{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 7.1 CHANGING PROMPT STYLES\n",
    "After finetuning the model with the Alpaca prompt style, try the Phi-3 prompt style\n",
    "shown in figure 7.4 and observe if it affects the response quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/ch07/01_main-chapter-code\n"
     ]
    }
   ],
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch07\", \"01_main-chapter-code\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"<|user|>\"\n",
    "        f\"\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    # 处理Input为空/非空的情况\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            # 拼接指令\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            # 拼接输出text\n",
    "            response_text = f\"\\n\\n<|assistant|>\\n{entry['output']}\"\n",
    "            # 合并指令+输出\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            # 编码上述信息\n",
    "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加输入和目标\n",
    "# 填充方法\n",
    "def custom_collate_fn(\n",
    "        batch, \n",
    "        pad_token_id=50256, \n",
    "        ignore_index=-100,\n",
    "        allowed_max_length=None,\n",
    "        device=\"cpu\"\n",
    "):\n",
    "    # 填充至当前batch的最大长度+1\n",
    "    # 至少会填充一个<|endoftext|>\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # 填充endoftext\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # 去除最后一个token, 作为输入\n",
    "        # 相对的，如果去掉第一个token，则作为目标\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # targets中仅保留一个<|endoftext|>，其余填充为ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        # 最大长度截断\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    # stack to batch\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# 将部分参数提前填充，并生成一个新的函数，以适配collate函数的要求\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 使用gpt2的bpe编码器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    # else:\n",
    "    #     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    #         text_data = file.read()\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "# 0.85、0.1、0.05\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "<|assistant|>\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# 测试format效果\n",
    "# Input为空\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n<|assistant|>\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout率\n",
    "    \"qkv_bias\": True         # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.748, Val loss 2.757\n",
      "Ep 1 (Step 000005): Train loss 1.676, Val loss 1.503\n",
      "Ep 1 (Step 000010): Train loss 1.170, Val loss 1.262\n",
      "Ep 1 (Step 000015): Train loss 1.168, Val loss 1.207\n",
      "Ep 1 (Step 000020): Train loss 1.062, Val loss 1.182\n",
      "Ep 1 (Step 000025): Train loss 1.028, Val loss 1.143\n",
      "Ep 1 (Step 000030): Train loss 1.084, Val loss 1.112\n",
      "Ep 1 (Step 000035): Train loss 0.953, Val loss 1.070\n",
      "Ep 1 (Step 000040): Train loss 0.909, Val loss 1.072\n",
      "Ep 1 (Step 000045): Train loss 0.855, Val loss 1.053\n",
      "Ep 1 (Step 000050): Train loss 0.899, Val loss 1.038\n",
      "Ep 1 (Step 000055): Train loss 1.006, Val loss 1.014\n",
      "Ep 1 (Step 000060): Train loss 0.957, Val loss 0.991\n",
      "Ep 1 (Step 000065): Train loss 0.876, Val loss 0.981\n",
      "Ep 1 (Step 000070): Train loss 0.750, Val loss 0.983\n",
      "Ep 1 (Step 000075): Train loss 0.792, Val loss 0.994\n",
      "Ep 1 (Step 000080): Train loss 0.797, Val loss 0.974\n",
      "Ep 1 (Step 000085): Train loss 0.677, Val loss 0.949\n",
      "Ep 1 (Step 000090): Train loss 0.737, Val loss 0.924\n",
      "Ep 1 (Step 000095): Train loss 0.694, Val loss 0.920\n",
      "Ep 1 (Step 000100): Train loss 0.680, Val loss 0.916\n",
      "Ep 1 (Step 000105): Train loss 0.745, Val loss 0.910\n",
      "Ep 1 (Step 000110): Train loss 0.736, Val loss 0.900\n",
      "Ep 1 (Step 000115): Train loss 0.681, Val loss 0.892\n",
      "<|user|> Convert the active sentence to passive: 'The chef cooks the meal every day.'  <|assistant|> The active sentence is 'The chef cooks the meal every day.'<|endoftext|>The following is a list of the most commonly used words in the English language.  1. The  2. The \n",
      "Ep 2 (Step 000120): Train loss 0.584, Val loss 0.897\n",
      "Ep 2 (Step 000125): Train loss 0.600, Val loss 0.923\n",
      "Ep 2 (Step 000130): Train loss 0.601, Val loss 0.921\n",
      "Ep 2 (Step 000135): Train loss 0.545, Val loss 0.936\n",
      "Ep 2 (Step 000140): Train loss 0.552, Val loss 0.918\n",
      "Ep 2 (Step 000145): Train loss 0.494, Val loss 0.912\n",
      "Ep 2 (Step 000150): Train loss 0.511, Val loss 0.912\n",
      "Ep 2 (Step 000155): Train loss 0.559, Val loss 0.910\n",
      "Ep 2 (Step 000160): Train loss 0.545, Val loss 0.914\n",
      "Ep 2 (Step 000165): Train loss 0.500, Val loss 0.918\n",
      "Ep 2 (Step 000170): Train loss 0.441, Val loss 0.925\n",
      "Ep 2 (Step 000175): Train loss 0.458, Val loss 0.918\n",
      "Ep 2 (Step 000180): Train loss 0.521, Val loss 0.891\n",
      "Ep 2 (Step 000185): Train loss 0.561, Val loss 0.891\n",
      "Ep 2 (Step 000190): Train loss 0.465, Val loss 0.878\n",
      "Ep 2 (Step 000195): Train loss 0.440, Val loss 0.860\n",
      "Ep 2 (Step 000200): Train loss 0.420, Val loss 0.861\n",
      "Ep 2 (Step 000205): Train loss 0.472, Val loss 0.850\n",
      "Ep 2 (Step 000210): Train loss 0.478, Val loss 0.853\n",
      "Ep 2 (Step 000215): Train loss 0.525, Val loss 0.861\n",
      "Ep 2 (Step 000220): Train loss 0.395, Val loss 0.875\n",
      "Ep 2 (Step 000225): Train loss 0.445, Val loss 0.899\n",
      "Ep 2 (Step 000230): Train loss 0.396, Val loss 0.880\n",
      "<|user|> Convert the active sentence to passive: 'The chef cooks the meal every day.'  <|assistant|> The meal is cooked every day by the chef.<|endoftext|>The following is a list of all the animals that are classified as reptiles.  ### Input: Snake  <|assistant|>\n",
      "Training completed in 2.66 minutes.\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")\n",
    "\n",
    "# 模型训练\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制损失曲线\n",
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "-------------------------------------\n",
      "<|user|>\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is cumulus.\n",
      "-------------------------------------\n",
      "<|user|>\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    # 输入格式化\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    # 生成回答\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    # 提取有效回答\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .replace(\"<|assistant|>\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    # 对比output\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:33<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# 保存测试集的结果\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .replace(\"<|assistant|>\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"instruction-data-with-response-phi3.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-medium355M-phi3-sft.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-phi3-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 效果评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = GPTModel(BASE_CONFIG)\n",
    "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))\n",
    "# model.to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "# 检测ollama是否正在运行\n",
    "# 运行ollama: ollama run llama3\n",
    "import psutil \n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Lanunch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "# 通过REST api访问ollama\n",
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "        prompt,\n",
    "        model=\"llama3\",\n",
    "        url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # 构造请求数据\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0.,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # dict转化json并编码\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # 解析返回结果\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    \n",
    "    return response_data\n",
    "\n",
    "model_name = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  36%|███▋      | 40/110 [00:16<00:35,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: A sonnet is a 14-line poem with a specific rhyme scheme and meter, often written in iambic pentameter.\n",
      "\n",
      "Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  42%|████▏     | 46/110 [00:18<00:25,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I'd rate this model's response as **80**.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  45%|████▌     | 50/110 [00:19<00:22,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Commence\n",
      "Score: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  75%|███████▌  | 83/110 [00:32<00:23,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I'd be happy to help!\n",
      "\n",
      "For the translation task, I would give myself a score of 100 because my response matches the correct Italian translation: \"Dove posso comprare i biglietti?\"\n",
      "\n",
      "As for the capital of Italy question, I would also give myself a score of 100 because my response is accurate and matches the correct answer: \"The capital of Italy is Rome.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [00:42<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 106 of 110\n",
      "Average score: 54.17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 封装上述功能\n",
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[85,\n",
       " 20,\n",
       " 98,\n",
       " 40,\n",
       " 95,\n",
       " 20,\n",
       " 10,\n",
       " 80,\n",
       " 20,\n",
       " 60,\n",
       " 95,\n",
       " 95,\n",
       " 20,\n",
       " 95,\n",
       " 0,\n",
       " 20,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 75,\n",
       " 20,\n",
       " 60,\n",
       " 40,\n",
       " 80,\n",
       " 80,\n",
       " 20,\n",
       " 20,\n",
       " 14,\n",
       " 95,\n",
       " 80,\n",
       " 95,\n",
       " 80,\n",
       " 92,\n",
       " 80,\n",
       " 95,\n",
       " 20,\n",
       " 100,\n",
       " 95,\n",
       " 20,\n",
       " 67,\n",
       " 90,\n",
       " 92,\n",
       " 50,\n",
       " 67,\n",
       " 20,\n",
       " 92,\n",
       " 95,\n",
       " 60,\n",
       " 95,\n",
       " 4,\n",
       " 4,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 50,\n",
       " 60,\n",
       " 80,\n",
       " 95,\n",
       " 60,\n",
       " 20,\n",
       " 4,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 67,\n",
       " 40,\n",
       " 20,\n",
       " 95,\n",
       " 67,\n",
       " 20,\n",
       " 95,\n",
       " 25,\n",
       " 95,\n",
       " 60,\n",
       " 95,\n",
       " 20,\n",
       " 100,\n",
       " 80,\n",
       " 95,\n",
       " 80,\n",
       " 60,\n",
       " 85,\n",
       " 82,\n",
       " 0,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 40,\n",
       " 20,\n",
       " 80,\n",
       " 40,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 67,\n",
       " 67,\n",
       " 20,\n",
       " 20,\n",
       " 95,\n",
       " 4,\n",
       " 20,\n",
       " 4,\n",
       " 85,\n",
       " 20,\n",
       " 40]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 7.2 INSTRUCTION AND INPUT MASKING\n",
    "After completing the chapter and finetuning the model with the InstructionDataset\n",
    "implemented in this section, replace the instruction and input tokens with the -100\n",
    "mask to implement the instruction masking method illustrated in Figure 7.13. Then,\n",
    "evaluate whether this has a positive effect on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要的思路是：在计算loss时，进行mask\n",
    "# 这里的loss计算，主要是使用torch.nn.functional.cross_entropy(logits, labels)\n",
    "# 将需要mask的token对应的label设置为-100，然后计算loss就能实现mask的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/ch07/01_main-chapter-code\n"
     ]
    }
   ],
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch07\", \"01_main-chapter-code\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    # else:\n",
    "    #     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    #         text_data = file.read()\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "# 0.85、0.1、0.05\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    # 处理Input为空/非空的情况\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "# 测试format效果\n",
    "# Input非空\n",
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout率\n",
    "    \"qkv_bias\": True         # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model_mask = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model_mask, params)\n",
    "model_mask.to(device)\n",
    "model_mask.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset_mask(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            # 拼接指令\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            # 拼接输出text\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            # 合并指令+输出\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            # 编码上述信息\n",
    "            self.encoded_texts.append(\n",
    "                {\n",
    "                    \"text\": tokenizer.encode(full_text),\n",
    "                    \"prefix_len\": len(tokenizer.encode(instruction_plus_input))\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加输入和目标\n",
    "# 填充方法\n",
    "def custom_collate_fn_mask(\n",
    "        batch, \n",
    "        pad_token_id=50256, \n",
    "        ignore_index=-100,\n",
    "        allowed_max_length=None,\n",
    "        device=\"cpu\",\n",
    "        ignore_prompt_tokens=False\n",
    "):\n",
    "    # 填充至当前batch的最大长度+1\n",
    "    # 至少会填充一个<|endoftext|>\n",
    "    if ignore_prompt_tokens:\n",
    "        batch_max_length = max(len(items[\"text\"])+1 for items in batch)\n",
    "    else:\n",
    "        batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for items in batch:\n",
    "        prefix_len = 0\n",
    "        if ignore_prompt_tokens:\n",
    "            item = items[\"text\"]\n",
    "            prefix_len = items[\"prefix_len\"]\n",
    "        else:\n",
    "            item = items\n",
    "\n",
    "        new_item = item.copy()\n",
    "        # 填充endoftext\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # 去除最后一个token, 作为输入\n",
    "        # 相对的，如果去掉第一个token，则作为目标\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # 将指令和输入的token设置为ignore_index\n",
    "        if ignore_prompt_tokens:\n",
    "            # 考虑targets右移了一位，故需要-1\n",
    "            targets[:prefix_len-1] = ignore_index\n",
    "\n",
    "        # targets中仅保留一个<|endoftext|>，其余填充为ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        # 最大长度截断\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    # stack to batch\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# 将部分参数提前填充，并生成一个新的函数，以适配collate函数的要求\n",
    "customized_collate_fn_mask = partial(\n",
    "    custom_collate_fn_mask,\n",
    "    device=device,\n",
    "    allowed_max_length=1024,\n",
    "    ignore_prompt_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 使用gpt2的bpe编码器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset_mask = InstructionDataset_mask(train_data, tokenizer)\n",
    "train_loader_mask = DataLoader(\n",
    "    train_dataset_mask,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn_mask,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset_mask = InstructionDataset_mask(val_data, tokenizer)\n",
    "val_loader_mask = DataLoader(\n",
    "    val_dataset_mask,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn_mask,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset_mask = InstructionDataset_mask(test_data, tokenizer)\n",
    "test_loader_mask = DataLoader(\n",
    "    test_dataset_mask,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn_mask,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "# 校验数据\n",
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader_mask:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198,  2061,   318,   262,  5931, 10451,   329,\n",
      "        21072,  6588,   378,    30,   198,   198, 21017, 18261,    25,   198,\n",
      "          464,  5931, 10451,   329, 21072,  6588,   378,   318, 11013,    17,\n",
      "         8220,    18,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "       device='mps:0')\n",
      "\n",
      "\n",
      "Targets:\n",
      " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,   198,   198, 21017, 18261,    25,   198,   464,\n",
      "         5931, 10451,   329, 21072,  6588,   378,   318, 11013,    17,  8220,\n",
      "           18,    13, 50256,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs:\\n\", inputs[1])\n",
    "print(\"\\n\\nTargets:\\n\", targets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the chemical formula for sodium carbonate?\n",
      "\n",
      "### Response:\n",
      "The chemical formula for sodium carbonate is Na2CO3.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(list(inputs[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Response:\n",
      "The chemical formula for sodium carbonate is Na2CO3.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "non_masked_targets = targets[1][targets[1] != -100]\n",
    "\n",
    "print(tokenizer.decode(list(non_masked_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.3913217067718504\n",
      "Validation loss: 2.2625606298446654\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")\n",
    "\n",
    "# 查看训练前的loss\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader_mask, model_mask, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader_mask, model_mask, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 1.636, Val loss 1.620\n",
      "Ep 1 (Step 000005): Train loss 1.060, Val loss 1.026\n",
      "Ep 1 (Step 000010): Train loss 0.881, Val loss 0.939\n",
      "Ep 1 (Step 000015): Train loss 0.878, Val loss 0.903\n",
      "Ep 1 (Step 000020): Train loss 0.817, Val loss 0.879\n",
      "Ep 1 (Step 000025): Train loss 0.738, Val loss 0.847\n",
      "Ep 1 (Step 000030): Train loss 0.779, Val loss 0.825\n",
      "Ep 1 (Step 000035): Train loss 0.645, Val loss 0.806\n",
      "Ep 1 (Step 000040): Train loss 0.757, Val loss 0.804\n",
      "Ep 1 (Step 000045): Train loss 0.564, Val loss 0.802\n",
      "Ep 1 (Step 000050): Train loss 0.661, Val loss 0.789\n",
      "Ep 1 (Step 000055): Train loss 0.893, Val loss 0.785\n",
      "Ep 1 (Step 000060): Train loss 0.669, Val loss 0.773\n",
      "Ep 1 (Step 000065): Train loss 0.563, Val loss 0.759\n",
      "Ep 1 (Step 000070): Train loss 0.523, Val loss 0.758\n",
      "Ep 1 (Step 000075): Train loss 0.533, Val loss 0.751\n",
      "Ep 1 (Step 000080): Train loss 0.557, Val loss 0.742\n",
      "Ep 1 (Step 000085): Train loss 0.481, Val loss 0.736\n",
      "Ep 1 (Step 000090): Train loss 0.526, Val loss 0.725\n",
      "Ep 1 (Step 000095): Train loss 0.392, Val loss 0.717\n",
      "Ep 1 (Step 000100): Train loss 0.485, Val loss 0.696\n",
      "Ep 1 (Step 000105): Train loss 0.520, Val loss 0.698\n",
      "Ep 1 (Step 000110): Train loss 0.539, Val loss 0.692\n",
      "Ep 1 (Step 000115): Train loss 0.438, Val loss 0.686\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The U.S. government is considering a new policy to protect the privacy of U.S. citizens abroad.  The new policy would require the government to\n",
      "Ep 2 (Step 000120): Train loss 0.378, Val loss 0.687\n",
      "Ep 2 (Step 000125): Train loss 0.358, Val loss 0.694\n",
      "Ep 2 (Step 000130): Train loss 0.337, Val loss 0.711\n",
      "Ep 2 (Step 000135): Train loss 0.271, Val loss 0.723\n",
      "Ep 2 (Step 000140): Train loss 0.278, Val loss 0.739\n",
      "Ep 2 (Step 000145): Train loss 0.260, Val loss 0.736\n",
      "Ep 2 (Step 000150): Train loss 0.201, Val loss 0.736\n",
      "Ep 2 (Step 000155): Train loss 0.282, Val loss 0.756\n",
      "Ep 2 (Step 000160): Train loss 0.366, Val loss 0.760\n",
      "Ep 2 (Step 000165): Train loss 0.273, Val loss 0.748\n",
      "Ep 2 (Step 000170): Train loss 0.168, Val loss 0.739\n",
      "Ep 2 (Step 000175): Train loss 0.218, Val loss 0.729\n",
      "Ep 2 (Step 000180): Train loss 0.310, Val loss 0.713\n",
      "Ep 2 (Step 000185): Train loss 0.321, Val loss 0.718\n",
      "Ep 2 (Step 000190): Train loss 0.188, Val loss 0.716\n",
      "Ep 2 (Step 000195): Train loss 0.198, Val loss 0.707\n",
      "Ep 2 (Step 000200): Train loss 0.153, Val loss 0.703\n",
      "Ep 2 (Step 000205): Train loss 0.230, Val loss 0.696\n",
      "Ep 2 (Step 000210): Train loss 0.258, Val loss 0.693\n",
      "Ep 2 (Step 000215): Train loss 0.295, Val loss 0.684\n",
      "Ep 2 (Step 000220): Train loss 0.167, Val loss 0.692\n",
      "Ep 2 (Step 000225): Train loss 0.143, Val loss 0.698\n",
      "Ep 2 (Step 000230): Train loss 0.126, Val loss 0.702\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The new year is a time of great change and excitement. It is a time when new ideas and new ideas are born. It is a time when new ideas are\n",
      "Training completed in 3.09 minutes.\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")\n",
    "\n",
    "# 模型训练\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_mask.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model_mask, train_loader_mask, val_loader_mask, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制损失曲线\n",
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> A thunderstorm is a type of cloud that typically forms when a strong wind blows from the northwest or southeast, creating a thunderstorm.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is George Bernard Shaw.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    # 输入格式化\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    # 生成回答\n",
    "    token_ids = generate(\n",
    "        model=model_mask,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    # 提取有效回答\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    # 对比output\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:16<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# 保存测试集的结果\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model_mask,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .replace(\"<|assistant|>\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"instruction-data-with-response-mask.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-medium355M-mask-sft.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-mask-sft.pth\"\n",
    "torch.save(model_mask.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "# 检测ollama是否正在运行\n",
    "# 运行ollama: ollama run llama3\n",
    "import psutil \n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Lanunch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "# 通过REST api访问ollama\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "        prompt,\n",
    "        model=\"llama3\",\n",
    "        url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # 构造请求数据\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0.,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # dict转化json并编码\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # 解析返回结果\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    \n",
    "    return response_data\n",
    "\n",
    "model_name = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"instruction-data-with-response-mask.json\", \"r\") as file:\n",
    "    test_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [00:35<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 48.58\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 封装上述功能\n",
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 7.3 FINETUNING ON THE ORIGINAL ALPACA DATASET\n",
    "The so-called Alpaca dataset by researchers at Stanford is one of the earliest and\n",
    "most popular openly shared instruction datasets, consisting of 52,002 entries. As an\n",
    "alternative to the instruction-data.json file we use in this chapter, consider\n",
    "finetuning an LLM on this dataset. The dataset is available at the following URL:\n",
    "https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
    "This dataset contains 52,002 entries, which is approximately 50 times more than\n",
    "those we used in this chapter, and most entries are longer as well. Thus, it's highly\n",
    "recommended to conduct the training using a GPU to accelerate the finetuning\n",
    "process. If you encounter out-of-memory errors, consider reducing the batch_size\n",
    "from 8 to 4, 2, or even 1. Additionally, lowering the allowed_max_length from 1024\n",
    "to 512 or 256 can further help manage memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/ch07/01_main-chapter-code\n"
     ]
    }
   ],
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch07\", \"01_main-chapter-code\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    # 处理Input为空/非空的情况\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            # 拼接指令\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            # 拼接输出text\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            # 合并指令+输出\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            # 编码上述信息\n",
    "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加输入和目标\n",
    "# 填充方法\n",
    "def custom_collate_fn(\n",
    "        batch, \n",
    "        pad_token_id=50256, \n",
    "        ignore_index=-100,\n",
    "        allowed_max_length=None,\n",
    "        device=\"cpu\"\n",
    "):\n",
    "    # 填充至当前batch的最大长度+1\n",
    "    # 至少会填充一个<|endoftext|>\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # 填充endoftext\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # 去除最后一个token, 作为输入\n",
    "        # 相对的，如果去掉第一个token，则作为目标\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # targets中仅保留一个<|endoftext|>，其余填充为ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        # 最大长度截断\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    # stack to batch\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# 将部分参数提前填充，并生成一个新的函数，以适配collate函数的要求\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=128 # 1024 -> 128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 使用gpt2的bpe编码器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    # else:\n",
    "    #     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    #         text_data = file.read()\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 52002\n"
     ]
    }
   ],
   "source": [
    "file_path = \"alpaca_data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com\"\n",
    "    \"/tatsu-lab/stanford_alpaca/main/alpaca_data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "# 0.85、0.1、0.05\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the flavor profile of the following type of cuisine\n",
      "\n",
      "### Input:\n",
      "Japanese\n",
      "\n",
      "<|assistant|>\n",
      "Japanese cuisine is characterized by its subtle and delicate flavors, featuring a combination of salty, sweet, sour, and umami flavors. It also utilizes fresh ingredients with a focus on preserving their natural flavors.\n"
     ]
    }
   ],
   "source": [
    "# 测试format效果\n",
    "# Input为空\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n<|assistant|>\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 1  # 8 -> 1\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout率\n",
    "    \"qkv_bias\": True         # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.975, Val loss 3.307\n",
      "Ep 1 (Step 000005): Train loss 2.657, Val loss 2.940\n",
      "Ep 1 (Step 000010): Train loss 2.541, Val loss 2.789\n",
      "Ep 1 (Step 000015): Train loss 2.129, Val loss 2.649\n",
      "Ep 1 (Step 000020): Train loss 2.099, Val loss 2.512\n",
      "Ep 1 (Step 000025): Train loss 1.983, Val loss 2.401\n",
      "Ep 1 (Step 000030): Train loss 1.855, Val loss 2.286\n",
      "Ep 1 (Step 000035): Train loss 1.601, Val loss 2.168\n",
      "Ep 1 (Step 000040): Train loss 1.498, Val loss 2.111\n",
      "Ep 1 (Step 000045): Train loss 1.440, Val loss 2.080\n",
      "Ep 1 (Step 000050): Train loss 1.299, Val loss 2.057\n",
      "Ep 1 (Step 000055): Train loss 1.545, Val loss 2.046\n",
      "Ep 1 (Step 000060): Train loss 1.569, Val loss 2.045\n",
      "Ep 1 (Step 000065): Train loss 1.369, Val loss 2.032\n",
      "Ep 1 (Step 000070): Train loss 1.690, Val loss 2.006\n",
      "Ep 1 (Step 000075): Train loss 1.644, Val loss 2.031\n",
      "Ep 1 (Step 000080): Train loss 2.023, Val loss 2.048\n",
      "Ep 1 (Step 000085): Train loss 1.546, Val loss 2.016\n",
      "Ep 1 (Step 000090): Train loss 1.809, Val loss 2.004\n",
      "Ep 1 (Step 000095): Train loss 1.560, Val loss 1.997\n",
      "Ep 1 (Step 000100): Train loss 1.465, Val loss 2.001\n",
      "Ep 1 (Step 000105): Train loss 1.464, Val loss 1.999\n",
      "Ep 1 (Step 000110): Train loss 1.234, Val loss 1.992\n",
      "Ep 1 (Step 000115): Train loss 1.583, Val loss 1.991\n",
      "Ep 1 (Step 000120): Train loss 1.616, Val loss 1.989\n",
      "Ep 1 (Step 000125): Train loss 1.366, Val loss 1.986\n",
      "Ep 1 (Step 000130): Train loss 1.558, Val loss 1.982\n",
      "Ep 1 (Step 000135): Train loss 1.348, Val loss 1.981\n",
      "Ep 1 (Step 000140): Train loss 1.389, Val loss 1.982\n",
      "Ep 1 (Step 000145): Train loss 1.441, Val loss 1.981\n",
      "Ep 1 (Step 000150): Train loss 1.554, Val loss 1.983\n",
      "Ep 1 (Step 000155): Train loss 1.274, Val loss 1.982\n",
      "Ep 1 (Step 000160): Train loss 1.531, Val loss 1.983\n",
      "Ep 1 (Step 000165): Train loss 1.351, Val loss 1.984\n",
      "Ep 1 (Step 000170): Train loss 1.069, Val loss 1.983\n",
      "Ep 1 (Step 000175): Train loss 1.501, Val loss 1.978\n",
      "Ep 1 (Step 000180): Train loss 1.599, Val loss 1.975\n",
      "Ep 1 (Step 000185): Train loss 1.250, Val loss 1.971\n",
      "Ep 1 (Step 000190): Train loss 1.619, Val loss 1.966\n",
      "Ep 1 (Step 000195): Train loss 1.569, Val loss 1.964\n",
      "Ep 1 (Step 000200): Train loss 1.401, Val loss 1.963\n",
      "Ep 1 (Step 000205): Train loss 1.254, Val loss 1.963\n",
      "Ep 1 (Step 000210): Train loss 1.418, Val loss 1.966\n",
      "Ep 1 (Step 000215): Train loss 1.411, Val loss 1.962\n",
      "Ep 1 (Step 000220): Train loss 1.400, Val loss 1.963\n",
      "Ep 1 (Step 000225): Train loss 1.611, Val loss 1.964\n",
      "Ep 1 (Step 000230): Train loss 1.754, Val loss 1.961\n",
      "Ep 1 (Step 000235): Train loss 1.102, Val loss 1.959\n",
      "Ep 1 (Step 000240): Train loss 1.571, Val loss 1.958\n",
      "Ep 1 (Step 000245): Train loss 1.379, Val loss 1.956\n",
      "Ep 1 (Step 000250): Train loss 1.348, Val loss 1.959\n",
      "Ep 1 (Step 000255): Train loss 1.362, Val loss 1.964\n",
      "Ep 1 (Step 000260): Train loss 1.246, Val loss 1.974\n",
      "Ep 1 (Step 000265): Train loss 1.288, Val loss 1.974\n",
      "Ep 1 (Step 000270): Train loss 1.487, Val loss 1.965\n",
      "Ep 1 (Step 000275): Train loss 1.493, Val loss 1.962\n",
      "Ep 1 (Step 000280): Train loss 1.733, Val loss 1.962\n",
      "Ep 1 (Step 000285): Train loss 1.550, Val loss 1.960\n",
      "Ep 1 (Step 000290): Train loss 1.199, Val loss 1.962\n",
      "Ep 1 (Step 000295): Train loss 1.408, Val loss 1.962\n",
      "Ep 1 (Step 000300): Train loss 1.642, Val loss 1.955\n",
      "Ep 1 (Step 000305): Train loss 1.586, Val loss 1.948\n",
      "Ep 1 (Step 000310): Train loss 1.471, Val loss 1.946\n",
      "Ep 1 (Step 000315): Train loss 1.448, Val loss 1.945\n",
      "Ep 1 (Step 000320): Train loss 1.395, Val loss 1.945\n",
      "Ep 1 (Step 000325): Train loss 1.375, Val loss 1.938\n",
      "Ep 1 (Step 000330): Train loss 1.610, Val loss 1.941\n",
      "Ep 1 (Step 000335): Train loss 1.453, Val loss 1.941\n",
      "Ep 1 (Step 000340): Train loss 1.361, Val loss 1.939\n",
      "Ep 1 (Step 000345): Train loss 1.392, Val loss 1.936\n",
      "Ep 1 (Step 000350): Train loss 1.241, Val loss 1.937\n",
      "Ep 1 (Step 000355): Train loss 1.453, Val loss 1.935\n",
      "Ep 1 (Step 000360): Train loss 1.409, Val loss 1.930\n",
      "Ep 1 (Step 000365): Train loss 1.710, Val loss 1.921\n",
      "Ep 1 (Step 000370): Train loss 1.586, Val loss 1.919\n",
      "Ep 1 (Step 000375): Train loss 1.610, Val loss 1.922\n",
      "Ep 1 (Step 000380): Train loss 1.603, Val loss 1.931\n",
      "Ep 1 (Step 000385): Train loss 1.657, Val loss 1.925\n",
      "Ep 1 (Step 000390): Train loss 1.413, Val loss 1.916\n",
      "Ep 1 (Step 000395): Train loss 1.450, Val loss 1.916\n",
      "Ep 1 (Step 000400): Train loss 1.444, Val loss 1.919\n",
      "Ep 1 (Step 000405): Train loss 1.295, Val loss 1.920\n",
      "Ep 1 (Step 000410): Train loss 1.458, Val loss 1.923\n",
      "Ep 1 (Step 000415): Train loss 1.246, Val loss 1.918\n",
      "Ep 1 (Step 000420): Train loss 1.233, Val loss 1.914\n",
      "Ep 1 (Step 000425): Train loss 1.588, Val loss 1.916\n",
      "Ep 1 (Step 000430): Train loss 1.251, Val loss 1.919\n",
      "Ep 1 (Step 000435): Train loss 1.336, Val loss 1.910\n",
      "Ep 1 (Step 000440): Train loss 1.380, Val loss 1.908\n",
      "Ep 1 (Step 000445): Train loss 1.337, Val loss 1.910\n",
      "Ep 1 (Step 000450): Train loss 1.372, Val loss 1.917\n",
      "Ep 1 (Step 000455): Train loss 1.414, Val loss 1.914\n",
      "Ep 1 (Step 000460): Train loss 1.571, Val loss 1.908\n",
      "Ep 1 (Step 000465): Train loss 1.443, Val loss 1.907\n",
      "Ep 1 (Step 000470): Train loss 1.098, Val loss 1.906\n",
      "Ep 1 (Step 000475): Train loss 1.604, Val loss 1.905\n",
      "Ep 1 (Step 000480): Train loss 1.514, Val loss 1.904\n",
      "Ep 1 (Step 000485): Train loss 1.429, Val loss 1.901\n",
      "Ep 1 (Step 000490): Train loss 1.168, Val loss 1.894\n",
      "Ep 1 (Step 000495): Train loss 1.574, Val loss 1.890\n",
      "Ep 1 (Step 000500): Train loss 1.626, Val loss 1.888\n",
      "Ep 1 (Step 000505): Train loss 1.386, Val loss 1.890\n",
      "Ep 1 (Step 000510): Train loss 1.485, Val loss 1.895\n",
      "Ep 1 (Step 000515): Train loss 1.252, Val loss 1.898\n",
      "Ep 1 (Step 000520): Train loss 1.512, Val loss 1.901\n",
      "Ep 1 (Step 000525): Train loss 1.353, Val loss 1.904\n",
      "Ep 1 (Step 000530): Train loss 1.513, Val loss 1.896\n",
      "Ep 1 (Step 000535): Train loss 1.186, Val loss 1.888\n",
      "Ep 1 (Step 000540): Train loss 1.396, Val loss 1.883\n",
      "Ep 1 (Step 000545): Train loss 1.157, Val loss 1.877\n",
      "Ep 1 (Step 000550): Train loss 1.500, Val loss 1.878\n",
      "Ep 1 (Step 000555): Train loss 1.676, Val loss 1.878\n",
      "Ep 1 (Step 000560): Train loss 1.061, Val loss 1.885\n",
      "Ep 1 (Step 000565): Train loss 1.534, Val loss 1.889\n",
      "Ep 1 (Step 000570): Train loss 1.586, Val loss 1.893\n",
      "Ep 1 (Step 000575): Train loss 1.345, Val loss 1.892\n",
      "Ep 1 (Step 000580): Train loss 1.484, Val loss 1.887\n",
      "Ep 1 (Step 000585): Train loss 1.343, Val loss 1.886\n",
      "Ep 1 (Step 000590): Train loss 1.309, Val loss 1.887\n",
      "Ep 1 (Step 000595): Train loss 1.332, Val loss 1.889\n",
      "Ep 1 (Step 000600): Train loss 1.713, Val loss 1.892\n",
      "Ep 1 (Step 000605): Train loss 1.555, Val loss 1.896\n",
      "Ep 1 (Step 000610): Train loss 1.604, Val loss 1.895\n",
      "Ep 1 (Step 000615): Train loss 1.381, Val loss 1.891\n",
      "Ep 1 (Step 000620): Train loss 1.426, Val loss 1.895\n",
      "Ep 1 (Step 000625): Train loss 1.477, Val loss 1.901\n",
      "Ep 1 (Step 000630): Train loss 1.130, Val loss 1.898\n",
      "Ep 1 (Step 000635): Train loss 1.595, Val loss 1.898\n",
      "Ep 1 (Step 000640): Train loss 1.060, Val loss 1.894\n",
      "Ep 1 (Step 000645): Train loss 1.377, Val loss 1.895\n",
      "Ep 1 (Step 000650): Train loss 1.046, Val loss 1.897\n",
      "Ep 1 (Step 000655): Train loss 1.149, Val loss 1.901\n",
      "Ep 1 (Step 000660): Train loss 1.328, Val loss 1.900\n",
      "Ep 1 (Step 000665): Train loss 1.603, Val loss 1.896\n",
      "Ep 1 (Step 000670): Train loss 1.484, Val loss 1.893\n",
      "Ep 1 (Step 000675): Train loss 1.278, Val loss 1.891\n",
      "Ep 1 (Step 000680): Train loss 1.388, Val loss 1.888\n",
      "Ep 1 (Step 000685): Train loss 1.430, Val loss 1.889\n",
      "Ep 1 (Step 000690): Train loss 1.468, Val loss 1.888\n",
      "Ep 1 (Step 000695): Train loss 1.382, Val loss 1.881\n",
      "Ep 1 (Step 000700): Train loss 1.314, Val loss 1.877\n",
      "Ep 1 (Step 000705): Train loss 1.471, Val loss 1.874\n",
      "Ep 1 (Step 000710): Train loss 1.267, Val loss 1.875\n",
      "Ep 1 (Step 000715): Train loss 1.544, Val loss 1.877\n",
      "Ep 1 (Step 000720): Train loss 1.321, Val loss 1.875\n",
      "Ep 1 (Step 000725): Train loss 1.399, Val loss 1.872\n",
      "Ep 1 (Step 000730): Train loss 1.490, Val loss 1.874\n",
      "Ep 1 (Step 000735): Train loss 1.023, Val loss 1.876\n",
      "Ep 1 (Step 000740): Train loss 1.571, Val loss 1.880\n",
      "Ep 1 (Step 000745): Train loss 1.485, Val loss 1.878\n",
      "Ep 1 (Step 000750): Train loss 1.649, Val loss 1.877\n",
      "Ep 1 (Step 000755): Train loss 1.404, Val loss 1.877\n",
      "Ep 1 (Step 000760): Train loss 1.325, Val loss 1.874\n",
      "Ep 1 (Step 000765): Train loss 1.363, Val loss 1.876\n",
      "Ep 1 (Step 000770): Train loss 1.264, Val loss 1.878\n",
      "Ep 1 (Step 000775): Train loss 1.187, Val loss 1.875\n",
      "Ep 1 (Step 000780): Train loss 1.292, Val loss 1.871\n",
      "Ep 1 (Step 000785): Train loss 1.413, Val loss 1.868\n",
      "Ep 1 (Step 000790): Train loss 1.509, Val loss 1.868\n",
      "Ep 1 (Step 000795): Train loss 1.647, Val loss 1.869\n",
      "Ep 1 (Step 000800): Train loss 1.234, Val loss 1.869\n",
      "Ep 1 (Step 000805): Train loss 1.168, Val loss 1.867\n",
      "Ep 1 (Step 000810): Train loss 1.353, Val loss 1.863\n",
      "Ep 1 (Step 000815): Train loss 1.226, Val loss 1.864\n",
      "Ep 1 (Step 000820): Train loss 1.301, Val loss 1.869\n",
      "Ep 1 (Step 000825): Train loss 1.339, Val loss 1.872\n",
      "Ep 1 (Step 000830): Train loss 1.631, Val loss 1.866\n",
      "Ep 1 (Step 000835): Train loss 1.576, Val loss 1.864\n",
      "Ep 1 (Step 000840): Train loss 1.408, Val loss 1.864\n",
      "Ep 1 (Step 000845): Train loss 1.188, Val loss 1.864\n",
      "Ep 1 (Step 000850): Train loss 1.504, Val loss 1.866\n",
      "Ep 1 (Step 000855): Train loss 1.529, Val loss 1.866\n",
      "Ep 1 (Step 000860): Train loss 1.919, Val loss 1.865\n",
      "Ep 1 (Step 000865): Train loss 1.222, Val loss 1.865\n",
      "Ep 1 (Step 000870): Train loss 1.463, Val loss 1.866\n",
      "Ep 1 (Step 000875): Train loss 1.362, Val loss 1.867\n",
      "Ep 1 (Step 000880): Train loss 1.232, Val loss 1.865\n",
      "Ep 1 (Step 000885): Train loss 1.349, Val loss 1.865\n",
      "Ep 1 (Step 000890): Train loss 1.480, Val loss 1.866\n",
      "Ep 1 (Step 000895): Train loss 1.146, Val loss 1.866\n",
      "Ep 1 (Step 000900): Train loss 1.544, Val loss 1.869\n",
      "Ep 1 (Step 000905): Train loss 1.635, Val loss 1.876\n",
      "Ep 1 (Step 000910): Train loss 1.575, Val loss 1.874\n",
      "Ep 1 (Step 000915): Train loss 1.619, Val loss 1.870\n",
      "Ep 1 (Step 000920): Train loss 1.328, Val loss 1.870\n",
      "Ep 1 (Step 000925): Train loss 1.638, Val loss 1.866\n",
      "Ep 1 (Step 000930): Train loss 1.511, Val loss 1.864\n",
      "Ep 1 (Step 000935): Train loss 1.413, Val loss 1.864\n",
      "Ep 1 (Step 000940): Train loss 1.567, Val loss 1.869\n",
      "Ep 1 (Step 000945): Train loss 1.789, Val loss 1.869\n",
      "Ep 1 (Step 000950): Train loss 1.601, Val loss 1.867\n",
      "Ep 1 (Step 000955): Train loss 1.240, Val loss 1.867\n",
      "Ep 1 (Step 000960): Train loss 1.177, Val loss 1.872\n",
      "Ep 1 (Step 000965): Train loss 1.420, Val loss 1.877\n",
      "Ep 1 (Step 000970): Train loss 1.351, Val loss 1.869\n",
      "Ep 1 (Step 000975): Train loss 1.316, Val loss 1.870\n",
      "Ep 1 (Step 000980): Train loss 1.556, Val loss 1.867\n",
      "Ep 1 (Step 000985): Train loss 1.255, Val loss 1.866\n",
      "Ep 1 (Step 000990): Train loss 1.582, Val loss 1.865\n",
      "Ep 1 (Step 000995): Train loss 1.417, Val loss 1.865\n",
      "Ep 1 (Step 001000): Train loss 1.100, Val loss 1.866\n",
      "Ep 1 (Step 001005): Train loss 1.038, Val loss 1.871\n",
      "Ep 1 (Step 001010): Train loss 1.189, Val loss 1.871\n",
      "Ep 1 (Step 001015): Train loss 1.508, Val loss 1.862\n",
      "Ep 1 (Step 001020): Train loss 1.625, Val loss 1.858\n",
      "Ep 1 (Step 001025): Train loss 1.308, Val loss 1.861\n",
      "Ep 1 (Step 001030): Train loss 1.382, Val loss 1.859\n",
      "Ep 1 (Step 001035): Train loss 1.390, Val loss 1.858\n",
      "Ep 1 (Step 001040): Train loss 1.147, Val loss 1.858\n",
      "Ep 1 (Step 001045): Train loss 1.373, Val loss 1.855\n",
      "Ep 1 (Step 001050): Train loss 1.323, Val loss 1.854\n",
      "Ep 1 (Step 001055): Train loss 1.434, Val loss 1.856\n",
      "Ep 1 (Step 001060): Train loss 1.482, Val loss 1.858\n",
      "Ep 1 (Step 001065): Train loss 1.189, Val loss 1.861\n",
      "Ep 1 (Step 001070): Train loss 1.453, Val loss 1.856\n",
      "Ep 1 (Step 001075): Train loss 1.121, Val loss 1.851\n",
      "Ep 1 (Step 001080): Train loss 1.144, Val loss 1.851\n",
      "Ep 1 (Step 001085): Train loss 1.627, Val loss 1.849\n",
      "Ep 1 (Step 001090): Train loss 1.336, Val loss 1.845\n",
      "Ep 1 (Step 001095): Train loss 1.590, Val loss 1.843\n",
      "Ep 1 (Step 001100): Train loss 1.546, Val loss 1.843\n",
      "Ep 1 (Step 001105): Train loss 1.159, Val loss 1.843\n",
      "Ep 1 (Step 001110): Train loss 1.325, Val loss 1.843\n",
      "Ep 1 (Step 001115): Train loss 1.422, Val loss 1.841\n",
      "Ep 1 (Step 001120): Train loss 1.456, Val loss 1.839\n",
      "Ep 1 (Step 001125): Train loss 1.202, Val loss 1.838\n",
      "Ep 1 (Step 001130): Train loss 1.370, Val loss 1.843\n",
      "Ep 1 (Step 001135): Train loss 1.542, Val loss 1.849\n",
      "Ep 1 (Step 001140): Train loss 1.683, Val loss 1.854\n",
      "Ep 1 (Step 001145): Train loss 1.469, Val loss 1.856\n",
      "Ep 1 (Step 001150): Train loss 1.252, Val loss 1.856\n",
      "Ep 1 (Step 001155): Train loss 1.316, Val loss 1.850\n",
      "Ep 1 (Step 001160): Train loss 1.248, Val loss 1.846\n",
      "Ep 1 (Step 001165): Train loss 1.178, Val loss 1.843\n",
      "Ep 1 (Step 001170): Train loss 1.534, Val loss 1.844\n",
      "Ep 1 (Step 001175): Train loss 1.264, Val loss 1.843\n",
      "Ep 1 (Step 001180): Train loss 1.427, Val loss 1.845\n",
      "Ep 1 (Step 001185): Train loss 1.268, Val loss 1.851\n",
      "Ep 1 (Step 001190): Train loss 1.619, Val loss 1.859\n",
      "Ep 1 (Step 001195): Train loss 1.298, Val loss 1.862\n",
      "Ep 1 (Step 001200): Train loss 1.173, Val loss 1.867\n",
      "Ep 1 (Step 001205): Train loss 1.145, Val loss 1.868\n",
      "Ep 1 (Step 001210): Train loss 1.219, Val loss 1.863\n",
      "Ep 1 (Step 001215): Train loss 1.299, Val loss 1.854\n",
      "Ep 1 (Step 001220): Train loss 1.583, Val loss 1.855\n",
      "Ep 1 (Step 001225): Train loss 1.297, Val loss 1.857\n",
      "Ep 1 (Step 001230): Train loss 1.245, Val loss 1.856\n",
      "Ep 1 (Step 001235): Train loss 1.169, Val loss 1.862\n",
      "Ep 1 (Step 001240): Train loss 1.712, Val loss 1.870\n",
      "Ep 1 (Step 001245): Train loss 1.675, Val loss 1.876\n",
      "Ep 1 (Step 001250): Train loss 1.402, Val loss 1.870\n",
      "Ep 1 (Step 001255): Train loss 1.326, Val loss 1.858\n",
      "Ep 1 (Step 001260): Train loss 1.330, Val loss 1.857\n",
      "Ep 1 (Step 001265): Train loss 1.578, Val loss 1.857\n",
      "Ep 1 (Step 001270): Train loss 1.292, Val loss 1.853\n",
      "Ep 1 (Step 001275): Train loss 0.993, Val loss 1.850\n",
      "Ep 1 (Step 001280): Train loss 1.261, Val loss 1.849\n",
      "Ep 1 (Step 001285): Train loss 1.301, Val loss 1.850\n",
      "Ep 1 (Step 001290): Train loss 1.383, Val loss 1.851\n",
      "Ep 1 (Step 001295): Train loss 1.402, Val loss 1.853\n",
      "Ep 1 (Step 001300): Train loss 1.346, Val loss 1.855\n",
      "Ep 1 (Step 001305): Train loss 1.303, Val loss 1.853\n",
      "Ep 1 (Step 001310): Train loss 1.453, Val loss 1.852\n",
      "Ep 1 (Step 001315): Train loss 1.400, Val loss 1.851\n",
      "Ep 1 (Step 001320): Train loss 1.316, Val loss 1.851\n",
      "Ep 1 (Step 001325): Train loss 1.065, Val loss 1.860\n",
      "Ep 1 (Step 001330): Train loss 1.183, Val loss 1.858\n",
      "Ep 1 (Step 001335): Train loss 1.180, Val loss 1.851\n",
      "Ep 1 (Step 001340): Train loss 1.016, Val loss 1.853\n",
      "Ep 1 (Step 001345): Train loss 1.399, Val loss 1.857\n",
      "Ep 1 (Step 001350): Train loss 1.421, Val loss 1.863\n",
      "Ep 1 (Step 001355): Train loss 1.622, Val loss 1.868\n",
      "Ep 1 (Step 001360): Train loss 1.317, Val loss 1.871\n",
      "Ep 1 (Step 001365): Train loss 1.109, Val loss 1.874\n",
      "Ep 1 (Step 001370): Train loss 1.340, Val loss 1.867\n",
      "Ep 1 (Step 001375): Train loss 1.252, Val loss 1.864\n",
      "Ep 1 (Step 001380): Train loss 1.180, Val loss 1.862\n",
      "Ep 1 (Step 001385): Train loss 1.486, Val loss 1.860\n",
      "Ep 1 (Step 001390): Train loss 1.316, Val loss 1.861\n",
      "Ep 1 (Step 001395): Train loss 1.308, Val loss 1.862\n",
      "Ep 1 (Step 001400): Train loss 1.283, Val loss 1.863\n",
      "Ep 1 (Step 001405): Train loss 1.471, Val loss 1.866\n",
      "Ep 1 (Step 001410): Train loss 1.350, Val loss 1.865\n",
      "Ep 1 (Step 001415): Train loss 1.392, Val loss 1.860\n",
      "Ep 1 (Step 001420): Train loss 1.402, Val loss 1.858\n",
      "Ep 1 (Step 001425): Train loss 1.305, Val loss 1.862\n",
      "Ep 1 (Step 001430): Train loss 1.517, Val loss 1.864\n",
      "Ep 1 (Step 001435): Train loss 1.300, Val loss 1.867\n",
      "Ep 1 (Step 001440): Train loss 1.354, Val loss 1.866\n",
      "Ep 1 (Step 001445): Train loss 1.419, Val loss 1.859\n",
      "Ep 1 (Step 001450): Train loss 1.089, Val loss 1.854\n",
      "Ep 1 (Step 001455): Train loss 1.382, Val loss 1.854\n",
      "Ep 1 (Step 001460): Train loss 1.102, Val loss 1.853\n",
      "Ep 1 (Step 001465): Train loss 1.306, Val loss 1.857\n",
      "Ep 1 (Step 001470): Train loss 1.478, Val loss 1.859\n",
      "Ep 1 (Step 001475): Train loss 1.284, Val loss 1.858\n",
      "Ep 1 (Step 001480): Train loss 1.333, Val loss 1.856\n",
      "Ep 1 (Step 001485): Train loss 1.476, Val loss 1.857\n",
      "Ep 1 (Step 001490): Train loss 1.306, Val loss 1.861\n",
      "Ep 1 (Step 001495): Train loss 1.325, Val loss 1.861\n",
      "Ep 1 (Step 001500): Train loss 1.174, Val loss 1.858\n",
      "Ep 1 (Step 001505): Train loss 1.291, Val loss 1.855\n",
      "Ep 1 (Step 001510): Train loss 1.542, Val loss 1.855\n",
      "Ep 1 (Step 001515): Train loss 0.933, Val loss 1.855\n",
      "Ep 1 (Step 001520): Train loss 1.318, Val loss 1.855\n",
      "Ep 1 (Step 001525): Train loss 1.341, Val loss 1.853\n",
      "Ep 1 (Step 001530): Train loss 1.226, Val loss 1.851\n",
      "Ep 1 (Step 001535): Train loss 1.196, Val loss 1.850\n",
      "Ep 1 (Step 001540): Train loss 1.403, Val loss 1.853\n",
      "Ep 1 (Step 001545): Train loss 1.124, Val loss 1.856\n",
      "Ep 1 (Step 001550): Train loss 1.793, Val loss 1.854\n",
      "Ep 1 (Step 001555): Train loss 1.522, Val loss 1.856\n",
      "Ep 1 (Step 001560): Train loss 1.191, Val loss 1.859\n",
      "Ep 1 (Step 001565): Train loss 1.281, Val loss 1.852\n",
      "Ep 1 (Step 001570): Train loss 1.360, Val loss 1.851\n",
      "Ep 1 (Step 001575): Train loss 1.540, Val loss 1.851\n",
      "Ep 1 (Step 001580): Train loss 1.674, Val loss 1.852\n",
      "Ep 1 (Step 001585): Train loss 1.573, Val loss 1.856\n",
      "Ep 1 (Step 001590): Train loss 1.300, Val loss 1.861\n",
      "Ep 1 (Step 001595): Train loss 1.429, Val loss 1.864\n",
      "Ep 1 (Step 001600): Train loss 1.674, Val loss 1.864\n",
      "Ep 1 (Step 001605): Train loss 1.512, Val loss 1.866\n",
      "Ep 1 (Step 001610): Train loss 1.076, Val loss 1.865\n",
      "Ep 1 (Step 001615): Train loss 1.762, Val loss 1.862\n",
      "Ep 1 (Step 001620): Train loss 1.476, Val loss 1.859\n",
      "Ep 1 (Step 001625): Train loss 1.839, Val loss 1.855\n",
      "Ep 1 (Step 001630): Train loss 1.315, Val loss 1.852\n",
      "Ep 1 (Step 001635): Train loss 1.269, Val loss 1.849\n",
      "Ep 1 (Step 001640): Train loss 1.506, Val loss 1.850\n",
      "Ep 1 (Step 001645): Train loss 1.491, Val loss 1.853\n",
      "Ep 1 (Step 001650): Train loss 1.056, Val loss 1.857\n",
      "Ep 1 (Step 001655): Train loss 1.491, Val loss 1.859\n",
      "Ep 1 (Step 001660): Train loss 1.154, Val loss 1.857\n",
      "Ep 1 (Step 001665): Train loss 1.372, Val loss 1.854\n",
      "Ep 1 (Step 001670): Train loss 1.421, Val loss 1.853\n",
      "Ep 1 (Step 001675): Train loss 1.473, Val loss 1.850\n",
      "Ep 1 (Step 001680): Train loss 1.268, Val loss 1.847\n",
      "Ep 1 (Step 001685): Train loss 1.490, Val loss 1.847\n",
      "Ep 1 (Step 001690): Train loss 1.314, Val loss 1.849\n",
      "Ep 1 (Step 001695): Train loss 1.263, Val loss 1.850\n",
      "Ep 1 (Step 001700): Train loss 1.261, Val loss 1.849\n",
      "Ep 1 (Step 001705): Train loss 1.439, Val loss 1.847\n",
      "Ep 1 (Step 001710): Train loss 1.108, Val loss 1.847\n",
      "Ep 1 (Step 001715): Train loss 1.525, Val loss 1.848\n",
      "Ep 1 (Step 001720): Train loss 1.428, Val loss 1.846\n",
      "Ep 1 (Step 001725): Train loss 1.237, Val loss 1.845\n",
      "Ep 1 (Step 001730): Train loss 1.250, Val loss 1.844\n",
      "Ep 1 (Step 001735): Train loss 1.394, Val loss 1.840\n",
      "Ep 1 (Step 001740): Train loss 1.356, Val loss 1.836\n",
      "Ep 1 (Step 001745): Train loss 1.429, Val loss 1.832\n",
      "Ep 1 (Step 001750): Train loss 1.464, Val loss 1.834\n",
      "Ep 1 (Step 001755): Train loss 1.283, Val loss 1.833\n",
      "Ep 1 (Step 001760): Train loss 1.376, Val loss 1.838\n",
      "Ep 1 (Step 001765): Train loss 1.227, Val loss 1.851\n",
      "Ep 1 (Step 001770): Train loss 1.377, Val loss 1.843\n",
      "Ep 1 (Step 001775): Train loss 1.447, Val loss 1.836\n",
      "Ep 1 (Step 001780): Train loss 1.224, Val loss 1.832\n",
      "Ep 1 (Step 001785): Train loss 1.340, Val loss 1.831\n",
      "Ep 1 (Step 001790): Train loss 1.271, Val loss 1.832\n",
      "Ep 1 (Step 001795): Train loss 1.184, Val loss 1.834\n",
      "Ep 1 (Step 001800): Train loss 1.335, Val loss 1.837\n",
      "Ep 1 (Step 001805): Train loss 1.244, Val loss 1.840\n",
      "Ep 1 (Step 001810): Train loss 1.565, Val loss 1.843\n",
      "Ep 1 (Step 001815): Train loss 1.273, Val loss 1.843\n",
      "Ep 1 (Step 001820): Train loss 1.289, Val loss 1.840\n",
      "Ep 1 (Step 001825): Train loss 1.158, Val loss 1.834\n",
      "Ep 1 (Step 001830): Train loss 1.420, Val loss 1.831\n",
      "Ep 1 (Step 001835): Train loss 1.398, Val loss 1.829\n",
      "Ep 1 (Step 001840): Train loss 1.061, Val loss 1.828\n",
      "Ep 1 (Step 001845): Train loss 1.557, Val loss 1.829\n",
      "Ep 1 (Step 001850): Train loss 1.385, Val loss 1.829\n",
      "Ep 1 (Step 001855): Train loss 1.411, Val loss 1.832\n",
      "Ep 1 (Step 001860): Train loss 1.193, Val loss 1.838\n",
      "Ep 1 (Step 001865): Train loss 1.426, Val loss 1.836\n",
      "Ep 1 (Step 001870): Train loss 1.411, Val loss 1.830\n",
      "Ep 1 (Step 001875): Train loss 1.356, Val loss 1.825\n",
      "Ep 1 (Step 001880): Train loss 1.089, Val loss 1.828\n",
      "Ep 1 (Step 001885): Train loss 1.265, Val loss 1.829\n",
      "Ep 1 (Step 001890): Train loss 1.252, Val loss 1.832\n",
      "Ep 1 (Step 001895): Train loss 1.507, Val loss 1.835\n",
      "Ep 1 (Step 001900): Train loss 1.675, Val loss 1.833\n",
      "Ep 1 (Step 001905): Train loss 1.403, Val loss 1.830\n",
      "Ep 1 (Step 001910): Train loss 1.497, Val loss 1.830\n",
      "Ep 1 (Step 001915): Train loss 1.493, Val loss 1.829\n",
      "Ep 1 (Step 001920): Train loss 1.233, Val loss 1.831\n",
      "Ep 1 (Step 001925): Train loss 1.274, Val loss 1.835\n",
      "Ep 1 (Step 001930): Train loss 1.487, Val loss 1.841\n",
      "Ep 1 (Step 001935): Train loss 1.573, Val loss 1.842\n",
      "Ep 1 (Step 001940): Train loss 1.390, Val loss 1.842\n",
      "Ep 1 (Step 001945): Train loss 1.624, Val loss 1.846\n",
      "Ep 1 (Step 001950): Train loss 1.262, Val loss 1.849\n",
      "Ep 1 (Step 001955): Train loss 1.488, Val loss 1.849\n",
      "Ep 1 (Step 001960): Train loss 1.172, Val loss 1.847\n",
      "Ep 1 (Step 001965): Train loss 1.277, Val loss 1.847\n",
      "Ep 1 (Step 001970): Train loss 1.377, Val loss 1.845\n",
      "Ep 1 (Step 001975): Train loss 1.262, Val loss 1.846\n",
      "Ep 1 (Step 001980): Train loss 1.431, Val loss 1.845\n",
      "Ep 1 (Step 001985): Train loss 1.420, Val loss 1.843\n",
      "Ep 1 (Step 001990): Train loss 1.590, Val loss 1.844\n",
      "Ep 1 (Step 001995): Train loss 1.442, Val loss 1.843\n",
      "Ep 1 (Step 002000): Train loss 1.387, Val loss 1.845\n",
      "Ep 1 (Step 002005): Train loss 1.594, Val loss 1.845\n",
      "Ep 1 (Step 002010): Train loss 1.392, Val loss 1.843\n",
      "Ep 1 (Step 002015): Train loss 1.524, Val loss 1.840\n",
      "Ep 1 (Step 002020): Train loss 1.282, Val loss 1.844\n",
      "Ep 1 (Step 002025): Train loss 1.555, Val loss 1.852\n",
      "Ep 1 (Step 002030): Train loss 1.281, Val loss 1.851\n",
      "Ep 1 (Step 002035): Train loss 1.286, Val loss 1.849\n",
      "Ep 1 (Step 002040): Train loss 1.430, Val loss 1.843\n",
      "Ep 1 (Step 002045): Train loss 1.331, Val loss 1.838\n",
      "Ep 1 (Step 002050): Train loss 1.240, Val loss 1.836\n",
      "Ep 1 (Step 002055): Train loss 1.489, Val loss 1.835\n",
      "Ep 1 (Step 002060): Train loss 1.193, Val loss 1.832\n",
      "Ep 1 (Step 002065): Train loss 1.003, Val loss 1.830\n",
      "Ep 1 (Step 002070): Train loss 1.090, Val loss 1.829\n",
      "Ep 1 (Step 002075): Train loss 1.550, Val loss 1.827\n",
      "Ep 1 (Step 002080): Train loss 1.151, Val loss 1.827\n",
      "Ep 1 (Step 002085): Train loss 1.238, Val loss 1.829\n",
      "Ep 1 (Step 002090): Train loss 1.096, Val loss 1.832\n",
      "Ep 1 (Step 002095): Train loss 1.355, Val loss 1.838\n",
      "Ep 1 (Step 002100): Train loss 1.117, Val loss 1.847\n",
      "Ep 1 (Step 002105): Train loss 1.297, Val loss 1.854\n",
      "Ep 1 (Step 002110): Train loss 1.610, Val loss 1.857\n",
      "Ep 1 (Step 002115): Train loss 1.627, Val loss 1.845\n",
      "Ep 1 (Step 002120): Train loss 1.448, Val loss 1.838\n",
      "Ep 1 (Step 002125): Train loss 1.255, Val loss 1.840\n",
      "Ep 1 (Step 002130): Train loss 1.201, Val loss 1.843\n",
      "Ep 1 (Step 002135): Train loss 1.424, Val loss 1.842\n",
      "Ep 1 (Step 002140): Train loss 1.367, Val loss 1.841\n",
      "Ep 1 (Step 002145): Train loss 1.426, Val loss 1.839\n",
      "Ep 1 (Step 002150): Train loss 1.391, Val loss 1.838\n",
      "Ep 1 (Step 002155): Train loss 1.385, Val loss 1.839\n",
      "Ep 1 (Step 002160): Train loss 1.402, Val loss 1.841\n",
      "Ep 1 (Step 002165): Train loss 1.188, Val loss 1.842\n",
      "Ep 1 (Step 002170): Train loss 1.360, Val loss 1.838\n",
      "Ep 1 (Step 002175): Train loss 1.282, Val loss 1.835\n",
      "Ep 1 (Step 002180): Train loss 1.255, Val loss 1.834\n",
      "Ep 1 (Step 002185): Train loss 1.558, Val loss 1.832\n",
      "Ep 1 (Step 002190): Train loss 1.628, Val loss 1.833\n",
      "Ep 1 (Step 002195): Train loss 1.174, Val loss 1.833\n",
      "Ep 1 (Step 002200): Train loss 1.363, Val loss 1.832\n",
      "Ep 1 (Step 002205): Train loss 1.513, Val loss 1.832\n",
      "Ep 1 (Step 002210): Train loss 1.282, Val loss 1.831\n",
      "Ep 1 (Step 002215): Train loss 1.393, Val loss 1.831\n",
      "Ep 1 (Step 002220): Train loss 1.583, Val loss 1.832\n",
      "Ep 1 (Step 002225): Train loss 1.412, Val loss 1.831\n",
      "Ep 1 (Step 002230): Train loss 1.362, Val loss 1.832\n",
      "Ep 1 (Step 002235): Train loss 1.460, Val loss 1.832\n",
      "Ep 1 (Step 002240): Train loss 1.198, Val loss 1.831\n",
      "Ep 1 (Step 002245): Train loss 1.218, Val loss 1.829\n",
      "Ep 1 (Step 002250): Train loss 1.395, Val loss 1.827\n",
      "Ep 1 (Step 002255): Train loss 1.393, Val loss 1.827\n",
      "Ep 1 (Step 002260): Train loss 1.407, Val loss 1.833\n",
      "Ep 1 (Step 002265): Train loss 1.351, Val loss 1.838\n",
      "Ep 1 (Step 002270): Train loss 1.505, Val loss 1.839\n",
      "Ep 1 (Step 002275): Train loss 1.128, Val loss 1.841\n",
      "Ep 1 (Step 002280): Train loss 1.362, Val loss 1.843\n",
      "Ep 1 (Step 002285): Train loss 1.548, Val loss 1.840\n",
      "Ep 1 (Step 002290): Train loss 1.448, Val loss 1.839\n",
      "Ep 1 (Step 002295): Train loss 1.669, Val loss 1.840\n",
      "Ep 1 (Step 002300): Train loss 1.238, Val loss 1.837\n",
      "Ep 1 (Step 002305): Train loss 1.174, Val loss 1.835\n",
      "Ep 1 (Step 002310): Train loss 1.259, Val loss 1.835\n",
      "Ep 1 (Step 002315): Train loss 0.969, Val loss 1.833\n",
      "Ep 1 (Step 002320): Train loss 1.140, Val loss 1.834\n",
      "Ep 1 (Step 002325): Train loss 1.357, Val loss 1.831\n",
      "Ep 1 (Step 002330): Train loss 1.590, Val loss 1.831\n",
      "Ep 1 (Step 002335): Train loss 1.167, Val loss 1.832\n",
      "Ep 1 (Step 002340): Train loss 1.045, Val loss 1.834\n",
      "Ep 1 (Step 002345): Train loss 1.100, Val loss 1.839\n",
      "Ep 1 (Step 002350): Train loss 1.487, Val loss 1.842\n",
      "Ep 1 (Step 002355): Train loss 1.341, Val loss 1.844\n",
      "Ep 1 (Step 002360): Train loss 1.330, Val loss 1.846\n",
      "Ep 1 (Step 002365): Train loss 1.451, Val loss 1.846\n",
      "Ep 1 (Step 002370): Train loss 1.443, Val loss 1.844\n",
      "Ep 1 (Step 002375): Train loss 1.172, Val loss 1.842\n",
      "Ep 1 (Step 002380): Train loss 1.425, Val loss 1.841\n",
      "Ep 1 (Step 002385): Train loss 1.309, Val loss 1.843\n",
      "Ep 1 (Step 002390): Train loss 1.090, Val loss 1.848\n",
      "Ep 1 (Step 002395): Train loss 1.252, Val loss 1.851\n",
      "Ep 1 (Step 002400): Train loss 1.431, Val loss 1.847\n",
      "Ep 1 (Step 002405): Train loss 1.566, Val loss 1.843\n",
      "Ep 1 (Step 002410): Train loss 1.103, Val loss 1.841\n",
      "Ep 1 (Step 002415): Train loss 1.042, Val loss 1.842\n",
      "Ep 1 (Step 002420): Train loss 1.435, Val loss 1.842\n",
      "Ep 1 (Step 002425): Train loss 1.312, Val loss 1.843\n",
      "Ep 1 (Step 002430): Train loss 1.315, Val loss 1.843\n",
      "Ep 1 (Step 002435): Train loss 1.419, Val loss 1.846\n",
      "Ep 1 (Step 002440): Train loss 1.431, Val loss 1.848\n",
      "Ep 1 (Step 002445): Train loss 1.419, Val loss 1.846\n",
      "Ep 1 (Step 002450): Train loss 1.474, Val loss 1.843\n",
      "Ep 1 (Step 002455): Train loss 1.029, Val loss 1.845\n",
      "Ep 1 (Step 002460): Train loss 1.274, Val loss 1.846\n",
      "Ep 1 (Step 002465): Train loss 1.006, Val loss 1.849\n",
      "Ep 1 (Step 002470): Train loss 1.305, Val loss 1.850\n",
      "Ep 1 (Step 002475): Train loss 1.131, Val loss 1.853\n",
      "Ep 1 (Step 002480): Train loss 1.270, Val loss 1.854\n",
      "Ep 1 (Step 002485): Train loss 1.506, Val loss 1.853\n",
      "Ep 1 (Step 002490): Train loss 1.455, Val loss 1.850\n",
      "Ep 1 (Step 002495): Train loss 1.337, Val loss 1.851\n",
      "Ep 1 (Step 002500): Train loss 1.147, Val loss 1.852\n",
      "Ep 1 (Step 002505): Train loss 1.268, Val loss 1.853\n",
      "Ep 1 (Step 002510): Train loss 1.263, Val loss 1.855\n",
      "Ep 1 (Step 002515): Train loss 1.111, Val loss 1.856\n",
      "Ep 1 (Step 002520): Train loss 1.113, Val loss 1.854\n",
      "Ep 1 (Step 002525): Train loss 1.163, Val loss 1.850\n",
      "Ep 1 (Step 002530): Train loss 1.308, Val loss 1.849\n",
      "Ep 1 (Step 002535): Train loss 1.386, Val loss 1.850\n",
      "Ep 1 (Step 002540): Train loss 1.274, Val loss 1.852\n",
      "Ep 1 (Step 002545): Train loss 1.355, Val loss 1.852\n",
      "Ep 1 (Step 002550): Train loss 1.486, Val loss 1.852\n",
      "Ep 1 (Step 002555): Train loss 1.321, Val loss 1.852\n",
      "Ep 1 (Step 002560): Train loss 1.134, Val loss 1.853\n",
      "Ep 1 (Step 002565): Train loss 1.671, Val loss 1.854\n",
      "Ep 1 (Step 002570): Train loss 1.155, Val loss 1.854\n",
      "Ep 1 (Step 002575): Train loss 1.435, Val loss 1.854\n",
      "Ep 1 (Step 002580): Train loss 1.250, Val loss 1.852\n",
      "Ep 1 (Step 002585): Train loss 1.317, Val loss 1.853\n",
      "Ep 1 (Step 002590): Train loss 1.124, Val loss 1.850\n",
      "Ep 1 (Step 002595): Train loss 1.056, Val loss 1.847\n",
      "Ep 1 (Step 002600): Train loss 1.569, Val loss 1.845\n",
      "Ep 1 (Step 002605): Train loss 1.336, Val loss 1.846\n",
      "Ep 1 (Step 002610): Train loss 1.453, Val loss 1.845\n",
      "Ep 1 (Step 002615): Train loss 1.131, Val loss 1.845\n",
      "Ep 1 (Step 002620): Train loss 1.373, Val loss 1.846\n",
      "Ep 1 (Step 002625): Train loss 1.704, Val loss 1.845\n",
      "Ep 1 (Step 002630): Train loss 1.259, Val loss 1.845\n",
      "Ep 1 (Step 002635): Train loss 1.358, Val loss 1.846\n",
      "Ep 1 (Step 002640): Train loss 1.433, Val loss 1.847\n",
      "Ep 1 (Step 002645): Train loss 1.472, Val loss 1.851\n",
      "Ep 1 (Step 002650): Train loss 1.242, Val loss 1.854\n",
      "Ep 1 (Step 002655): Train loss 1.359, Val loss 1.854\n",
      "Ep 1 (Step 002660): Train loss 1.576, Val loss 1.848\n",
      "Ep 1 (Step 002665): Train loss 1.291, Val loss 1.845\n",
      "Ep 1 (Step 002670): Train loss 1.252, Val loss 1.843\n",
      "Ep 1 (Step 002675): Train loss 1.139, Val loss 1.844\n",
      "Ep 1 (Step 002680): Train loss 1.179, Val loss 1.846\n",
      "Ep 1 (Step 002685): Train loss 1.578, Val loss 1.849\n",
      "Ep 1 (Step 002690): Train loss 1.521, Val loss 1.853\n",
      "Ep 1 (Step 002695): Train loss 1.338, Val loss 1.855\n",
      "Ep 1 (Step 002700): Train loss 1.410, Val loss 1.855\n",
      "Ep 1 (Step 002705): Train loss 1.368, Val loss 1.853\n",
      "Ep 1 (Step 002710): Train loss 1.313, Val loss 1.853\n",
      "Ep 1 (Step 002715): Train loss 1.416, Val loss 1.852\n",
      "Ep 1 (Step 002720): Train loss 1.572, Val loss 1.851\n",
      "Ep 1 (Step 002725): Train loss 1.239, Val loss 1.850\n",
      "Ep 1 (Step 002730): Train loss 1.091, Val loss 1.851\n",
      "Ep 1 (Step 002735): Train loss 1.462, Val loss 1.851\n",
      "Ep 1 (Step 002740): Train loss 1.348, Val loss 1.847\n",
      "Ep 1 (Step 002745): Train loss 1.366, Val loss 1.845\n",
      "Ep 1 (Step 002750): Train loss 0.883, Val loss 1.845\n",
      "Ep 1 (Step 002755): Train loss 1.307, Val loss 1.847\n",
      "Ep 1 (Step 002760): Train loss 1.197, Val loss 1.851\n",
      "Ep 1 (Step 002765): Train loss 1.224, Val loss 1.850\n",
      "Ep 1 (Step 002770): Train loss 1.421, Val loss 1.850\n",
      "Ep 1 (Step 002775): Train loss 0.986, Val loss 1.849\n",
      "Ep 1 (Step 002780): Train loss 1.217, Val loss 1.849\n",
      "Ep 1 (Step 002785): Train loss 1.432, Val loss 1.849\n",
      "Ep 1 (Step 002790): Train loss 1.244, Val loss 1.849\n",
      "Ep 1 (Step 002795): Train loss 1.542, Val loss 1.845\n",
      "Ep 1 (Step 002800): Train loss 1.212, Val loss 1.843\n",
      "Ep 1 (Step 002805): Train loss 1.321, Val loss 1.844\n",
      "Ep 1 (Step 002810): Train loss 1.649, Val loss 1.848\n",
      "Ep 1 (Step 002815): Train loss 1.164, Val loss 1.852\n",
      "Ep 1 (Step 002820): Train loss 1.254, Val loss 1.854\n",
      "Ep 1 (Step 002825): Train loss 1.571, Val loss 1.856\n",
      "Ep 1 (Step 002830): Train loss 1.265, Val loss 1.854\n",
      "Ep 1 (Step 002835): Train loss 1.423, Val loss 1.851\n",
      "Ep 1 (Step 002840): Train loss 1.384, Val loss 1.846\n",
      "Ep 1 (Step 002845): Train loss 1.310, Val loss 1.843\n",
      "Ep 1 (Step 002850): Train loss 1.298, Val loss 1.843\n",
      "Ep 1 (Step 002855): Train loss 1.436, Val loss 1.842\n",
      "Ep 1 (Step 002860): Train loss 1.119, Val loss 1.841\n",
      "Ep 1 (Step 002865): Train loss 1.211, Val loss 1.841\n",
      "Ep 1 (Step 002870): Train loss 1.226, Val loss 1.844\n",
      "Ep 1 (Step 002875): Train loss 1.416, Val loss 1.848\n",
      "Ep 1 (Step 002880): Train loss 1.424, Val loss 1.845\n",
      "Ep 1 (Step 002885): Train loss 1.598, Val loss 1.841\n",
      "Ep 1 (Step 002890): Train loss 1.324, Val loss 1.838\n",
      "Ep 1 (Step 002895): Train loss 1.342, Val loss 1.836\n",
      "Ep 1 (Step 002900): Train loss 1.304, Val loss 1.837\n",
      "Ep 1 (Step 002905): Train loss 1.279, Val loss 1.836\n",
      "Ep 1 (Step 002910): Train loss 1.402, Val loss 1.838\n",
      "Ep 1 (Step 002915): Train loss 1.315, Val loss 1.839\n",
      "Ep 1 (Step 002920): Train loss 1.421, Val loss 1.841\n",
      "Ep 1 (Step 002925): Train loss 1.642, Val loss 1.843\n",
      "Ep 1 (Step 002930): Train loss 1.360, Val loss 1.842\n",
      "Ep 1 (Step 002935): Train loss 1.172, Val loss 1.842\n",
      "Ep 1 (Step 002940): Train loss 1.463, Val loss 1.842\n",
      "Ep 1 (Step 002945): Train loss 1.230, Val loss 1.843\n",
      "Ep 1 (Step 002950): Train loss 1.319, Val loss 1.841\n",
      "Ep 1 (Step 002955): Train loss 1.131, Val loss 1.839\n",
      "Ep 1 (Step 002960): Train loss 1.348, Val loss 1.836\n",
      "Ep 1 (Step 002965): Train loss 1.234, Val loss 1.834\n",
      "Ep 1 (Step 002970): Train loss 1.199, Val loss 1.831\n",
      "Ep 1 (Step 002975): Train loss 1.567, Val loss 1.831\n",
      "Ep 1 (Step 002980): Train loss 1.272, Val loss 1.830\n",
      "Ep 1 (Step 002985): Train loss 1.113, Val loss 1.828\n",
      "Ep 1 (Step 002990): Train loss 1.400, Val loss 1.827\n",
      "Ep 1 (Step 002995): Train loss 1.216, Val loss 1.827\n",
      "Ep 1 (Step 003000): Train loss 1.215, Val loss 1.827\n",
      "Ep 1 (Step 003005): Train loss 1.381, Val loss 1.829\n",
      "Ep 1 (Step 003010): Train loss 1.596, Val loss 1.835\n",
      "Ep 1 (Step 003015): Train loss 1.390, Val loss 1.838\n",
      "Ep 1 (Step 003020): Train loss 1.540, Val loss 1.838\n",
      "Ep 1 (Step 003025): Train loss 1.128, Val loss 1.838\n",
      "Ep 1 (Step 003030): Train loss 1.350, Val loss 1.841\n",
      "Ep 1 (Step 003035): Train loss 0.979, Val loss 1.843\n",
      "Ep 1 (Step 003040): Train loss 1.059, Val loss 1.846\n",
      "Ep 1 (Step 003045): Train loss 1.515, Val loss 1.846\n",
      "Ep 1 (Step 003050): Train loss 1.426, Val loss 1.845\n",
      "Ep 1 (Step 003055): Train loss 1.239, Val loss 1.847\n",
      "Ep 1 (Step 003060): Train loss 1.190, Val loss 1.850\n",
      "Ep 1 (Step 003065): Train loss 1.454, Val loss 1.852\n",
      "Ep 1 (Step 003070): Train loss 1.738, Val loss 1.852\n",
      "Ep 1 (Step 003075): Train loss 1.340, Val loss 1.849\n",
      "Ep 1 (Step 003080): Train loss 1.570, Val loss 1.847\n",
      "Ep 1 (Step 003085): Train loss 1.315, Val loss 1.846\n",
      "Ep 1 (Step 003090): Train loss 1.334, Val loss 1.845\n",
      "Ep 1 (Step 003095): Train loss 1.467, Val loss 1.844\n",
      "Ep 1 (Step 003100): Train loss 1.348, Val loss 1.845\n",
      "Ep 1 (Step 003105): Train loss 1.339, Val loss 1.843\n",
      "Ep 1 (Step 003110): Train loss 1.542, Val loss 1.839\n",
      "Ep 1 (Step 003115): Train loss 1.129, Val loss 1.837\n",
      "Ep 1 (Step 003120): Train loss 1.320, Val loss 1.837\n",
      "Ep 1 (Step 003125): Train loss 1.191, Val loss 1.838\n",
      "Ep 1 (Step 003130): Train loss 1.245, Val loss 1.841\n",
      "Ep 1 (Step 003135): Train loss 1.213, Val loss 1.846\n",
      "Ep 1 (Step 003140): Train loss 1.272, Val loss 1.850\n",
      "Ep 1 (Step 003145): Train loss 1.340, Val loss 1.852\n",
      "Ep 1 (Step 003150): Train loss 1.550, Val loss 1.848\n",
      "Ep 1 (Step 003155): Train loss 1.197, Val loss 1.842\n",
      "Ep 1 (Step 003160): Train loss 1.266, Val loss 1.839\n",
      "Ep 1 (Step 003165): Train loss 1.280, Val loss 1.840\n",
      "Ep 1 (Step 003170): Train loss 1.114, Val loss 1.843\n",
      "Ep 1 (Step 003175): Train loss 1.198, Val loss 1.846\n",
      "Ep 1 (Step 003180): Train loss 1.588, Val loss 1.849\n",
      "Ep 1 (Step 003185): Train loss 1.320, Val loss 1.852\n",
      "Ep 1 (Step 003190): Train loss 0.993, Val loss 1.854\n",
      "Ep 1 (Step 003195): Train loss 1.179, Val loss 1.857\n",
      "Ep 1 (Step 003200): Train loss 1.225, Val loss 1.862\n",
      "Ep 1 (Step 003205): Train loss 1.193, Val loss 1.866\n",
      "Ep 1 (Step 003210): Train loss 1.146, Val loss 1.867\n",
      "Ep 1 (Step 003215): Train loss 1.479, Val loss 1.866\n",
      "Ep 1 (Step 003220): Train loss 1.201, Val loss 1.860\n",
      "Ep 1 (Step 003225): Train loss 1.311, Val loss 1.851\n",
      "Ep 1 (Step 003230): Train loss 1.281, Val loss 1.845\n",
      "Ep 1 (Step 003235): Train loss 1.283, Val loss 1.841\n",
      "Ep 1 (Step 003240): Train loss 1.424, Val loss 1.841\n",
      "Ep 1 (Step 003245): Train loss 1.830, Val loss 1.842\n",
      "Ep 1 (Step 003250): Train loss 1.309, Val loss 1.844\n",
      "Ep 1 (Step 003255): Train loss 1.155, Val loss 1.847\n",
      "Ep 1 (Step 003260): Train loss 0.983, Val loss 1.851\n",
      "Ep 1 (Step 003265): Train loss 1.485, Val loss 1.850\n",
      "Ep 1 (Step 003270): Train loss 1.378, Val loss 1.848\n",
      "Ep 1 (Step 003275): Train loss 1.437, Val loss 1.849\n",
      "Ep 1 (Step 003280): Train loss 1.223, Val loss 1.850\n",
      "Ep 1 (Step 003285): Train loss 1.417, Val loss 1.849\n",
      "Ep 1 (Step 003290): Train loss 1.235, Val loss 1.852\n",
      "Ep 1 (Step 003295): Train loss 1.413, Val loss 1.856\n",
      "Ep 1 (Step 003300): Train loss 1.169, Val loss 1.856\n",
      "Ep 1 (Step 003305): Train loss 1.464, Val loss 1.855\n",
      "Ep 1 (Step 003310): Train loss 1.558, Val loss 1.856\n",
      "Ep 1 (Step 003315): Train loss 1.203, Val loss 1.853\n",
      "Ep 1 (Step 003320): Train loss 1.253, Val loss 1.851\n",
      "Ep 1 (Step 003325): Train loss 1.260, Val loss 1.849\n",
      "Ep 1 (Step 003330): Train loss 1.237, Val loss 1.850\n",
      "Ep 1 (Step 003335): Train loss 1.256, Val loss 1.854\n",
      "Ep 1 (Step 003340): Train loss 1.057, Val loss 1.858\n",
      "Ep 1 (Step 003345): Train loss 0.883, Val loss 1.856\n",
      "Ep 1 (Step 003350): Train loss 1.257, Val loss 1.855\n",
      "Ep 1 (Step 003355): Train loss 1.383, Val loss 1.858\n",
      "Ep 1 (Step 003360): Train loss 1.262, Val loss 1.860\n",
      "Ep 1 (Step 003365): Train loss 1.202, Val loss 1.860\n",
      "Ep 1 (Step 003370): Train loss 1.177, Val loss 1.859\n",
      "Ep 1 (Step 003375): Train loss 1.758, Val loss 1.858\n",
      "Ep 1 (Step 003380): Train loss 1.661, Val loss 1.858\n",
      "Ep 1 (Step 003385): Train loss 1.370, Val loss 1.857\n",
      "Ep 1 (Step 003390): Train loss 1.267, Val loss 1.854\n",
      "Ep 1 (Step 003395): Train loss 1.291, Val loss 1.852\n",
      "Ep 1 (Step 003400): Train loss 1.444, Val loss 1.852\n",
      "Ep 1 (Step 003405): Train loss 1.355, Val loss 1.852\n",
      "Ep 1 (Step 003410): Train loss 1.416, Val loss 1.854\n",
      "Ep 1 (Step 003415): Train loss 1.162, Val loss 1.859\n",
      "Ep 1 (Step 003420): Train loss 1.245, Val loss 1.864\n",
      "Ep 1 (Step 003425): Train loss 1.290, Val loss 1.865\n",
      "Ep 1 (Step 003430): Train loss 1.438, Val loss 1.865\n",
      "Ep 1 (Step 003435): Train loss 0.991, Val loss 1.863\n",
      "Ep 1 (Step 003440): Train loss 1.084, Val loss 1.861\n",
      "Ep 1 (Step 003445): Train loss 1.289, Val loss 1.860\n",
      "Ep 1 (Step 003450): Train loss 1.139, Val loss 1.858\n",
      "Ep 1 (Step 003455): Train loss 1.176, Val loss 1.856\n",
      "Ep 1 (Step 003460): Train loss 1.264, Val loss 1.856\n",
      "Ep 1 (Step 003465): Train loss 0.905, Val loss 1.856\n",
      "Ep 1 (Step 003470): Train loss 1.495, Val loss 1.859\n",
      "Ep 1 (Step 003475): Train loss 1.387, Val loss 1.862\n",
      "Ep 1 (Step 003480): Train loss 1.241, Val loss 1.862\n",
      "Ep 1 (Step 003485): Train loss 1.247, Val loss 1.866\n",
      "Ep 1 (Step 003490): Train loss 1.375, Val loss 1.868\n",
      "Ep 1 (Step 003495): Train loss 1.208, Val loss 1.870\n",
      "Ep 1 (Step 003500): Train loss 1.169, Val loss 1.868\n",
      "Ep 1 (Step 003505): Train loss 1.475, Val loss 1.866\n",
      "Ep 1 (Step 003510): Train loss 1.173, Val loss 1.865\n",
      "Ep 1 (Step 003515): Train loss 1.298, Val loss 1.866\n",
      "Ep 1 (Step 003520): Train loss 1.399, Val loss 1.868\n",
      "Ep 1 (Step 003525): Train loss 1.473, Val loss 1.865\n",
      "Ep 1 (Step 003530): Train loss 1.237, Val loss 1.862\n",
      "Ep 1 (Step 003535): Train loss 1.305, Val loss 1.858\n",
      "Ep 1 (Step 003540): Train loss 1.489, Val loss 1.857\n",
      "Ep 1 (Step 003545): Train loss 1.092, Val loss 1.857\n",
      "Ep 1 (Step 003550): Train loss 1.183, Val loss 1.856\n",
      "Ep 1 (Step 003555): Train loss 1.145, Val loss 1.853\n",
      "Ep 1 (Step 003560): Train loss 1.475, Val loss 1.854\n",
      "Ep 1 (Step 003565): Train loss 1.187, Val loss 1.857\n",
      "Ep 1 (Step 003570): Train loss 0.916, Val loss 1.857\n",
      "Ep 1 (Step 003575): Train loss 1.110, Val loss 1.853\n",
      "Ep 1 (Step 003580): Train loss 1.150, Val loss 1.848\n",
      "Ep 1 (Step 003585): Train loss 1.354, Val loss 1.845\n",
      "Ep 1 (Step 003590): Train loss 1.711, Val loss 1.845\n",
      "Ep 1 (Step 003595): Train loss 1.501, Val loss 1.847\n",
      "Ep 1 (Step 003600): Train loss 1.491, Val loss 1.848\n",
      "Ep 1 (Step 003605): Train loss 1.484, Val loss 1.850\n",
      "Ep 1 (Step 003610): Train loss 1.443, Val loss 1.851\n",
      "Ep 1 (Step 003615): Train loss 1.047, Val loss 1.854\n",
      "Ep 1 (Step 003620): Train loss 1.317, Val loss 1.856\n",
      "Ep 1 (Step 003625): Train loss 1.522, Val loss 1.856\n",
      "Ep 1 (Step 003630): Train loss 1.421, Val loss 1.854\n",
      "Ep 1 (Step 003635): Train loss 1.411, Val loss 1.850\n",
      "Ep 1 (Step 003640): Train loss 1.245, Val loss 1.848\n",
      "Ep 1 (Step 003645): Train loss 1.331, Val loss 1.848\n",
      "Ep 1 (Step 003650): Train loss 1.227, Val loss 1.848\n",
      "Ep 1 (Step 003655): Train loss 1.381, Val loss 1.849\n",
      "Ep 1 (Step 003660): Train loss 1.225, Val loss 1.850\n",
      "Ep 1 (Step 003665): Train loss 1.771, Val loss 1.850\n",
      "Ep 1 (Step 003670): Train loss 1.498, Val loss 1.850\n",
      "Ep 1 (Step 003675): Train loss 1.121, Val loss 1.846\n",
      "Ep 1 (Step 003680): Train loss 1.421, Val loss 1.843\n",
      "Ep 1 (Step 003685): Train loss 1.013, Val loss 1.842\n",
      "Ep 1 (Step 003690): Train loss 1.699, Val loss 1.841\n",
      "Ep 1 (Step 003695): Train loss 1.287, Val loss 1.840\n",
      "Ep 1 (Step 003700): Train loss 1.396, Val loss 1.838\n",
      "Ep 1 (Step 003705): Train loss 1.231, Val loss 1.837\n",
      "Ep 1 (Step 003710): Train loss 1.253, Val loss 1.835\n",
      "Ep 1 (Step 003715): Train loss 1.079, Val loss 1.834\n",
      "Ep 1 (Step 003720): Train loss 1.240, Val loss 1.834\n",
      "Ep 1 (Step 003725): Train loss 1.658, Val loss 1.832\n",
      "Ep 1 (Step 003730): Train loss 1.468, Val loss 1.830\n",
      "Ep 1 (Step 003735): Train loss 1.142, Val loss 1.829\n",
      "Ep 1 (Step 003740): Train loss 1.339, Val loss 1.829\n",
      "Ep 1 (Step 003745): Train loss 1.492, Val loss 1.830\n",
      "Ep 1 (Step 003750): Train loss 1.382, Val loss 1.831\n",
      "Ep 1 (Step 003755): Train loss 1.246, Val loss 1.829\n",
      "Ep 1 (Step 003760): Train loss 1.339, Val loss 1.828\n",
      "Ep 1 (Step 003765): Train loss 1.078, Val loss 1.828\n",
      "Ep 1 (Step 003770): Train loss 1.359, Val loss 1.827\n",
      "Ep 1 (Step 003775): Train loss 1.387, Val loss 1.827\n",
      "Ep 1 (Step 003780): Train loss 1.252, Val loss 1.830\n",
      "Ep 1 (Step 003785): Train loss 1.159, Val loss 1.835\n",
      "Ep 1 (Step 003790): Train loss 1.307, Val loss 1.838\n",
      "Ep 1 (Step 003795): Train loss 1.355, Val loss 1.838\n",
      "Ep 1 (Step 003800): Train loss 1.446, Val loss 1.835\n",
      "Ep 1 (Step 003805): Train loss 1.168, Val loss 1.831\n",
      "Ep 1 (Step 003810): Train loss 1.161, Val loss 1.829\n",
      "Ep 1 (Step 003815): Train loss 1.125, Val loss 1.830\n",
      "Ep 1 (Step 003820): Train loss 1.358, Val loss 1.832\n",
      "Ep 1 (Step 003825): Train loss 1.249, Val loss 1.834\n",
      "Ep 1 (Step 003830): Train loss 1.038, Val loss 1.830\n",
      "Ep 1 (Step 003835): Train loss 1.475, Val loss 1.825\n",
      "Ep 1 (Step 003840): Train loss 1.574, Val loss 1.820\n",
      "Ep 1 (Step 003845): Train loss 1.085, Val loss 1.817\n",
      "Ep 1 (Step 003850): Train loss 1.152, Val loss 1.814\n",
      "Ep 1 (Step 003855): Train loss 1.111, Val loss 1.811\n",
      "Ep 1 (Step 003860): Train loss 1.593, Val loss 1.811\n",
      "Ep 1 (Step 003865): Train loss 1.218, Val loss 1.808\n",
      "Ep 1 (Step 003870): Train loss 1.234, Val loss 1.806\n",
      "Ep 1 (Step 003875): Train loss 1.359, Val loss 1.806\n",
      "Ep 1 (Step 003880): Train loss 1.777, Val loss 1.806\n",
      "Ep 1 (Step 003885): Train loss 1.845, Val loss 1.806\n",
      "Ep 1 (Step 003890): Train loss 1.388, Val loss 1.806\n",
      "Ep 1 (Step 003895): Train loss 1.251, Val loss 1.805\n",
      "Ep 1 (Step 003900): Train loss 1.003, Val loss 1.806\n",
      "Ep 1 (Step 003905): Train loss 1.466, Val loss 1.809\n",
      "Ep 1 (Step 003910): Train loss 1.310, Val loss 1.810\n",
      "Ep 1 (Step 003915): Train loss 1.091, Val loss 1.810\n",
      "Ep 1 (Step 003920): Train loss 1.420, Val loss 1.811\n",
      "Ep 1 (Step 003925): Train loss 1.493, Val loss 1.810\n",
      "Ep 1 (Step 003930): Train loss 1.602, Val loss 1.810\n",
      "Ep 1 (Step 003935): Train loss 1.546, Val loss 1.812\n",
      "Ep 1 (Step 003940): Train loss 1.174, Val loss 1.817\n",
      "Ep 1 (Step 003945): Train loss 1.386, Val loss 1.818\n",
      "Ep 1 (Step 003950): Train loss 1.392, Val loss 1.817\n",
      "Ep 1 (Step 003955): Train loss 1.455, Val loss 1.817\n",
      "Ep 1 (Step 003960): Train loss 1.736, Val loss 1.818\n",
      "Ep 1 (Step 003965): Train loss 1.340, Val loss 1.819\n",
      "Ep 1 (Step 003970): Train loss 1.065, Val loss 1.819\n",
      "Ep 1 (Step 003975): Train loss 1.313, Val loss 1.818\n",
      "Ep 1 (Step 003980): Train loss 1.345, Val loss 1.818\n",
      "Ep 1 (Step 003985): Train loss 1.274, Val loss 1.819\n",
      "Ep 1 (Step 003990): Train loss 1.322, Val loss 1.819\n",
      "Ep 1 (Step 003995): Train loss 1.301, Val loss 1.819\n",
      "Ep 1 (Step 004000): Train loss 1.220, Val loss 1.819\n",
      "Ep 1 (Step 004005): Train loss 1.232, Val loss 1.819\n",
      "Ep 1 (Step 004010): Train loss 1.286, Val loss 1.820\n",
      "Ep 1 (Step 004015): Train loss 1.504, Val loss 1.822\n",
      "Ep 1 (Step 004020): Train loss 1.595, Val loss 1.824\n",
      "Ep 1 (Step 004025): Train loss 1.274, Val loss 1.826\n",
      "Ep 1 (Step 004030): Train loss 1.312, Val loss 1.828\n",
      "Ep 1 (Step 004035): Train loss 1.376, Val loss 1.829\n",
      "Ep 1 (Step 004040): Train loss 1.194, Val loss 1.830\n",
      "Ep 1 (Step 004045): Train loss 1.126, Val loss 1.831\n",
      "Ep 1 (Step 004050): Train loss 1.396, Val loss 1.836\n",
      "Ep 1 (Step 004055): Train loss 1.370, Val loss 1.835\n",
      "Ep 1 (Step 004060): Train loss 1.360, Val loss 1.828\n",
      "Ep 1 (Step 004065): Train loss 1.130, Val loss 1.822\n",
      "Ep 1 (Step 004070): Train loss 1.403, Val loss 1.819\n",
      "Ep 1 (Step 004075): Train loss 1.474, Val loss 1.819\n",
      "Ep 1 (Step 004080): Train loss 1.433, Val loss 1.821\n",
      "Ep 1 (Step 004085): Train loss 1.111, Val loss 1.823\n",
      "Ep 1 (Step 004090): Train loss 1.241, Val loss 1.823\n",
      "Ep 1 (Step 004095): Train loss 1.089, Val loss 1.825\n",
      "Ep 1 (Step 004100): Train loss 1.190, Val loss 1.826\n",
      "Ep 1 (Step 004105): Train loss 1.439, Val loss 1.826\n",
      "Ep 1 (Step 004110): Train loss 1.322, Val loss 1.824\n",
      "Ep 1 (Step 004115): Train loss 1.355, Val loss 1.822\n",
      "Ep 1 (Step 004120): Train loss 1.662, Val loss 1.820\n",
      "Ep 1 (Step 004125): Train loss 1.242, Val loss 1.816\n",
      "Ep 1 (Step 004130): Train loss 1.192, Val loss 1.811\n",
      "Ep 1 (Step 004135): Train loss 1.112, Val loss 1.810\n",
      "Ep 1 (Step 004140): Train loss 1.228, Val loss 1.810\n",
      "Ep 1 (Step 004145): Train loss 1.119, Val loss 1.810\n",
      "Ep 1 (Step 004150): Train loss 1.398, Val loss 1.810\n",
      "Ep 1 (Step 004155): Train loss 1.160, Val loss 1.811\n",
      "Ep 1 (Step 004160): Train loss 1.063, Val loss 1.813\n",
      "Ep 1 (Step 004165): Train loss 1.169, Val loss 1.814\n",
      "Ep 1 (Step 004170): Train loss 1.179, Val loss 1.813\n",
      "Ep 1 (Step 004175): Train loss 0.943, Val loss 1.813\n",
      "Ep 1 (Step 004180): Train loss 1.459, Val loss 1.814\n",
      "Ep 1 (Step 004185): Train loss 1.479, Val loss 1.815\n",
      "Ep 1 (Step 004190): Train loss 1.496, Val loss 1.819\n",
      "Ep 1 (Step 004195): Train loss 1.123, Val loss 1.821\n",
      "Ep 1 (Step 004200): Train loss 1.320, Val loss 1.821\n",
      "Ep 1 (Step 004205): Train loss 1.507, Val loss 1.818\n",
      "Ep 1 (Step 004210): Train loss 1.119, Val loss 1.817\n",
      "Ep 1 (Step 004215): Train loss 1.276, Val loss 1.817\n",
      "Ep 1 (Step 004220): Train loss 1.428, Val loss 1.816\n",
      "Ep 1 (Step 004225): Train loss 1.407, Val loss 1.815\n",
      "Ep 1 (Step 004230): Train loss 1.043, Val loss 1.814\n",
      "Ep 1 (Step 004235): Train loss 1.008, Val loss 1.816\n",
      "Ep 1 (Step 004240): Train loss 1.098, Val loss 1.817\n",
      "Ep 1 (Step 004245): Train loss 1.220, Val loss 1.815\n",
      "Ep 1 (Step 004250): Train loss 1.510, Val loss 1.812\n",
      "Ep 1 (Step 004255): Train loss 1.411, Val loss 1.807\n",
      "Ep 1 (Step 004260): Train loss 1.356, Val loss 1.803\n",
      "Ep 1 (Step 004265): Train loss 1.371, Val loss 1.802\n",
      "Ep 1 (Step 004270): Train loss 1.346, Val loss 1.802\n",
      "Ep 1 (Step 004275): Train loss 1.592, Val loss 1.804\n",
      "Ep 1 (Step 004280): Train loss 1.325, Val loss 1.805\n",
      "Ep 1 (Step 004285): Train loss 1.181, Val loss 1.803\n",
      "Ep 1 (Step 004290): Train loss 1.251, Val loss 1.804\n",
      "Ep 1 (Step 004295): Train loss 1.166, Val loss 1.806\n",
      "Ep 1 (Step 004300): Train loss 1.071, Val loss 1.808\n",
      "Ep 1 (Step 004305): Train loss 1.378, Val loss 1.809\n",
      "Ep 1 (Step 004310): Train loss 1.452, Val loss 1.812\n",
      "Ep 1 (Step 004315): Train loss 1.177, Val loss 1.816\n",
      "Ep 1 (Step 004320): Train loss 1.407, Val loss 1.819\n",
      "Ep 1 (Step 004325): Train loss 1.281, Val loss 1.819\n",
      "Ep 1 (Step 004330): Train loss 1.512, Val loss 1.818\n",
      "Ep 1 (Step 004335): Train loss 1.132, Val loss 1.817\n",
      "Ep 1 (Step 004340): Train loss 0.985, Val loss 1.817\n",
      "Ep 1 (Step 004345): Train loss 1.478, Val loss 1.818\n",
      "Ep 1 (Step 004350): Train loss 1.473, Val loss 1.818\n",
      "Ep 1 (Step 004355): Train loss 1.066, Val loss 1.818\n",
      "Ep 1 (Step 004360): Train loss 1.317, Val loss 1.817\n",
      "Ep 1 (Step 004365): Train loss 1.487, Val loss 1.816\n",
      "Ep 1 (Step 004370): Train loss 1.272, Val loss 1.815\n",
      "Ep 1 (Step 004375): Train loss 1.488, Val loss 1.816\n",
      "Ep 1 (Step 004380): Train loss 1.202, Val loss 1.816\n",
      "Ep 1 (Step 004385): Train loss 1.151, Val loss 1.817\n",
      "Ep 1 (Step 004390): Train loss 1.380, Val loss 1.818\n",
      "Ep 1 (Step 004395): Train loss 1.211, Val loss 1.818\n",
      "Ep 1 (Step 004400): Train loss 1.230, Val loss 1.819\n",
      "Ep 1 (Step 004405): Train loss 1.322, Val loss 1.819\n",
      "Ep 1 (Step 004410): Train loss 1.452, Val loss 1.823\n",
      "Ep 1 (Step 004415): Train loss 1.440, Val loss 1.826\n",
      "Ep 1 (Step 004420): Train loss 1.275, Val loss 1.825\n",
      "Ep 1 (Step 004425): Train loss 1.301, Val loss 1.823\n",
      "Ep 1 (Step 004430): Train loss 1.390, Val loss 1.819\n",
      "Ep 1 (Step 004435): Train loss 1.407, Val loss 1.817\n",
      "Ep 1 (Step 004440): Train loss 1.371, Val loss 1.813\n",
      "Ep 1 (Step 004445): Train loss 1.356, Val loss 1.811\n",
      "Ep 1 (Step 004450): Train loss 1.575, Val loss 1.810\n",
      "Ep 1 (Step 004455): Train loss 1.657, Val loss 1.811\n",
      "Ep 1 (Step 004460): Train loss 1.009, Val loss 1.811\n",
      "Ep 1 (Step 004465): Train loss 1.319, Val loss 1.811\n",
      "Ep 1 (Step 004470): Train loss 1.170, Val loss 1.809\n",
      "Ep 1 (Step 004475): Train loss 1.347, Val loss 1.807\n",
      "Ep 1 (Step 004480): Train loss 1.434, Val loss 1.811\n",
      "Ep 1 (Step 004485): Train loss 1.266, Val loss 1.818\n",
      "Ep 1 (Step 004490): Train loss 1.332, Val loss 1.822\n",
      "Ep 1 (Step 004495): Train loss 1.343, Val loss 1.822\n",
      "Ep 1 (Step 004500): Train loss 1.536, Val loss 1.815\n",
      "Ep 1 (Step 004505): Train loss 1.450, Val loss 1.809\n",
      "Ep 1 (Step 004510): Train loss 1.289, Val loss 1.808\n",
      "Ep 1 (Step 004515): Train loss 1.418, Val loss 1.807\n",
      "Ep 1 (Step 004520): Train loss 1.259, Val loss 1.806\n",
      "Ep 1 (Step 004525): Train loss 1.460, Val loss 1.807\n",
      "Ep 1 (Step 004530): Train loss 1.209, Val loss 1.810\n",
      "Ep 1 (Step 004535): Train loss 1.320, Val loss 1.814\n",
      "Ep 1 (Step 004540): Train loss 1.446, Val loss 1.817\n",
      "Ep 1 (Step 004545): Train loss 1.216, Val loss 1.815\n",
      "Ep 1 (Step 004550): Train loss 1.305, Val loss 1.812\n",
      "Ep 1 (Step 004555): Train loss 1.318, Val loss 1.810\n",
      "Ep 1 (Step 004560): Train loss 1.361, Val loss 1.814\n",
      "Ep 1 (Step 004565): Train loss 1.422, Val loss 1.819\n",
      "Ep 1 (Step 004570): Train loss 1.490, Val loss 1.822\n",
      "Ep 1 (Step 004575): Train loss 1.366, Val loss 1.824\n",
      "Ep 1 (Step 004580): Train loss 1.391, Val loss 1.826\n",
      "Ep 1 (Step 004585): Train loss 1.172, Val loss 1.828\n",
      "Ep 1 (Step 004590): Train loss 1.107, Val loss 1.827\n",
      "Ep 1 (Step 004595): Train loss 1.288, Val loss 1.824\n",
      "Ep 1 (Step 004600): Train loss 1.118, Val loss 1.825\n",
      "Ep 1 (Step 004605): Train loss 1.322, Val loss 1.825\n",
      "Ep 1 (Step 004610): Train loss 1.566, Val loss 1.824\n",
      "Ep 1 (Step 004615): Train loss 1.048, Val loss 1.820\n",
      "Ep 1 (Step 004620): Train loss 1.124, Val loss 1.820\n",
      "Ep 1 (Step 004625): Train loss 1.394, Val loss 1.821\n",
      "Ep 1 (Step 004630): Train loss 0.973, Val loss 1.822\n",
      "Ep 1 (Step 004635): Train loss 1.286, Val loss 1.822\n",
      "Ep 1 (Step 004640): Train loss 1.158, Val loss 1.823\n",
      "Ep 1 (Step 004645): Train loss 1.274, Val loss 1.823\n",
      "Ep 1 (Step 004650): Train loss 1.064, Val loss 1.818\n",
      "Ep 1 (Step 004655): Train loss 0.944, Val loss 1.814\n",
      "Ep 1 (Step 004660): Train loss 1.073, Val loss 1.814\n",
      "Ep 1 (Step 004665): Train loss 1.148, Val loss 1.815\n",
      "Ep 1 (Step 004670): Train loss 1.191, Val loss 1.816\n",
      "Ep 1 (Step 004675): Train loss 1.316, Val loss 1.819\n",
      "Ep 1 (Step 004680): Train loss 1.367, Val loss 1.820\n",
      "Ep 1 (Step 004685): Train loss 1.526, Val loss 1.822\n",
      "Ep 1 (Step 004690): Train loss 1.333, Val loss 1.825\n",
      "Ep 1 (Step 004695): Train loss 1.508, Val loss 1.827\n",
      "Ep 1 (Step 004700): Train loss 1.165, Val loss 1.827\n",
      "Ep 1 (Step 004705): Train loss 1.604, Val loss 1.828\n",
      "Ep 1 (Step 004710): Train loss 1.323, Val loss 1.827\n",
      "Ep 1 (Step 004715): Train loss 1.143, Val loss 1.823\n",
      "Ep 1 (Step 004720): Train loss 1.095, Val loss 1.822\n",
      "Ep 1 (Step 004725): Train loss 1.101, Val loss 1.824\n",
      "Ep 1 (Step 004730): Train loss 1.503, Val loss 1.827\n",
      "Ep 1 (Step 004735): Train loss 1.356, Val loss 1.831\n",
      "Ep 1 (Step 004740): Train loss 0.947, Val loss 1.834\n",
      "Ep 1 (Step 004745): Train loss 1.168, Val loss 1.834\n",
      "Ep 1 (Step 004750): Train loss 1.465, Val loss 1.834\n",
      "Ep 1 (Step 004755): Train loss 1.131, Val loss 1.834\n",
      "Ep 1 (Step 004760): Train loss 1.056, Val loss 1.836\n",
      "Ep 1 (Step 004765): Train loss 1.523, Val loss 1.837\n",
      "Ep 1 (Step 004770): Train loss 1.289, Val loss 1.836\n",
      "Ep 1 (Step 004775): Train loss 1.259, Val loss 1.832\n",
      "Ep 1 (Step 004780): Train loss 1.399, Val loss 1.829\n",
      "Ep 1 (Step 004785): Train loss 1.030, Val loss 1.826\n",
      "Ep 1 (Step 004790): Train loss 1.345, Val loss 1.826\n",
      "Ep 1 (Step 004795): Train loss 1.454, Val loss 1.827\n",
      "Ep 1 (Step 004800): Train loss 1.021, Val loss 1.828\n",
      "Ep 1 (Step 004805): Train loss 1.134, Val loss 1.827\n",
      "Ep 1 (Step 004810): Train loss 1.210, Val loss 1.826\n",
      "Ep 1 (Step 004815): Train loss 1.200, Val loss 1.822\n",
      "Ep 1 (Step 004820): Train loss 1.416, Val loss 1.818\n",
      "Ep 1 (Step 004825): Train loss 1.365, Val loss 1.816\n",
      "Ep 1 (Step 004830): Train loss 1.149, Val loss 1.815\n",
      "Ep 1 (Step 004835): Train loss 1.248, Val loss 1.817\n",
      "Ep 1 (Step 004840): Train loss 1.176, Val loss 1.819\n",
      "Ep 1 (Step 004845): Train loss 1.208, Val loss 1.823\n",
      "Ep 1 (Step 004850): Train loss 1.255, Val loss 1.826\n",
      "Ep 1 (Step 004855): Train loss 1.196, Val loss 1.827\n",
      "Ep 1 (Step 004860): Train loss 1.312, Val loss 1.827\n",
      "Ep 1 (Step 004865): Train loss 1.329, Val loss 1.829\n",
      "Ep 1 (Step 004870): Train loss 1.333, Val loss 1.828\n",
      "Ep 1 (Step 004875): Train loss 1.343, Val loss 1.823\n",
      "Ep 1 (Step 004880): Train loss 1.400, Val loss 1.820\n",
      "Ep 1 (Step 004885): Train loss 1.082, Val loss 1.818\n",
      "Ep 1 (Step 004890): Train loss 1.464, Val loss 1.819\n",
      "Ep 1 (Step 004895): Train loss 1.390, Val loss 1.818\n",
      "Ep 1 (Step 004900): Train loss 1.547, Val loss 1.814\n",
      "Ep 1 (Step 004905): Train loss 1.345, Val loss 1.812\n",
      "Ep 1 (Step 004910): Train loss 1.453, Val loss 1.812\n",
      "Ep 1 (Step 004915): Train loss 1.292, Val loss 1.814\n",
      "Ep 1 (Step 004920): Train loss 1.230, Val loss 1.815\n",
      "Ep 1 (Step 004925): Train loss 1.292, Val loss 1.817\n",
      "Ep 1 (Step 004930): Train loss 1.288, Val loss 1.819\n",
      "Ep 1 (Step 004935): Train loss 1.214, Val loss 1.821\n",
      "Ep 1 (Step 004940): Train loss 1.171, Val loss 1.820\n",
      "Ep 1 (Step 004945): Train loss 1.167, Val loss 1.816\n",
      "Ep 1 (Step 004950): Train loss 1.346, Val loss 1.813\n",
      "Ep 1 (Step 004955): Train loss 1.356, Val loss 1.812\n",
      "Ep 1 (Step 004960): Train loss 1.514, Val loss 1.813\n",
      "Ep 1 (Step 004965): Train loss 1.764, Val loss 1.815\n",
      "Ep 1 (Step 004970): Train loss 1.339, Val loss 1.818\n",
      "Ep 1 (Step 004975): Train loss 0.992, Val loss 1.819\n",
      "Ep 1 (Step 004980): Train loss 1.374, Val loss 1.819\n",
      "Ep 1 (Step 004985): Train loss 1.298, Val loss 1.819\n",
      "Ep 1 (Step 004990): Train loss 0.985, Val loss 1.820\n",
      "Ep 1 (Step 004995): Train loss 1.664, Val loss 1.820\n",
      "Ep 1 (Step 005000): Train loss 1.409, Val loss 1.820\n",
      "Ep 1 (Step 005005): Train loss 1.105, Val loss 1.819\n",
      "Ep 1 (Step 005010): Train loss 1.197, Val loss 1.817\n",
      "Ep 1 (Step 005015): Train loss 1.096, Val loss 1.812\n",
      "Ep 1 (Step 005020): Train loss 1.176, Val loss 1.810\n",
      "Ep 1 (Step 005025): Train loss 1.466, Val loss 1.808\n",
      "Ep 1 (Step 005030): Train loss 1.122, Val loss 1.805\n",
      "Ep 1 (Step 005035): Train loss 1.305, Val loss 1.802\n",
      "Ep 1 (Step 005040): Train loss 0.852, Val loss 1.799\n",
      "Ep 1 (Step 005045): Train loss 1.489, Val loss 1.797\n",
      "Ep 1 (Step 005050): Train loss 1.270, Val loss 1.798\n",
      "Ep 1 (Step 005055): Train loss 1.172, Val loss 1.798\n",
      "Ep 1 (Step 005060): Train loss 1.057, Val loss 1.800\n",
      "Ep 1 (Step 005065): Train loss 1.058, Val loss 1.803\n",
      "Ep 1 (Step 005070): Train loss 1.343, Val loss 1.807\n",
      "Ep 1 (Step 005075): Train loss 1.526, Val loss 1.809\n",
      "Ep 1 (Step 005080): Train loss 1.452, Val loss 1.808\n",
      "Ep 1 (Step 005085): Train loss 1.501, Val loss 1.808\n",
      "Ep 1 (Step 005090): Train loss 1.073, Val loss 1.809\n",
      "Ep 1 (Step 005095): Train loss 1.543, Val loss 1.809\n",
      "Ep 1 (Step 005100): Train loss 1.414, Val loss 1.809\n",
      "Ep 1 (Step 005105): Train loss 1.370, Val loss 1.808\n",
      "Ep 1 (Step 005110): Train loss 1.269, Val loss 1.808\n",
      "Ep 1 (Step 005115): Train loss 1.451, Val loss 1.808\n",
      "Ep 1 (Step 005120): Train loss 1.162, Val loss 1.809\n",
      "Ep 1 (Step 005125): Train loss 1.187, Val loss 1.810\n",
      "Ep 1 (Step 005130): Train loss 0.912, Val loss 1.807\n",
      "Ep 1 (Step 005135): Train loss 1.224, Val loss 1.805\n",
      "Ep 1 (Step 005140): Train loss 1.046, Val loss 1.805\n",
      "Ep 1 (Step 005145): Train loss 1.385, Val loss 1.807\n",
      "Ep 1 (Step 005150): Train loss 1.308, Val loss 1.807\n",
      "Ep 1 (Step 005155): Train loss 1.240, Val loss 1.805\n",
      "Ep 1 (Step 005160): Train loss 1.358, Val loss 1.805\n",
      "Ep 1 (Step 005165): Train loss 1.376, Val loss 1.807\n",
      "Ep 1 (Step 005170): Train loss 1.275, Val loss 1.809\n",
      "Ep 1 (Step 005175): Train loss 1.378, Val loss 1.809\n",
      "Ep 1 (Step 005180): Train loss 1.271, Val loss 1.811\n",
      "Ep 1 (Step 005185): Train loss 1.369, Val loss 1.813\n",
      "Ep 1 (Step 005190): Train loss 1.213, Val loss 1.816\n",
      "Ep 1 (Step 005195): Train loss 1.434, Val loss 1.819\n",
      "Ep 1 (Step 005200): Train loss 1.546, Val loss 1.820\n",
      "Ep 1 (Step 005205): Train loss 0.889, Val loss 1.822\n",
      "Ep 1 (Step 005210): Train loss 1.389, Val loss 1.824\n",
      "Ep 1 (Step 005215): Train loss 1.245, Val loss 1.825\n",
      "Ep 1 (Step 005220): Train loss 1.167, Val loss 1.822\n",
      "Ep 1 (Step 005225): Train loss 1.047, Val loss 1.815\n",
      "Ep 1 (Step 005230): Train loss 1.376, Val loss 1.809\n",
      "Ep 1 (Step 005235): Train loss 1.286, Val loss 1.807\n",
      "Ep 1 (Step 005240): Train loss 1.680, Val loss 1.805\n",
      "Ep 1 (Step 005245): Train loss 1.221, Val loss 1.803\n",
      "Ep 1 (Step 005250): Train loss 1.393, Val loss 1.801\n",
      "Ep 1 (Step 005255): Train loss 1.407, Val loss 1.798\n",
      "Ep 1 (Step 005260): Train loss 1.032, Val loss 1.799\n",
      "Ep 1 (Step 005265): Train loss 1.213, Val loss 1.800\n",
      "Ep 1 (Step 005270): Train loss 1.048, Val loss 1.799\n",
      "Ep 1 (Step 005275): Train loss 1.380, Val loss 1.795\n",
      "Ep 1 (Step 005280): Train loss 1.268, Val loss 1.793\n",
      "Ep 1 (Step 005285): Train loss 0.998, Val loss 1.793\n",
      "Ep 1 (Step 005290): Train loss 1.267, Val loss 1.794\n",
      "Ep 1 (Step 005295): Train loss 1.268, Val loss 1.797\n",
      "Ep 1 (Step 005300): Train loss 1.112, Val loss 1.799\n",
      "Ep 1 (Step 005305): Train loss 1.245, Val loss 1.803\n",
      "Ep 1 (Step 005310): Train loss 1.263, Val loss 1.806\n",
      "Ep 1 (Step 005315): Train loss 1.428, Val loss 1.807\n",
      "Ep 1 (Step 005320): Train loss 1.295, Val loss 1.807\n",
      "Ep 1 (Step 005325): Train loss 1.391, Val loss 1.806\n",
      "Ep 1 (Step 005330): Train loss 1.324, Val loss 1.807\n",
      "Ep 1 (Step 005335): Train loss 1.314, Val loss 1.808\n",
      "Ep 1 (Step 005340): Train loss 1.168, Val loss 1.809\n",
      "Ep 1 (Step 005345): Train loss 1.409, Val loss 1.806\n",
      "Ep 1 (Step 005350): Train loss 1.132, Val loss 1.805\n",
      "Ep 1 (Step 005355): Train loss 1.266, Val loss 1.804\n",
      "Ep 1 (Step 005360): Train loss 1.134, Val loss 1.805\n",
      "Ep 1 (Step 005365): Train loss 1.089, Val loss 1.805\n",
      "Ep 1 (Step 005370): Train loss 1.389, Val loss 1.806\n",
      "Ep 1 (Step 005375): Train loss 1.189, Val loss 1.803\n",
      "Ep 1 (Step 005380): Train loss 1.058, Val loss 1.802\n",
      "Ep 1 (Step 005385): Train loss 1.261, Val loss 1.801\n",
      "Ep 1 (Step 005390): Train loss 1.339, Val loss 1.802\n",
      "Ep 1 (Step 005395): Train loss 1.072, Val loss 1.803\n",
      "Ep 1 (Step 005400): Train loss 1.226, Val loss 1.804\n",
      "Ep 1 (Step 005405): Train loss 1.044, Val loss 1.804\n",
      "Ep 1 (Step 005410): Train loss 1.190, Val loss 1.804\n",
      "Ep 1 (Step 005415): Train loss 1.344, Val loss 1.800\n",
      "Ep 1 (Step 005420): Train loss 1.126, Val loss 1.795\n",
      "Ep 1 (Step 005425): Train loss 1.410, Val loss 1.791\n",
      "Ep 1 (Step 005430): Train loss 1.275, Val loss 1.790\n",
      "Ep 1 (Step 005435): Train loss 1.191, Val loss 1.791\n",
      "Ep 1 (Step 005440): Train loss 1.389, Val loss 1.792\n",
      "Ep 1 (Step 005445): Train loss 1.426, Val loss 1.795\n",
      "Ep 1 (Step 005450): Train loss 1.313, Val loss 1.797\n",
      "Ep 1 (Step 005455): Train loss 1.238, Val loss 1.799\n",
      "Ep 1 (Step 005460): Train loss 1.401, Val loss 1.802\n",
      "Ep 1 (Step 005465): Train loss 1.236, Val loss 1.806\n",
      "Ep 1 (Step 005470): Train loss 1.380, Val loss 1.807\n",
      "Ep 1 (Step 005475): Train loss 1.228, Val loss 1.808\n",
      "Ep 1 (Step 005480): Train loss 1.507, Val loss 1.809\n",
      "Ep 1 (Step 005485): Train loss 1.359, Val loss 1.811\n",
      "Ep 1 (Step 005490): Train loss 1.248, Val loss 1.814\n",
      "Ep 1 (Step 005495): Train loss 1.399, Val loss 1.814\n",
      "Ep 1 (Step 005500): Train loss 1.208, Val loss 1.815\n",
      "Ep 1 (Step 005505): Train loss 1.494, Val loss 1.815\n",
      "Ep 1 (Step 005510): Train loss 1.119, Val loss 1.812\n",
      "Ep 1 (Step 005515): Train loss 1.289, Val loss 1.807\n",
      "Ep 1 (Step 005520): Train loss 1.176, Val loss 1.804\n",
      "Ep 1 (Step 005525): Train loss 1.305, Val loss 1.801\n",
      "Ep 1 (Step 005530): Train loss 1.357, Val loss 1.800\n",
      "Ep 1 (Step 005535): Train loss 1.034, Val loss 1.803\n",
      "Ep 1 (Step 005540): Train loss 1.587, Val loss 1.806\n",
      "Ep 1 (Step 005545): Train loss 0.939, Val loss 1.806\n",
      "Ep 1 (Step 005550): Train loss 1.492, Val loss 1.802\n",
      "Ep 1 (Step 005555): Train loss 1.258, Val loss 1.796\n",
      "Ep 1 (Step 005560): Train loss 1.232, Val loss 1.793\n",
      "Ep 1 (Step 005565): Train loss 1.307, Val loss 1.791\n",
      "Ep 1 (Step 005570): Train loss 1.303, Val loss 1.790\n",
      "Ep 1 (Step 005575): Train loss 1.318, Val loss 1.790\n",
      "Ep 1 (Step 005580): Train loss 0.976, Val loss 1.791\n",
      "Ep 1 (Step 005585): Train loss 1.185, Val loss 1.792\n",
      "Ep 1 (Step 005590): Train loss 1.423, Val loss 1.795\n",
      "Ep 1 (Step 005595): Train loss 1.329, Val loss 1.797\n",
      "Ep 1 (Step 005600): Train loss 1.108, Val loss 1.797\n",
      "Ep 1 (Step 005605): Train loss 1.243, Val loss 1.794\n",
      "Ep 1 (Step 005610): Train loss 1.247, Val loss 1.793\n",
      "Ep 1 (Step 005615): Train loss 1.357, Val loss 1.792\n",
      "Ep 1 (Step 005620): Train loss 1.248, Val loss 1.791\n",
      "Ep 1 (Step 005625): Train loss 1.056, Val loss 1.789\n",
      "Ep 1 (Step 005630): Train loss 1.129, Val loss 1.790\n",
      "Ep 1 (Step 005635): Train loss 1.318, Val loss 1.791\n",
      "Ep 1 (Step 005640): Train loss 1.313, Val loss 1.793\n",
      "Ep 1 (Step 005645): Train loss 1.614, Val loss 1.795\n",
      "Ep 1 (Step 005650): Train loss 1.024, Val loss 1.796\n",
      "Ep 1 (Step 005655): Train loss 1.430, Val loss 1.798\n",
      "Ep 1 (Step 005660): Train loss 1.426, Val loss 1.801\n",
      "Ep 1 (Step 005665): Train loss 1.415, Val loss 1.802\n",
      "Ep 1 (Step 005670): Train loss 1.101, Val loss 1.801\n",
      "Ep 1 (Step 005675): Train loss 1.550, Val loss 1.800\n",
      "Ep 1 (Step 005680): Train loss 1.084, Val loss 1.799\n",
      "Ep 1 (Step 005685): Train loss 1.448, Val loss 1.798\n",
      "Ep 1 (Step 005690): Train loss 1.232, Val loss 1.799\n",
      "Ep 1 (Step 005695): Train loss 1.283, Val loss 1.802\n",
      "Ep 1 (Step 005700): Train loss 1.232, Val loss 1.804\n",
      "Ep 1 (Step 005705): Train loss 1.375, Val loss 1.807\n",
      "Ep 1 (Step 005710): Train loss 1.413, Val loss 1.807\n",
      "Ep 1 (Step 005715): Train loss 1.380, Val loss 1.807\n",
      "Ep 1 (Step 005720): Train loss 1.118, Val loss 1.807\n",
      "Ep 1 (Step 005725): Train loss 1.437, Val loss 1.808\n",
      "Ep 1 (Step 005730): Train loss 1.335, Val loss 1.810\n",
      "Ep 1 (Step 005735): Train loss 1.477, Val loss 1.815\n",
      "Ep 1 (Step 005740): Train loss 1.512, Val loss 1.821\n",
      "Ep 1 (Step 005745): Train loss 1.345, Val loss 1.824\n",
      "Ep 1 (Step 005750): Train loss 1.260, Val loss 1.824\n",
      "Ep 1 (Step 005755): Train loss 1.048, Val loss 1.822\n",
      "Ep 1 (Step 005760): Train loss 1.067, Val loss 1.818\n",
      "Ep 1 (Step 005765): Train loss 1.156, Val loss 1.817\n",
      "Ep 1 (Step 005770): Train loss 1.383, Val loss 1.815\n",
      "Ep 1 (Step 005775): Train loss 1.646, Val loss 1.814\n",
      "Ep 1 (Step 005780): Train loss 1.573, Val loss 1.814\n",
      "Ep 1 (Step 005785): Train loss 1.270, Val loss 1.814\n",
      "Ep 1 (Step 005790): Train loss 1.163, Val loss 1.813\n",
      "Ep 1 (Step 005795): Train loss 1.143, Val loss 1.814\n",
      "Ep 1 (Step 005800): Train loss 1.341, Val loss 1.813\n",
      "Ep 1 (Step 005805): Train loss 1.479, Val loss 1.812\n",
      "Ep 1 (Step 005810): Train loss 1.325, Val loss 1.810\n",
      "Ep 1 (Step 005815): Train loss 1.101, Val loss 1.809\n",
      "Ep 1 (Step 005820): Train loss 1.306, Val loss 1.808\n",
      "Ep 1 (Step 005825): Train loss 1.663, Val loss 1.807\n",
      "Ep 1 (Step 005830): Train loss 1.165, Val loss 1.807\n",
      "Ep 1 (Step 005835): Train loss 1.236, Val loss 1.810\n",
      "Ep 1 (Step 005840): Train loss 1.275, Val loss 1.809\n",
      "Ep 1 (Step 005845): Train loss 1.557, Val loss 1.807\n",
      "Ep 1 (Step 005850): Train loss 1.344, Val loss 1.807\n",
      "Ep 1 (Step 005855): Train loss 1.282, Val loss 1.810\n",
      "Ep 1 (Step 005860): Train loss 1.362, Val loss 1.812\n",
      "Ep 1 (Step 005865): Train loss 1.111, Val loss 1.812\n",
      "Ep 1 (Step 005870): Train loss 1.258, Val loss 1.810\n",
      "Ep 1 (Step 005875): Train loss 1.127, Val loss 1.812\n",
      "Ep 1 (Step 005880): Train loss 1.443, Val loss 1.814\n",
      "Ep 1 (Step 005885): Train loss 1.399, Val loss 1.815\n",
      "Ep 1 (Step 005890): Train loss 0.875, Val loss 1.814\n",
      "Ep 1 (Step 005895): Train loss 1.240, Val loss 1.813\n",
      "Ep 1 (Step 005900): Train loss 1.226, Val loss 1.810\n",
      "Ep 1 (Step 005905): Train loss 1.202, Val loss 1.806\n",
      "Ep 1 (Step 005910): Train loss 1.335, Val loss 1.804\n",
      "Ep 1 (Step 005915): Train loss 1.340, Val loss 1.803\n",
      "Ep 1 (Step 005920): Train loss 1.191, Val loss 1.803\n",
      "Ep 1 (Step 005925): Train loss 1.211, Val loss 1.804\n",
      "Ep 1 (Step 005930): Train loss 1.191, Val loss 1.806\n",
      "Ep 1 (Step 005935): Train loss 1.380, Val loss 1.806\n",
      "Ep 1 (Step 005940): Train loss 1.148, Val loss 1.806\n",
      "Ep 1 (Step 005945): Train loss 1.169, Val loss 1.808\n",
      "Ep 1 (Step 005950): Train loss 1.172, Val loss 1.809\n",
      "Ep 1 (Step 005955): Train loss 1.429, Val loss 1.809\n",
      "Ep 1 (Step 005960): Train loss 1.744, Val loss 1.807\n",
      "Ep 1 (Step 005965): Train loss 1.434, Val loss 1.805\n",
      "Ep 1 (Step 005970): Train loss 1.070, Val loss 1.802\n",
      "Ep 1 (Step 005975): Train loss 1.262, Val loss 1.800\n",
      "Ep 1 (Step 005980): Train loss 1.153, Val loss 1.799\n",
      "Ep 1 (Step 005985): Train loss 1.044, Val loss 1.799\n",
      "Ep 1 (Step 005990): Train loss 1.235, Val loss 1.799\n",
      "Ep 1 (Step 005995): Train loss 1.368, Val loss 1.800\n",
      "Ep 1 (Step 006000): Train loss 1.244, Val loss 1.802\n",
      "Ep 1 (Step 006005): Train loss 1.157, Val loss 1.805\n",
      "Ep 1 (Step 006010): Train loss 1.436, Val loss 1.813\n",
      "Ep 1 (Step 006015): Train loss 1.263, Val loss 1.816\n",
      "Ep 1 (Step 006020): Train loss 1.265, Val loss 1.816\n",
      "Ep 1 (Step 006025): Train loss 1.434, Val loss 1.808\n",
      "Ep 1 (Step 006030): Train loss 1.381, Val loss 1.804\n",
      "Ep 1 (Step 006035): Train loss 1.486, Val loss 1.804\n",
      "Ep 1 (Step 006040): Train loss 1.645, Val loss 1.805\n",
      "Ep 1 (Step 006045): Train loss 1.559, Val loss 1.805\n",
      "Ep 1 (Step 006050): Train loss 1.189, Val loss 1.803\n",
      "Ep 1 (Step 006055): Train loss 1.191, Val loss 1.799\n",
      "Ep 1 (Step 006060): Train loss 1.143, Val loss 1.796\n",
      "Ep 1 (Step 006065): Train loss 1.515, Val loss 1.793\n",
      "Ep 1 (Step 006070): Train loss 1.139, Val loss 1.790\n",
      "Ep 1 (Step 006075): Train loss 1.369, Val loss 1.789\n",
      "Ep 1 (Step 006080): Train loss 1.221, Val loss 1.790\n",
      "Ep 1 (Step 006085): Train loss 1.319, Val loss 1.790\n",
      "Ep 1 (Step 006090): Train loss 1.112, Val loss 1.789\n",
      "Ep 1 (Step 006095): Train loss 1.363, Val loss 1.790\n",
      "Ep 1 (Step 006100): Train loss 1.280, Val loss 1.792\n",
      "Ep 1 (Step 006105): Train loss 1.365, Val loss 1.794\n",
      "Ep 1 (Step 006110): Train loss 1.379, Val loss 1.795\n",
      "Ep 1 (Step 006115): Train loss 1.146, Val loss 1.796\n",
      "Ep 1 (Step 006120): Train loss 1.267, Val loss 1.795\n",
      "Ep 1 (Step 006125): Train loss 1.252, Val loss 1.798\n",
      "Ep 1 (Step 006130): Train loss 1.259, Val loss 1.801\n",
      "Ep 1 (Step 006135): Train loss 1.009, Val loss 1.802\n",
      "Ep 1 (Step 006140): Train loss 1.576, Val loss 1.802\n",
      "Ep 1 (Step 006145): Train loss 1.237, Val loss 1.798\n",
      "Ep 1 (Step 006150): Train loss 1.315, Val loss 1.793\n",
      "Ep 1 (Step 006155): Train loss 1.311, Val loss 1.790\n",
      "Ep 1 (Step 006160): Train loss 1.564, Val loss 1.791\n",
      "Ep 1 (Step 006165): Train loss 1.137, Val loss 1.794\n",
      "Ep 1 (Step 006170): Train loss 1.224, Val loss 1.799\n",
      "Ep 1 (Step 006175): Train loss 1.425, Val loss 1.802\n",
      "Ep 1 (Step 006180): Train loss 1.337, Val loss 1.800\n",
      "Ep 1 (Step 006185): Train loss 1.226, Val loss 1.798\n",
      "Ep 1 (Step 006190): Train loss 1.255, Val loss 1.798\n",
      "Ep 1 (Step 006195): Train loss 1.300, Val loss 1.798\n",
      "Ep 1 (Step 006200): Train loss 1.186, Val loss 1.800\n",
      "Ep 1 (Step 006205): Train loss 1.443, Val loss 1.802\n",
      "Ep 1 (Step 006210): Train loss 1.130, Val loss 1.805\n",
      "Ep 1 (Step 006215): Train loss 1.252, Val loss 1.804\n",
      "Ep 1 (Step 006220): Train loss 1.207, Val loss 1.803\n",
      "Ep 1 (Step 006225): Train loss 1.202, Val loss 1.802\n",
      "Ep 1 (Step 006230): Train loss 1.327, Val loss 1.802\n",
      "Ep 1 (Step 006235): Train loss 1.179, Val loss 1.805\n",
      "Ep 1 (Step 006240): Train loss 1.521, Val loss 1.809\n",
      "Ep 1 (Step 006245): Train loss 1.165, Val loss 1.812\n",
      "Ep 1 (Step 006250): Train loss 1.114, Val loss 1.815\n",
      "Ep 1 (Step 006255): Train loss 1.399, Val loss 1.818\n",
      "Ep 1 (Step 006260): Train loss 1.320, Val loss 1.818\n",
      "Ep 1 (Step 006265): Train loss 1.380, Val loss 1.816\n",
      "Ep 1 (Step 006270): Train loss 0.966, Val loss 1.816\n",
      "Ep 1 (Step 006275): Train loss 1.682, Val loss 1.815\n",
      "Ep 1 (Step 006280): Train loss 1.211, Val loss 1.812\n",
      "Ep 1 (Step 006285): Train loss 1.290, Val loss 1.810\n",
      "Ep 1 (Step 006290): Train loss 1.324, Val loss 1.808\n",
      "Ep 1 (Step 006295): Train loss 1.272, Val loss 1.807\n",
      "Ep 1 (Step 006300): Train loss 1.458, Val loss 1.805\n",
      "Ep 1 (Step 006305): Train loss 1.100, Val loss 1.804\n",
      "Ep 1 (Step 006310): Train loss 1.452, Val loss 1.804\n",
      "Ep 1 (Step 006315): Train loss 1.456, Val loss 1.804\n",
      "Ep 1 (Step 006320): Train loss 1.279, Val loss 1.803\n",
      "Ep 1 (Step 006325): Train loss 1.457, Val loss 1.802\n",
      "Ep 1 (Step 006330): Train loss 1.403, Val loss 1.801\n",
      "Ep 1 (Step 006335): Train loss 1.178, Val loss 1.804\n",
      "Ep 1 (Step 006340): Train loss 1.196, Val loss 1.806\n",
      "Ep 1 (Step 006345): Train loss 1.276, Val loss 1.804\n",
      "Ep 1 (Step 006350): Train loss 1.288, Val loss 1.800\n",
      "Ep 1 (Step 006355): Train loss 1.576, Val loss 1.800\n",
      "Ep 1 (Step 006360): Train loss 1.359, Val loss 1.801\n",
      "Ep 1 (Step 006365): Train loss 0.906, Val loss 1.802\n",
      "Ep 1 (Step 006370): Train loss 1.046, Val loss 1.802\n",
      "Ep 1 (Step 006375): Train loss 1.588, Val loss 1.801\n",
      "Ep 1 (Step 006380): Train loss 1.491, Val loss 1.803\n",
      "Ep 1 (Step 006385): Train loss 1.498, Val loss 1.804\n",
      "Ep 1 (Step 006390): Train loss 1.122, Val loss 1.803\n",
      "Ep 1 (Step 006395): Train loss 1.172, Val loss 1.803\n",
      "Ep 1 (Step 006400): Train loss 1.106, Val loss 1.801\n",
      "Ep 1 (Step 006405): Train loss 1.491, Val loss 1.800\n",
      "Ep 1 (Step 006410): Train loss 1.247, Val loss 1.801\n",
      "Ep 1 (Step 006415): Train loss 1.184, Val loss 1.802\n",
      "Ep 1 (Step 006420): Train loss 1.267, Val loss 1.801\n",
      "Ep 1 (Step 006425): Train loss 1.283, Val loss 1.799\n",
      "Ep 1 (Step 006430): Train loss 1.259, Val loss 1.797\n",
      "Ep 1 (Step 006435): Train loss 1.448, Val loss 1.798\n",
      "Ep 1 (Step 006440): Train loss 1.260, Val loss 1.799\n",
      "Ep 1 (Step 006445): Train loss 1.095, Val loss 1.800\n",
      "Ep 1 (Step 006450): Train loss 1.410, Val loss 1.801\n",
      "Ep 1 (Step 006455): Train loss 1.210, Val loss 1.802\n",
      "Ep 1 (Step 006460): Train loss 1.223, Val loss 1.803\n",
      "Ep 1 (Step 006465): Train loss 1.319, Val loss 1.804\n",
      "Ep 1 (Step 006470): Train loss 1.408, Val loss 1.803\n",
      "Ep 1 (Step 006475): Train loss 1.545, Val loss 1.803\n",
      "Ep 1 (Step 006480): Train loss 1.285, Val loss 1.803\n",
      "Ep 1 (Step 006485): Train loss 1.242, Val loss 1.804\n",
      "Ep 1 (Step 006490): Train loss 1.381, Val loss 1.802\n",
      "Ep 1 (Step 006495): Train loss 1.474, Val loss 1.802\n",
      "Ep 1 (Step 006500): Train loss 1.195, Val loss 1.802\n",
      "Ep 1 (Step 006505): Train loss 1.340, Val loss 1.802\n",
      "Ep 1 (Step 006510): Train loss 1.567, Val loss 1.803\n",
      "Ep 1 (Step 006515): Train loss 1.336, Val loss 1.804\n",
      "Ep 1 (Step 006520): Train loss 1.353, Val loss 1.803\n",
      "Ep 1 (Step 006525): Train loss 1.616, Val loss 1.800\n",
      "Ep 1 (Step 006530): Train loss 1.173, Val loss 1.798\n",
      "Ep 1 (Step 006535): Train loss 1.246, Val loss 1.798\n",
      "Ep 1 (Step 006540): Train loss 1.397, Val loss 1.800\n",
      "Ep 1 (Step 006545): Train loss 1.211, Val loss 1.798\n",
      "Ep 1 (Step 006550): Train loss 1.655, Val loss 1.799\n",
      "Ep 1 (Step 006555): Train loss 1.238, Val loss 1.800\n",
      "Ep 1 (Step 006560): Train loss 1.287, Val loss 1.801\n",
      "Ep 1 (Step 006565): Train loss 1.312, Val loss 1.806\n",
      "Ep 1 (Step 006570): Train loss 1.165, Val loss 1.808\n",
      "Ep 1 (Step 006575): Train loss 1.561, Val loss 1.810\n",
      "Ep 1 (Step 006580): Train loss 1.276, Val loss 1.811\n",
      "Ep 1 (Step 006585): Train loss 1.333, Val loss 1.810\n",
      "Ep 1 (Step 006590): Train loss 1.368, Val loss 1.808\n",
      "Ep 1 (Step 006595): Train loss 1.235, Val loss 1.808\n",
      "Ep 1 (Step 006600): Train loss 1.360, Val loss 1.807\n",
      "Ep 1 (Step 006605): Train loss 1.345, Val loss 1.804\n",
      "Ep 1 (Step 006610): Train loss 1.650, Val loss 1.803\n",
      "Ep 1 (Step 006615): Train loss 1.243, Val loss 1.803\n",
      "Ep 1 (Step 006620): Train loss 1.194, Val loss 1.802\n",
      "Ep 1 (Step 006625): Train loss 0.999, Val loss 1.802\n",
      "Ep 1 (Step 006630): Train loss 1.530, Val loss 1.802\n",
      "Ep 1 (Step 006635): Train loss 1.291, Val loss 1.796\n",
      "Ep 1 (Step 006640): Train loss 1.397, Val loss 1.794\n",
      "Ep 1 (Step 006645): Train loss 1.028, Val loss 1.794\n",
      "Ep 1 (Step 006650): Train loss 1.438, Val loss 1.795\n",
      "Ep 1 (Step 006655): Train loss 1.524, Val loss 1.794\n",
      "Ep 1 (Step 006660): Train loss 1.207, Val loss 1.794\n",
      "Ep 1 (Step 006665): Train loss 1.482, Val loss 1.795\n",
      "Ep 1 (Step 006670): Train loss 1.042, Val loss 1.796\n",
      "Ep 1 (Step 006675): Train loss 1.263, Val loss 1.795\n",
      "Ep 1 (Step 006680): Train loss 1.162, Val loss 1.796\n",
      "Ep 1 (Step 006685): Train loss 1.316, Val loss 1.796\n",
      "Ep 1 (Step 006690): Train loss 1.136, Val loss 1.797\n",
      "Ep 1 (Step 006695): Train loss 1.206, Val loss 1.799\n",
      "Ep 1 (Step 006700): Train loss 0.997, Val loss 1.800\n",
      "Ep 1 (Step 006705): Train loss 1.292, Val loss 1.799\n",
      "Ep 1 (Step 006710): Train loss 1.532, Val loss 1.796\n",
      "Ep 1 (Step 006715): Train loss 1.487, Val loss 1.795\n",
      "Ep 1 (Step 006720): Train loss 1.491, Val loss 1.794\n",
      "Ep 1 (Step 006725): Train loss 1.187, Val loss 1.793\n",
      "Ep 1 (Step 006730): Train loss 1.285, Val loss 1.791\n",
      "Ep 1 (Step 006735): Train loss 1.216, Val loss 1.788\n",
      "Ep 1 (Step 006740): Train loss 1.437, Val loss 1.788\n",
      "Ep 1 (Step 006745): Train loss 1.155, Val loss 1.790\n",
      "Ep 1 (Step 006750): Train loss 1.504, Val loss 1.791\n",
      "Ep 1 (Step 006755): Train loss 1.100, Val loss 1.790\n",
      "Ep 1 (Step 006760): Train loss 1.151, Val loss 1.789\n",
      "Ep 1 (Step 006765): Train loss 1.468, Val loss 1.789\n",
      "Ep 1 (Step 006770): Train loss 1.017, Val loss 1.787\n",
      "Ep 1 (Step 006775): Train loss 1.357, Val loss 1.786\n",
      "Ep 1 (Step 006780): Train loss 1.366, Val loss 1.786\n",
      "Ep 1 (Step 006785): Train loss 1.233, Val loss 1.786\n",
      "Ep 1 (Step 006790): Train loss 1.426, Val loss 1.786\n",
      "Ep 1 (Step 006795): Train loss 1.101, Val loss 1.786\n",
      "Ep 1 (Step 006800): Train loss 1.339, Val loss 1.787\n",
      "Ep 1 (Step 006805): Train loss 1.079, Val loss 1.791\n",
      "Ep 1 (Step 006810): Train loss 1.415, Val loss 1.794\n",
      "Ep 1 (Step 006815): Train loss 1.513, Val loss 1.798\n",
      "Ep 1 (Step 006820): Train loss 1.169, Val loss 1.800\n",
      "Ep 1 (Step 006825): Train loss 1.242, Val loss 1.800\n",
      "Ep 1 (Step 006830): Train loss 1.516, Val loss 1.800\n",
      "Ep 1 (Step 006835): Train loss 1.081, Val loss 1.798\n",
      "Ep 1 (Step 006840): Train loss 1.060, Val loss 1.796\n",
      "Ep 1 (Step 006845): Train loss 1.177, Val loss 1.795\n",
      "Ep 1 (Step 006850): Train loss 1.168, Val loss 1.794\n",
      "Ep 1 (Step 006855): Train loss 1.494, Val loss 1.793\n",
      "Ep 1 (Step 006860): Train loss 1.284, Val loss 1.797\n",
      "Ep 1 (Step 006865): Train loss 1.298, Val loss 1.797\n",
      "Ep 1 (Step 006870): Train loss 1.724, Val loss 1.792\n",
      "Ep 1 (Step 006875): Train loss 1.153, Val loss 1.787\n",
      "Ep 1 (Step 006880): Train loss 1.050, Val loss 1.785\n",
      "Ep 1 (Step 006885): Train loss 1.165, Val loss 1.784\n",
      "Ep 1 (Step 006890): Train loss 1.273, Val loss 1.782\n",
      "Ep 1 (Step 006895): Train loss 1.285, Val loss 1.780\n",
      "Ep 1 (Step 006900): Train loss 1.350, Val loss 1.779\n",
      "Ep 1 (Step 006905): Train loss 0.947, Val loss 1.780\n",
      "Ep 1 (Step 006910): Train loss 1.059, Val loss 1.782\n",
      "Ep 1 (Step 006915): Train loss 1.452, Val loss 1.783\n",
      "Ep 1 (Step 006920): Train loss 1.643, Val loss 1.784\n",
      "Ep 1 (Step 006925): Train loss 1.193, Val loss 1.783\n",
      "Ep 1 (Step 006930): Train loss 1.115, Val loss 1.783\n",
      "Ep 1 (Step 006935): Train loss 1.409, Val loss 1.784\n",
      "Ep 1 (Step 006940): Train loss 1.530, Val loss 1.784\n",
      "Ep 1 (Step 006945): Train loss 1.471, Val loss 1.787\n",
      "Ep 1 (Step 006950): Train loss 1.297, Val loss 1.791\n",
      "Ep 1 (Step 006955): Train loss 0.974, Val loss 1.794\n",
      "Ep 1 (Step 006960): Train loss 1.204, Val loss 1.797\n",
      "Ep 1 (Step 006965): Train loss 1.396, Val loss 1.794\n",
      "Ep 1 (Step 006970): Train loss 1.176, Val loss 1.788\n",
      "Ep 1 (Step 006975): Train loss 0.901, Val loss 1.784\n",
      "Ep 1 (Step 006980): Train loss 1.284, Val loss 1.783\n",
      "Ep 1 (Step 006985): Train loss 1.239, Val loss 1.782\n",
      "Ep 1 (Step 006990): Train loss 1.558, Val loss 1.781\n",
      "Ep 1 (Step 006995): Train loss 1.166, Val loss 1.780\n",
      "Ep 1 (Step 007000): Train loss 1.163, Val loss 1.781\n",
      "Ep 1 (Step 007005): Train loss 1.306, Val loss 1.782\n",
      "Ep 1 (Step 007010): Train loss 1.772, Val loss 1.784\n",
      "Ep 1 (Step 007015): Train loss 1.503, Val loss 1.783\n",
      "Ep 1 (Step 007020): Train loss 1.578, Val loss 1.782\n",
      "Ep 1 (Step 007025): Train loss 1.246, Val loss 1.783\n",
      "Ep 1 (Step 007030): Train loss 1.304, Val loss 1.786\n",
      "Ep 1 (Step 007035): Train loss 1.244, Val loss 1.788\n",
      "Ep 1 (Step 007040): Train loss 1.252, Val loss 1.789\n",
      "Ep 1 (Step 007045): Train loss 1.339, Val loss 1.789\n",
      "Ep 1 (Step 007050): Train loss 1.776, Val loss 1.786\n",
      "Ep 1 (Step 007055): Train loss 1.161, Val loss 1.786\n",
      "Ep 1 (Step 007060): Train loss 1.218, Val loss 1.786\n",
      "Ep 1 (Step 007065): Train loss 1.265, Val loss 1.790\n",
      "Ep 1 (Step 007070): Train loss 1.126, Val loss 1.795\n",
      "Ep 1 (Step 007075): Train loss 1.382, Val loss 1.798\n",
      "Ep 1 (Step 007080): Train loss 0.995, Val loss 1.798\n",
      "Ep 1 (Step 007085): Train loss 1.280, Val loss 1.794\n",
      "Ep 1 (Step 007090): Train loss 1.299, Val loss 1.790\n",
      "Ep 1 (Step 007095): Train loss 1.172, Val loss 1.790\n",
      "Ep 1 (Step 007100): Train loss 1.255, Val loss 1.790\n",
      "Ep 1 (Step 007105): Train loss 1.219, Val loss 1.787\n",
      "Ep 1 (Step 007110): Train loss 1.493, Val loss 1.784\n",
      "Ep 1 (Step 007115): Train loss 1.541, Val loss 1.782\n",
      "Ep 1 (Step 007120): Train loss 1.284, Val loss 1.781\n",
      "Ep 1 (Step 007125): Train loss 1.429, Val loss 1.782\n",
      "Ep 1 (Step 007130): Train loss 1.197, Val loss 1.783\n",
      "Ep 1 (Step 007135): Train loss 1.154, Val loss 1.784\n",
      "Ep 1 (Step 007140): Train loss 1.311, Val loss 1.783\n",
      "Ep 1 (Step 007145): Train loss 1.211, Val loss 1.782\n",
      "Ep 1 (Step 007150): Train loss 1.390, Val loss 1.780\n",
      "Ep 1 (Step 007155): Train loss 1.506, Val loss 1.779\n",
      "Ep 1 (Step 007160): Train loss 1.210, Val loss 1.777\n",
      "Ep 1 (Step 007165): Train loss 1.413, Val loss 1.778\n",
      "Ep 1 (Step 007170): Train loss 1.272, Val loss 1.777\n",
      "Ep 1 (Step 007175): Train loss 1.215, Val loss 1.778\n",
      "Ep 1 (Step 007180): Train loss 1.201, Val loss 1.779\n",
      "Ep 1 (Step 007185): Train loss 1.205, Val loss 1.782\n",
      "Ep 1 (Step 007190): Train loss 1.209, Val loss 1.784\n",
      "Ep 1 (Step 007195): Train loss 1.362, Val loss 1.784\n",
      "Ep 1 (Step 007200): Train loss 1.105, Val loss 1.785\n",
      "Ep 1 (Step 007205): Train loss 1.141, Val loss 1.787\n",
      "Ep 1 (Step 007210): Train loss 1.325, Val loss 1.787\n",
      "Ep 1 (Step 007215): Train loss 1.216, Val loss 1.788\n",
      "Ep 1 (Step 007220): Train loss 1.217, Val loss 1.786\n",
      "Ep 1 (Step 007225): Train loss 1.023, Val loss 1.782\n",
      "Ep 1 (Step 007230): Train loss 1.429, Val loss 1.780\n",
      "Ep 1 (Step 007235): Train loss 1.612, Val loss 1.780\n",
      "Ep 1 (Step 007240): Train loss 1.342, Val loss 1.780\n",
      "Ep 1 (Step 007245): Train loss 1.381, Val loss 1.780\n",
      "Ep 1 (Step 007250): Train loss 1.361, Val loss 1.780\n",
      "Ep 1 (Step 007255): Train loss 1.192, Val loss 1.778\n",
      "Ep 1 (Step 007260): Train loss 1.305, Val loss 1.778\n",
      "Ep 1 (Step 007265): Train loss 1.293, Val loss 1.778\n",
      "Ep 1 (Step 007270): Train loss 1.574, Val loss 1.778\n",
      "Ep 1 (Step 007275): Train loss 1.189, Val loss 1.777\n",
      "Ep 1 (Step 007280): Train loss 1.609, Val loss 1.775\n",
      "Ep 1 (Step 007285): Train loss 1.298, Val loss 1.774\n",
      "Ep 1 (Step 007290): Train loss 1.459, Val loss 1.774\n",
      "Ep 1 (Step 007295): Train loss 1.130, Val loss 1.775\n",
      "Ep 1 (Step 007300): Train loss 1.372, Val loss 1.778\n",
      "Ep 1 (Step 007305): Train loss 1.207, Val loss 1.779\n",
      "Ep 1 (Step 007310): Train loss 1.092, Val loss 1.781\n",
      "Ep 1 (Step 007315): Train loss 1.420, Val loss 1.783\n",
      "Ep 1 (Step 007320): Train loss 1.005, Val loss 1.786\n",
      "Ep 1 (Step 007325): Train loss 1.184, Val loss 1.787\n",
      "Ep 1 (Step 007330): Train loss 1.260, Val loss 1.787\n",
      "Ep 1 (Step 007335): Train loss 1.391, Val loss 1.786\n",
      "Ep 1 (Step 007340): Train loss 1.273, Val loss 1.787\n",
      "Ep 1 (Step 007345): Train loss 1.479, Val loss 1.786\n",
      "Ep 1 (Step 007350): Train loss 1.098, Val loss 1.785\n",
      "Ep 1 (Step 007355): Train loss 1.367, Val loss 1.783\n",
      "Ep 1 (Step 007360): Train loss 1.169, Val loss 1.783\n",
      "Ep 1 (Step 007365): Train loss 1.016, Val loss 1.783\n",
      "Ep 1 (Step 007370): Train loss 1.423, Val loss 1.783\n",
      "Ep 1 (Step 007375): Train loss 1.514, Val loss 1.783\n",
      "Ep 1 (Step 007380): Train loss 1.228, Val loss 1.780\n",
      "Ep 1 (Step 007385): Train loss 1.252, Val loss 1.779\n",
      "Ep 1 (Step 007390): Train loss 1.528, Val loss 1.778\n",
      "Ep 1 (Step 007395): Train loss 1.045, Val loss 1.777\n",
      "Ep 1 (Step 007400): Train loss 1.076, Val loss 1.779\n",
      "Ep 1 (Step 007405): Train loss 1.234, Val loss 1.782\n",
      "Ep 1 (Step 007410): Train loss 1.351, Val loss 1.787\n",
      "Ep 1 (Step 007415): Train loss 1.388, Val loss 1.792\n",
      "Ep 1 (Step 007420): Train loss 1.377, Val loss 1.800\n",
      "Ep 1 (Step 007425): Train loss 1.196, Val loss 1.806\n",
      "Ep 1 (Step 007430): Train loss 1.235, Val loss 1.811\n",
      "Ep 1 (Step 007435): Train loss 1.117, Val loss 1.809\n",
      "Ep 1 (Step 007440): Train loss 1.085, Val loss 1.808\n",
      "Ep 1 (Step 007445): Train loss 1.314, Val loss 1.807\n",
      "Ep 1 (Step 007450): Train loss 0.936, Val loss 1.803\n",
      "Ep 1 (Step 007455): Train loss 1.241, Val loss 1.802\n",
      "Ep 1 (Step 007460): Train loss 1.407, Val loss 1.801\n",
      "Ep 1 (Step 007465): Train loss 1.582, Val loss 1.799\n",
      "Ep 1 (Step 007470): Train loss 1.043, Val loss 1.793\n",
      "Ep 1 (Step 007475): Train loss 1.466, Val loss 1.787\n",
      "Ep 1 (Step 007480): Train loss 1.269, Val loss 1.782\n",
      "Ep 1 (Step 007485): Train loss 1.376, Val loss 1.778\n",
      "Ep 1 (Step 007490): Train loss 1.183, Val loss 1.777\n",
      "Ep 1 (Step 007495): Train loss 1.580, Val loss 1.779\n",
      "Ep 1 (Step 007500): Train loss 1.064, Val loss 1.783\n",
      "Ep 1 (Step 007505): Train loss 1.104, Val loss 1.786\n",
      "Ep 1 (Step 007510): Train loss 1.156, Val loss 1.787\n",
      "Ep 1 (Step 007515): Train loss 1.133, Val loss 1.786\n",
      "Ep 1 (Step 007520): Train loss 1.442, Val loss 1.783\n",
      "Ep 1 (Step 007525): Train loss 1.389, Val loss 1.782\n",
      "Ep 1 (Step 007530): Train loss 1.532, Val loss 1.781\n",
      "Ep 1 (Step 007535): Train loss 1.189, Val loss 1.780\n",
      "Ep 1 (Step 007540): Train loss 1.369, Val loss 1.782\n",
      "Ep 1 (Step 007545): Train loss 1.281, Val loss 1.783\n",
      "Ep 1 (Step 007550): Train loss 1.001, Val loss 1.781\n",
      "Ep 1 (Step 007555): Train loss 1.091, Val loss 1.783\n",
      "Ep 1 (Step 007560): Train loss 1.333, Val loss 1.784\n",
      "Ep 1 (Step 007565): Train loss 1.023, Val loss 1.781\n",
      "Ep 1 (Step 007570): Train loss 1.178, Val loss 1.780\n",
      "Ep 1 (Step 007575): Train loss 1.089, Val loss 1.780\n",
      "Ep 1 (Step 007580): Train loss 1.497, Val loss 1.781\n",
      "Ep 1 (Step 007585): Train loss 1.510, Val loss 1.782\n",
      "Ep 1 (Step 007590): Train loss 1.039, Val loss 1.785\n",
      "Ep 1 (Step 007595): Train loss 1.392, Val loss 1.787\n",
      "Ep 1 (Step 007600): Train loss 1.091, Val loss 1.789\n",
      "Ep 1 (Step 007605): Train loss 1.212, Val loss 1.788\n",
      "Ep 1 (Step 007610): Train loss 1.112, Val loss 1.789\n",
      "Ep 1 (Step 007615): Train loss 1.452, Val loss 1.789\n",
      "Ep 1 (Step 007620): Train loss 1.409, Val loss 1.788\n",
      "Ep 1 (Step 007625): Train loss 1.045, Val loss 1.788\n",
      "Ep 1 (Step 007630): Train loss 1.587, Val loss 1.788\n",
      "Ep 1 (Step 007635): Train loss 1.444, Val loss 1.789\n",
      "Ep 1 (Step 007640): Train loss 1.311, Val loss 1.793\n",
      "Ep 1 (Step 007645): Train loss 1.387, Val loss 1.797\n",
      "Ep 1 (Step 007650): Train loss 1.445, Val loss 1.797\n",
      "Ep 1 (Step 007655): Train loss 1.114, Val loss 1.798\n",
      "Ep 1 (Step 007660): Train loss 1.405, Val loss 1.796\n",
      "Ep 1 (Step 007665): Train loss 1.278, Val loss 1.794\n",
      "Ep 1 (Step 007670): Train loss 1.575, Val loss 1.793\n",
      "Ep 1 (Step 007675): Train loss 1.198, Val loss 1.795\n",
      "Ep 1 (Step 007680): Train loss 1.112, Val loss 1.798\n",
      "Ep 1 (Step 007685): Train loss 1.400, Val loss 1.801\n",
      "Ep 1 (Step 007690): Train loss 1.433, Val loss 1.804\n",
      "Ep 1 (Step 007695): Train loss 1.329, Val loss 1.804\n",
      "Ep 1 (Step 007700): Train loss 0.994, Val loss 1.801\n",
      "Ep 1 (Step 007705): Train loss 1.230, Val loss 1.796\n",
      "Ep 1 (Step 007710): Train loss 1.481, Val loss 1.793\n",
      "Ep 1 (Step 007715): Train loss 1.306, Val loss 1.793\n",
      "Ep 1 (Step 007720): Train loss 1.072, Val loss 1.793\n",
      "Ep 1 (Step 007725): Train loss 1.313, Val loss 1.793\n",
      "Ep 1 (Step 007730): Train loss 1.190, Val loss 1.792\n",
      "Ep 1 (Step 007735): Train loss 1.275, Val loss 1.792\n",
      "Ep 1 (Step 007740): Train loss 0.958, Val loss 1.793\n",
      "Ep 1 (Step 007745): Train loss 1.154, Val loss 1.794\n",
      "Ep 1 (Step 007750): Train loss 1.361, Val loss 1.795\n",
      "Ep 1 (Step 007755): Train loss 1.213, Val loss 1.795\n",
      "Ep 1 (Step 007760): Train loss 1.051, Val loss 1.793\n",
      "Ep 1 (Step 007765): Train loss 1.315, Val loss 1.793\n",
      "Ep 1 (Step 007770): Train loss 1.220, Val loss 1.794\n",
      "Ep 1 (Step 007775): Train loss 1.249, Val loss 1.792\n",
      "Ep 1 (Step 007780): Train loss 0.981, Val loss 1.791\n",
      "Ep 1 (Step 007785): Train loss 1.250, Val loss 1.786\n",
      "Ep 1 (Step 007790): Train loss 1.227, Val loss 1.783\n",
      "Ep 1 (Step 007795): Train loss 1.401, Val loss 1.783\n",
      "Ep 1 (Step 007800): Train loss 1.118, Val loss 1.783\n",
      "Ep 1 (Step 007805): Train loss 1.104, Val loss 1.783\n",
      "Ep 1 (Step 007810): Train loss 1.011, Val loss 1.783\n",
      "Ep 1 (Step 007815): Train loss 1.025, Val loss 1.780\n",
      "Ep 1 (Step 007820): Train loss 1.326, Val loss 1.781\n",
      "Ep 1 (Step 007825): Train loss 1.357, Val loss 1.781\n",
      "Ep 1 (Step 007830): Train loss 1.046, Val loss 1.780\n",
      "Ep 1 (Step 007835): Train loss 1.104, Val loss 1.779\n",
      "Ep 1 (Step 007840): Train loss 1.510, Val loss 1.780\n",
      "Ep 1 (Step 007845): Train loss 1.293, Val loss 1.782\n",
      "Ep 1 (Step 007850): Train loss 1.339, Val loss 1.784\n",
      "Ep 1 (Step 007855): Train loss 1.130, Val loss 1.784\n",
      "Ep 1 (Step 007860): Train loss 1.162, Val loss 1.783\n",
      "Ep 1 (Step 007865): Train loss 1.359, Val loss 1.785\n",
      "Ep 1 (Step 007870): Train loss 1.321, Val loss 1.786\n",
      "Ep 1 (Step 007875): Train loss 1.025, Val loss 1.787\n",
      "Ep 1 (Step 007880): Train loss 1.319, Val loss 1.785\n",
      "Ep 1 (Step 007885): Train loss 1.408, Val loss 1.782\n",
      "Ep 1 (Step 007890): Train loss 1.163, Val loss 1.782\n",
      "Ep 1 (Step 007895): Train loss 1.514, Val loss 1.784\n",
      "Ep 1 (Step 007900): Train loss 1.381, Val loss 1.786\n",
      "Ep 1 (Step 007905): Train loss 1.262, Val loss 1.785\n",
      "Ep 1 (Step 007910): Train loss 1.451, Val loss 1.782\n",
      "Ep 1 (Step 007915): Train loss 1.284, Val loss 1.781\n",
      "Ep 1 (Step 007920): Train loss 0.883, Val loss 1.781\n",
      "Ep 1 (Step 007925): Train loss 1.699, Val loss 1.780\n",
      "Ep 1 (Step 007930): Train loss 1.173, Val loss 1.781\n",
      "Ep 1 (Step 007935): Train loss 1.256, Val loss 1.783\n",
      "Ep 1 (Step 007940): Train loss 1.592, Val loss 1.783\n",
      "Ep 1 (Step 007945): Train loss 1.282, Val loss 1.784\n",
      "Ep 1 (Step 007950): Train loss 1.354, Val loss 1.786\n",
      "Ep 1 (Step 007955): Train loss 1.053, Val loss 1.787\n",
      "Ep 1 (Step 007960): Train loss 1.283, Val loss 1.786\n",
      "Ep 1 (Step 007965): Train loss 1.275, Val loss 1.787\n",
      "Ep 1 (Step 007970): Train loss 1.406, Val loss 1.790\n",
      "Ep 1 (Step 007975): Train loss 1.205, Val loss 1.792\n",
      "Ep 1 (Step 007980): Train loss 0.899, Val loss 1.792\n",
      "Ep 1 (Step 007985): Train loss 1.355, Val loss 1.788\n",
      "Ep 1 (Step 007990): Train loss 1.390, Val loss 1.787\n",
      "Ep 1 (Step 007995): Train loss 1.285, Val loss 1.786\n",
      "Ep 1 (Step 008000): Train loss 1.009, Val loss 1.786\n",
      "Ep 1 (Step 008005): Train loss 1.125, Val loss 1.783\n",
      "Ep 1 (Step 008010): Train loss 0.983, Val loss 1.782\n",
      "Ep 1 (Step 008015): Train loss 1.410, Val loss 1.779\n",
      "Ep 1 (Step 008020): Train loss 1.655, Val loss 1.777\n",
      "Ep 1 (Step 008025): Train loss 1.387, Val loss 1.773\n",
      "Ep 1 (Step 008030): Train loss 1.374, Val loss 1.770\n",
      "Ep 1 (Step 008035): Train loss 1.190, Val loss 1.770\n",
      "Ep 1 (Step 008040): Train loss 1.439, Val loss 1.771\n",
      "Ep 1 (Step 008045): Train loss 1.115, Val loss 1.773\n",
      "Ep 1 (Step 008050): Train loss 1.459, Val loss 1.773\n",
      "Ep 1 (Step 008055): Train loss 1.321, Val loss 1.774\n",
      "Ep 1 (Step 008060): Train loss 1.111, Val loss 1.775\n",
      "Ep 1 (Step 008065): Train loss 1.209, Val loss 1.775\n",
      "Ep 1 (Step 008070): Train loss 1.191, Val loss 1.775\n",
      "Ep 1 (Step 008075): Train loss 0.829, Val loss 1.777\n",
      "Ep 1 (Step 008080): Train loss 1.489, Val loss 1.780\n",
      "Ep 1 (Step 008085): Train loss 1.291, Val loss 1.782\n",
      "Ep 1 (Step 008090): Train loss 1.124, Val loss 1.784\n",
      "Ep 1 (Step 008095): Train loss 1.428, Val loss 1.784\n",
      "Ep 1 (Step 008100): Train loss 1.176, Val loss 1.783\n",
      "Ep 1 (Step 008105): Train loss 1.411, Val loss 1.783\n",
      "Ep 1 (Step 008110): Train loss 1.571, Val loss 1.782\n",
      "Ep 1 (Step 008115): Train loss 0.874, Val loss 1.783\n",
      "Ep 1 (Step 008120): Train loss 1.292, Val loss 1.783\n",
      "Ep 1 (Step 008125): Train loss 1.457, Val loss 1.781\n",
      "Ep 1 (Step 008130): Train loss 1.041, Val loss 1.782\n",
      "Ep 1 (Step 008135): Train loss 1.691, Val loss 1.785\n",
      "Ep 1 (Step 008140): Train loss 1.349, Val loss 1.785\n",
      "Ep 1 (Step 008145): Train loss 1.166, Val loss 1.785\n",
      "Ep 1 (Step 008150): Train loss 1.368, Val loss 1.788\n",
      "Ep 1 (Step 008155): Train loss 1.304, Val loss 1.788\n",
      "Ep 1 (Step 008160): Train loss 1.063, Val loss 1.785\n",
      "Ep 1 (Step 008165): Train loss 0.982, Val loss 1.784\n",
      "Ep 1 (Step 008170): Train loss 1.283, Val loss 1.782\n",
      "Ep 1 (Step 008175): Train loss 1.427, Val loss 1.782\n",
      "Ep 1 (Step 008180): Train loss 1.357, Val loss 1.782\n",
      "Ep 1 (Step 008185): Train loss 1.568, Val loss 1.783\n",
      "Ep 1 (Step 008190): Train loss 1.542, Val loss 1.784\n",
      "Ep 1 (Step 008195): Train loss 1.424, Val loss 1.786\n",
      "Ep 1 (Step 008200): Train loss 1.629, Val loss 1.786\n",
      "Ep 1 (Step 008205): Train loss 1.153, Val loss 1.786\n",
      "Ep 1 (Step 008210): Train loss 1.466, Val loss 1.783\n",
      "Ep 1 (Step 008215): Train loss 1.379, Val loss 1.781\n",
      "Ep 1 (Step 008220): Train loss 1.116, Val loss 1.781\n",
      "Ep 1 (Step 008225): Train loss 1.321, Val loss 1.782\n",
      "Ep 1 (Step 008230): Train loss 1.344, Val loss 1.783\n",
      "Ep 1 (Step 008235): Train loss 1.464, Val loss 1.785\n",
      "Ep 1 (Step 008240): Train loss 1.381, Val loss 1.785\n",
      "Ep 1 (Step 008245): Train loss 1.614, Val loss 1.785\n",
      "Ep 1 (Step 008250): Train loss 1.437, Val loss 1.784\n",
      "Ep 1 (Step 008255): Train loss 1.303, Val loss 1.783\n",
      "Ep 1 (Step 008260): Train loss 1.213, Val loss 1.782\n",
      "Ep 1 (Step 008265): Train loss 1.056, Val loss 1.782\n",
      "Ep 1 (Step 008270): Train loss 1.104, Val loss 1.783\n",
      "Ep 1 (Step 008275): Train loss 1.280, Val loss 1.785\n",
      "Ep 1 (Step 008280): Train loss 1.026, Val loss 1.785\n",
      "Ep 1 (Step 008285): Train loss 1.204, Val loss 1.784\n",
      "Ep 1 (Step 008290): Train loss 1.272, Val loss 1.784\n",
      "Ep 1 (Step 008295): Train loss 1.152, Val loss 1.784\n",
      "Ep 1 (Step 008300): Train loss 1.410, Val loss 1.785\n",
      "Ep 1 (Step 008305): Train loss 1.262, Val loss 1.789\n",
      "Ep 1 (Step 008310): Train loss 1.093, Val loss 1.791\n",
      "Ep 1 (Step 008315): Train loss 1.286, Val loss 1.792\n",
      "Ep 1 (Step 008320): Train loss 1.177, Val loss 1.791\n",
      "Ep 1 (Step 008325): Train loss 1.123, Val loss 1.787\n",
      "Ep 1 (Step 008330): Train loss 1.355, Val loss 1.784\n",
      "Ep 1 (Step 008335): Train loss 1.432, Val loss 1.783\n",
      "Ep 1 (Step 008340): Train loss 1.265, Val loss 1.783\n",
      "Ep 1 (Step 008345): Train loss 1.103, Val loss 1.782\n",
      "Ep 1 (Step 008350): Train loss 1.476, Val loss 1.781\n",
      "Ep 1 (Step 008355): Train loss 1.375, Val loss 1.781\n",
      "Ep 1 (Step 008360): Train loss 1.298, Val loss 1.780\n",
      "Ep 1 (Step 008365): Train loss 1.348, Val loss 1.782\n",
      "Ep 1 (Step 008370): Train loss 1.139, Val loss 1.787\n",
      "Ep 1 (Step 008375): Train loss 1.230, Val loss 1.791\n",
      "Ep 1 (Step 008380): Train loss 0.943, Val loss 1.793\n",
      "Ep 1 (Step 008385): Train loss 1.067, Val loss 1.792\n",
      "Ep 1 (Step 008390): Train loss 1.354, Val loss 1.792\n",
      "Ep 1 (Step 008395): Train loss 1.046, Val loss 1.792\n",
      "Ep 1 (Step 008400): Train loss 1.312, Val loss 1.789\n",
      "Ep 1 (Step 008405): Train loss 1.117, Val loss 1.786\n",
      "Ep 1 (Step 008410): Train loss 1.263, Val loss 1.786\n",
      "Ep 1 (Step 008415): Train loss 1.255, Val loss 1.786\n",
      "Ep 1 (Step 008420): Train loss 1.114, Val loss 1.787\n",
      "Ep 1 (Step 008425): Train loss 1.561, Val loss 1.787\n",
      "Ep 1 (Step 008430): Train loss 1.198, Val loss 1.787\n",
      "Ep 1 (Step 008435): Train loss 1.182, Val loss 1.787\n",
      "Ep 1 (Step 008440): Train loss 1.197, Val loss 1.786\n",
      "Ep 1 (Step 008445): Train loss 1.553, Val loss 1.784\n",
      "Ep 1 (Step 008450): Train loss 1.003, Val loss 1.783\n",
      "Ep 1 (Step 008455): Train loss 1.230, Val loss 1.783\n",
      "Ep 1 (Step 008460): Train loss 1.641, Val loss 1.783\n",
      "Ep 1 (Step 008465): Train loss 1.241, Val loss 1.785\n",
      "Ep 1 (Step 008470): Train loss 1.092, Val loss 1.786\n",
      "Ep 1 (Step 008475): Train loss 1.291, Val loss 1.786\n",
      "Ep 1 (Step 008480): Train loss 1.362, Val loss 1.787\n",
      "Ep 1 (Step 008485): Train loss 0.953, Val loss 1.787\n",
      "Ep 1 (Step 008490): Train loss 1.438, Val loss 1.785\n",
      "Ep 1 (Step 008495): Train loss 1.336, Val loss 1.782\n",
      "Ep 1 (Step 008500): Train loss 1.247, Val loss 1.781\n",
      "Ep 1 (Step 008505): Train loss 1.178, Val loss 1.781\n",
      "Ep 1 (Step 008510): Train loss 1.549, Val loss 1.783\n",
      "Ep 1 (Step 008515): Train loss 0.948, Val loss 1.781\n",
      "Ep 1 (Step 008520): Train loss 1.276, Val loss 1.778\n",
      "Ep 1 (Step 008525): Train loss 0.929, Val loss 1.774\n",
      "Ep 1 (Step 008530): Train loss 1.641, Val loss 1.771\n",
      "Ep 1 (Step 008535): Train loss 1.169, Val loss 1.770\n",
      "Ep 1 (Step 008540): Train loss 1.323, Val loss 1.770\n",
      "Ep 1 (Step 008545): Train loss 1.112, Val loss 1.770\n",
      "Ep 1 (Step 008550): Train loss 1.205, Val loss 1.771\n",
      "Ep 1 (Step 008555): Train loss 1.356, Val loss 1.772\n",
      "Ep 1 (Step 008560): Train loss 1.136, Val loss 1.772\n",
      "Ep 1 (Step 008565): Train loss 1.185, Val loss 1.773\n",
      "Ep 1 (Step 008570): Train loss 1.291, Val loss 1.772\n",
      "Ep 1 (Step 008575): Train loss 1.605, Val loss 1.772\n",
      "Ep 1 (Step 008580): Train loss 1.248, Val loss 1.776\n",
      "Ep 1 (Step 008585): Train loss 1.195, Val loss 1.779\n",
      "Ep 1 (Step 008590): Train loss 1.334, Val loss 1.781\n",
      "Ep 1 (Step 008595): Train loss 1.133, Val loss 1.781\n",
      "Ep 1 (Step 008600): Train loss 0.995, Val loss 1.782\n",
      "Ep 1 (Step 008605): Train loss 1.468, Val loss 1.781\n",
      "Ep 1 (Step 008610): Train loss 0.980, Val loss 1.778\n",
      "Ep 1 (Step 008615): Train loss 1.257, Val loss 1.775\n",
      "Ep 1 (Step 008620): Train loss 1.203, Val loss 1.772\n",
      "Ep 1 (Step 008625): Train loss 1.337, Val loss 1.771\n",
      "Ep 1 (Step 008630): Train loss 1.203, Val loss 1.771\n",
      "Ep 1 (Step 008635): Train loss 1.150, Val loss 1.772\n",
      "Ep 1 (Step 008640): Train loss 1.148, Val loss 1.773\n",
      "Ep 1 (Step 008645): Train loss 1.228, Val loss 1.775\n",
      "Ep 1 (Step 008650): Train loss 1.217, Val loss 1.777\n",
      "Ep 1 (Step 008655): Train loss 1.546, Val loss 1.778\n",
      "Ep 1 (Step 008660): Train loss 1.353, Val loss 1.777\n",
      "Ep 1 (Step 008665): Train loss 1.643, Val loss 1.778\n",
      "Ep 1 (Step 008670): Train loss 1.394, Val loss 1.780\n",
      "Ep 1 (Step 008675): Train loss 1.194, Val loss 1.783\n",
      "Ep 1 (Step 008680): Train loss 1.029, Val loss 1.789\n",
      "Ep 1 (Step 008685): Train loss 1.197, Val loss 1.791\n",
      "Ep 1 (Step 008690): Train loss 0.935, Val loss 1.791\n",
      "Ep 1 (Step 008695): Train loss 0.937, Val loss 1.790\n",
      "Ep 1 (Step 008700): Train loss 1.189, Val loss 1.791\n",
      "Ep 1 (Step 008705): Train loss 1.366, Val loss 1.793\n",
      "Ep 1 (Step 008710): Train loss 1.000, Val loss 1.792\n",
      "Ep 1 (Step 008715): Train loss 1.251, Val loss 1.792\n",
      "Ep 1 (Step 008720): Train loss 1.245, Val loss 1.793\n",
      "Ep 1 (Step 008725): Train loss 1.339, Val loss 1.792\n",
      "Ep 1 (Step 008730): Train loss 1.120, Val loss 1.793\n",
      "Ep 1 (Step 008735): Train loss 1.175, Val loss 1.794\n",
      "Ep 1 (Step 008740): Train loss 1.295, Val loss 1.792\n",
      "Ep 1 (Step 008745): Train loss 1.228, Val loss 1.792\n",
      "Ep 1 (Step 008750): Train loss 0.993, Val loss 1.790\n",
      "Ep 1 (Step 008755): Train loss 1.285, Val loss 1.787\n",
      "Ep 1 (Step 008760): Train loss 1.099, Val loss 1.787\n",
      "Ep 1 (Step 008765): Train loss 1.273, Val loss 1.787\n",
      "Ep 1 (Step 008770): Train loss 1.567, Val loss 1.788\n",
      "Ep 1 (Step 008775): Train loss 1.450, Val loss 1.786\n",
      "Ep 1 (Step 008780): Train loss 1.337, Val loss 1.784\n",
      "Ep 1 (Step 008785): Train loss 0.985, Val loss 1.783\n",
      "Ep 1 (Step 008790): Train loss 1.204, Val loss 1.781\n",
      "Ep 1 (Step 008795): Train loss 1.255, Val loss 1.779\n",
      "Ep 1 (Step 008800): Train loss 1.014, Val loss 1.779\n",
      "Ep 1 (Step 008805): Train loss 1.139, Val loss 1.780\n",
      "Ep 1 (Step 008810): Train loss 1.367, Val loss 1.782\n",
      "Ep 1 (Step 008815): Train loss 1.335, Val loss 1.781\n",
      "Ep 1 (Step 008820): Train loss 1.217, Val loss 1.781\n",
      "Ep 1 (Step 008825): Train loss 1.135, Val loss 1.781\n",
      "Ep 1 (Step 008830): Train loss 1.449, Val loss 1.781\n",
      "Ep 1 (Step 008835): Train loss 1.106, Val loss 1.781\n",
      "Ep 1 (Step 008840): Train loss 1.251, Val loss 1.780\n",
      "Ep 1 (Step 008845): Train loss 1.150, Val loss 1.777\n",
      "Ep 1 (Step 008850): Train loss 1.355, Val loss 1.775\n",
      "Ep 1 (Step 008855): Train loss 1.486, Val loss 1.775\n",
      "Ep 1 (Step 008860): Train loss 1.100, Val loss 1.777\n",
      "Ep 1 (Step 008865): Train loss 1.389, Val loss 1.779\n",
      "Ep 1 (Step 008870): Train loss 1.325, Val loss 1.780\n",
      "Ep 1 (Step 008875): Train loss 1.174, Val loss 1.778\n",
      "Ep 1 (Step 008880): Train loss 1.148, Val loss 1.776\n",
      "Ep 1 (Step 008885): Train loss 1.326, Val loss 1.773\n",
      "Ep 1 (Step 008890): Train loss 1.554, Val loss 1.772\n",
      "Ep 1 (Step 008895): Train loss 1.071, Val loss 1.772\n",
      "Ep 1 (Step 008900): Train loss 1.255, Val loss 1.772\n",
      "Ep 1 (Step 008905): Train loss 1.106, Val loss 1.773\n",
      "Ep 1 (Step 008910): Train loss 1.155, Val loss 1.774\n",
      "Ep 1 (Step 008915): Train loss 1.052, Val loss 1.774\n",
      "Ep 1 (Step 008920): Train loss 1.149, Val loss 1.774\n",
      "Ep 1 (Step 008925): Train loss 0.978, Val loss 1.774\n",
      "Ep 1 (Step 008930): Train loss 1.364, Val loss 1.771\n",
      "Ep 1 (Step 008935): Train loss 1.728, Val loss 1.767\n",
      "Ep 1 (Step 008940): Train loss 1.222, Val loss 1.766\n",
      "Ep 1 (Step 008945): Train loss 1.524, Val loss 1.767\n",
      "Ep 1 (Step 008950): Train loss 1.181, Val loss 1.770\n",
      "Ep 1 (Step 008955): Train loss 1.355, Val loss 1.775\n",
      "Ep 1 (Step 008960): Train loss 1.446, Val loss 1.779\n",
      "Ep 1 (Step 008965): Train loss 1.133, Val loss 1.779\n",
      "Ep 1 (Step 008970): Train loss 1.478, Val loss 1.779\n",
      "Ep 1 (Step 008975): Train loss 1.556, Val loss 1.779\n",
      "Ep 1 (Step 008980): Train loss 1.285, Val loss 1.778\n",
      "Ep 1 (Step 008985): Train loss 1.372, Val loss 1.775\n",
      "Ep 1 (Step 008990): Train loss 1.250, Val loss 1.777\n",
      "Ep 1 (Step 008995): Train loss 1.450, Val loss 1.779\n",
      "Ep 1 (Step 009000): Train loss 0.870, Val loss 1.780\n",
      "Ep 1 (Step 009005): Train loss 1.362, Val loss 1.780\n",
      "Ep 1 (Step 009010): Train loss 1.037, Val loss 1.777\n",
      "Ep 1 (Step 009015): Train loss 1.092, Val loss 1.774\n",
      "Ep 1 (Step 009020): Train loss 1.329, Val loss 1.771\n",
      "Ep 1 (Step 009025): Train loss 1.259, Val loss 1.770\n",
      "Ep 1 (Step 009030): Train loss 1.502, Val loss 1.770\n",
      "Ep 1 (Step 009035): Train loss 1.077, Val loss 1.770\n",
      "Ep 1 (Step 009040): Train loss 1.137, Val loss 1.773\n",
      "Ep 1 (Step 009045): Train loss 1.123, Val loss 1.776\n",
      "Ep 1 (Step 009050): Train loss 1.212, Val loss 1.780\n",
      "Ep 1 (Step 009055): Train loss 1.153, Val loss 1.783\n",
      "Ep 1 (Step 009060): Train loss 1.441, Val loss 1.783\n",
      "Ep 1 (Step 009065): Train loss 1.068, Val loss 1.783\n",
      "Ep 1 (Step 009070): Train loss 1.237, Val loss 1.785\n",
      "Ep 1 (Step 009075): Train loss 1.490, Val loss 1.785\n",
      "Ep 1 (Step 009080): Train loss 1.478, Val loss 1.784\n",
      "Ep 1 (Step 009085): Train loss 1.125, Val loss 1.782\n",
      "Ep 1 (Step 009090): Train loss 1.379, Val loss 1.781\n",
      "Ep 1 (Step 009095): Train loss 1.272, Val loss 1.783\n",
      "Ep 1 (Step 009100): Train loss 1.313, Val loss 1.783\n",
      "Ep 1 (Step 009105): Train loss 1.432, Val loss 1.784\n",
      "Ep 1 (Step 009110): Train loss 1.584, Val loss 1.784\n",
      "Ep 1 (Step 009115): Train loss 1.098, Val loss 1.785\n",
      "Ep 1 (Step 009120): Train loss 1.302, Val loss 1.785\n",
      "Ep 1 (Step 009125): Train loss 1.262, Val loss 1.785\n",
      "Ep 1 (Step 009130): Train loss 1.152, Val loss 1.782\n",
      "Ep 1 (Step 009135): Train loss 1.247, Val loss 1.780\n",
      "Ep 1 (Step 009140): Train loss 1.504, Val loss 1.775\n",
      "Ep 1 (Step 009145): Train loss 1.235, Val loss 1.773\n",
      "Ep 1 (Step 009150): Train loss 1.198, Val loss 1.772\n",
      "Ep 1 (Step 009155): Train loss 1.146, Val loss 1.771\n",
      "Ep 1 (Step 009160): Train loss 1.174, Val loss 1.771\n",
      "Ep 1 (Step 009165): Train loss 1.360, Val loss 1.773\n",
      "Ep 1 (Step 009170): Train loss 1.195, Val loss 1.774\n",
      "Ep 1 (Step 009175): Train loss 1.117, Val loss 1.774\n",
      "Ep 1 (Step 009180): Train loss 1.369, Val loss 1.775\n",
      "Ep 1 (Step 009185): Train loss 1.339, Val loss 1.775\n",
      "Ep 1 (Step 009190): Train loss 1.161, Val loss 1.776\n",
      "Ep 1 (Step 009195): Train loss 1.350, Val loss 1.778\n",
      "Ep 1 (Step 009200): Train loss 1.332, Val loss 1.779\n",
      "Ep 1 (Step 009205): Train loss 1.108, Val loss 1.778\n",
      "Ep 1 (Step 009210): Train loss 1.224, Val loss 1.777\n",
      "Ep 1 (Step 009215): Train loss 1.284, Val loss 1.778\n",
      "Ep 1 (Step 009220): Train loss 1.308, Val loss 1.781\n",
      "Ep 1 (Step 009225): Train loss 1.262, Val loss 1.785\n",
      "Ep 1 (Step 009230): Train loss 1.235, Val loss 1.790\n",
      "Ep 1 (Step 009235): Train loss 1.298, Val loss 1.791\n",
      "Ep 1 (Step 009240): Train loss 1.338, Val loss 1.791\n",
      "Ep 1 (Step 009245): Train loss 1.189, Val loss 1.792\n",
      "Ep 1 (Step 009250): Train loss 1.302, Val loss 1.791\n",
      "Ep 1 (Step 009255): Train loss 1.273, Val loss 1.792\n",
      "Ep 1 (Step 009260): Train loss 1.343, Val loss 1.793\n",
      "Ep 1 (Step 009265): Train loss 1.211, Val loss 1.795\n",
      "Ep 1 (Step 009270): Train loss 0.974, Val loss 1.796\n",
      "Ep 1 (Step 009275): Train loss 1.226, Val loss 1.799\n",
      "Ep 1 (Step 009280): Train loss 1.418, Val loss 1.798\n",
      "Ep 1 (Step 009285): Train loss 0.995, Val loss 1.795\n",
      "Ep 1 (Step 009290): Train loss 1.388, Val loss 1.793\n",
      "Ep 1 (Step 009295): Train loss 1.309, Val loss 1.791\n",
      "Ep 1 (Step 009300): Train loss 1.284, Val loss 1.792\n",
      "Ep 1 (Step 009305): Train loss 1.347, Val loss 1.793\n",
      "Ep 1 (Step 009310): Train loss 1.193, Val loss 1.792\n",
      "Ep 1 (Step 009315): Train loss 1.473, Val loss 1.792\n",
      "Ep 1 (Step 009320): Train loss 1.208, Val loss 1.793\n",
      "Ep 1 (Step 009325): Train loss 1.089, Val loss 1.796\n",
      "Ep 1 (Step 009330): Train loss 1.160, Val loss 1.798\n",
      "Ep 1 (Step 009335): Train loss 1.133, Val loss 1.797\n",
      "Ep 1 (Step 009340): Train loss 1.247, Val loss 1.798\n",
      "Ep 1 (Step 009345): Train loss 1.225, Val loss 1.801\n",
      "Ep 1 (Step 009350): Train loss 1.339, Val loss 1.802\n",
      "Ep 1 (Step 009355): Train loss 0.987, Val loss 1.800\n",
      "Ep 1 (Step 009360): Train loss 1.328, Val loss 1.797\n",
      "Ep 1 (Step 009365): Train loss 1.243, Val loss 1.797\n",
      "Ep 1 (Step 009370): Train loss 1.085, Val loss 1.797\n",
      "Ep 1 (Step 009375): Train loss 1.508, Val loss 1.797\n",
      "Ep 1 (Step 009380): Train loss 1.152, Val loss 1.799\n",
      "Ep 1 (Step 009385): Train loss 1.139, Val loss 1.801\n",
      "Ep 1 (Step 009390): Train loss 1.286, Val loss 1.801\n",
      "Ep 1 (Step 009395): Train loss 1.056, Val loss 1.802\n",
      "Ep 1 (Step 009400): Train loss 1.157, Val loss 1.802\n",
      "Ep 1 (Step 009405): Train loss 1.306, Val loss 1.804\n",
      "Ep 1 (Step 009410): Train loss 1.194, Val loss 1.803\n",
      "Ep 1 (Step 009415): Train loss 1.097, Val loss 1.802\n",
      "Ep 1 (Step 009420): Train loss 0.979, Val loss 1.802\n",
      "Ep 1 (Step 009425): Train loss 1.305, Val loss 1.802\n",
      "Ep 1 (Step 009430): Train loss 1.526, Val loss 1.800\n",
      "Ep 1 (Step 009435): Train loss 1.019, Val loss 1.796\n",
      "Ep 1 (Step 009440): Train loss 1.254, Val loss 1.793\n",
      "Ep 1 (Step 009445): Train loss 1.074, Val loss 1.793\n",
      "Ep 1 (Step 009450): Train loss 1.309, Val loss 1.795\n",
      "Ep 1 (Step 009455): Train loss 1.187, Val loss 1.796\n",
      "Ep 1 (Step 009460): Train loss 1.259, Val loss 1.796\n",
      "Ep 1 (Step 009465): Train loss 1.129, Val loss 1.794\n",
      "Ep 1 (Step 009470): Train loss 1.100, Val loss 1.790\n",
      "Ep 1 (Step 009475): Train loss 1.277, Val loss 1.786\n",
      "Ep 1 (Step 009480): Train loss 1.170, Val loss 1.784\n",
      "Ep 1 (Step 009485): Train loss 1.370, Val loss 1.785\n",
      "Ep 1 (Step 009490): Train loss 1.251, Val loss 1.787\n",
      "Ep 1 (Step 009495): Train loss 1.550, Val loss 1.789\n",
      "Ep 1 (Step 009500): Train loss 1.061, Val loss 1.791\n",
      "Ep 1 (Step 009505): Train loss 1.257, Val loss 1.793\n",
      "Ep 1 (Step 009510): Train loss 1.281, Val loss 1.796\n",
      "Ep 1 (Step 009515): Train loss 1.351, Val loss 1.799\n",
      "Ep 1 (Step 009520): Train loss 1.130, Val loss 1.799\n",
      "Ep 1 (Step 009525): Train loss 1.603, Val loss 1.797\n",
      "Ep 1 (Step 009530): Train loss 1.295, Val loss 1.798\n",
      "Ep 1 (Step 009535): Train loss 1.504, Val loss 1.798\n",
      "Ep 1 (Step 009540): Train loss 1.386, Val loss 1.798\n",
      "Ep 1 (Step 009545): Train loss 1.184, Val loss 1.797\n",
      "Ep 1 (Step 009550): Train loss 1.022, Val loss 1.795\n",
      "Ep 1 (Step 009555): Train loss 1.232, Val loss 1.793\n",
      "Ep 1 (Step 009560): Train loss 1.018, Val loss 1.792\n",
      "Ep 1 (Step 009565): Train loss 1.135, Val loss 1.792\n",
      "Ep 1 (Step 009570): Train loss 1.171, Val loss 1.791\n",
      "Ep 1 (Step 009575): Train loss 1.114, Val loss 1.787\n",
      "Ep 1 (Step 009580): Train loss 1.504, Val loss 1.784\n",
      "Ep 1 (Step 009585): Train loss 1.076, Val loss 1.783\n",
      "Ep 1 (Step 009590): Train loss 1.255, Val loss 1.782\n",
      "Ep 1 (Step 009595): Train loss 1.123, Val loss 1.782\n",
      "Ep 1 (Step 009600): Train loss 1.153, Val loss 1.783\n",
      "Ep 1 (Step 009605): Train loss 1.332, Val loss 1.784\n",
      "Ep 1 (Step 009610): Train loss 1.254, Val loss 1.783\n",
      "Ep 1 (Step 009615): Train loss 1.208, Val loss 1.784\n",
      "Ep 1 (Step 009620): Train loss 1.071, Val loss 1.786\n",
      "Ep 1 (Step 009625): Train loss 1.315, Val loss 1.788\n",
      "Ep 1 (Step 009630): Train loss 1.507, Val loss 1.789\n",
      "Ep 1 (Step 009635): Train loss 0.942, Val loss 1.792\n",
      "Ep 1 (Step 009640): Train loss 1.462, Val loss 1.793\n",
      "Ep 1 (Step 009645): Train loss 1.256, Val loss 1.794\n",
      "Ep 1 (Step 009650): Train loss 1.354, Val loss 1.794\n",
      "Ep 1 (Step 009655): Train loss 1.178, Val loss 1.794\n",
      "Ep 1 (Step 009660): Train loss 1.155, Val loss 1.792\n",
      "Ep 1 (Step 009665): Train loss 1.507, Val loss 1.789\n",
      "Ep 1 (Step 009670): Train loss 1.131, Val loss 1.788\n",
      "Ep 1 (Step 009675): Train loss 1.491, Val loss 1.786\n",
      "Ep 1 (Step 009680): Train loss 1.232, Val loss 1.786\n",
      "Ep 1 (Step 009685): Train loss 1.218, Val loss 1.785\n",
      "Ep 1 (Step 009690): Train loss 1.167, Val loss 1.783\n",
      "Ep 1 (Step 009695): Train loss 1.269, Val loss 1.784\n",
      "Ep 1 (Step 009700): Train loss 1.212, Val loss 1.787\n",
      "Ep 1 (Step 009705): Train loss 1.137, Val loss 1.788\n",
      "Ep 1 (Step 009710): Train loss 1.270, Val loss 1.788\n",
      "Ep 1 (Step 009715): Train loss 1.439, Val loss 1.788\n",
      "Ep 1 (Step 009720): Train loss 1.476, Val loss 1.789\n",
      "Ep 1 (Step 009725): Train loss 1.128, Val loss 1.789\n",
      "Ep 1 (Step 009730): Train loss 1.243, Val loss 1.790\n",
      "Ep 1 (Step 009735): Train loss 1.020, Val loss 1.791\n",
      "Ep 1 (Step 009740): Train loss 1.166, Val loss 1.791\n",
      "Ep 1 (Step 009745): Train loss 0.996, Val loss 1.791\n",
      "Ep 1 (Step 009750): Train loss 1.158, Val loss 1.787\n",
      "Ep 1 (Step 009755): Train loss 1.003, Val loss 1.786\n",
      "Ep 1 (Step 009760): Train loss 1.415, Val loss 1.788\n",
      "Ep 1 (Step 009765): Train loss 1.340, Val loss 1.789\n",
      "Ep 1 (Step 009770): Train loss 1.064, Val loss 1.792\n",
      "Ep 1 (Step 009775): Train loss 1.522, Val loss 1.794\n",
      "Ep 1 (Step 009780): Train loss 1.361, Val loss 1.793\n",
      "Ep 1 (Step 009785): Train loss 1.487, Val loss 1.789\n",
      "Ep 1 (Step 009790): Train loss 1.276, Val loss 1.787\n",
      "Ep 1 (Step 009795): Train loss 1.170, Val loss 1.786\n",
      "Ep 1 (Step 009800): Train loss 1.041, Val loss 1.784\n",
      "Ep 1 (Step 009805): Train loss 1.282, Val loss 1.781\n",
      "Ep 1 (Step 009810): Train loss 1.104, Val loss 1.780\n",
      "Ep 1 (Step 009815): Train loss 1.395, Val loss 1.780\n",
      "Ep 1 (Step 009820): Train loss 1.254, Val loss 1.780\n",
      "Ep 1 (Step 009825): Train loss 0.944, Val loss 1.783\n",
      "Ep 1 (Step 009830): Train loss 1.273, Val loss 1.784\n",
      "Ep 1 (Step 009835): Train loss 1.288, Val loss 1.785\n",
      "Ep 1 (Step 009840): Train loss 1.002, Val loss 1.787\n",
      "Ep 1 (Step 009845): Train loss 1.380, Val loss 1.793\n",
      "Ep 1 (Step 009850): Train loss 1.319, Val loss 1.794\n",
      "Ep 1 (Step 009855): Train loss 0.905, Val loss 1.793\n",
      "Ep 1 (Step 009860): Train loss 1.270, Val loss 1.792\n",
      "Ep 1 (Step 009865): Train loss 1.147, Val loss 1.792\n",
      "Ep 1 (Step 009870): Train loss 1.164, Val loss 1.793\n",
      "Ep 1 (Step 009875): Train loss 1.063, Val loss 1.794\n",
      "Ep 1 (Step 009880): Train loss 1.571, Val loss 1.793\n",
      "Ep 1 (Step 009885): Train loss 1.194, Val loss 1.792\n",
      "Ep 1 (Step 009890): Train loss 1.360, Val loss 1.792\n",
      "Ep 1 (Step 009895): Train loss 1.501, Val loss 1.788\n",
      "Ep 1 (Step 009900): Train loss 1.305, Val loss 1.787\n",
      "Ep 1 (Step 009905): Train loss 1.035, Val loss 1.788\n",
      "Ep 1 (Step 009910): Train loss 1.388, Val loss 1.789\n",
      "Ep 1 (Step 009915): Train loss 1.482, Val loss 1.788\n",
      "Ep 1 (Step 009920): Train loss 1.178, Val loss 1.788\n",
      "Ep 1 (Step 009925): Train loss 1.087, Val loss 1.788\n",
      "Ep 1 (Step 009930): Train loss 1.204, Val loss 1.786\n",
      "Ep 1 (Step 009935): Train loss 1.342, Val loss 1.784\n",
      "Ep 1 (Step 009940): Train loss 1.326, Val loss 1.781\n",
      "Ep 1 (Step 009945): Train loss 1.045, Val loss 1.779\n",
      "Ep 1 (Step 009950): Train loss 1.195, Val loss 1.777\n",
      "Ep 1 (Step 009955): Train loss 1.433, Val loss 1.777\n",
      "Ep 1 (Step 009960): Train loss 1.279, Val loss 1.777\n",
      "Ep 1 (Step 009965): Train loss 1.100, Val loss 1.777\n",
      "Ep 1 (Step 009970): Train loss 1.084, Val loss 1.775\n",
      "Ep 1 (Step 009975): Train loss 1.488, Val loss 1.776\n",
      "Ep 1 (Step 009980): Train loss 1.190, Val loss 1.778\n",
      "Ep 1 (Step 009985): Train loss 1.611, Val loss 1.779\n",
      "Ep 1 (Step 009990): Train loss 1.285, Val loss 1.780\n",
      "Ep 1 (Step 009995): Train loss 1.272, Val loss 1.779\n",
      "Ep 1 (Step 010000): Train loss 1.279, Val loss 1.778\n",
      "Ep 1 (Step 010005): Train loss 1.069, Val loss 1.778\n",
      "Ep 1 (Step 010010): Train loss 1.206, Val loss 1.779\n",
      "Ep 1 (Step 010015): Train loss 1.278, Val loss 1.779\n",
      "Ep 1 (Step 010020): Train loss 1.223, Val loss 1.781\n",
      "Ep 1 (Step 010025): Train loss 1.273, Val loss 1.782\n",
      "Ep 1 (Step 010030): Train loss 1.286, Val loss 1.782\n",
      "Ep 1 (Step 010035): Train loss 1.268, Val loss 1.782\n",
      "Ep 1 (Step 010040): Train loss 1.275, Val loss 1.784\n",
      "Ep 1 (Step 010045): Train loss 0.979, Val loss 1.783\n",
      "Ep 1 (Step 010050): Train loss 1.200, Val loss 1.778\n",
      "Ep 1 (Step 010055): Train loss 1.247, Val loss 1.775\n",
      "Ep 1 (Step 010060): Train loss 1.015, Val loss 1.773\n",
      "Ep 1 (Step 010065): Train loss 1.566, Val loss 1.771\n",
      "Ep 1 (Step 010070): Train loss 1.123, Val loss 1.769\n",
      "Ep 1 (Step 010075): Train loss 1.030, Val loss 1.769\n",
      "Ep 1 (Step 010080): Train loss 1.053, Val loss 1.770\n",
      "Ep 1 (Step 010085): Train loss 1.234, Val loss 1.773\n",
      "Ep 1 (Step 010090): Train loss 1.002, Val loss 1.775\n",
      "Ep 1 (Step 010095): Train loss 1.116, Val loss 1.777\n",
      "Ep 1 (Step 010100): Train loss 1.362, Val loss 1.779\n",
      "Ep 1 (Step 010105): Train loss 1.133, Val loss 1.781\n",
      "Ep 1 (Step 010110): Train loss 1.250, Val loss 1.784\n",
      "Ep 1 (Step 010115): Train loss 1.345, Val loss 1.782\n",
      "Ep 1 (Step 010120): Train loss 1.326, Val loss 1.779\n",
      "Ep 1 (Step 010125): Train loss 1.394, Val loss 1.778\n",
      "Ep 1 (Step 010130): Train loss 1.089, Val loss 1.778\n",
      "Ep 1 (Step 010135): Train loss 1.535, Val loss 1.778\n",
      "Ep 1 (Step 010140): Train loss 1.125, Val loss 1.779\n",
      "Ep 1 (Step 010145): Train loss 1.230, Val loss 1.782\n",
      "Ep 1 (Step 010150): Train loss 1.065, Val loss 1.785\n",
      "Ep 1 (Step 010155): Train loss 1.027, Val loss 1.788\n",
      "Ep 1 (Step 010160): Train loss 1.164, Val loss 1.787\n",
      "Ep 1 (Step 010165): Train loss 1.262, Val loss 1.785\n",
      "Ep 1 (Step 010170): Train loss 1.336, Val loss 1.783\n",
      "Ep 1 (Step 010175): Train loss 1.188, Val loss 1.781\n",
      "Ep 1 (Step 010180): Train loss 1.171, Val loss 1.778\n",
      "Ep 1 (Step 010185): Train loss 1.482, Val loss 1.777\n",
      "Ep 1 (Step 010190): Train loss 1.264, Val loss 1.774\n",
      "Ep 1 (Step 010195): Train loss 1.512, Val loss 1.774\n",
      "Ep 1 (Step 010200): Train loss 1.455, Val loss 1.774\n",
      "Ep 1 (Step 010205): Train loss 1.153, Val loss 1.776\n",
      "Ep 1 (Step 010210): Train loss 1.354, Val loss 1.775\n",
      "Ep 1 (Step 010215): Train loss 1.381, Val loss 1.775\n",
      "Ep 1 (Step 010220): Train loss 1.142, Val loss 1.773\n",
      "Ep 1 (Step 010225): Train loss 1.143, Val loss 1.771\n",
      "Ep 1 (Step 010230): Train loss 1.133, Val loss 1.770\n",
      "Ep 1 (Step 010235): Train loss 1.082, Val loss 1.769\n",
      "Ep 1 (Step 010240): Train loss 1.667, Val loss 1.770\n",
      "Ep 1 (Step 010245): Train loss 1.274, Val loss 1.771\n",
      "Ep 1 (Step 010250): Train loss 0.944, Val loss 1.772\n",
      "Ep 1 (Step 010255): Train loss 1.472, Val loss 1.772\n",
      "Ep 1 (Step 010260): Train loss 1.408, Val loss 1.772\n",
      "Ep 1 (Step 010265): Train loss 1.383, Val loss 1.771\n",
      "Ep 1 (Step 010270): Train loss 1.291, Val loss 1.772\n",
      "Ep 1 (Step 010275): Train loss 1.244, Val loss 1.775\n",
      "Ep 1 (Step 010280): Train loss 1.239, Val loss 1.775\n",
      "Ep 1 (Step 010285): Train loss 1.162, Val loss 1.776\n",
      "Ep 1 (Step 010290): Train loss 1.200, Val loss 1.774\n",
      "Ep 1 (Step 010295): Train loss 1.341, Val loss 1.771\n",
      "Ep 1 (Step 010300): Train loss 1.226, Val loss 1.768\n",
      "Ep 1 (Step 010305): Train loss 1.393, Val loss 1.766\n",
      "Ep 1 (Step 010310): Train loss 1.235, Val loss 1.766\n",
      "Ep 1 (Step 010315): Train loss 1.429, Val loss 1.767\n",
      "Ep 1 (Step 010320): Train loss 1.508, Val loss 1.768\n",
      "Ep 1 (Step 010325): Train loss 0.884, Val loss 1.769\n",
      "Ep 1 (Step 010330): Train loss 1.243, Val loss 1.770\n",
      "Ep 1 (Step 010335): Train loss 1.021, Val loss 1.770\n",
      "Ep 1 (Step 010340): Train loss 1.298, Val loss 1.771\n",
      "Ep 1 (Step 010345): Train loss 1.527, Val loss 1.771\n",
      "Ep 1 (Step 010350): Train loss 1.301, Val loss 1.770\n",
      "Ep 1 (Step 010355): Train loss 1.299, Val loss 1.771\n",
      "Ep 1 (Step 010360): Train loss 1.259, Val loss 1.772\n",
      "Ep 1 (Step 010365): Train loss 1.373, Val loss 1.771\n",
      "Ep 1 (Step 010370): Train loss 1.211, Val loss 1.771\n",
      "Ep 1 (Step 010375): Train loss 1.091, Val loss 1.770\n",
      "Ep 1 (Step 010380): Train loss 1.031, Val loss 1.768\n",
      "Ep 1 (Step 010385): Train loss 1.288, Val loss 1.768\n",
      "Ep 1 (Step 010390): Train loss 1.255, Val loss 1.767\n",
      "Ep 1 (Step 010395): Train loss 1.325, Val loss 1.768\n",
      "Ep 1 (Step 010400): Train loss 1.193, Val loss 1.769\n",
      "Ep 1 (Step 010405): Train loss 1.281, Val loss 1.770\n",
      "Ep 1 (Step 010410): Train loss 1.204, Val loss 1.770\n",
      "Ep 1 (Step 010415): Train loss 1.215, Val loss 1.771\n",
      "Ep 1 (Step 010420): Train loss 1.172, Val loss 1.772\n",
      "Ep 1 (Step 010425): Train loss 1.062, Val loss 1.775\n",
      "Ep 1 (Step 010430): Train loss 1.307, Val loss 1.778\n",
      "Ep 1 (Step 010435): Train loss 1.278, Val loss 1.780\n",
      "Ep 1 (Step 010440): Train loss 1.492, Val loss 1.782\n",
      "Ep 1 (Step 010445): Train loss 1.057, Val loss 1.782\n",
      "Ep 1 (Step 010450): Train loss 1.337, Val loss 1.781\n",
      "Ep 1 (Step 010455): Train loss 0.972, Val loss 1.783\n",
      "Ep 1 (Step 010460): Train loss 1.141, Val loss 1.786\n",
      "Ep 1 (Step 010465): Train loss 1.438, Val loss 1.788\n",
      "Ep 1 (Step 010470): Train loss 1.384, Val loss 1.790\n",
      "Ep 1 (Step 010475): Train loss 1.510, Val loss 1.790\n",
      "Ep 1 (Step 010480): Train loss 1.288, Val loss 1.792\n",
      "Ep 1 (Step 010485): Train loss 1.430, Val loss 1.786\n",
      "Ep 1 (Step 010490): Train loss 1.197, Val loss 1.782\n",
      "Ep 1 (Step 010495): Train loss 1.249, Val loss 1.780\n",
      "Ep 1 (Step 010500): Train loss 1.222, Val loss 1.781\n",
      "Ep 1 (Step 010505): Train loss 1.331, Val loss 1.782\n",
      "Ep 1 (Step 010510): Train loss 1.500, Val loss 1.783\n",
      "Ep 1 (Step 010515): Train loss 1.268, Val loss 1.785\n",
      "Ep 1 (Step 010520): Train loss 1.260, Val loss 1.787\n",
      "Ep 1 (Step 010525): Train loss 1.202, Val loss 1.788\n",
      "Ep 1 (Step 010530): Train loss 1.283, Val loss 1.789\n",
      "Ep 1 (Step 010535): Train loss 1.357, Val loss 1.788\n",
      "Ep 1 (Step 010540): Train loss 1.231, Val loss 1.786\n",
      "Ep 1 (Step 010545): Train loss 1.158, Val loss 1.785\n",
      "Ep 1 (Step 010550): Train loss 1.363, Val loss 1.784\n",
      "Ep 1 (Step 010555): Train loss 1.473, Val loss 1.783\n",
      "Ep 1 (Step 010560): Train loss 1.553, Val loss 1.782\n",
      "Ep 1 (Step 010565): Train loss 1.157, Val loss 1.782\n",
      "Ep 1 (Step 010570): Train loss 1.325, Val loss 1.781\n",
      "Ep 1 (Step 010575): Train loss 1.348, Val loss 1.779\n",
      "Ep 1 (Step 010580): Train loss 1.195, Val loss 1.778\n",
      "Ep 1 (Step 010585): Train loss 1.182, Val loss 1.777\n",
      "Ep 1 (Step 010590): Train loss 1.304, Val loss 1.775\n",
      "Ep 1 (Step 010595): Train loss 1.366, Val loss 1.774\n",
      "Ep 1 (Step 010600): Train loss 1.016, Val loss 1.770\n",
      "Ep 1 (Step 010605): Train loss 1.477, Val loss 1.768\n",
      "Ep 1 (Step 010610): Train loss 1.124, Val loss 1.767\n",
      "Ep 1 (Step 010615): Train loss 1.064, Val loss 1.767\n",
      "Ep 1 (Step 010620): Train loss 1.220, Val loss 1.767\n",
      "Ep 1 (Step 010625): Train loss 1.100, Val loss 1.764\n",
      "Ep 1 (Step 010630): Train loss 1.123, Val loss 1.764\n",
      "Ep 1 (Step 010635): Train loss 1.219, Val loss 1.764\n",
      "Ep 1 (Step 010640): Train loss 1.324, Val loss 1.763\n",
      "Ep 1 (Step 010645): Train loss 1.120, Val loss 1.764\n",
      "Ep 1 (Step 010650): Train loss 1.170, Val loss 1.765\n",
      "Ep 1 (Step 010655): Train loss 1.283, Val loss 1.766\n",
      "Ep 1 (Step 010660): Train loss 0.961, Val loss 1.765\n",
      "Ep 1 (Step 010665): Train loss 1.284, Val loss 1.763\n",
      "Ep 1 (Step 010670): Train loss 1.131, Val loss 1.761\n",
      "Ep 1 (Step 010675): Train loss 1.213, Val loss 1.760\n",
      "Ep 1 (Step 010680): Train loss 1.322, Val loss 1.760\n",
      "Ep 1 (Step 010685): Train loss 1.249, Val loss 1.760\n",
      "Ep 1 (Step 010690): Train loss 1.323, Val loss 1.762\n",
      "Ep 1 (Step 010695): Train loss 1.250, Val loss 1.765\n",
      "Ep 1 (Step 010700): Train loss 1.208, Val loss 1.766\n",
      "Ep 1 (Step 010705): Train loss 1.108, Val loss 1.766\n",
      "Ep 1 (Step 010710): Train loss 1.011, Val loss 1.767\n",
      "Ep 1 (Step 010715): Train loss 1.282, Val loss 1.768\n",
      "Ep 1 (Step 010720): Train loss 1.247, Val loss 1.770\n",
      "Ep 1 (Step 010725): Train loss 1.131, Val loss 1.772\n",
      "Ep 1 (Step 010730): Train loss 1.279, Val loss 1.774\n",
      "Ep 1 (Step 010735): Train loss 1.195, Val loss 1.774\n",
      "Ep 1 (Step 010740): Train loss 1.324, Val loss 1.775\n",
      "Ep 1 (Step 010745): Train loss 1.390, Val loss 1.778\n",
      "Ep 1 (Step 010750): Train loss 1.410, Val loss 1.780\n",
      "Ep 1 (Step 010755): Train loss 1.154, Val loss 1.781\n",
      "Ep 1 (Step 010760): Train loss 1.317, Val loss 1.781\n",
      "Ep 1 (Step 010765): Train loss 1.452, Val loss 1.780\n",
      "Ep 1 (Step 010770): Train loss 1.141, Val loss 1.779\n",
      "Ep 1 (Step 010775): Train loss 1.432, Val loss 1.777\n",
      "Ep 1 (Step 010780): Train loss 1.416, Val loss 1.774\n",
      "Ep 1 (Step 010785): Train loss 1.258, Val loss 1.775\n",
      "Ep 1 (Step 010790): Train loss 1.410, Val loss 1.774\n",
      "Ep 1 (Step 010795): Train loss 1.371, Val loss 1.775\n",
      "Ep 1 (Step 010800): Train loss 1.467, Val loss 1.777\n",
      "Ep 1 (Step 010805): Train loss 1.235, Val loss 1.778\n",
      "Ep 1 (Step 010810): Train loss 1.288, Val loss 1.780\n",
      "Ep 1 (Step 010815): Train loss 1.330, Val loss 1.782\n",
      "Ep 1 (Step 010820): Train loss 1.403, Val loss 1.782\n",
      "Ep 1 (Step 010825): Train loss 1.296, Val loss 1.781\n",
      "Ep 1 (Step 010830): Train loss 1.326, Val loss 1.780\n",
      "Ep 1 (Step 010835): Train loss 1.238, Val loss 1.780\n",
      "Ep 1 (Step 010840): Train loss 1.035, Val loss 1.780\n",
      "Ep 1 (Step 010845): Train loss 1.544, Val loss 1.780\n",
      "Ep 1 (Step 010850): Train loss 1.429, Val loss 1.783\n",
      "Ep 1 (Step 010855): Train loss 1.280, Val loss 1.785\n",
      "Ep 1 (Step 010860): Train loss 1.333, Val loss 1.785\n",
      "Ep 1 (Step 010865): Train loss 1.145, Val loss 1.785\n",
      "Ep 1 (Step 010870): Train loss 1.179, Val loss 1.786\n",
      "Ep 1 (Step 010875): Train loss 0.936, Val loss 1.786\n",
      "Ep 1 (Step 010880): Train loss 1.492, Val loss 1.785\n",
      "Ep 1 (Step 010885): Train loss 1.328, Val loss 1.783\n",
      "Ep 1 (Step 010890): Train loss 1.157, Val loss 1.784\n",
      "Ep 1 (Step 010895): Train loss 1.161, Val loss 1.785\n",
      "Ep 1 (Step 010900): Train loss 1.336, Val loss 1.785\n",
      "Ep 1 (Step 010905): Train loss 1.514, Val loss 1.785\n",
      "Ep 1 (Step 010910): Train loss 1.201, Val loss 1.784\n",
      "Ep 1 (Step 010915): Train loss 1.345, Val loss 1.783\n",
      "Ep 1 (Step 010920): Train loss 1.149, Val loss 1.783\n",
      "Ep 1 (Step 010925): Train loss 1.402, Val loss 1.782\n",
      "Ep 1 (Step 010930): Train loss 1.070, Val loss 1.784\n",
      "Ep 1 (Step 010935): Train loss 1.492, Val loss 1.782\n",
      "Ep 1 (Step 010940): Train loss 1.321, Val loss 1.783\n",
      "Ep 1 (Step 010945): Train loss 1.484, Val loss 1.783\n",
      "Ep 1 (Step 010950): Train loss 1.294, Val loss 1.781\n",
      "Ep 1 (Step 010955): Train loss 1.383, Val loss 1.778\n",
      "Ep 1 (Step 010960): Train loss 1.320, Val loss 1.778\n",
      "Ep 1 (Step 010965): Train loss 0.870, Val loss 1.779\n",
      "Ep 1 (Step 010970): Train loss 1.233, Val loss 1.779\n",
      "Ep 1 (Step 010975): Train loss 1.202, Val loss 1.779\n",
      "Ep 1 (Step 010980): Train loss 0.952, Val loss 1.778\n",
      "Ep 1 (Step 010985): Train loss 1.558, Val loss 1.777\n",
      "Ep 1 (Step 010990): Train loss 1.189, Val loss 1.777\n",
      "Ep 1 (Step 010995): Train loss 1.334, Val loss 1.782\n",
      "Ep 1 (Step 011000): Train loss 1.196, Val loss 1.784\n",
      "Ep 1 (Step 011005): Train loss 1.150, Val loss 1.784\n",
      "Ep 1 (Step 011010): Train loss 1.132, Val loss 1.782\n",
      "Ep 1 (Step 011015): Train loss 1.506, Val loss 1.780\n",
      "Ep 1 (Step 011020): Train loss 1.367, Val loss 1.776\n",
      "Ep 1 (Step 011025): Train loss 1.203, Val loss 1.775\n",
      "Ep 1 (Step 011030): Train loss 1.280, Val loss 1.774\n",
      "Ep 1 (Step 011035): Train loss 1.227, Val loss 1.773\n",
      "Ep 1 (Step 011040): Train loss 1.427, Val loss 1.773\n",
      "Ep 1 (Step 011045): Train loss 1.182, Val loss 1.772\n",
      "Ep 1 (Step 011050): Train loss 1.123, Val loss 1.770\n",
      "Ep 1 (Step 011055): Train loss 1.471, Val loss 1.769\n",
      "Ep 1 (Step 011060): Train loss 1.142, Val loss 1.770\n",
      "Ep 1 (Step 011065): Train loss 1.310, Val loss 1.771\n",
      "Ep 1 (Step 011070): Train loss 1.164, Val loss 1.773\n",
      "Ep 1 (Step 011075): Train loss 1.109, Val loss 1.774\n",
      "Ep 1 (Step 011080): Train loss 1.247, Val loss 1.773\n",
      "Ep 1 (Step 011085): Train loss 1.215, Val loss 1.773\n",
      "Ep 1 (Step 011090): Train loss 1.556, Val loss 1.773\n",
      "Ep 1 (Step 011095): Train loss 1.536, Val loss 1.772\n",
      "Ep 1 (Step 011100): Train loss 1.201, Val loss 1.770\n",
      "Ep 1 (Step 011105): Train loss 1.294, Val loss 1.768\n",
      "Ep 1 (Step 011110): Train loss 1.319, Val loss 1.767\n",
      "Ep 1 (Step 011115): Train loss 1.340, Val loss 1.768\n",
      "Ep 1 (Step 011120): Train loss 1.373, Val loss 1.769\n",
      "Ep 1 (Step 011125): Train loss 1.227, Val loss 1.770\n",
      "Ep 1 (Step 011130): Train loss 1.411, Val loss 1.771\n",
      "Ep 1 (Step 011135): Train loss 1.175, Val loss 1.772\n",
      "Ep 1 (Step 011140): Train loss 1.331, Val loss 1.772\n",
      "Ep 1 (Step 011145): Train loss 1.267, Val loss 1.771\n",
      "Ep 1 (Step 011150): Train loss 1.518, Val loss 1.770\n",
      "Ep 1 (Step 011155): Train loss 1.474, Val loss 1.770\n",
      "Ep 1 (Step 011160): Train loss 1.606, Val loss 1.772\n",
      "Ep 1 (Step 011165): Train loss 1.173, Val loss 1.776\n",
      "Ep 1 (Step 011170): Train loss 1.517, Val loss 1.777\n",
      "Ep 1 (Step 011175): Train loss 1.735, Val loss 1.778\n",
      "Ep 1 (Step 011180): Train loss 1.390, Val loss 1.779\n",
      "Ep 1 (Step 011185): Train loss 1.310, Val loss 1.779\n",
      "Ep 1 (Step 011190): Train loss 1.000, Val loss 1.782\n",
      "Ep 1 (Step 011195): Train loss 1.201, Val loss 1.785\n",
      "Ep 1 (Step 011200): Train loss 1.101, Val loss 1.786\n",
      "Ep 1 (Step 011205): Train loss 1.427, Val loss 1.787\n",
      "Ep 1 (Step 011210): Train loss 1.057, Val loss 1.787\n",
      "Ep 1 (Step 011215): Train loss 1.210, Val loss 1.786\n",
      "Ep 1 (Step 011220): Train loss 1.286, Val loss 1.787\n",
      "Ep 1 (Step 011225): Train loss 1.230, Val loss 1.787\n",
      "Ep 1 (Step 011230): Train loss 1.417, Val loss 1.786\n",
      "Ep 1 (Step 011235): Train loss 1.311, Val loss 1.783\n",
      "Ep 1 (Step 011240): Train loss 1.719, Val loss 1.782\n",
      "Ep 1 (Step 011245): Train loss 1.308, Val loss 1.783\n",
      "Ep 1 (Step 011250): Train loss 1.460, Val loss 1.782\n",
      "Ep 1 (Step 011255): Train loss 1.462, Val loss 1.781\n",
      "Ep 1 (Step 011260): Train loss 1.602, Val loss 1.779\n",
      "Ep 1 (Step 011265): Train loss 1.323, Val loss 1.777\n",
      "Ep 1 (Step 011270): Train loss 1.068, Val loss 1.775\n",
      "Ep 1 (Step 011275): Train loss 1.216, Val loss 1.776\n",
      "Ep 1 (Step 011280): Train loss 1.131, Val loss 1.778\n",
      "Ep 1 (Step 011285): Train loss 1.094, Val loss 1.781\n",
      "Ep 1 (Step 011290): Train loss 1.578, Val loss 1.783\n",
      "Ep 1 (Step 011295): Train loss 0.917, Val loss 1.782\n",
      "Ep 1 (Step 011300): Train loss 1.518, Val loss 1.782\n",
      "Ep 1 (Step 011305): Train loss 1.159, Val loss 1.781\n",
      "Ep 1 (Step 011310): Train loss 1.075, Val loss 1.781\n",
      "Ep 1 (Step 011315): Train loss 1.409, Val loss 1.781\n",
      "Ep 1 (Step 011320): Train loss 1.179, Val loss 1.782\n",
      "Ep 1 (Step 011325): Train loss 1.232, Val loss 1.778\n",
      "Ep 1 (Step 011330): Train loss 1.392, Val loss 1.777\n",
      "Ep 1 (Step 011335): Train loss 1.106, Val loss 1.777\n",
      "Ep 1 (Step 011340): Train loss 1.262, Val loss 1.778\n",
      "Ep 1 (Step 011345): Train loss 1.090, Val loss 1.779\n",
      "Ep 1 (Step 011350): Train loss 1.305, Val loss 1.780\n",
      "Ep 1 (Step 011355): Train loss 1.263, Val loss 1.780\n",
      "Ep 1 (Step 011360): Train loss 1.258, Val loss 1.778\n",
      "Ep 1 (Step 011365): Train loss 1.121, Val loss 1.776\n",
      "Ep 1 (Step 011370): Train loss 1.472, Val loss 1.775\n",
      "Ep 1 (Step 011375): Train loss 1.060, Val loss 1.774\n",
      "Ep 1 (Step 011380): Train loss 1.220, Val loss 1.774\n",
      "Ep 1 (Step 011385): Train loss 0.956, Val loss 1.773\n",
      "Ep 1 (Step 011390): Train loss 1.413, Val loss 1.773\n",
      "Ep 1 (Step 011395): Train loss 1.218, Val loss 1.775\n",
      "Ep 1 (Step 011400): Train loss 1.280, Val loss 1.777\n",
      "Ep 1 (Step 011405): Train loss 0.964, Val loss 1.778\n",
      "Ep 1 (Step 011410): Train loss 1.464, Val loss 1.778\n",
      "Ep 1 (Step 011415): Train loss 1.104, Val loss 1.777\n",
      "Ep 1 (Step 011420): Train loss 1.120, Val loss 1.777\n",
      "Ep 1 (Step 011425): Train loss 1.452, Val loss 1.775\n",
      "Ep 1 (Step 011430): Train loss 1.322, Val loss 1.774\n",
      "Ep 1 (Step 011435): Train loss 1.382, Val loss 1.774\n",
      "Ep 1 (Step 011440): Train loss 1.009, Val loss 1.773\n",
      "Ep 1 (Step 011445): Train loss 1.101, Val loss 1.771\n",
      "Ep 1 (Step 011450): Train loss 1.389, Val loss 1.769\n",
      "Ep 1 (Step 011455): Train loss 1.409, Val loss 1.768\n",
      "Ep 1 (Step 011460): Train loss 1.271, Val loss 1.769\n",
      "Ep 1 (Step 011465): Train loss 0.981, Val loss 1.768\n",
      "Ep 1 (Step 011470): Train loss 1.201, Val loss 1.769\n",
      "Ep 1 (Step 011475): Train loss 1.167, Val loss 1.770\n",
      "Ep 1 (Step 011480): Train loss 1.322, Val loss 1.772\n",
      "Ep 1 (Step 011485): Train loss 1.237, Val loss 1.772\n",
      "Ep 1 (Step 011490): Train loss 1.201, Val loss 1.774\n",
      "Ep 1 (Step 011495): Train loss 1.276, Val loss 1.775\n",
      "Ep 1 (Step 011500): Train loss 1.460, Val loss 1.776\n",
      "Ep 1 (Step 011505): Train loss 1.030, Val loss 1.778\n",
      "Ep 1 (Step 011510): Train loss 1.285, Val loss 1.779\n",
      "Ep 1 (Step 011515): Train loss 1.109, Val loss 1.777\n",
      "Ep 1 (Step 011520): Train loss 1.380, Val loss 1.776\n",
      "Ep 1 (Step 011525): Train loss 1.321, Val loss 1.778\n",
      "Ep 1 (Step 011530): Train loss 1.402, Val loss 1.781\n",
      "Ep 1 (Step 011535): Train loss 1.187, Val loss 1.782\n",
      "Ep 1 (Step 011540): Train loss 1.013, Val loss 1.782\n",
      "Ep 1 (Step 011545): Train loss 1.236, Val loss 1.781\n",
      "Ep 1 (Step 011550): Train loss 1.335, Val loss 1.782\n",
      "Ep 1 (Step 011555): Train loss 1.121, Val loss 1.781\n",
      "Ep 1 (Step 011560): Train loss 1.172, Val loss 1.780\n",
      "Ep 1 (Step 011565): Train loss 1.563, Val loss 1.782\n",
      "Ep 1 (Step 011570): Train loss 0.998, Val loss 1.785\n",
      "Ep 1 (Step 011575): Train loss 1.336, Val loss 1.786\n",
      "Ep 1 (Step 011580): Train loss 0.887, Val loss 1.787\n",
      "Ep 1 (Step 011585): Train loss 0.940, Val loss 1.788\n",
      "Ep 1 (Step 011590): Train loss 1.178, Val loss 1.786\n",
      "Ep 1 (Step 011595): Train loss 1.183, Val loss 1.786\n",
      "Ep 1 (Step 011600): Train loss 1.225, Val loss 1.787\n",
      "Ep 1 (Step 011605): Train loss 1.476, Val loss 1.788\n",
      "Ep 1 (Step 011610): Train loss 1.314, Val loss 1.790\n",
      "Ep 1 (Step 011615): Train loss 0.970, Val loss 1.791\n",
      "Ep 1 (Step 011620): Train loss 1.298, Val loss 1.790\n",
      "Ep 1 (Step 011625): Train loss 1.011, Val loss 1.788\n",
      "Ep 1 (Step 011630): Train loss 1.245, Val loss 1.786\n",
      "Ep 1 (Step 011635): Train loss 1.219, Val loss 1.784\n",
      "Ep 1 (Step 011640): Train loss 1.182, Val loss 1.782\n",
      "Ep 1 (Step 011645): Train loss 1.271, Val loss 1.783\n",
      "Ep 1 (Step 011650): Train loss 1.402, Val loss 1.783\n",
      "Ep 1 (Step 011655): Train loss 1.526, Val loss 1.785\n",
      "Ep 1 (Step 011660): Train loss 1.335, Val loss 1.787\n",
      "Ep 1 (Step 011665): Train loss 1.214, Val loss 1.786\n",
      "Ep 1 (Step 011670): Train loss 1.268, Val loss 1.786\n",
      "Ep 1 (Step 011675): Train loss 1.125, Val loss 1.786\n",
      "Ep 1 (Step 011680): Train loss 1.556, Val loss 1.787\n",
      "Ep 1 (Step 011685): Train loss 1.224, Val loss 1.788\n",
      "Ep 1 (Step 011690): Train loss 1.174, Val loss 1.788\n",
      "Ep 1 (Step 011695): Train loss 1.548, Val loss 1.788\n",
      "Ep 1 (Step 011700): Train loss 1.295, Val loss 1.787\n",
      "Ep 1 (Step 011705): Train loss 1.284, Val loss 1.784\n",
      "Ep 1 (Step 011710): Train loss 1.291, Val loss 1.781\n",
      "Ep 1 (Step 011715): Train loss 1.116, Val loss 1.778\n",
      "Ep 1 (Step 011720): Train loss 1.278, Val loss 1.777\n",
      "Ep 1 (Step 011725): Train loss 1.269, Val loss 1.776\n",
      "Ep 1 (Step 011730): Train loss 1.545, Val loss 1.776\n",
      "Ep 1 (Step 011735): Train loss 1.193, Val loss 1.776\n",
      "Ep 1 (Step 011740): Train loss 1.284, Val loss 1.776\n",
      "Ep 1 (Step 011745): Train loss 1.304, Val loss 1.775\n",
      "Ep 1 (Step 011750): Train loss 1.025, Val loss 1.776\n",
      "Ep 1 (Step 011755): Train loss 1.328, Val loss 1.774\n",
      "Ep 1 (Step 011760): Train loss 1.117, Val loss 1.772\n",
      "Ep 1 (Step 011765): Train loss 1.143, Val loss 1.772\n",
      "Ep 1 (Step 011770): Train loss 1.234, Val loss 1.773\n",
      "Ep 1 (Step 011775): Train loss 1.252, Val loss 1.776\n",
      "Ep 1 (Step 011780): Train loss 1.460, Val loss 1.779\n",
      "Ep 1 (Step 011785): Train loss 1.176, Val loss 1.781\n",
      "Ep 1 (Step 011790): Train loss 1.173, Val loss 1.782\n",
      "Ep 1 (Step 011795): Train loss 1.268, Val loss 1.783\n",
      "Ep 1 (Step 011800): Train loss 1.247, Val loss 1.784\n",
      "Ep 1 (Step 011805): Train loss 1.259, Val loss 1.783\n",
      "Ep 1 (Step 011810): Train loss 1.201, Val loss 1.783\n",
      "Ep 1 (Step 011815): Train loss 1.209, Val loss 1.781\n",
      "Ep 1 (Step 011820): Train loss 1.359, Val loss 1.781\n",
      "Ep 1 (Step 011825): Train loss 1.233, Val loss 1.782\n",
      "Ep 1 (Step 011830): Train loss 1.408, Val loss 1.781\n",
      "Ep 1 (Step 011835): Train loss 1.297, Val loss 1.779\n",
      "Ep 1 (Step 011840): Train loss 1.181, Val loss 1.777\n",
      "Ep 1 (Step 011845): Train loss 1.171, Val loss 1.775\n",
      "Ep 1 (Step 011850): Train loss 1.149, Val loss 1.774\n",
      "Ep 1 (Step 011855): Train loss 1.262, Val loss 1.774\n",
      "Ep 1 (Step 011860): Train loss 1.257, Val loss 1.774\n",
      "Ep 1 (Step 011865): Train loss 1.148, Val loss 1.774\n",
      "Ep 1 (Step 011870): Train loss 1.312, Val loss 1.775\n",
      "Ep 1 (Step 011875): Train loss 1.107, Val loss 1.777\n",
      "Ep 1 (Step 011880): Train loss 1.347, Val loss 1.781\n",
      "Ep 1 (Step 011885): Train loss 1.248, Val loss 1.785\n",
      "Ep 1 (Step 011890): Train loss 1.125, Val loss 1.787\n",
      "Ep 1 (Step 011895): Train loss 1.222, Val loss 1.787\n",
      "Ep 1 (Step 011900): Train loss 1.160, Val loss 1.784\n",
      "Ep 1 (Step 011905): Train loss 1.193, Val loss 1.780\n",
      "Ep 1 (Step 011910): Train loss 1.224, Val loss 1.779\n",
      "Ep 1 (Step 011915): Train loss 1.199, Val loss 1.778\n",
      "Ep 1 (Step 011920): Train loss 1.255, Val loss 1.780\n",
      "Ep 1 (Step 011925): Train loss 1.402, Val loss 1.782\n",
      "Ep 1 (Step 011930): Train loss 1.329, Val loss 1.782\n",
      "Ep 1 (Step 011935): Train loss 1.262, Val loss 1.781\n",
      "Ep 1 (Step 011940): Train loss 1.155, Val loss 1.782\n",
      "Ep 1 (Step 011945): Train loss 1.072, Val loss 1.781\n",
      "Ep 1 (Step 011950): Train loss 1.409, Val loss 1.781\n",
      "Ep 1 (Step 011955): Train loss 1.128, Val loss 1.781\n",
      "Ep 1 (Step 011960): Train loss 1.137, Val loss 1.780\n",
      "Ep 1 (Step 011965): Train loss 1.234, Val loss 1.779\n",
      "Ep 1 (Step 011970): Train loss 1.293, Val loss 1.779\n",
      "Ep 1 (Step 011975): Train loss 1.242, Val loss 1.778\n",
      "Ep 1 (Step 011980): Train loss 1.082, Val loss 1.775\n",
      "Ep 1 (Step 011985): Train loss 1.257, Val loss 1.773\n",
      "Ep 1 (Step 011990): Train loss 1.082, Val loss 1.773\n",
      "Ep 1 (Step 011995): Train loss 1.228, Val loss 1.776\n",
      "Ep 1 (Step 012000): Train loss 1.015, Val loss 1.778\n",
      "Ep 1 (Step 012005): Train loss 1.306, Val loss 1.777\n",
      "Ep 1 (Step 012010): Train loss 1.398, Val loss 1.779\n",
      "Ep 1 (Step 012015): Train loss 1.047, Val loss 1.779\n",
      "Ep 1 (Step 012020): Train loss 1.064, Val loss 1.779\n",
      "Ep 1 (Step 012025): Train loss 1.070, Val loss 1.779\n",
      "Ep 1 (Step 012030): Train loss 1.100, Val loss 1.779\n",
      "Ep 1 (Step 012035): Train loss 1.129, Val loss 1.778\n",
      "Ep 1 (Step 012040): Train loss 1.240, Val loss 1.779\n",
      "Ep 1 (Step 012045): Train loss 1.435, Val loss 1.780\n",
      "Ep 1 (Step 012050): Train loss 1.370, Val loss 1.781\n",
      "Ep 1 (Step 012055): Train loss 1.148, Val loss 1.781\n",
      "Ep 1 (Step 012060): Train loss 1.211, Val loss 1.779\n",
      "Ep 1 (Step 012065): Train loss 1.327, Val loss 1.777\n",
      "Ep 1 (Step 012070): Train loss 1.312, Val loss 1.775\n",
      "Ep 1 (Step 012075): Train loss 1.298, Val loss 1.775\n",
      "Ep 1 (Step 012080): Train loss 1.271, Val loss 1.774\n",
      "Ep 1 (Step 012085): Train loss 1.357, Val loss 1.773\n",
      "Ep 1 (Step 012090): Train loss 1.100, Val loss 1.774\n",
      "Ep 1 (Step 012095): Train loss 1.223, Val loss 1.774\n",
      "Ep 1 (Step 012100): Train loss 1.172, Val loss 1.775\n",
      "Ep 1 (Step 012105): Train loss 1.468, Val loss 1.775\n",
      "Ep 1 (Step 012110): Train loss 1.165, Val loss 1.775\n",
      "Ep 1 (Step 012115): Train loss 1.303, Val loss 1.776\n",
      "Ep 1 (Step 012120): Train loss 1.322, Val loss 1.777\n",
      "Ep 1 (Step 012125): Train loss 1.463, Val loss 1.776\n",
      "Ep 1 (Step 012130): Train loss 1.221, Val loss 1.775\n",
      "Ep 1 (Step 012135): Train loss 1.165, Val loss 1.775\n",
      "Ep 1 (Step 012140): Train loss 1.456, Val loss 1.774\n",
      "Ep 1 (Step 012145): Train loss 1.381, Val loss 1.775\n",
      "Ep 1 (Step 012150): Train loss 1.356, Val loss 1.774\n",
      "Ep 1 (Step 012155): Train loss 1.330, Val loss 1.774\n",
      "Ep 1 (Step 012160): Train loss 1.306, Val loss 1.774\n",
      "Ep 1 (Step 012165): Train loss 1.094, Val loss 1.774\n",
      "Ep 1 (Step 012170): Train loss 1.373, Val loss 1.774\n",
      "Ep 1 (Step 012175): Train loss 1.211, Val loss 1.775\n",
      "Ep 1 (Step 012180): Train loss 1.075, Val loss 1.775\n",
      "Ep 1 (Step 012185): Train loss 1.242, Val loss 1.775\n",
      "Ep 1 (Step 012190): Train loss 0.855, Val loss 1.777\n",
      "Ep 1 (Step 012195): Train loss 0.971, Val loss 1.777\n",
      "Ep 1 (Step 012200): Train loss 1.068, Val loss 1.777\n",
      "Ep 1 (Step 012205): Train loss 1.426, Val loss 1.775\n",
      "Ep 1 (Step 012210): Train loss 1.218, Val loss 1.774\n",
      "Ep 1 (Step 012215): Train loss 1.173, Val loss 1.774\n",
      "Ep 1 (Step 012220): Train loss 1.034, Val loss 1.774\n",
      "Ep 1 (Step 012225): Train loss 1.053, Val loss 1.774\n",
      "Ep 1 (Step 012230): Train loss 1.197, Val loss 1.771\n",
      "Ep 1 (Step 012235): Train loss 1.092, Val loss 1.771\n",
      "Ep 1 (Step 012240): Train loss 1.384, Val loss 1.772\n",
      "Ep 1 (Step 012245): Train loss 1.306, Val loss 1.775\n",
      "Ep 1 (Step 012250): Train loss 1.223, Val loss 1.777\n",
      "Ep 1 (Step 012255): Train loss 1.150, Val loss 1.776\n",
      "Ep 1 (Step 012260): Train loss 1.005, Val loss 1.776\n",
      "Ep 1 (Step 012265): Train loss 1.429, Val loss 1.776\n",
      "Ep 1 (Step 012270): Train loss 1.203, Val loss 1.776\n",
      "Ep 1 (Step 012275): Train loss 1.100, Val loss 1.776\n",
      "Ep 1 (Step 012280): Train loss 1.031, Val loss 1.774\n",
      "Ep 1 (Step 012285): Train loss 1.358, Val loss 1.772\n",
      "Ep 1 (Step 012290): Train loss 1.190, Val loss 1.771\n",
      "Ep 1 (Step 012295): Train loss 1.334, Val loss 1.770\n",
      "Ep 1 (Step 012300): Train loss 1.206, Val loss 1.770\n",
      "Ep 1 (Step 012305): Train loss 1.102, Val loss 1.771\n",
      "Ep 1 (Step 012310): Train loss 1.118, Val loss 1.773\n",
      "Ep 1 (Step 012315): Train loss 1.288, Val loss 1.774\n",
      "Ep 1 (Step 012320): Train loss 1.366, Val loss 1.776\n",
      "Ep 1 (Step 012325): Train loss 1.143, Val loss 1.778\n",
      "Ep 1 (Step 012330): Train loss 1.328, Val loss 1.777\n",
      "Ep 1 (Step 012335): Train loss 1.292, Val loss 1.778\n",
      "Ep 1 (Step 012340): Train loss 1.353, Val loss 1.780\n",
      "Ep 1 (Step 012345): Train loss 1.022, Val loss 1.782\n",
      "Ep 1 (Step 012350): Train loss 1.093, Val loss 1.784\n",
      "Ep 1 (Step 012355): Train loss 1.003, Val loss 1.785\n",
      "Ep 1 (Step 012360): Train loss 1.178, Val loss 1.787\n",
      "Ep 1 (Step 012365): Train loss 1.284, Val loss 1.788\n",
      "Ep 1 (Step 012370): Train loss 1.136, Val loss 1.787\n",
      "Ep 1 (Step 012375): Train loss 0.875, Val loss 1.785\n",
      "Ep 1 (Step 012380): Train loss 1.132, Val loss 1.784\n",
      "Ep 1 (Step 012385): Train loss 1.128, Val loss 1.785\n",
      "Ep 1 (Step 012390): Train loss 1.065, Val loss 1.786\n",
      "Ep 1 (Step 012395): Train loss 1.061, Val loss 1.788\n",
      "Ep 1 (Step 012400): Train loss 1.327, Val loss 1.791\n",
      "Ep 1 (Step 012405): Train loss 1.256, Val loss 1.794\n",
      "Ep 1 (Step 012410): Train loss 1.334, Val loss 1.794\n",
      "Ep 1 (Step 012415): Train loss 1.171, Val loss 1.794\n",
      "Ep 1 (Step 012420): Train loss 1.250, Val loss 1.791\n",
      "Ep 1 (Step 012425): Train loss 1.200, Val loss 1.791\n",
      "Ep 1 (Step 012430): Train loss 1.327, Val loss 1.791\n",
      "Ep 1 (Step 012435): Train loss 1.112, Val loss 1.791\n",
      "Ep 1 (Step 012440): Train loss 1.231, Val loss 1.793\n",
      "Ep 1 (Step 012445): Train loss 1.370, Val loss 1.792\n",
      "Ep 1 (Step 012450): Train loss 1.287, Val loss 1.792\n",
      "Ep 1 (Step 012455): Train loss 1.088, Val loss 1.790\n",
      "Ep 1 (Step 012460): Train loss 1.192, Val loss 1.788\n",
      "Ep 1 (Step 012465): Train loss 1.515, Val loss 1.786\n",
      "Ep 1 (Step 012470): Train loss 1.220, Val loss 1.785\n",
      "Ep 1 (Step 012475): Train loss 1.671, Val loss 1.784\n",
      "Ep 1 (Step 012480): Train loss 1.207, Val loss 1.784\n",
      "Ep 1 (Step 012485): Train loss 1.524, Val loss 1.787\n",
      "Ep 1 (Step 012490): Train loss 1.296, Val loss 1.791\n",
      "Ep 1 (Step 012495): Train loss 1.337, Val loss 1.792\n",
      "Ep 1 (Step 012500): Train loss 1.247, Val loss 1.793\n",
      "Ep 1 (Step 012505): Train loss 1.184, Val loss 1.791\n",
      "Ep 1 (Step 012510): Train loss 1.110, Val loss 1.791\n",
      "Ep 1 (Step 012515): Train loss 1.349, Val loss 1.790\n",
      "Ep 1 (Step 012520): Train loss 1.178, Val loss 1.788\n",
      "Ep 1 (Step 012525): Train loss 1.306, Val loss 1.787\n",
      "Ep 1 (Step 012530): Train loss 1.145, Val loss 1.787\n",
      "Ep 1 (Step 012535): Train loss 1.096, Val loss 1.786\n",
      "Ep 1 (Step 012540): Train loss 1.325, Val loss 1.786\n",
      "Ep 1 (Step 012545): Train loss 1.275, Val loss 1.783\n",
      "Ep 1 (Step 012550): Train loss 1.000, Val loss 1.782\n",
      "Ep 1 (Step 012555): Train loss 1.469, Val loss 1.780\n",
      "Ep 1 (Step 012560): Train loss 0.994, Val loss 1.778\n",
      "Ep 1 (Step 012565): Train loss 1.072, Val loss 1.779\n",
      "Ep 1 (Step 012570): Train loss 1.100, Val loss 1.781\n",
      "Ep 1 (Step 012575): Train loss 1.201, Val loss 1.782\n",
      "Ep 1 (Step 012580): Train loss 1.253, Val loss 1.785\n",
      "Ep 1 (Step 012585): Train loss 1.084, Val loss 1.789\n",
      "Ep 1 (Step 012590): Train loss 1.323, Val loss 1.790\n",
      "Ep 1 (Step 012595): Train loss 1.487, Val loss 1.792\n",
      "Ep 1 (Step 012600): Train loss 1.071, Val loss 1.792\n",
      "Ep 1 (Step 012605): Train loss 1.386, Val loss 1.792\n",
      "Ep 1 (Step 012610): Train loss 0.948, Val loss 1.790\n",
      "Ep 1 (Step 012615): Train loss 1.330, Val loss 1.788\n",
      "Ep 1 (Step 012620): Train loss 1.500, Val loss 1.787\n",
      "Ep 1 (Step 012625): Train loss 1.472, Val loss 1.788\n",
      "Ep 1 (Step 012630): Train loss 1.205, Val loss 1.790\n",
      "Ep 1 (Step 012635): Train loss 1.437, Val loss 1.792\n",
      "Ep 1 (Step 012640): Train loss 1.282, Val loss 1.793\n",
      "Ep 1 (Step 012645): Train loss 1.219, Val loss 1.794\n",
      "Ep 1 (Step 012650): Train loss 1.038, Val loss 1.790\n",
      "Ep 1 (Step 012655): Train loss 1.239, Val loss 1.787\n",
      "Ep 1 (Step 012660): Train loss 1.074, Val loss 1.787\n",
      "Ep 1 (Step 012665): Train loss 1.358, Val loss 1.787\n",
      "Ep 1 (Step 012670): Train loss 1.309, Val loss 1.787\n",
      "Ep 1 (Step 012675): Train loss 1.323, Val loss 1.786\n",
      "Ep 1 (Step 012680): Train loss 1.176, Val loss 1.785\n",
      "Ep 1 (Step 012685): Train loss 0.992, Val loss 1.786\n",
      "Ep 1 (Step 012690): Train loss 1.588, Val loss 1.786\n",
      "Ep 1 (Step 012695): Train loss 1.516, Val loss 1.783\n",
      "Ep 1 (Step 012700): Train loss 1.189, Val loss 1.780\n",
      "Ep 1 (Step 012705): Train loss 1.133, Val loss 1.781\n",
      "Ep 1 (Step 012710): Train loss 1.181, Val loss 1.782\n",
      "Ep 1 (Step 012715): Train loss 1.186, Val loss 1.784\n",
      "Ep 1 (Step 012720): Train loss 1.212, Val loss 1.786\n",
      "Ep 1 (Step 012725): Train loss 1.476, Val loss 1.789\n",
      "Ep 1 (Step 012730): Train loss 1.598, Val loss 1.790\n",
      "Ep 1 (Step 012735): Train loss 1.163, Val loss 1.790\n",
      "Ep 1 (Step 012740): Train loss 1.465, Val loss 1.789\n",
      "Ep 1 (Step 012745): Train loss 1.183, Val loss 1.788\n",
      "Ep 1 (Step 012750): Train loss 1.173, Val loss 1.787\n",
      "Ep 1 (Step 012755): Train loss 1.414, Val loss 1.788\n",
      "Ep 1 (Step 012760): Train loss 1.275, Val loss 1.787\n",
      "Ep 1 (Step 012765): Train loss 1.218, Val loss 1.786\n",
      "Ep 1 (Step 012770): Train loss 1.091, Val loss 1.786\n",
      "Ep 1 (Step 012775): Train loss 1.486, Val loss 1.789\n",
      "Ep 1 (Step 012780): Train loss 1.047, Val loss 1.792\n",
      "Ep 1 (Step 012785): Train loss 1.103, Val loss 1.794\n",
      "Ep 1 (Step 012790): Train loss 1.121, Val loss 1.795\n",
      "Ep 1 (Step 012795): Train loss 1.323, Val loss 1.795\n",
      "Ep 1 (Step 012800): Train loss 1.139, Val loss 1.792\n",
      "Ep 1 (Step 012805): Train loss 1.200, Val loss 1.785\n",
      "Ep 1 (Step 012810): Train loss 1.274, Val loss 1.781\n",
      "Ep 1 (Step 012815): Train loss 1.414, Val loss 1.779\n",
      "Ep 1 (Step 012820): Train loss 1.360, Val loss 1.780\n",
      "Ep 1 (Step 012825): Train loss 1.237, Val loss 1.780\n",
      "Ep 1 (Step 012830): Train loss 1.311, Val loss 1.780\n",
      "Ep 1 (Step 012835): Train loss 1.364, Val loss 1.781\n",
      "Ep 1 (Step 012840): Train loss 1.180, Val loss 1.781\n",
      "Ep 1 (Step 012845): Train loss 1.114, Val loss 1.781\n",
      "Ep 1 (Step 012850): Train loss 1.330, Val loss 1.782\n",
      "Ep 1 (Step 012855): Train loss 1.283, Val loss 1.783\n",
      "Ep 1 (Step 012860): Train loss 1.121, Val loss 1.784\n",
      "Ep 1 (Step 012865): Train loss 1.262, Val loss 1.786\n",
      "Ep 1 (Step 012870): Train loss 1.167, Val loss 1.788\n",
      "Ep 1 (Step 012875): Train loss 1.491, Val loss 1.789\n",
      "Ep 1 (Step 012880): Train loss 1.105, Val loss 1.789\n",
      "Ep 1 (Step 012885): Train loss 1.083, Val loss 1.791\n",
      "Ep 1 (Step 012890): Train loss 1.251, Val loss 1.793\n",
      "Ep 1 (Step 012895): Train loss 1.278, Val loss 1.793\n",
      "Ep 1 (Step 012900): Train loss 1.343, Val loss 1.790\n",
      "Ep 1 (Step 012905): Train loss 1.176, Val loss 1.788\n",
      "Ep 1 (Step 012910): Train loss 0.860, Val loss 1.785\n",
      "Ep 1 (Step 012915): Train loss 1.298, Val loss 1.784\n",
      "Ep 1 (Step 012920): Train loss 1.205, Val loss 1.781\n",
      "Ep 1 (Step 012925): Train loss 1.244, Val loss 1.779\n",
      "Ep 1 (Step 012930): Train loss 1.091, Val loss 1.779\n",
      "Ep 1 (Step 012935): Train loss 1.163, Val loss 1.779\n",
      "Ep 1 (Step 012940): Train loss 1.236, Val loss 1.781\n",
      "Ep 1 (Step 012945): Train loss 1.265, Val loss 1.782\n",
      "Ep 1 (Step 012950): Train loss 0.920, Val loss 1.781\n",
      "Ep 1 (Step 012955): Train loss 1.352, Val loss 1.780\n",
      "Ep 1 (Step 012960): Train loss 1.303, Val loss 1.778\n",
      "Ep 1 (Step 012965): Train loss 1.265, Val loss 1.776\n",
      "Ep 1 (Step 012970): Train loss 1.242, Val loss 1.774\n",
      "Ep 1 (Step 012975): Train loss 1.130, Val loss 1.773\n",
      "Ep 1 (Step 012980): Train loss 0.955, Val loss 1.773\n",
      "Ep 1 (Step 012985): Train loss 1.398, Val loss 1.773\n",
      "Ep 1 (Step 012990): Train loss 1.190, Val loss 1.774\n",
      "Ep 1 (Step 012995): Train loss 1.396, Val loss 1.775\n",
      "Ep 1 (Step 013000): Train loss 1.224, Val loss 1.774\n",
      "Ep 1 (Step 013005): Train loss 1.062, Val loss 1.773\n",
      "Ep 1 (Step 013010): Train loss 0.975, Val loss 1.774\n",
      "Ep 1 (Step 013015): Train loss 1.144, Val loss 1.774\n",
      "Ep 1 (Step 013020): Train loss 1.321, Val loss 1.776\n",
      "Ep 1 (Step 013025): Train loss 1.394, Val loss 1.779\n",
      "Ep 1 (Step 013030): Train loss 1.193, Val loss 1.782\n",
      "Ep 1 (Step 013035): Train loss 1.280, Val loss 1.782\n",
      "Ep 1 (Step 013040): Train loss 1.189, Val loss 1.782\n",
      "Ep 1 (Step 013045): Train loss 1.242, Val loss 1.781\n",
      "Ep 1 (Step 013050): Train loss 1.030, Val loss 1.780\n",
      "Ep 1 (Step 013055): Train loss 1.145, Val loss 1.780\n",
      "Ep 1 (Step 013060): Train loss 1.280, Val loss 1.780\n",
      "Ep 1 (Step 013065): Train loss 1.237, Val loss 1.782\n",
      "Ep 1 (Step 013070): Train loss 1.464, Val loss 1.782\n",
      "Ep 1 (Step 013075): Train loss 1.324, Val loss 1.783\n",
      "Ep 1 (Step 013080): Train loss 1.280, Val loss 1.783\n",
      "Ep 1 (Step 013085): Train loss 1.026, Val loss 1.783\n",
      "Ep 1 (Step 013090): Train loss 0.956, Val loss 1.785\n",
      "Ep 1 (Step 013095): Train loss 1.486, Val loss 1.784\n",
      "Ep 1 (Step 013100): Train loss 1.270, Val loss 1.784\n",
      "Ep 1 (Step 013105): Train loss 1.122, Val loss 1.783\n",
      "Ep 1 (Step 013110): Train loss 1.364, Val loss 1.781\n",
      "Ep 1 (Step 013115): Train loss 1.024, Val loss 1.778\n",
      "Ep 1 (Step 013120): Train loss 1.286, Val loss 1.776\n",
      "Ep 1 (Step 013125): Train loss 1.251, Val loss 1.774\n",
      "Ep 1 (Step 013130): Train loss 1.159, Val loss 1.772\n",
      "Ep 1 (Step 013135): Train loss 1.263, Val loss 1.771\n",
      "Ep 1 (Step 013140): Train loss 1.302, Val loss 1.770\n",
      "Ep 1 (Step 013145): Train loss 1.268, Val loss 1.770\n",
      "Ep 1 (Step 013150): Train loss 1.484, Val loss 1.769\n",
      "Ep 1 (Step 013155): Train loss 1.420, Val loss 1.770\n",
      "Ep 1 (Step 013160): Train loss 1.416, Val loss 1.770\n",
      "Ep 1 (Step 013165): Train loss 0.981, Val loss 1.773\n",
      "Ep 1 (Step 013170): Train loss 1.556, Val loss 1.776\n",
      "Ep 1 (Step 013175): Train loss 1.430, Val loss 1.777\n",
      "Ep 1 (Step 013180): Train loss 1.048, Val loss 1.777\n",
      "Ep 1 (Step 013185): Train loss 1.379, Val loss 1.777\n",
      "Ep 1 (Step 013190): Train loss 1.375, Val loss 1.776\n",
      "Ep 1 (Step 013195): Train loss 1.400, Val loss 1.776\n",
      "Ep 1 (Step 013200): Train loss 1.152, Val loss 1.774\n",
      "Ep 1 (Step 013205): Train loss 1.223, Val loss 1.772\n",
      "Ep 1 (Step 013210): Train loss 1.146, Val loss 1.772\n",
      "Ep 1 (Step 013215): Train loss 1.078, Val loss 1.773\n",
      "Ep 1 (Step 013220): Train loss 1.391, Val loss 1.773\n",
      "Ep 1 (Step 013225): Train loss 1.267, Val loss 1.771\n",
      "Ep 1 (Step 013230): Train loss 1.096, Val loss 1.770\n",
      "Ep 1 (Step 013235): Train loss 0.897, Val loss 1.771\n",
      "Ep 1 (Step 013240): Train loss 1.280, Val loss 1.771\n",
      "Ep 1 (Step 013245): Train loss 0.961, Val loss 1.774\n",
      "Ep 1 (Step 013250): Train loss 1.261, Val loss 1.775\n",
      "Ep 1 (Step 013255): Train loss 1.391, Val loss 1.776\n",
      "Ep 1 (Step 013260): Train loss 0.920, Val loss 1.776\n",
      "Ep 1 (Step 013265): Train loss 1.361, Val loss 1.777\n",
      "Ep 1 (Step 013270): Train loss 1.496, Val loss 1.777\n",
      "Ep 1 (Step 013275): Train loss 1.096, Val loss 1.776\n",
      "Ep 1 (Step 013280): Train loss 1.133, Val loss 1.775\n",
      "Ep 1 (Step 013285): Train loss 1.388, Val loss 1.776\n",
      "Ep 1 (Step 013290): Train loss 1.268, Val loss 1.776\n",
      "Ep 1 (Step 013295): Train loss 1.065, Val loss 1.776\n",
      "Ep 1 (Step 013300): Train loss 1.171, Val loss 1.777\n",
      "Ep 1 (Step 013305): Train loss 1.116, Val loss 1.779\n",
      "Ep 1 (Step 013310): Train loss 1.193, Val loss 1.779\n",
      "Ep 1 (Step 013315): Train loss 1.092, Val loss 1.777\n",
      "Ep 1 (Step 013320): Train loss 1.517, Val loss 1.776\n",
      "Ep 1 (Step 013325): Train loss 1.105, Val loss 1.775\n",
      "Ep 1 (Step 013330): Train loss 1.136, Val loss 1.774\n",
      "Ep 1 (Step 013335): Train loss 0.978, Val loss 1.774\n",
      "Ep 1 (Step 013340): Train loss 1.481, Val loss 1.775\n",
      "Ep 1 (Step 013345): Train loss 1.065, Val loss 1.778\n",
      "Ep 1 (Step 013350): Train loss 1.214, Val loss 1.778\n",
      "Ep 1 (Step 013355): Train loss 1.407, Val loss 1.776\n",
      "Ep 1 (Step 013360): Train loss 1.052, Val loss 1.776\n",
      "Ep 1 (Step 013365): Train loss 1.558, Val loss 1.776\n",
      "Ep 1 (Step 013370): Train loss 1.236, Val loss 1.776\n",
      "Ep 1 (Step 013375): Train loss 1.191, Val loss 1.776\n",
      "Ep 1 (Step 013380): Train loss 1.052, Val loss 1.773\n",
      "Ep 1 (Step 013385): Train loss 1.388, Val loss 1.771\n",
      "Ep 1 (Step 013390): Train loss 1.427, Val loss 1.768\n",
      "Ep 1 (Step 013395): Train loss 1.188, Val loss 1.766\n",
      "Ep 1 (Step 013400): Train loss 0.907, Val loss 1.766\n",
      "Ep 1 (Step 013405): Train loss 1.199, Val loss 1.767\n",
      "Ep 1 (Step 013410): Train loss 1.349, Val loss 1.770\n",
      "Ep 1 (Step 013415): Train loss 1.043, Val loss 1.771\n",
      "Ep 1 (Step 013420): Train loss 1.299, Val loss 1.771\n",
      "Ep 1 (Step 013425): Train loss 1.502, Val loss 1.770\n",
      "Ep 1 (Step 013430): Train loss 1.236, Val loss 1.772\n",
      "Ep 1 (Step 013435): Train loss 1.223, Val loss 1.771\n",
      "Ep 1 (Step 013440): Train loss 1.026, Val loss 1.770\n",
      "Ep 1 (Step 013445): Train loss 1.280, Val loss 1.769\n",
      "Ep 1 (Step 013450): Train loss 1.303, Val loss 1.767\n",
      "Ep 1 (Step 013455): Train loss 1.156, Val loss 1.766\n",
      "Ep 1 (Step 013460): Train loss 0.924, Val loss 1.768\n",
      "Ep 1 (Step 013465): Train loss 1.541, Val loss 1.769\n",
      "Ep 1 (Step 013470): Train loss 0.868, Val loss 1.771\n",
      "Ep 1 (Step 013475): Train loss 1.443, Val loss 1.771\n",
      "Ep 1 (Step 013480): Train loss 1.142, Val loss 1.770\n",
      "Ep 1 (Step 013485): Train loss 1.238, Val loss 1.769\n",
      "Ep 1 (Step 013490): Train loss 1.326, Val loss 1.767\n",
      "Ep 1 (Step 013495): Train loss 1.237, Val loss 1.765\n",
      "Ep 1 (Step 013500): Train loss 1.182, Val loss 1.764\n",
      "Ep 1 (Step 013505): Train loss 1.280, Val loss 1.766\n",
      "Ep 1 (Step 013510): Train loss 1.719, Val loss 1.768\n",
      "Ep 1 (Step 013515): Train loss 1.195, Val loss 1.770\n",
      "Ep 1 (Step 013520): Train loss 1.419, Val loss 1.771\n",
      "Ep 1 (Step 013525): Train loss 1.258, Val loss 1.772\n",
      "Ep 1 (Step 013530): Train loss 1.753, Val loss 1.773\n",
      "Ep 1 (Step 013535): Train loss 1.083, Val loss 1.775\n",
      "Ep 1 (Step 013540): Train loss 1.055, Val loss 1.778\n",
      "Ep 1 (Step 013545): Train loss 1.201, Val loss 1.780\n",
      "Ep 1 (Step 013550): Train loss 1.476, Val loss 1.781\n",
      "Ep 1 (Step 013555): Train loss 1.293, Val loss 1.783\n",
      "Ep 1 (Step 013560): Train loss 1.014, Val loss 1.783\n",
      "Ep 1 (Step 013565): Train loss 1.270, Val loss 1.783\n",
      "Ep 1 (Step 013570): Train loss 1.244, Val loss 1.782\n",
      "Ep 1 (Step 013575): Train loss 0.971, Val loss 1.781\n",
      "Ep 1 (Step 013580): Train loss 1.265, Val loss 1.781\n",
      "Ep 1 (Step 013585): Train loss 1.341, Val loss 1.780\n",
      "Ep 1 (Step 013590): Train loss 1.133, Val loss 1.781\n",
      "Ep 1 (Step 013595): Train loss 1.255, Val loss 1.780\n",
      "Ep 1 (Step 013600): Train loss 1.181, Val loss 1.780\n",
      "Ep 1 (Step 013605): Train loss 1.261, Val loss 1.782\n",
      "Ep 1 (Step 013610): Train loss 1.246, Val loss 1.785\n",
      "Ep 1 (Step 013615): Train loss 1.334, Val loss 1.790\n",
      "Ep 1 (Step 013620): Train loss 1.231, Val loss 1.792\n",
      "Ep 1 (Step 013625): Train loss 1.010, Val loss 1.793\n",
      "Ep 1 (Step 013630): Train loss 1.410, Val loss 1.793\n",
      "Ep 1 (Step 013635): Train loss 1.050, Val loss 1.789\n",
      "Ep 1 (Step 013640): Train loss 1.420, Val loss 1.785\n",
      "Ep 1 (Step 013645): Train loss 1.035, Val loss 1.780\n",
      "Ep 1 (Step 013650): Train loss 1.191, Val loss 1.777\n",
      "Ep 1 (Step 013655): Train loss 1.194, Val loss 1.776\n",
      "Ep 1 (Step 013660): Train loss 1.196, Val loss 1.776\n",
      "Ep 1 (Step 013665): Train loss 1.091, Val loss 1.777\n",
      "Ep 1 (Step 013670): Train loss 1.255, Val loss 1.778\n",
      "Ep 1 (Step 013675): Train loss 1.400, Val loss 1.779\n",
      "Ep 1 (Step 013680): Train loss 1.320, Val loss 1.778\n",
      "Ep 1 (Step 013685): Train loss 1.153, Val loss 1.782\n",
      "Ep 1 (Step 013690): Train loss 1.516, Val loss 1.784\n",
      "Ep 1 (Step 013695): Train loss 1.360, Val loss 1.783\n",
      "Ep 1 (Step 013700): Train loss 1.158, Val loss 1.782\n",
      "Ep 1 (Step 013705): Train loss 1.278, Val loss 1.779\n",
      "Ep 1 (Step 013710): Train loss 1.011, Val loss 1.774\n",
      "Ep 1 (Step 013715): Train loss 1.278, Val loss 1.772\n",
      "Ep 1 (Step 013720): Train loss 1.391, Val loss 1.770\n",
      "Ep 1 (Step 013725): Train loss 1.055, Val loss 1.767\n",
      "Ep 1 (Step 013730): Train loss 1.204, Val loss 1.765\n",
      "Ep 1 (Step 013735): Train loss 1.348, Val loss 1.764\n",
      "Ep 1 (Step 013740): Train loss 1.016, Val loss 1.764\n",
      "Ep 1 (Step 013745): Train loss 1.277, Val loss 1.765\n",
      "Ep 1 (Step 013750): Train loss 1.440, Val loss 1.766\n",
      "Ep 1 (Step 013755): Train loss 1.125, Val loss 1.769\n",
      "Ep 1 (Step 013760): Train loss 1.432, Val loss 1.772\n",
      "Ep 1 (Step 013765): Train loss 0.918, Val loss 1.772\n",
      "Ep 1 (Step 013770): Train loss 1.163, Val loss 1.770\n",
      "Ep 1 (Step 013775): Train loss 1.166, Val loss 1.769\n",
      "Ep 1 (Step 013780): Train loss 1.258, Val loss 1.769\n",
      "Ep 1 (Step 013785): Train loss 1.363, Val loss 1.768\n",
      "Ep 1 (Step 013790): Train loss 1.204, Val loss 1.767\n",
      "Ep 1 (Step 013795): Train loss 1.262, Val loss 1.765\n",
      "Ep 1 (Step 013800): Train loss 1.532, Val loss 1.763\n",
      "Ep 1 (Step 013805): Train loss 1.181, Val loss 1.763\n",
      "Ep 1 (Step 013810): Train loss 1.339, Val loss 1.762\n",
      "Ep 1 (Step 013815): Train loss 1.419, Val loss 1.761\n",
      "Ep 1 (Step 013820): Train loss 1.224, Val loss 1.762\n",
      "Ep 1 (Step 013825): Train loss 1.162, Val loss 1.766\n",
      "Ep 1 (Step 013830): Train loss 1.405, Val loss 1.769\n",
      "Ep 1 (Step 013835): Train loss 1.227, Val loss 1.769\n",
      "Ep 1 (Step 013840): Train loss 1.184, Val loss 1.766\n",
      "Ep 1 (Step 013845): Train loss 1.328, Val loss 1.764\n",
      "Ep 1 (Step 013850): Train loss 0.863, Val loss 1.760\n",
      "Ep 1 (Step 013855): Train loss 1.026, Val loss 1.759\n",
      "Ep 1 (Step 013860): Train loss 1.527, Val loss 1.759\n",
      "Ep 1 (Step 013865): Train loss 1.222, Val loss 1.759\n",
      "Ep 1 (Step 013870): Train loss 0.911, Val loss 1.762\n",
      "Ep 1 (Step 013875): Train loss 1.329, Val loss 1.766\n",
      "Ep 1 (Step 013880): Train loss 1.157, Val loss 1.769\n",
      "Ep 1 (Step 013885): Train loss 1.489, Val loss 1.770\n",
      "Ep 1 (Step 013890): Train loss 1.501, Val loss 1.771\n",
      "Ep 1 (Step 013895): Train loss 1.077, Val loss 1.772\n",
      "Ep 1 (Step 013900): Train loss 1.080, Val loss 1.771\n",
      "Ep 1 (Step 013905): Train loss 1.291, Val loss 1.771\n",
      "Ep 1 (Step 013910): Train loss 1.297, Val loss 1.770\n",
      "Ep 1 (Step 013915): Train loss 1.237, Val loss 1.772\n",
      "Ep 1 (Step 013920): Train loss 1.355, Val loss 1.774\n",
      "Ep 1 (Step 013925): Train loss 0.976, Val loss 1.774\n",
      "Ep 1 (Step 013930): Train loss 1.003, Val loss 1.773\n",
      "Ep 1 (Step 013935): Train loss 1.320, Val loss 1.773\n",
      "Ep 1 (Step 013940): Train loss 1.044, Val loss 1.773\n",
      "Ep 1 (Step 013945): Train loss 1.313, Val loss 1.771\n",
      "Ep 1 (Step 013950): Train loss 0.977, Val loss 1.770\n",
      "Ep 1 (Step 013955): Train loss 1.160, Val loss 1.770\n",
      "Ep 1 (Step 013960): Train loss 1.207, Val loss 1.772\n",
      "Ep 1 (Step 013965): Train loss 1.042, Val loss 1.773\n",
      "Ep 1 (Step 013970): Train loss 1.273, Val loss 1.774\n",
      "Ep 1 (Step 013975): Train loss 1.187, Val loss 1.776\n",
      "Ep 1 (Step 013980): Train loss 1.518, Val loss 1.776\n",
      "Ep 1 (Step 013985): Train loss 1.273, Val loss 1.777\n",
      "Ep 1 (Step 013990): Train loss 0.985, Val loss 1.777\n",
      "Ep 1 (Step 013995): Train loss 1.050, Val loss 1.775\n",
      "Ep 1 (Step 014000): Train loss 0.979, Val loss 1.773\n",
      "Ep 1 (Step 014005): Train loss 1.629, Val loss 1.773\n",
      "Ep 1 (Step 014010): Train loss 1.081, Val loss 1.771\n",
      "Ep 1 (Step 014015): Train loss 1.134, Val loss 1.771\n",
      "Ep 1 (Step 014020): Train loss 1.075, Val loss 1.771\n",
      "Ep 1 (Step 014025): Train loss 1.263, Val loss 1.772\n",
      "Ep 1 (Step 014030): Train loss 1.011, Val loss 1.773\n",
      "Ep 1 (Step 014035): Train loss 1.412, Val loss 1.773\n",
      "Ep 1 (Step 014040): Train loss 1.349, Val loss 1.775\n",
      "Ep 1 (Step 014045): Train loss 1.469, Val loss 1.774\n",
      "Ep 1 (Step 014050): Train loss 1.080, Val loss 1.771\n",
      "Ep 1 (Step 014055): Train loss 1.249, Val loss 1.771\n",
      "Ep 1 (Step 014060): Train loss 1.592, Val loss 1.771\n",
      "Ep 1 (Step 014065): Train loss 1.173, Val loss 1.772\n",
      "Ep 1 (Step 014070): Train loss 1.205, Val loss 1.773\n",
      "Ep 1 (Step 014075): Train loss 1.269, Val loss 1.773\n",
      "Ep 1 (Step 014080): Train loss 1.473, Val loss 1.774\n",
      "Ep 1 (Step 014085): Train loss 0.999, Val loss 1.776\n",
      "Ep 1 (Step 014090): Train loss 1.034, Val loss 1.778\n",
      "Ep 1 (Step 014095): Train loss 1.179, Val loss 1.779\n",
      "Ep 1 (Step 014100): Train loss 1.417, Val loss 1.779\n",
      "Ep 1 (Step 014105): Train loss 1.379, Val loss 1.777\n",
      "Ep 1 (Step 014110): Train loss 1.381, Val loss 1.776\n",
      "Ep 1 (Step 014115): Train loss 1.136, Val loss 1.776\n",
      "Ep 1 (Step 014120): Train loss 1.174, Val loss 1.775\n",
      "Ep 1 (Step 014125): Train loss 1.190, Val loss 1.775\n",
      "Ep 1 (Step 014130): Train loss 1.151, Val loss 1.776\n",
      "Ep 1 (Step 014135): Train loss 1.496, Val loss 1.776\n",
      "Ep 1 (Step 014140): Train loss 1.297, Val loss 1.776\n",
      "Ep 1 (Step 014145): Train loss 1.271, Val loss 1.774\n",
      "Ep 1 (Step 014150): Train loss 1.383, Val loss 1.772\n",
      "Ep 1 (Step 014155): Train loss 1.364, Val loss 1.771\n",
      "Ep 1 (Step 014160): Train loss 1.162, Val loss 1.771\n",
      "Ep 1 (Step 014165): Train loss 1.151, Val loss 1.770\n",
      "Ep 1 (Step 014170): Train loss 1.219, Val loss 1.771\n",
      "Ep 1 (Step 014175): Train loss 0.998, Val loss 1.771\n",
      "Ep 1 (Step 014180): Train loss 1.184, Val loss 1.770\n",
      "Ep 1 (Step 014185): Train loss 1.096, Val loss 1.768\n",
      "Ep 1 (Step 014190): Train loss 1.017, Val loss 1.767\n",
      "Ep 1 (Step 014195): Train loss 1.197, Val loss 1.767\n",
      "Ep 1 (Step 014200): Train loss 1.148, Val loss 1.767\n",
      "Ep 1 (Step 014205): Train loss 1.622, Val loss 1.766\n",
      "Ep 1 (Step 014210): Train loss 1.473, Val loss 1.765\n",
      "Ep 1 (Step 014215): Train loss 1.098, Val loss 1.764\n",
      "Ep 1 (Step 014220): Train loss 1.076, Val loss 1.764\n",
      "Ep 1 (Step 014225): Train loss 1.107, Val loss 1.766\n",
      "Ep 1 (Step 014230): Train loss 1.417, Val loss 1.767\n",
      "Ep 1 (Step 014235): Train loss 1.533, Val loss 1.768\n",
      "Ep 1 (Step 014240): Train loss 0.999, Val loss 1.768\n",
      "Ep 1 (Step 014245): Train loss 1.090, Val loss 1.769\n",
      "Ep 1 (Step 014250): Train loss 1.143, Val loss 1.768\n",
      "Ep 1 (Step 014255): Train loss 0.912, Val loss 1.770\n",
      "Ep 1 (Step 014260): Train loss 1.176, Val loss 1.769\n",
      "Ep 1 (Step 014265): Train loss 1.303, Val loss 1.770\n",
      "Ep 1 (Step 014270): Train loss 1.337, Val loss 1.771\n",
      "Ep 1 (Step 014275): Train loss 0.918, Val loss 1.772\n",
      "Ep 1 (Step 014280): Train loss 1.320, Val loss 1.772\n",
      "Ep 1 (Step 014285): Train loss 1.217, Val loss 1.774\n",
      "Ep 1 (Step 014290): Train loss 1.468, Val loss 1.775\n",
      "Ep 1 (Step 014295): Train loss 1.027, Val loss 1.775\n",
      "Ep 1 (Step 014300): Train loss 1.258, Val loss 1.775\n",
      "Ep 1 (Step 014305): Train loss 1.128, Val loss 1.774\n",
      "Ep 1 (Step 014310): Train loss 1.237, Val loss 1.777\n",
      "Ep 1 (Step 014315): Train loss 1.524, Val loss 1.778\n",
      "Ep 1 (Step 014320): Train loss 1.315, Val loss 1.778\n",
      "Ep 1 (Step 014325): Train loss 1.064, Val loss 1.776\n",
      "Ep 1 (Step 014330): Train loss 1.052, Val loss 1.776\n",
      "Ep 1 (Step 014335): Train loss 1.247, Val loss 1.774\n",
      "Ep 1 (Step 014340): Train loss 1.105, Val loss 1.773\n",
      "Ep 1 (Step 014345): Train loss 1.209, Val loss 1.773\n",
      "Ep 1 (Step 014350): Train loss 1.053, Val loss 1.772\n",
      "Ep 1 (Step 014355): Train loss 1.369, Val loss 1.770\n",
      "Ep 1 (Step 014360): Train loss 1.451, Val loss 1.769\n",
      "Ep 1 (Step 014365): Train loss 1.190, Val loss 1.771\n",
      "Ep 1 (Step 014370): Train loss 1.460, Val loss 1.773\n",
      "Ep 1 (Step 014375): Train loss 1.294, Val loss 1.774\n",
      "Ep 1 (Step 014380): Train loss 1.096, Val loss 1.774\n",
      "Ep 1 (Step 014385): Train loss 1.248, Val loss 1.773\n",
      "Ep 1 (Step 014390): Train loss 1.178, Val loss 1.773\n",
      "Ep 1 (Step 014395): Train loss 1.135, Val loss 1.773\n",
      "Ep 1 (Step 014400): Train loss 1.244, Val loss 1.773\n",
      "Ep 1 (Step 014405): Train loss 1.192, Val loss 1.775\n",
      "Ep 1 (Step 014410): Train loss 1.615, Val loss 1.773\n",
      "Ep 1 (Step 014415): Train loss 0.933, Val loss 1.772\n",
      "Ep 1 (Step 014420): Train loss 1.314, Val loss 1.773\n",
      "Ep 1 (Step 014425): Train loss 1.189, Val loss 1.774\n",
      "Ep 1 (Step 014430): Train loss 1.274, Val loss 1.774\n",
      "Ep 1 (Step 014435): Train loss 0.969, Val loss 1.773\n",
      "Ep 1 (Step 014440): Train loss 1.413, Val loss 1.773\n",
      "Ep 1 (Step 014445): Train loss 1.289, Val loss 1.773\n",
      "Ep 1 (Step 014450): Train loss 1.092, Val loss 1.774\n",
      "Ep 1 (Step 014455): Train loss 1.277, Val loss 1.774\n",
      "Ep 1 (Step 014460): Train loss 1.297, Val loss 1.774\n",
      "Ep 1 (Step 014465): Train loss 1.274, Val loss 1.772\n",
      "Ep 1 (Step 014470): Train loss 1.162, Val loss 1.771\n",
      "Ep 1 (Step 014475): Train loss 1.080, Val loss 1.770\n",
      "Ep 1 (Step 014480): Train loss 1.551, Val loss 1.770\n",
      "Ep 1 (Step 014485): Train loss 1.287, Val loss 1.771\n",
      "Ep 1 (Step 014490): Train loss 1.238, Val loss 1.772\n",
      "Ep 1 (Step 014495): Train loss 1.521, Val loss 1.773\n",
      "Ep 1 (Step 014500): Train loss 1.041, Val loss 1.773\n",
      "Ep 1 (Step 014505): Train loss 1.027, Val loss 1.775\n",
      "Ep 1 (Step 014510): Train loss 1.159, Val loss 1.778\n",
      "Ep 1 (Step 014515): Train loss 1.290, Val loss 1.779\n",
      "Ep 1 (Step 014520): Train loss 1.266, Val loss 1.780\n",
      "Ep 1 (Step 014525): Train loss 1.365, Val loss 1.781\n",
      "Ep 1 (Step 014530): Train loss 1.222, Val loss 1.780\n",
      "Ep 1 (Step 014535): Train loss 1.233, Val loss 1.780\n",
      "Ep 1 (Step 014540): Train loss 1.286, Val loss 1.779\n",
      "Ep 1 (Step 014545): Train loss 1.125, Val loss 1.778\n",
      "Ep 1 (Step 014550): Train loss 1.215, Val loss 1.778\n",
      "Ep 1 (Step 014555): Train loss 1.097, Val loss 1.779\n",
      "Ep 1 (Step 014560): Train loss 0.982, Val loss 1.781\n",
      "Ep 1 (Step 014565): Train loss 1.392, Val loss 1.780\n",
      "Ep 1 (Step 014570): Train loss 1.437, Val loss 1.779\n",
      "Ep 1 (Step 014575): Train loss 1.524, Val loss 1.777\n",
      "Ep 1 (Step 014580): Train loss 1.193, Val loss 1.776\n",
      "Ep 1 (Step 014585): Train loss 1.177, Val loss 1.777\n",
      "Ep 1 (Step 014590): Train loss 1.199, Val loss 1.777\n",
      "Ep 1 (Step 014595): Train loss 1.113, Val loss 1.775\n",
      "Ep 1 (Step 014600): Train loss 1.077, Val loss 1.772\n",
      "Ep 1 (Step 014605): Train loss 1.273, Val loss 1.770\n",
      "Ep 1 (Step 014610): Train loss 0.973, Val loss 1.771\n",
      "Ep 1 (Step 014615): Train loss 1.364, Val loss 1.770\n",
      "Ep 1 (Step 014620): Train loss 0.907, Val loss 1.770\n",
      "Ep 1 (Step 014625): Train loss 1.174, Val loss 1.771\n",
      "Ep 1 (Step 014630): Train loss 1.416, Val loss 1.773\n",
      "Ep 1 (Step 014635): Train loss 1.209, Val loss 1.778\n",
      "Ep 1 (Step 014640): Train loss 1.286, Val loss 1.781\n",
      "Ep 1 (Step 014645): Train loss 1.437, Val loss 1.783\n",
      "Ep 1 (Step 014650): Train loss 1.387, Val loss 1.783\n",
      "Ep 1 (Step 014655): Train loss 1.203, Val loss 1.784\n",
      "Ep 1 (Step 014660): Train loss 1.200, Val loss 1.784\n",
      "Ep 1 (Step 014665): Train loss 1.145, Val loss 1.783\n",
      "Ep 1 (Step 014670): Train loss 1.213, Val loss 1.781\n",
      "Ep 1 (Step 014675): Train loss 1.052, Val loss 1.781\n",
      "Ep 1 (Step 014680): Train loss 1.237, Val loss 1.781\n",
      "Ep 1 (Step 014685): Train loss 1.594, Val loss 1.782\n",
      "Ep 1 (Step 014690): Train loss 1.324, Val loss 1.781\n",
      "Ep 1 (Step 014695): Train loss 1.078, Val loss 1.781\n",
      "Ep 1 (Step 014700): Train loss 1.007, Val loss 1.780\n",
      "Ep 1 (Step 014705): Train loss 1.240, Val loss 1.778\n",
      "Ep 1 (Step 014710): Train loss 0.936, Val loss 1.776\n",
      "Ep 1 (Step 014715): Train loss 1.185, Val loss 1.774\n",
      "Ep 1 (Step 014720): Train loss 1.023, Val loss 1.773\n",
      "Ep 1 (Step 014725): Train loss 1.205, Val loss 1.771\n",
      "Ep 1 (Step 014730): Train loss 1.314, Val loss 1.771\n",
      "Ep 1 (Step 014735): Train loss 1.118, Val loss 1.774\n",
      "Ep 1 (Step 014740): Train loss 1.291, Val loss 1.775\n",
      "Ep 1 (Step 014745): Train loss 1.340, Val loss 1.776\n",
      "Ep 1 (Step 014750): Train loss 1.201, Val loss 1.777\n",
      "Ep 1 (Step 014755): Train loss 1.099, Val loss 1.778\n",
      "Ep 1 (Step 014760): Train loss 1.458, Val loss 1.779\n",
      "Ep 1 (Step 014765): Train loss 1.031, Val loss 1.778\n",
      "Ep 1 (Step 014770): Train loss 1.178, Val loss 1.778\n",
      "Ep 1 (Step 014775): Train loss 1.009, Val loss 1.777\n",
      "Ep 1 (Step 014780): Train loss 1.050, Val loss 1.776\n",
      "Ep 1 (Step 014785): Train loss 0.908, Val loss 1.773\n",
      "Ep 1 (Step 014790): Train loss 1.078, Val loss 1.772\n",
      "Ep 1 (Step 014795): Train loss 1.180, Val loss 1.770\n",
      "Ep 1 (Step 014800): Train loss 1.411, Val loss 1.768\n",
      "Ep 1 (Step 014805): Train loss 1.356, Val loss 1.767\n",
      "Ep 1 (Step 014810): Train loss 1.038, Val loss 1.767\n",
      "Ep 1 (Step 014815): Train loss 1.443, Val loss 1.766\n",
      "Ep 1 (Step 014820): Train loss 0.918, Val loss 1.767\n",
      "Ep 1 (Step 014825): Train loss 1.350, Val loss 1.768\n",
      "Ep 1 (Step 014830): Train loss 1.371, Val loss 1.768\n",
      "Ep 1 (Step 014835): Train loss 1.419, Val loss 1.769\n",
      "Ep 1 (Step 014840): Train loss 1.362, Val loss 1.771\n",
      "Ep 1 (Step 014845): Train loss 1.152, Val loss 1.772\n",
      "Ep 1 (Step 014850): Train loss 1.081, Val loss 1.772\n",
      "Ep 1 (Step 014855): Train loss 1.162, Val loss 1.771\n",
      "Ep 1 (Step 014860): Train loss 1.285, Val loss 1.772\n",
      "Ep 1 (Step 014865): Train loss 1.093, Val loss 1.772\n",
      "Ep 1 (Step 014870): Train loss 1.247, Val loss 1.771\n",
      "Ep 1 (Step 014875): Train loss 1.221, Val loss 1.771\n",
      "Ep 1 (Step 014880): Train loss 1.172, Val loss 1.771\n",
      "Ep 1 (Step 014885): Train loss 1.016, Val loss 1.772\n",
      "Ep 1 (Step 014890): Train loss 0.932, Val loss 1.773\n",
      "Ep 1 (Step 014895): Train loss 1.035, Val loss 1.773\n",
      "Ep 1 (Step 014900): Train loss 1.181, Val loss 1.773\n",
      "Ep 1 (Step 014905): Train loss 1.082, Val loss 1.773\n",
      "Ep 1 (Step 014910): Train loss 1.278, Val loss 1.774\n",
      "Ep 1 (Step 014915): Train loss 0.918, Val loss 1.776\n",
      "Ep 1 (Step 014920): Train loss 1.362, Val loss 1.777\n",
      "Ep 1 (Step 014925): Train loss 1.393, Val loss 1.779\n",
      "Ep 1 (Step 014930): Train loss 1.070, Val loss 1.781\n",
      "Ep 1 (Step 014935): Train loss 1.513, Val loss 1.781\n",
      "Ep 1 (Step 014940): Train loss 0.902, Val loss 1.780\n",
      "Ep 1 (Step 014945): Train loss 1.020, Val loss 1.779\n",
      "Ep 1 (Step 014950): Train loss 1.186, Val loss 1.780\n",
      "Ep 1 (Step 014955): Train loss 1.118, Val loss 1.781\n",
      "Ep 1 (Step 014960): Train loss 1.198, Val loss 1.782\n",
      "Ep 1 (Step 014965): Train loss 0.841, Val loss 1.784\n",
      "Ep 1 (Step 014970): Train loss 1.247, Val loss 1.785\n",
      "Ep 1 (Step 014975): Train loss 1.345, Val loss 1.785\n",
      "Ep 1 (Step 014980): Train loss 1.546, Val loss 1.785\n",
      "Ep 1 (Step 014985): Train loss 1.110, Val loss 1.785\n",
      "Ep 1 (Step 014990): Train loss 1.218, Val loss 1.786\n",
      "Ep 1 (Step 014995): Train loss 0.995, Val loss 1.787\n",
      "Ep 1 (Step 015000): Train loss 1.239, Val loss 1.788\n",
      "Ep 1 (Step 015005): Train loss 1.515, Val loss 1.785\n",
      "Ep 1 (Step 015010): Train loss 1.335, Val loss 1.785\n",
      "Ep 1 (Step 015015): Train loss 1.386, Val loss 1.785\n",
      "Ep 1 (Step 015020): Train loss 1.279, Val loss 1.785\n",
      "Ep 1 (Step 015025): Train loss 1.074, Val loss 1.786\n",
      "Ep 1 (Step 015030): Train loss 1.058, Val loss 1.786\n",
      "Ep 1 (Step 015035): Train loss 1.508, Val loss 1.788\n",
      "Ep 1 (Step 015040): Train loss 0.959, Val loss 1.789\n",
      "Ep 1 (Step 015045): Train loss 1.129, Val loss 1.788\n",
      "Ep 1 (Step 015050): Train loss 1.182, Val loss 1.789\n",
      "Ep 1 (Step 015055): Train loss 1.266, Val loss 1.788\n",
      "Ep 1 (Step 015060): Train loss 1.386, Val loss 1.788\n",
      "Ep 1 (Step 015065): Train loss 1.114, Val loss 1.787\n",
      "Ep 1 (Step 015070): Train loss 1.358, Val loss 1.787\n",
      "Ep 1 (Step 015075): Train loss 1.281, Val loss 1.787\n",
      "Ep 1 (Step 015080): Train loss 1.209, Val loss 1.789\n",
      "Ep 1 (Step 015085): Train loss 1.145, Val loss 1.787\n",
      "Ep 1 (Step 015090): Train loss 1.067, Val loss 1.787\n",
      "Ep 1 (Step 015095): Train loss 1.019, Val loss 1.787\n",
      "Ep 1 (Step 015100): Train loss 1.231, Val loss 1.787\n",
      "Ep 1 (Step 015105): Train loss 1.101, Val loss 1.789\n",
      "Ep 1 (Step 015110): Train loss 1.171, Val loss 1.791\n",
      "Ep 1 (Step 015115): Train loss 1.244, Val loss 1.793\n",
      "Ep 1 (Step 015120): Train loss 1.343, Val loss 1.792\n",
      "Ep 1 (Step 015125): Train loss 1.173, Val loss 1.791\n",
      "Ep 1 (Step 015130): Train loss 1.198, Val loss 1.789\n",
      "Ep 1 (Step 015135): Train loss 1.185, Val loss 1.788\n",
      "Ep 1 (Step 015140): Train loss 1.378, Val loss 1.789\n",
      "Ep 1 (Step 015145): Train loss 1.219, Val loss 1.791\n",
      "Ep 1 (Step 015150): Train loss 1.165, Val loss 1.794\n",
      "Ep 1 (Step 015155): Train loss 1.264, Val loss 1.795\n",
      "Ep 1 (Step 015160): Train loss 1.263, Val loss 1.795\n",
      "Ep 1 (Step 015165): Train loss 0.960, Val loss 1.795\n",
      "Ep 1 (Step 015170): Train loss 1.182, Val loss 1.793\n",
      "Ep 1 (Step 015175): Train loss 0.945, Val loss 1.791\n",
      "Ep 1 (Step 015180): Train loss 0.801, Val loss 1.789\n",
      "Ep 1 (Step 015185): Train loss 1.074, Val loss 1.787\n",
      "Ep 1 (Step 015190): Train loss 1.414, Val loss 1.786\n",
      "Ep 1 (Step 015195): Train loss 1.235, Val loss 1.785\n",
      "Ep 1 (Step 015200): Train loss 1.070, Val loss 1.785\n",
      "Ep 1 (Step 015205): Train loss 0.985, Val loss 1.785\n",
      "Ep 1 (Step 015210): Train loss 1.161, Val loss 1.784\n",
      "Ep 1 (Step 015215): Train loss 1.199, Val loss 1.784\n",
      "Ep 1 (Step 015220): Train loss 1.281, Val loss 1.784\n",
      "Ep 1 (Step 015225): Train loss 1.275, Val loss 1.785\n",
      "Ep 1 (Step 015230): Train loss 1.223, Val loss 1.785\n",
      "Ep 1 (Step 015235): Train loss 1.285, Val loss 1.787\n",
      "Ep 1 (Step 015240): Train loss 1.059, Val loss 1.789\n",
      "Ep 1 (Step 015245): Train loss 1.119, Val loss 1.789\n",
      "Ep 1 (Step 015250): Train loss 1.103, Val loss 1.786\n",
      "Ep 1 (Step 015255): Train loss 1.167, Val loss 1.783\n",
      "Ep 1 (Step 015260): Train loss 1.270, Val loss 1.781\n",
      "Ep 1 (Step 015265): Train loss 1.317, Val loss 1.780\n",
      "Ep 1 (Step 015270): Train loss 1.021, Val loss 1.779\n",
      "Ep 1 (Step 015275): Train loss 1.159, Val loss 1.780\n",
      "Ep 1 (Step 015280): Train loss 1.299, Val loss 1.780\n",
      "Ep 1 (Step 015285): Train loss 1.201, Val loss 1.780\n",
      "Ep 1 (Step 015290): Train loss 1.114, Val loss 1.780\n",
      "Ep 1 (Step 015295): Train loss 1.204, Val loss 1.779\n",
      "Ep 1 (Step 015300): Train loss 1.111, Val loss 1.778\n",
      "Ep 1 (Step 015305): Train loss 1.120, Val loss 1.779\n",
      "Ep 1 (Step 015310): Train loss 1.189, Val loss 1.779\n",
      "Ep 1 (Step 015315): Train loss 1.262, Val loss 1.780\n",
      "Ep 1 (Step 015320): Train loss 1.690, Val loss 1.782\n",
      "Ep 1 (Step 015325): Train loss 1.095, Val loss 1.782\n",
      "Ep 1 (Step 015330): Train loss 1.119, Val loss 1.782\n",
      "Ep 1 (Step 015335): Train loss 1.158, Val loss 1.782\n",
      "Ep 1 (Step 015340): Train loss 1.014, Val loss 1.781\n",
      "Ep 1 (Step 015345): Train loss 1.604, Val loss 1.781\n",
      "Ep 1 (Step 015350): Train loss 1.195, Val loss 1.780\n",
      "Ep 1 (Step 015355): Train loss 1.033, Val loss 1.779\n",
      "Ep 1 (Step 015360): Train loss 1.285, Val loss 1.777\n",
      "Ep 1 (Step 015365): Train loss 1.073, Val loss 1.775\n",
      "Ep 1 (Step 015370): Train loss 1.061, Val loss 1.775\n",
      "Ep 1 (Step 015375): Train loss 1.191, Val loss 1.776\n",
      "Ep 1 (Step 015380): Train loss 1.345, Val loss 1.776\n",
      "Ep 1 (Step 015385): Train loss 1.589, Val loss 1.778\n",
      "Ep 1 (Step 015390): Train loss 0.986, Val loss 1.781\n",
      "Ep 1 (Step 015395): Train loss 0.993, Val loss 1.785\n",
      "Ep 1 (Step 015400): Train loss 1.224, Val loss 1.788\n",
      "Ep 1 (Step 015405): Train loss 1.152, Val loss 1.789\n",
      "Ep 1 (Step 015410): Train loss 1.260, Val loss 1.789\n",
      "Ep 1 (Step 015415): Train loss 1.432, Val loss 1.789\n",
      "Ep 1 (Step 015420): Train loss 1.115, Val loss 1.790\n",
      "Ep 1 (Step 015425): Train loss 1.305, Val loss 1.792\n",
      "Ep 1 (Step 015430): Train loss 1.280, Val loss 1.795\n",
      "Ep 1 (Step 015435): Train loss 1.414, Val loss 1.796\n",
      "Ep 1 (Step 015440): Train loss 1.276, Val loss 1.795\n",
      "Ep 1 (Step 015445): Train loss 1.070, Val loss 1.792\n",
      "Ep 1 (Step 015450): Train loss 1.256, Val loss 1.789\n",
      "Ep 1 (Step 015455): Train loss 1.244, Val loss 1.787\n",
      "Ep 1 (Step 015460): Train loss 1.344, Val loss 1.784\n",
      "Ep 1 (Step 015465): Train loss 1.268, Val loss 1.782\n",
      "Ep 1 (Step 015470): Train loss 1.331, Val loss 1.782\n",
      "Ep 1 (Step 015475): Train loss 1.331, Val loss 1.781\n",
      "Ep 1 (Step 015480): Train loss 1.331, Val loss 1.783\n",
      "Ep 1 (Step 015485): Train loss 1.152, Val loss 1.786\n",
      "Ep 1 (Step 015490): Train loss 1.114, Val loss 1.788\n",
      "Ep 1 (Step 015495): Train loss 1.463, Val loss 1.788\n",
      "Ep 1 (Step 015500): Train loss 1.162, Val loss 1.788\n",
      "Ep 1 (Step 015505): Train loss 1.333, Val loss 1.786\n",
      "Ep 1 (Step 015510): Train loss 1.024, Val loss 1.784\n",
      "Ep 1 (Step 015515): Train loss 1.298, Val loss 1.784\n",
      "Ep 1 (Step 015520): Train loss 1.344, Val loss 1.783\n",
      "Ep 1 (Step 015525): Train loss 1.138, Val loss 1.782\n",
      "Ep 1 (Step 015530): Train loss 1.152, Val loss 1.784\n",
      "Ep 1 (Step 015535): Train loss 0.967, Val loss 1.787\n",
      "Ep 1 (Step 015540): Train loss 1.141, Val loss 1.789\n",
      "Ep 1 (Step 015545): Train loss 0.923, Val loss 1.791\n",
      "Ep 1 (Step 015550): Train loss 1.291, Val loss 1.793\n",
      "Ep 1 (Step 015555): Train loss 1.226, Val loss 1.794\n",
      "Ep 1 (Step 015560): Train loss 1.465, Val loss 1.793\n",
      "Ep 1 (Step 015565): Train loss 1.236, Val loss 1.791\n",
      "Ep 1 (Step 015570): Train loss 1.217, Val loss 1.790\n",
      "Ep 1 (Step 015575): Train loss 1.021, Val loss 1.788\n",
      "Ep 1 (Step 015580): Train loss 1.153, Val loss 1.787\n",
      "Ep 1 (Step 015585): Train loss 1.396, Val loss 1.785\n",
      "Ep 1 (Step 015590): Train loss 0.962, Val loss 1.783\n",
      "Ep 1 (Step 015595): Train loss 1.201, Val loss 1.779\n",
      "Ep 1 (Step 015600): Train loss 1.147, Val loss 1.774\n",
      "Ep 1 (Step 015605): Train loss 1.257, Val loss 1.773\n",
      "Ep 1 (Step 015610): Train loss 1.104, Val loss 1.776\n",
      "Ep 1 (Step 015615): Train loss 1.349, Val loss 1.780\n",
      "Ep 1 (Step 015620): Train loss 0.948, Val loss 1.783\n",
      "Ep 1 (Step 015625): Train loss 1.004, Val loss 1.784\n",
      "Ep 1 (Step 015630): Train loss 1.147, Val loss 1.782\n",
      "Ep 1 (Step 015635): Train loss 1.278, Val loss 1.781\n",
      "Ep 1 (Step 015640): Train loss 1.408, Val loss 1.779\n",
      "Ep 1 (Step 015645): Train loss 1.097, Val loss 1.778\n",
      "Ep 1 (Step 015650): Train loss 1.077, Val loss 1.777\n",
      "Ep 1 (Step 015655): Train loss 1.286, Val loss 1.776\n",
      "Ep 1 (Step 015660): Train loss 1.078, Val loss 1.774\n",
      "Ep 1 (Step 015665): Train loss 1.064, Val loss 1.772\n",
      "Ep 1 (Step 015670): Train loss 0.994, Val loss 1.773\n",
      "Ep 1 (Step 015675): Train loss 1.390, Val loss 1.773\n",
      "Ep 1 (Step 015680): Train loss 1.045, Val loss 1.774\n",
      "Ep 1 (Step 015685): Train loss 1.225, Val loss 1.776\n",
      "Ep 1 (Step 015690): Train loss 1.180, Val loss 1.778\n",
      "Ep 1 (Step 015695): Train loss 1.238, Val loss 1.781\n",
      "Ep 1 (Step 015700): Train loss 1.258, Val loss 1.784\n",
      "Ep 1 (Step 015705): Train loss 0.802, Val loss 1.782\n",
      "Ep 1 (Step 015710): Train loss 1.161, Val loss 1.782\n",
      "Ep 1 (Step 015715): Train loss 1.409, Val loss 1.781\n",
      "Ep 1 (Step 015720): Train loss 1.210, Val loss 1.782\n",
      "Ep 1 (Step 015725): Train loss 1.241, Val loss 1.782\n",
      "Ep 1 (Step 015730): Train loss 1.128, Val loss 1.783\n",
      "Ep 1 (Step 015735): Train loss 1.222, Val loss 1.783\n",
      "Ep 1 (Step 015740): Train loss 1.157, Val loss 1.782\n",
      "Ep 1 (Step 015745): Train loss 1.124, Val loss 1.780\n",
      "Ep 1 (Step 015750): Train loss 1.106, Val loss 1.779\n",
      "Ep 1 (Step 015755): Train loss 1.191, Val loss 1.778\n",
      "Ep 1 (Step 015760): Train loss 1.223, Val loss 1.779\n",
      "Ep 1 (Step 015765): Train loss 1.367, Val loss 1.780\n",
      "Ep 1 (Step 015770): Train loss 1.391, Val loss 1.780\n",
      "Ep 1 (Step 015775): Train loss 1.176, Val loss 1.781\n",
      "Ep 1 (Step 015780): Train loss 1.387, Val loss 1.781\n",
      "Ep 1 (Step 015785): Train loss 1.495, Val loss 1.776\n",
      "Ep 1 (Step 015790): Train loss 1.234, Val loss 1.773\n",
      "Ep 1 (Step 015795): Train loss 1.078, Val loss 1.770\n",
      "Ep 1 (Step 015800): Train loss 1.201, Val loss 1.768\n",
      "Ep 1 (Step 015805): Train loss 1.315, Val loss 1.767\n",
      "Ep 1 (Step 015810): Train loss 1.358, Val loss 1.768\n",
      "Ep 1 (Step 015815): Train loss 1.140, Val loss 1.769\n",
      "Ep 1 (Step 015820): Train loss 1.475, Val loss 1.770\n",
      "Ep 1 (Step 015825): Train loss 1.183, Val loss 1.772\n",
      "Ep 1 (Step 015830): Train loss 1.371, Val loss 1.776\n",
      "Ep 1 (Step 015835): Train loss 1.312, Val loss 1.778\n",
      "Ep 1 (Step 015840): Train loss 1.197, Val loss 1.779\n",
      "Ep 1 (Step 015845): Train loss 1.558, Val loss 1.779\n",
      "Ep 1 (Step 015850): Train loss 1.308, Val loss 1.779\n",
      "Ep 1 (Step 015855): Train loss 1.105, Val loss 1.780\n",
      "Ep 1 (Step 015860): Train loss 1.087, Val loss 1.781\n",
      "Ep 1 (Step 015865): Train loss 1.175, Val loss 1.782\n",
      "Ep 1 (Step 015870): Train loss 1.357, Val loss 1.783\n",
      "Ep 1 (Step 015875): Train loss 1.273, Val loss 1.784\n",
      "Ep 1 (Step 015880): Train loss 1.233, Val loss 1.785\n",
      "Ep 1 (Step 015885): Train loss 1.299, Val loss 1.785\n",
      "Ep 1 (Step 015890): Train loss 1.345, Val loss 1.785\n",
      "Ep 1 (Step 015895): Train loss 1.567, Val loss 1.783\n",
      "Ep 1 (Step 015900): Train loss 1.122, Val loss 1.783\n",
      "Ep 1 (Step 015905): Train loss 0.997, Val loss 1.782\n",
      "Ep 1 (Step 015910): Train loss 1.410, Val loss 1.783\n",
      "Ep 1 (Step 015915): Train loss 0.906, Val loss 1.783\n",
      "Ep 1 (Step 015920): Train loss 1.305, Val loss 1.785\n",
      "Ep 1 (Step 015925): Train loss 1.342, Val loss 1.788\n",
      "Ep 1 (Step 015930): Train loss 1.210, Val loss 1.791\n",
      "Ep 1 (Step 015935): Train loss 1.406, Val loss 1.792\n",
      "Ep 1 (Step 015940): Train loss 1.380, Val loss 1.793\n",
      "Ep 1 (Step 015945): Train loss 1.316, Val loss 1.794\n",
      "Ep 1 (Step 015950): Train loss 1.385, Val loss 1.793\n",
      "Ep 1 (Step 015955): Train loss 1.229, Val loss 1.791\n",
      "Ep 1 (Step 015960): Train loss 1.196, Val loss 1.789\n",
      "Ep 1 (Step 015965): Train loss 1.033, Val loss 1.788\n",
      "Ep 1 (Step 015970): Train loss 1.125, Val loss 1.787\n",
      "Ep 1 (Step 015975): Train loss 1.352, Val loss 1.784\n",
      "Ep 1 (Step 015980): Train loss 1.286, Val loss 1.780\n",
      "Ep 1 (Step 015985): Train loss 1.206, Val loss 1.777\n",
      "Ep 1 (Step 015990): Train loss 1.171, Val loss 1.775\n",
      "Ep 1 (Step 015995): Train loss 1.045, Val loss 1.772\n",
      "Ep 1 (Step 016000): Train loss 1.220, Val loss 1.771\n",
      "Ep 1 (Step 016005): Train loss 0.993, Val loss 1.771\n",
      "Ep 1 (Step 016010): Train loss 1.038, Val loss 1.772\n",
      "Ep 1 (Step 016015): Train loss 1.093, Val loss 1.773\n",
      "Ep 1 (Step 016020): Train loss 1.112, Val loss 1.773\n",
      "Ep 1 (Step 016025): Train loss 1.201, Val loss 1.775\n",
      "Ep 1 (Step 016030): Train loss 1.080, Val loss 1.773\n",
      "Ep 1 (Step 016035): Train loss 1.580, Val loss 1.771\n",
      "Ep 1 (Step 016040): Train loss 1.468, Val loss 1.770\n",
      "Ep 1 (Step 016045): Train loss 1.057, Val loss 1.768\n",
      "Ep 1 (Step 016050): Train loss 1.059, Val loss 1.767\n",
      "Ep 1 (Step 016055): Train loss 1.156, Val loss 1.767\n",
      "Ep 1 (Step 016060): Train loss 1.359, Val loss 1.767\n",
      "Ep 1 (Step 016065): Train loss 0.849, Val loss 1.769\n",
      "Ep 1 (Step 016070): Train loss 1.281, Val loss 1.771\n",
      "Ep 1 (Step 016075): Train loss 0.909, Val loss 1.772\n",
      "Ep 1 (Step 016080): Train loss 1.197, Val loss 1.773\n",
      "Ep 1 (Step 016085): Train loss 1.310, Val loss 1.774\n",
      "Ep 1 (Step 016090): Train loss 1.172, Val loss 1.774\n",
      "Ep 1 (Step 016095): Train loss 1.113, Val loss 1.773\n",
      "Ep 1 (Step 016100): Train loss 1.322, Val loss 1.772\n",
      "Ep 1 (Step 016105): Train loss 1.194, Val loss 1.771\n",
      "Ep 1 (Step 016110): Train loss 1.295, Val loss 1.770\n",
      "Ep 1 (Step 016115): Train loss 1.240, Val loss 1.770\n",
      "Ep 1 (Step 016120): Train loss 1.437, Val loss 1.771\n",
      "Ep 1 (Step 016125): Train loss 0.975, Val loss 1.771\n",
      "Ep 1 (Step 016130): Train loss 1.052, Val loss 1.773\n",
      "Ep 1 (Step 016135): Train loss 1.220, Val loss 1.772\n",
      "Ep 1 (Step 016140): Train loss 0.909, Val loss 1.772\n",
      "Ep 1 (Step 016145): Train loss 1.328, Val loss 1.771\n",
      "Ep 1 (Step 016150): Train loss 1.106, Val loss 1.771\n",
      "Ep 1 (Step 016155): Train loss 1.095, Val loss 1.767\n",
      "Ep 1 (Step 016160): Train loss 1.207, Val loss 1.765\n",
      "Ep 1 (Step 016165): Train loss 1.372, Val loss 1.765\n",
      "Ep 1 (Step 016170): Train loss 1.185, Val loss 1.766\n",
      "Ep 1 (Step 016175): Train loss 1.248, Val loss 1.768\n",
      "Ep 1 (Step 016180): Train loss 1.538, Val loss 1.770\n",
      "Ep 1 (Step 016185): Train loss 0.846, Val loss 1.772\n",
      "Ep 1 (Step 016190): Train loss 1.099, Val loss 1.775\n",
      "Ep 1 (Step 016195): Train loss 0.999, Val loss 1.775\n",
      "Ep 1 (Step 016200): Train loss 1.085, Val loss 1.774\n",
      "Ep 1 (Step 016205): Train loss 1.154, Val loss 1.773\n",
      "Ep 1 (Step 016210): Train loss 1.117, Val loss 1.773\n",
      "Ep 1 (Step 016215): Train loss 1.278, Val loss 1.774\n",
      "Ep 1 (Step 016220): Train loss 0.916, Val loss 1.776\n",
      "Ep 1 (Step 016225): Train loss 1.045, Val loss 1.777\n",
      "Ep 1 (Step 016230): Train loss 1.444, Val loss 1.779\n",
      "Ep 1 (Step 016235): Train loss 0.985, Val loss 1.782\n",
      "Ep 1 (Step 016240): Train loss 1.229, Val loss 1.784\n",
      "Ep 1 (Step 016245): Train loss 1.282, Val loss 1.785\n",
      "Ep 1 (Step 016250): Train loss 1.124, Val loss 1.785\n",
      "Ep 1 (Step 016255): Train loss 1.400, Val loss 1.782\n",
      "Ep 1 (Step 016260): Train loss 1.239, Val loss 1.778\n",
      "Ep 1 (Step 016265): Train loss 1.249, Val loss 1.776\n",
      "Ep 1 (Step 016270): Train loss 1.323, Val loss 1.775\n",
      "Ep 1 (Step 016275): Train loss 1.198, Val loss 1.773\n",
      "Ep 1 (Step 016280): Train loss 1.422, Val loss 1.773\n",
      "Ep 1 (Step 016285): Train loss 1.274, Val loss 1.774\n",
      "Ep 1 (Step 016290): Train loss 0.906, Val loss 1.774\n",
      "Ep 1 (Step 016295): Train loss 1.018, Val loss 1.776\n",
      "Ep 1 (Step 016300): Train loss 1.186, Val loss 1.776\n",
      "Ep 1 (Step 016305): Train loss 0.765, Val loss 1.776\n",
      "Ep 1 (Step 016310): Train loss 1.024, Val loss 1.775\n",
      "Ep 1 (Step 016315): Train loss 1.288, Val loss 1.775\n",
      "Ep 1 (Step 016320): Train loss 1.259, Val loss 1.776\n",
      "Ep 1 (Step 016325): Train loss 1.036, Val loss 1.777\n",
      "Ep 1 (Step 016330): Train loss 1.152, Val loss 1.778\n",
      "Ep 1 (Step 016335): Train loss 1.335, Val loss 1.777\n",
      "Ep 1 (Step 016340): Train loss 1.326, Val loss 1.776\n",
      "Ep 1 (Step 016345): Train loss 1.035, Val loss 1.776\n",
      "Ep 1 (Step 016350): Train loss 0.853, Val loss 1.776\n",
      "Ep 1 (Step 016355): Train loss 0.958, Val loss 1.775\n",
      "Ep 1 (Step 016360): Train loss 1.248, Val loss 1.773\n",
      "Ep 1 (Step 016365): Train loss 1.071, Val loss 1.770\n",
      "Ep 1 (Step 016370): Train loss 1.387, Val loss 1.768\n",
      "Ep 1 (Step 016375): Train loss 1.113, Val loss 1.767\n",
      "Ep 1 (Step 016380): Train loss 1.225, Val loss 1.766\n",
      "Ep 1 (Step 016385): Train loss 1.178, Val loss 1.766\n",
      "Ep 1 (Step 016390): Train loss 1.230, Val loss 1.766\n",
      "Ep 1 (Step 016395): Train loss 1.247, Val loss 1.767\n",
      "Ep 1 (Step 016400): Train loss 1.189, Val loss 1.768\n",
      "Ep 1 (Step 016405): Train loss 1.275, Val loss 1.769\n",
      "Ep 1 (Step 016410): Train loss 1.214, Val loss 1.770\n",
      "Ep 1 (Step 016415): Train loss 1.108, Val loss 1.770\n",
      "Ep 1 (Step 016420): Train loss 1.174, Val loss 1.769\n",
      "Ep 1 (Step 016425): Train loss 1.160, Val loss 1.766\n",
      "Ep 1 (Step 016430): Train loss 1.080, Val loss 1.766\n",
      "Ep 1 (Step 016435): Train loss 1.472, Val loss 1.767\n",
      "Ep 1 (Step 016440): Train loss 1.158, Val loss 1.769\n",
      "Ep 1 (Step 016445): Train loss 1.309, Val loss 1.771\n",
      "Ep 1 (Step 016450): Train loss 1.416, Val loss 1.772\n",
      "Ep 1 (Step 016455): Train loss 1.220, Val loss 1.771\n",
      "Ep 1 (Step 016460): Train loss 1.066, Val loss 1.770\n",
      "Ep 1 (Step 016465): Train loss 1.188, Val loss 1.769\n",
      "Ep 1 (Step 016470): Train loss 1.086, Val loss 1.768\n",
      "Ep 1 (Step 016475): Train loss 1.418, Val loss 1.768\n",
      "Ep 1 (Step 016480): Train loss 1.630, Val loss 1.768\n",
      "Ep 1 (Step 016485): Train loss 0.929, Val loss 1.767\n",
      "Ep 1 (Step 016490): Train loss 1.091, Val loss 1.768\n",
      "Ep 1 (Step 016495): Train loss 0.932, Val loss 1.771\n",
      "Ep 1 (Step 016500): Train loss 1.531, Val loss 1.773\n",
      "Ep 1 (Step 016505): Train loss 1.344, Val loss 1.774\n",
      "Ep 1 (Step 016510): Train loss 1.129, Val loss 1.773\n",
      "Ep 1 (Step 016515): Train loss 1.175, Val loss 1.772\n",
      "Ep 1 (Step 016520): Train loss 1.441, Val loss 1.772\n",
      "Ep 1 (Step 016525): Train loss 1.115, Val loss 1.770\n",
      "Ep 1 (Step 016530): Train loss 1.346, Val loss 1.770\n",
      "Ep 1 (Step 016535): Train loss 1.172, Val loss 1.771\n",
      "Ep 1 (Step 016540): Train loss 1.088, Val loss 1.772\n",
      "Ep 1 (Step 016545): Train loss 1.136, Val loss 1.774\n",
      "Ep 1 (Step 016550): Train loss 1.256, Val loss 1.776\n",
      "Ep 1 (Step 016555): Train loss 1.320, Val loss 1.777\n",
      "Ep 1 (Step 016560): Train loss 0.982, Val loss 1.777\n",
      "Ep 1 (Step 016565): Train loss 1.525, Val loss 1.777\n",
      "Ep 1 (Step 016570): Train loss 1.200, Val loss 1.778\n",
      "Ep 1 (Step 016575): Train loss 1.270, Val loss 1.779\n",
      "Ep 1 (Step 016580): Train loss 1.159, Val loss 1.779\n",
      "Ep 1 (Step 016585): Train loss 1.296, Val loss 1.776\n",
      "Ep 1 (Step 016590): Train loss 1.063, Val loss 1.775\n",
      "Ep 1 (Step 016595): Train loss 1.209, Val loss 1.774\n",
      "Ep 1 (Step 016600): Train loss 1.280, Val loss 1.773\n",
      "Ep 1 (Step 016605): Train loss 1.244, Val loss 1.771\n",
      "Ep 1 (Step 016610): Train loss 1.079, Val loss 1.770\n",
      "Ep 1 (Step 016615): Train loss 1.318, Val loss 1.772\n",
      "Ep 1 (Step 016620): Train loss 1.111, Val loss 1.773\n",
      "Ep 1 (Step 016625): Train loss 1.089, Val loss 1.773\n",
      "Ep 1 (Step 016630): Train loss 1.077, Val loss 1.773\n",
      "Ep 1 (Step 016635): Train loss 1.426, Val loss 1.773\n",
      "Ep 1 (Step 016640): Train loss 1.334, Val loss 1.771\n",
      "Ep 1 (Step 016645): Train loss 1.348, Val loss 1.769\n",
      "Ep 1 (Step 016650): Train loss 0.885, Val loss 1.769\n",
      "Ep 1 (Step 016655): Train loss 1.152, Val loss 1.769\n",
      "Ep 1 (Step 016660): Train loss 1.280, Val loss 1.769\n",
      "Ep 1 (Step 016665): Train loss 0.965, Val loss 1.769\n",
      "Ep 1 (Step 016670): Train loss 1.097, Val loss 1.768\n",
      "Ep 1 (Step 016675): Train loss 1.206, Val loss 1.768\n",
      "Ep 1 (Step 016680): Train loss 1.176, Val loss 1.768\n",
      "Ep 1 (Step 016685): Train loss 1.259, Val loss 1.768\n",
      "Ep 1 (Step 016690): Train loss 1.110, Val loss 1.770\n",
      "Ep 1 (Step 016695): Train loss 1.018, Val loss 1.771\n",
      "Ep 1 (Step 016700): Train loss 1.095, Val loss 1.774\n",
      "Ep 1 (Step 016705): Train loss 1.275, Val loss 1.776\n",
      "Ep 1 (Step 016710): Train loss 1.307, Val loss 1.776\n",
      "Ep 1 (Step 016715): Train loss 1.123, Val loss 1.777\n",
      "Ep 1 (Step 016720): Train loss 1.012, Val loss 1.779\n",
      "Ep 1 (Step 016725): Train loss 1.072, Val loss 1.782\n",
      "Ep 1 (Step 016730): Train loss 1.358, Val loss 1.784\n",
      "Ep 1 (Step 016735): Train loss 1.284, Val loss 1.785\n",
      "Ep 1 (Step 016740): Train loss 1.086, Val loss 1.788\n",
      "Ep 1 (Step 016745): Train loss 1.569, Val loss 1.790\n",
      "Ep 1 (Step 016750): Train loss 1.239, Val loss 1.791\n",
      "Ep 1 (Step 016755): Train loss 1.133, Val loss 1.792\n",
      "Ep 1 (Step 016760): Train loss 1.140, Val loss 1.793\n",
      "Ep 1 (Step 016765): Train loss 1.020, Val loss 1.794\n",
      "Ep 1 (Step 016770): Train loss 1.349, Val loss 1.796\n",
      "Ep 1 (Step 016775): Train loss 1.532, Val loss 1.799\n",
      "Ep 1 (Step 016780): Train loss 1.402, Val loss 1.799\n",
      "Ep 1 (Step 016785): Train loss 1.333, Val loss 1.798\n",
      "Ep 1 (Step 016790): Train loss 1.223, Val loss 1.796\n",
      "Ep 1 (Step 016795): Train loss 1.043, Val loss 1.796\n",
      "Ep 1 (Step 016800): Train loss 1.138, Val loss 1.799\n",
      "Ep 1 (Step 016805): Train loss 1.289, Val loss 1.803\n",
      "Ep 1 (Step 016810): Train loss 1.271, Val loss 1.804\n",
      "Ep 1 (Step 016815): Train loss 1.410, Val loss 1.802\n",
      "Ep 1 (Step 016820): Train loss 1.068, Val loss 1.798\n",
      "Ep 1 (Step 016825): Train loss 1.289, Val loss 1.791\n",
      "Ep 1 (Step 016830): Train loss 1.177, Val loss 1.788\n",
      "Ep 1 (Step 016835): Train loss 1.448, Val loss 1.787\n",
      "Ep 1 (Step 016840): Train loss 1.485, Val loss 1.787\n",
      "Ep 1 (Step 016845): Train loss 1.375, Val loss 1.787\n",
      "Ep 1 (Step 016850): Train loss 1.315, Val loss 1.789\n",
      "Ep 1 (Step 016855): Train loss 1.288, Val loss 1.789\n",
      "Ep 1 (Step 016860): Train loss 1.320, Val loss 1.789\n",
      "Ep 1 (Step 016865): Train loss 1.233, Val loss 1.789\n",
      "Ep 1 (Step 016870): Train loss 1.165, Val loss 1.789\n",
      "Ep 1 (Step 016875): Train loss 1.046, Val loss 1.788\n",
      "Ep 1 (Step 016880): Train loss 1.177, Val loss 1.787\n",
      "Ep 1 (Step 016885): Train loss 1.544, Val loss 1.788\n",
      "Ep 1 (Step 016890): Train loss 1.301, Val loss 1.788\n",
      "Ep 1 (Step 016895): Train loss 1.175, Val loss 1.787\n",
      "Ep 1 (Step 016900): Train loss 1.435, Val loss 1.787\n",
      "Ep 1 (Step 016905): Train loss 1.191, Val loss 1.788\n",
      "Ep 1 (Step 016910): Train loss 1.280, Val loss 1.788\n",
      "Ep 1 (Step 016915): Train loss 1.397, Val loss 1.787\n",
      "Ep 1 (Step 016920): Train loss 0.987, Val loss 1.787\n",
      "Ep 1 (Step 016925): Train loss 1.427, Val loss 1.786\n",
      "Ep 1 (Step 016930): Train loss 1.070, Val loss 1.786\n",
      "Ep 1 (Step 016935): Train loss 1.210, Val loss 1.787\n",
      "Ep 1 (Step 016940): Train loss 1.051, Val loss 1.787\n",
      "Ep 1 (Step 016945): Train loss 1.074, Val loss 1.783\n",
      "Ep 1 (Step 016950): Train loss 1.207, Val loss 1.781\n",
      "Ep 1 (Step 016955): Train loss 0.998, Val loss 1.781\n",
      "Ep 1 (Step 016960): Train loss 1.291, Val loss 1.780\n",
      "Ep 1 (Step 016965): Train loss 0.971, Val loss 1.778\n",
      "Ep 1 (Step 016970): Train loss 1.061, Val loss 1.779\n",
      "Ep 1 (Step 016975): Train loss 1.231, Val loss 1.781\n",
      "Ep 1 (Step 016980): Train loss 1.241, Val loss 1.783\n",
      "Ep 1 (Step 016985): Train loss 1.078, Val loss 1.785\n",
      "Ep 1 (Step 016990): Train loss 1.219, Val loss 1.788\n",
      "Ep 1 (Step 016995): Train loss 0.949, Val loss 1.789\n",
      "Ep 1 (Step 017000): Train loss 1.414, Val loss 1.789\n",
      "Ep 1 (Step 017005): Train loss 1.317, Val loss 1.789\n",
      "Ep 1 (Step 017010): Train loss 1.305, Val loss 1.788\n",
      "Ep 1 (Step 017015): Train loss 1.201, Val loss 1.787\n",
      "Ep 1 (Step 017020): Train loss 1.220, Val loss 1.787\n",
      "Ep 1 (Step 017025): Train loss 1.100, Val loss 1.786\n",
      "Ep 1 (Step 017030): Train loss 1.249, Val loss 1.786\n",
      "Ep 1 (Step 017035): Train loss 1.129, Val loss 1.786\n",
      "Ep 1 (Step 017040): Train loss 1.458, Val loss 1.785\n",
      "Ep 1 (Step 017045): Train loss 0.908, Val loss 1.783\n",
      "Ep 1 (Step 017050): Train loss 1.059, Val loss 1.784\n",
      "Ep 1 (Step 017055): Train loss 1.086, Val loss 1.783\n",
      "Ep 1 (Step 017060): Train loss 1.140, Val loss 1.781\n",
      "Ep 1 (Step 017065): Train loss 1.172, Val loss 1.780\n",
      "Ep 1 (Step 017070): Train loss 1.210, Val loss 1.780\n",
      "Ep 1 (Step 017075): Train loss 1.392, Val loss 1.781\n",
      "Ep 1 (Step 017080): Train loss 1.492, Val loss 1.782\n",
      "Ep 1 (Step 017085): Train loss 1.006, Val loss 1.783\n",
      "Ep 1 (Step 017090): Train loss 1.212, Val loss 1.781\n",
      "Ep 1 (Step 017095): Train loss 1.259, Val loss 1.779\n",
      "Ep 1 (Step 017100): Train loss 1.439, Val loss 1.780\n",
      "Ep 1 (Step 017105): Train loss 1.306, Val loss 1.781\n",
      "Ep 1 (Step 017110): Train loss 1.450, Val loss 1.783\n",
      "Ep 1 (Step 017115): Train loss 1.125, Val loss 1.786\n",
      "Ep 1 (Step 017120): Train loss 1.332, Val loss 1.788\n",
      "Ep 1 (Step 017125): Train loss 1.410, Val loss 1.788\n",
      "Ep 1 (Step 017130): Train loss 1.212, Val loss 1.790\n",
      "Ep 1 (Step 017135): Train loss 1.146, Val loss 1.790\n",
      "Ep 1 (Step 017140): Train loss 0.940, Val loss 1.790\n",
      "Ep 1 (Step 017145): Train loss 1.396, Val loss 1.790\n",
      "Ep 1 (Step 017150): Train loss 1.252, Val loss 1.790\n",
      "Ep 1 (Step 017155): Train loss 1.293, Val loss 1.790\n",
      "Ep 1 (Step 017160): Train loss 0.999, Val loss 1.791\n",
      "Ep 1 (Step 017165): Train loss 1.117, Val loss 1.791\n",
      "Ep 1 (Step 017170): Train loss 1.147, Val loss 1.790\n",
      "Ep 1 (Step 017175): Train loss 1.255, Val loss 1.790\n",
      "Ep 1 (Step 017180): Train loss 1.282, Val loss 1.789\n",
      "Ep 1 (Step 017185): Train loss 1.342, Val loss 1.789\n",
      "Ep 1 (Step 017190): Train loss 1.019, Val loss 1.788\n",
      "Ep 1 (Step 017195): Train loss 1.158, Val loss 1.788\n",
      "Ep 1 (Step 017200): Train loss 1.071, Val loss 1.785\n",
      "Ep 1 (Step 017205): Train loss 1.140, Val loss 1.785\n",
      "Ep 1 (Step 017210): Train loss 1.162, Val loss 1.786\n",
      "Ep 1 (Step 017215): Train loss 1.269, Val loss 1.787\n",
      "Ep 1 (Step 017220): Train loss 0.955, Val loss 1.788\n",
      "Ep 1 (Step 017225): Train loss 1.526, Val loss 1.787\n",
      "Ep 1 (Step 017230): Train loss 0.821, Val loss 1.787\n",
      "Ep 1 (Step 017235): Train loss 1.013, Val loss 1.788\n",
      "Ep 1 (Step 017240): Train loss 1.008, Val loss 1.787\n",
      "Ep 1 (Step 017245): Train loss 1.316, Val loss 1.784\n",
      "Ep 1 (Step 017250): Train loss 1.313, Val loss 1.780\n",
      "Ep 1 (Step 017255): Train loss 1.290, Val loss 1.778\n",
      "Ep 1 (Step 017260): Train loss 1.141, Val loss 1.778\n",
      "Ep 1 (Step 017265): Train loss 1.265, Val loss 1.777\n",
      "Ep 1 (Step 017270): Train loss 1.196, Val loss 1.776\n",
      "Ep 1 (Step 017275): Train loss 1.169, Val loss 1.777\n",
      "Ep 1 (Step 017280): Train loss 1.235, Val loss 1.777\n",
      "Ep 1 (Step 017285): Train loss 1.362, Val loss 1.777\n",
      "Ep 1 (Step 017290): Train loss 1.290, Val loss 1.778\n",
      "Ep 1 (Step 017295): Train loss 1.003, Val loss 1.780\n",
      "Ep 1 (Step 017300): Train loss 1.341, Val loss 1.783\n",
      "Ep 1 (Step 017305): Train loss 1.174, Val loss 1.784\n",
      "Ep 1 (Step 017310): Train loss 0.962, Val loss 1.785\n",
      "Ep 1 (Step 017315): Train loss 1.266, Val loss 1.785\n",
      "Ep 1 (Step 017320): Train loss 1.197, Val loss 1.783\n",
      "Ep 1 (Step 017325): Train loss 0.891, Val loss 1.780\n",
      "Ep 1 (Step 017330): Train loss 0.913, Val loss 1.778\n",
      "Ep 1 (Step 017335): Train loss 1.062, Val loss 1.777\n",
      "Ep 1 (Step 017340): Train loss 1.251, Val loss 1.777\n",
      "Ep 1 (Step 017345): Train loss 1.230, Val loss 1.779\n",
      "Ep 1 (Step 017350): Train loss 1.284, Val loss 1.783\n",
      "Ep 1 (Step 017355): Train loss 0.913, Val loss 1.788\n",
      "Ep 1 (Step 017360): Train loss 1.254, Val loss 1.792\n",
      "Ep 1 (Step 017365): Train loss 1.098, Val loss 1.795\n",
      "Ep 1 (Step 017370): Train loss 1.462, Val loss 1.797\n",
      "Ep 1 (Step 017375): Train loss 1.466, Val loss 1.796\n",
      "Ep 1 (Step 017380): Train loss 1.114, Val loss 1.794\n",
      "Ep 1 (Step 017385): Train loss 1.116, Val loss 1.791\n",
      "Ep 1 (Step 017390): Train loss 1.407, Val loss 1.791\n",
      "Ep 1 (Step 017395): Train loss 1.608, Val loss 1.791\n",
      "Ep 1 (Step 017400): Train loss 1.188, Val loss 1.791\n",
      "Ep 1 (Step 017405): Train loss 1.127, Val loss 1.791\n",
      "Ep 1 (Step 017410): Train loss 1.264, Val loss 1.790\n",
      "Ep 1 (Step 017415): Train loss 1.081, Val loss 1.789\n",
      "Ep 1 (Step 017420): Train loss 1.316, Val loss 1.788\n",
      "Ep 1 (Step 017425): Train loss 0.877, Val loss 1.787\n",
      "Ep 1 (Step 017430): Train loss 1.100, Val loss 1.788\n",
      "Ep 1 (Step 017435): Train loss 0.991, Val loss 1.789\n",
      "Ep 1 (Step 017440): Train loss 0.989, Val loss 1.790\n",
      "Ep 1 (Step 017445): Train loss 1.414, Val loss 1.792\n",
      "Ep 1 (Step 017450): Train loss 0.981, Val loss 1.793\n",
      "Ep 1 (Step 017455): Train loss 1.343, Val loss 1.794\n",
      "Ep 1 (Step 017460): Train loss 1.228, Val loss 1.795\n",
      "Ep 1 (Step 017465): Train loss 1.476, Val loss 1.794\n",
      "Ep 1 (Step 017470): Train loss 1.262, Val loss 1.795\n",
      "Ep 1 (Step 017475): Train loss 1.341, Val loss 1.795\n",
      "Ep 1 (Step 017480): Train loss 1.201, Val loss 1.796\n",
      "Ep 1 (Step 017485): Train loss 1.401, Val loss 1.796\n",
      "Ep 1 (Step 017490): Train loss 1.080, Val loss 1.795\n",
      "Ep 1 (Step 017495): Train loss 1.064, Val loss 1.794\n",
      "Ep 1 (Step 017500): Train loss 0.931, Val loss 1.794\n",
      "Ep 1 (Step 017505): Train loss 0.902, Val loss 1.794\n",
      "Ep 1 (Step 017510): Train loss 1.281, Val loss 1.794\n",
      "Ep 1 (Step 017515): Train loss 1.089, Val loss 1.795\n",
      "Ep 1 (Step 017520): Train loss 1.027, Val loss 1.795\n",
      "Ep 1 (Step 017525): Train loss 1.327, Val loss 1.794\n",
      "Ep 1 (Step 017530): Train loss 1.024, Val loss 1.793\n",
      "Ep 1 (Step 017535): Train loss 1.028, Val loss 1.793\n",
      "Ep 1 (Step 017540): Train loss 1.314, Val loss 1.794\n",
      "Ep 1 (Step 017545): Train loss 1.100, Val loss 1.793\n",
      "Ep 1 (Step 017550): Train loss 1.264, Val loss 1.793\n",
      "Ep 1 (Step 017555): Train loss 1.047, Val loss 1.792\n",
      "Ep 1 (Step 017560): Train loss 1.145, Val loss 1.791\n",
      "Ep 1 (Step 017565): Train loss 1.341, Val loss 1.791\n",
      "Ep 1 (Step 017570): Train loss 1.338, Val loss 1.791\n",
      "Ep 1 (Step 017575): Train loss 0.949, Val loss 1.792\n",
      "Ep 1 (Step 017580): Train loss 0.941, Val loss 1.793\n",
      "Ep 1 (Step 017585): Train loss 1.004, Val loss 1.793\n",
      "Ep 1 (Step 017590): Train loss 1.291, Val loss 1.794\n",
      "Ep 1 (Step 017595): Train loss 1.319, Val loss 1.799\n",
      "Ep 1 (Step 017600): Train loss 1.052, Val loss 1.805\n",
      "Ep 1 (Step 017605): Train loss 1.153, Val loss 1.810\n",
      "Ep 1 (Step 017610): Train loss 1.287, Val loss 1.812\n",
      "Ep 1 (Step 017615): Train loss 1.418, Val loss 1.812\n",
      "Ep 1 (Step 017620): Train loss 1.402, Val loss 1.811\n",
      "Ep 1 (Step 017625): Train loss 1.653, Val loss 1.810\n",
      "Ep 1 (Step 017630): Train loss 1.155, Val loss 1.807\n",
      "Ep 1 (Step 017635): Train loss 1.275, Val loss 1.806\n",
      "Ep 1 (Step 017640): Train loss 1.177, Val loss 1.803\n",
      "Ep 1 (Step 017645): Train loss 1.161, Val loss 1.801\n",
      "Ep 1 (Step 017650): Train loss 1.287, Val loss 1.800\n",
      "Ep 1 (Step 017655): Train loss 1.367, Val loss 1.798\n",
      "Ep 1 (Step 017660): Train loss 1.203, Val loss 1.799\n",
      "Ep 1 (Step 017665): Train loss 1.382, Val loss 1.798\n",
      "Ep 1 (Step 017670): Train loss 1.159, Val loss 1.797\n",
      "Ep 1 (Step 017675): Train loss 1.289, Val loss 1.797\n",
      "Ep 1 (Step 017680): Train loss 1.143, Val loss 1.798\n",
      "Ep 1 (Step 017685): Train loss 1.018, Val loss 1.798\n",
      "Ep 1 (Step 017690): Train loss 1.388, Val loss 1.798\n",
      "Ep 1 (Step 017695): Train loss 1.141, Val loss 1.798\n",
      "Ep 1 (Step 017700): Train loss 1.277, Val loss 1.798\n",
      "Ep 1 (Step 017705): Train loss 1.232, Val loss 1.797\n",
      "Ep 1 (Step 017710): Train loss 1.236, Val loss 1.797\n",
      "Ep 1 (Step 017715): Train loss 1.355, Val loss 1.797\n",
      "Ep 1 (Step 017720): Train loss 1.040, Val loss 1.797\n",
      "Ep 1 (Step 017725): Train loss 1.469, Val loss 1.795\n",
      "Ep 1 (Step 017730): Train loss 1.035, Val loss 1.792\n",
      "Ep 1 (Step 017735): Train loss 1.255, Val loss 1.789\n",
      "Ep 1 (Step 017740): Train loss 1.199, Val loss 1.787\n",
      "Ep 1 (Step 017745): Train loss 1.073, Val loss 1.785\n",
      "Ep 1 (Step 017750): Train loss 0.971, Val loss 1.785\n",
      "Ep 1 (Step 017755): Train loss 1.420, Val loss 1.785\n",
      "Ep 1 (Step 017760): Train loss 1.176, Val loss 1.787\n",
      "Ep 1 (Step 017765): Train loss 1.109, Val loss 1.790\n",
      "Ep 1 (Step 017770): Train loss 1.243, Val loss 1.792\n",
      "Ep 1 (Step 017775): Train loss 0.993, Val loss 1.793\n",
      "Ep 1 (Step 017780): Train loss 1.340, Val loss 1.793\n",
      "Ep 1 (Step 017785): Train loss 1.099, Val loss 1.792\n",
      "Ep 1 (Step 017790): Train loss 1.182, Val loss 1.791\n",
      "Ep 1 (Step 017795): Train loss 1.057, Val loss 1.790\n",
      "Ep 1 (Step 017800): Train loss 1.036, Val loss 1.789\n",
      "Ep 1 (Step 017805): Train loss 1.197, Val loss 1.788\n",
      "Ep 1 (Step 017810): Train loss 1.207, Val loss 1.787\n",
      "Ep 1 (Step 017815): Train loss 1.067, Val loss 1.789\n",
      "Ep 1 (Step 017820): Train loss 1.270, Val loss 1.790\n",
      "Ep 1 (Step 017825): Train loss 1.356, Val loss 1.791\n",
      "Ep 1 (Step 017830): Train loss 1.140, Val loss 1.792\n",
      "Ep 1 (Step 017835): Train loss 1.011, Val loss 1.793\n",
      "Ep 1 (Step 017840): Train loss 1.102, Val loss 1.792\n",
      "Ep 1 (Step 017845): Train loss 1.276, Val loss 1.792\n",
      "Ep 1 (Step 017850): Train loss 1.294, Val loss 1.790\n",
      "Ep 1 (Step 017855): Train loss 1.029, Val loss 1.790\n",
      "Ep 1 (Step 017860): Train loss 1.066, Val loss 1.787\n",
      "Ep 1 (Step 017865): Train loss 1.243, Val loss 1.783\n",
      "Ep 1 (Step 017870): Train loss 1.156, Val loss 1.782\n",
      "Ep 1 (Step 017875): Train loss 1.199, Val loss 1.781\n",
      "Ep 1 (Step 017880): Train loss 1.098, Val loss 1.780\n",
      "Ep 1 (Step 017885): Train loss 1.319, Val loss 1.780\n",
      "Ep 1 (Step 017890): Train loss 1.215, Val loss 1.782\n",
      "Ep 1 (Step 017895): Train loss 1.274, Val loss 1.784\n",
      "Ep 1 (Step 017900): Train loss 1.529, Val loss 1.786\n",
      "Ep 1 (Step 017905): Train loss 0.862, Val loss 1.787\n",
      "Ep 1 (Step 017910): Train loss 1.182, Val loss 1.787\n",
      "Ep 1 (Step 017915): Train loss 1.316, Val loss 1.787\n",
      "Ep 1 (Step 017920): Train loss 1.108, Val loss 1.788\n",
      "Ep 1 (Step 017925): Train loss 1.103, Val loss 1.789\n",
      "Ep 1 (Step 017930): Train loss 1.128, Val loss 1.788\n",
      "Ep 1 (Step 017935): Train loss 1.434, Val loss 1.785\n",
      "Ep 1 (Step 017940): Train loss 1.273, Val loss 1.786\n",
      "Ep 1 (Step 017945): Train loss 1.361, Val loss 1.785\n",
      "Ep 1 (Step 017950): Train loss 1.242, Val loss 1.782\n",
      "Ep 1 (Step 017955): Train loss 1.398, Val loss 1.781\n",
      "Ep 1 (Step 017960): Train loss 1.239, Val loss 1.782\n",
      "Ep 1 (Step 017965): Train loss 1.163, Val loss 1.784\n",
      "Ep 1 (Step 017970): Train loss 1.019, Val loss 1.785\n",
      "Ep 1 (Step 017975): Train loss 1.573, Val loss 1.788\n",
      "Ep 1 (Step 017980): Train loss 1.144, Val loss 1.792\n",
      "Ep 1 (Step 017985): Train loss 0.972, Val loss 1.792\n",
      "Ep 1 (Step 017990): Train loss 1.206, Val loss 1.792\n",
      "Ep 1 (Step 017995): Train loss 1.252, Val loss 1.791\n",
      "Ep 1 (Step 018000): Train loss 1.114, Val loss 1.789\n",
      "Ep 1 (Step 018005): Train loss 1.153, Val loss 1.786\n",
      "Ep 1 (Step 018010): Train loss 0.887, Val loss 1.785\n",
      "Ep 1 (Step 018015): Train loss 0.723, Val loss 1.784\n",
      "Ep 1 (Step 018020): Train loss 1.178, Val loss 1.785\n",
      "Ep 1 (Step 018025): Train loss 1.267, Val loss 1.784\n",
      "Ep 1 (Step 018030): Train loss 1.438, Val loss 1.780\n",
      "Ep 1 (Step 018035): Train loss 1.212, Val loss 1.777\n",
      "Ep 1 (Step 018040): Train loss 1.192, Val loss 1.775\n",
      "Ep 1 (Step 018045): Train loss 1.419, Val loss 1.773\n",
      "Ep 1 (Step 018050): Train loss 1.272, Val loss 1.774\n",
      "Ep 1 (Step 018055): Train loss 1.214, Val loss 1.775\n",
      "Ep 1 (Step 018060): Train loss 1.090, Val loss 1.778\n",
      "Ep 1 (Step 018065): Train loss 1.399, Val loss 1.782\n",
      "Ep 1 (Step 018070): Train loss 1.068, Val loss 1.784\n",
      "Ep 1 (Step 018075): Train loss 1.095, Val loss 1.785\n",
      "Ep 1 (Step 018080): Train loss 1.235, Val loss 1.786\n",
      "Ep 1 (Step 018085): Train loss 1.126, Val loss 1.787\n",
      "Ep 1 (Step 018090): Train loss 1.067, Val loss 1.786\n",
      "Ep 1 (Step 018095): Train loss 0.957, Val loss 1.786\n",
      "Ep 1 (Step 018100): Train loss 1.249, Val loss 1.785\n",
      "Ep 1 (Step 018105): Train loss 1.160, Val loss 1.784\n",
      "Ep 1 (Step 018110): Train loss 1.039, Val loss 1.786\n",
      "Ep 1 (Step 018115): Train loss 1.294, Val loss 1.787\n",
      "Ep 1 (Step 018120): Train loss 1.246, Val loss 1.788\n",
      "Ep 1 (Step 018125): Train loss 1.212, Val loss 1.788\n",
      "Ep 1 (Step 018130): Train loss 1.294, Val loss 1.787\n",
      "Ep 1 (Step 018135): Train loss 1.352, Val loss 1.786\n",
      "Ep 1 (Step 018140): Train loss 1.435, Val loss 1.784\n",
      "Ep 1 (Step 018145): Train loss 1.017, Val loss 1.782\n",
      "Ep 1 (Step 018150): Train loss 1.399, Val loss 1.781\n",
      "Ep 1 (Step 018155): Train loss 1.202, Val loss 1.780\n",
      "Ep 1 (Step 018160): Train loss 1.162, Val loss 1.782\n",
      "Ep 1 (Step 018165): Train loss 1.042, Val loss 1.783\n",
      "Ep 1 (Step 018170): Train loss 1.176, Val loss 1.784\n",
      "Ep 1 (Step 018175): Train loss 1.274, Val loss 1.783\n",
      "Ep 1 (Step 018180): Train loss 0.942, Val loss 1.781\n",
      "Ep 1 (Step 018185): Train loss 1.372, Val loss 1.780\n",
      "Ep 1 (Step 018190): Train loss 1.049, Val loss 1.777\n",
      "Ep 1 (Step 018195): Train loss 0.974, Val loss 1.775\n",
      "Ep 1 (Step 018200): Train loss 1.148, Val loss 1.774\n",
      "Ep 1 (Step 018205): Train loss 1.179, Val loss 1.774\n",
      "Ep 1 (Step 018210): Train loss 1.325, Val loss 1.775\n",
      "Ep 1 (Step 018215): Train loss 1.158, Val loss 1.774\n",
      "Ep 1 (Step 018220): Train loss 1.228, Val loss 1.774\n",
      "Ep 1 (Step 018225): Train loss 1.617, Val loss 1.774\n",
      "Ep 1 (Step 018230): Train loss 1.372, Val loss 1.775\n",
      "Ep 1 (Step 018235): Train loss 1.052, Val loss 1.775\n",
      "Ep 1 (Step 018240): Train loss 1.278, Val loss 1.776\n",
      "Ep 1 (Step 018245): Train loss 1.130, Val loss 1.777\n",
      "Ep 1 (Step 018250): Train loss 1.293, Val loss 1.777\n",
      "Ep 1 (Step 018255): Train loss 1.174, Val loss 1.775\n",
      "Ep 1 (Step 018260): Train loss 1.095, Val loss 1.774\n",
      "Ep 1 (Step 018265): Train loss 1.020, Val loss 1.772\n",
      "Ep 1 (Step 018270): Train loss 1.262, Val loss 1.770\n",
      "Ep 1 (Step 018275): Train loss 1.434, Val loss 1.769\n",
      "Ep 1 (Step 018280): Train loss 1.008, Val loss 1.769\n",
      "Ep 1 (Step 018285): Train loss 1.074, Val loss 1.767\n",
      "Ep 1 (Step 018290): Train loss 1.119, Val loss 1.767\n",
      "Ep 1 (Step 018295): Train loss 0.952, Val loss 1.767\n",
      "Ep 1 (Step 018300): Train loss 1.400, Val loss 1.765\n",
      "Ep 1 (Step 018305): Train loss 1.022, Val loss 1.765\n",
      "Ep 1 (Step 018310): Train loss 1.320, Val loss 1.766\n",
      "Ep 1 (Step 018315): Train loss 1.135, Val loss 1.767\n",
      "Ep 1 (Step 018320): Train loss 1.036, Val loss 1.768\n",
      "Ep 1 (Step 018325): Train loss 1.407, Val loss 1.768\n",
      "Ep 1 (Step 018330): Train loss 1.307, Val loss 1.769\n",
      "Ep 1 (Step 018335): Train loss 1.033, Val loss 1.770\n",
      "Ep 1 (Step 018340): Train loss 1.034, Val loss 1.770\n",
      "Ep 1 (Step 018345): Train loss 1.132, Val loss 1.771\n",
      "Ep 1 (Step 018350): Train loss 1.248, Val loss 1.772\n",
      "Ep 1 (Step 018355): Train loss 1.035, Val loss 1.773\n",
      "Ep 1 (Step 018360): Train loss 1.292, Val loss 1.775\n",
      "Ep 1 (Step 018365): Train loss 1.082, Val loss 1.776\n",
      "Ep 1 (Step 018370): Train loss 1.369, Val loss 1.776\n",
      "Ep 1 (Step 018375): Train loss 1.282, Val loss 1.775\n",
      "Ep 1 (Step 018380): Train loss 0.934, Val loss 1.777\n",
      "Ep 1 (Step 018385): Train loss 1.226, Val loss 1.778\n",
      "Ep 1 (Step 018390): Train loss 1.054, Val loss 1.778\n",
      "Ep 1 (Step 018395): Train loss 1.076, Val loss 1.778\n",
      "Ep 1 (Step 018400): Train loss 1.153, Val loss 1.778\n",
      "Ep 1 (Step 018405): Train loss 1.016, Val loss 1.779\n",
      "Ep 1 (Step 018410): Train loss 1.627, Val loss 1.780\n",
      "Ep 1 (Step 018415): Train loss 1.073, Val loss 1.782\n",
      "Ep 1 (Step 018420): Train loss 1.587, Val loss 1.784\n",
      "Ep 1 (Step 018425): Train loss 1.115, Val loss 1.784\n",
      "Ep 1 (Step 018430): Train loss 1.275, Val loss 1.784\n",
      "Ep 1 (Step 018435): Train loss 0.987, Val loss 1.781\n",
      "Ep 1 (Step 018440): Train loss 1.501, Val loss 1.779\n",
      "Ep 1 (Step 018445): Train loss 1.258, Val loss 1.776\n",
      "Ep 1 (Step 018450): Train loss 1.046, Val loss 1.773\n",
      "Ep 1 (Step 018455): Train loss 1.272, Val loss 1.772\n",
      "Ep 1 (Step 018460): Train loss 1.359, Val loss 1.774\n",
      "Ep 1 (Step 018465): Train loss 1.137, Val loss 1.775\n",
      "Ep 1 (Step 018470): Train loss 0.983, Val loss 1.775\n",
      "Ep 1 (Step 018475): Train loss 1.386, Val loss 1.776\n",
      "Ep 1 (Step 018480): Train loss 1.131, Val loss 1.778\n",
      "Ep 1 (Step 018485): Train loss 1.270, Val loss 1.780\n",
      "Ep 1 (Step 018490): Train loss 1.329, Val loss 1.781\n",
      "Ep 1 (Step 018495): Train loss 1.266, Val loss 1.782\n",
      "Ep 1 (Step 018500): Train loss 1.072, Val loss 1.783\n",
      "Ep 1 (Step 018505): Train loss 1.236, Val loss 1.785\n",
      "Ep 1 (Step 018510): Train loss 1.157, Val loss 1.787\n",
      "Ep 1 (Step 018515): Train loss 1.119, Val loss 1.789\n",
      "Ep 1 (Step 018520): Train loss 1.117, Val loss 1.790\n",
      "Ep 1 (Step 018525): Train loss 1.195, Val loss 1.790\n",
      "Ep 1 (Step 018530): Train loss 1.177, Val loss 1.790\n",
      "Ep 1 (Step 018535): Train loss 1.328, Val loss 1.789\n",
      "Ep 1 (Step 018540): Train loss 1.462, Val loss 1.790\n",
      "Ep 1 (Step 018545): Train loss 1.322, Val loss 1.792\n",
      "Ep 1 (Step 018550): Train loss 1.135, Val loss 1.793\n",
      "Ep 1 (Step 018555): Train loss 1.019, Val loss 1.793\n",
      "Ep 1 (Step 018560): Train loss 1.303, Val loss 1.793\n",
      "Ep 1 (Step 018565): Train loss 1.301, Val loss 1.793\n",
      "Ep 1 (Step 018570): Train loss 1.152, Val loss 1.790\n",
      "Ep 1 (Step 018575): Train loss 0.812, Val loss 1.788\n",
      "Ep 1 (Step 018580): Train loss 1.077, Val loss 1.788\n",
      "Ep 1 (Step 018585): Train loss 1.252, Val loss 1.787\n",
      "Ep 1 (Step 018590): Train loss 1.185, Val loss 1.785\n",
      "Ep 1 (Step 018595): Train loss 1.291, Val loss 1.784\n",
      "Ep 1 (Step 018600): Train loss 1.190, Val loss 1.782\n",
      "Ep 1 (Step 018605): Train loss 1.316, Val loss 1.782\n",
      "Ep 1 (Step 018610): Train loss 1.312, Val loss 1.781\n",
      "Ep 1 (Step 018615): Train loss 1.132, Val loss 1.781\n",
      "Ep 1 (Step 018620): Train loss 1.290, Val loss 1.782\n",
      "Ep 1 (Step 018625): Train loss 1.058, Val loss 1.783\n",
      "Ep 1 (Step 018630): Train loss 1.140, Val loss 1.785\n",
      "Ep 1 (Step 018635): Train loss 1.169, Val loss 1.787\n",
      "Ep 1 (Step 018640): Train loss 1.624, Val loss 1.789\n",
      "Ep 1 (Step 018645): Train loss 1.039, Val loss 1.790\n",
      "Ep 1 (Step 018650): Train loss 1.170, Val loss 1.792\n",
      "Ep 1 (Step 018655): Train loss 1.102, Val loss 1.792\n",
      "Ep 1 (Step 018660): Train loss 1.393, Val loss 1.791\n",
      "Ep 1 (Step 018665): Train loss 1.284, Val loss 1.791\n",
      "Ep 1 (Step 018670): Train loss 1.151, Val loss 1.791\n",
      "Ep 1 (Step 018675): Train loss 0.988, Val loss 1.793\n",
      "Ep 1 (Step 018680): Train loss 1.221, Val loss 1.791\n",
      "Ep 1 (Step 018685): Train loss 1.170, Val loss 1.789\n",
      "Ep 1 (Step 018690): Train loss 1.358, Val loss 1.788\n",
      "Ep 1 (Step 018695): Train loss 1.104, Val loss 1.789\n",
      "Ep 1 (Step 018700): Train loss 1.208, Val loss 1.790\n",
      "Ep 1 (Step 018705): Train loss 1.228, Val loss 1.790\n",
      "Ep 1 (Step 018710): Train loss 1.088, Val loss 1.792\n",
      "Ep 1 (Step 018715): Train loss 1.013, Val loss 1.792\n",
      "Ep 1 (Step 018720): Train loss 1.096, Val loss 1.792\n",
      "Ep 1 (Step 018725): Train loss 1.197, Val loss 1.791\n",
      "Ep 1 (Step 018730): Train loss 1.309, Val loss 1.790\n",
      "Ep 1 (Step 018735): Train loss 1.443, Val loss 1.791\n",
      "Ep 1 (Step 018740): Train loss 1.194, Val loss 1.791\n",
      "Ep 1 (Step 018745): Train loss 1.076, Val loss 1.791\n",
      "Ep 1 (Step 018750): Train loss 0.868, Val loss 1.790\n",
      "Ep 1 (Step 018755): Train loss 1.240, Val loss 1.791\n",
      "Ep 1 (Step 018760): Train loss 1.092, Val loss 1.791\n",
      "Ep 1 (Step 018765): Train loss 1.255, Val loss 1.788\n",
      "Ep 1 (Step 018770): Train loss 1.026, Val loss 1.787\n",
      "Ep 1 (Step 018775): Train loss 1.277, Val loss 1.783\n",
      "Ep 1 (Step 018780): Train loss 1.288, Val loss 1.782\n",
      "Ep 1 (Step 018785): Train loss 1.031, Val loss 1.781\n",
      "Ep 1 (Step 018790): Train loss 1.211, Val loss 1.779\n",
      "Ep 1 (Step 018795): Train loss 1.265, Val loss 1.777\n",
      "Ep 1 (Step 018800): Train loss 1.052, Val loss 1.776\n",
      "Ep 1 (Step 018805): Train loss 0.993, Val loss 1.775\n",
      "Ep 1 (Step 018810): Train loss 1.215, Val loss 1.777\n",
      "Ep 1 (Step 018815): Train loss 1.228, Val loss 1.779\n",
      "Ep 1 (Step 018820): Train loss 1.477, Val loss 1.782\n",
      "Ep 1 (Step 018825): Train loss 1.037, Val loss 1.782\n",
      "Ep 1 (Step 018830): Train loss 1.243, Val loss 1.782\n",
      "Ep 1 (Step 018835): Train loss 1.100, Val loss 1.782\n",
      "Ep 1 (Step 018840): Train loss 1.616, Val loss 1.782\n",
      "Ep 1 (Step 018845): Train loss 1.373, Val loss 1.783\n",
      "Ep 1 (Step 018850): Train loss 1.005, Val loss 1.783\n",
      "Ep 1 (Step 018855): Train loss 1.193, Val loss 1.783\n",
      "Ep 1 (Step 018860): Train loss 1.226, Val loss 1.785\n",
      "Ep 1 (Step 018865): Train loss 1.250, Val loss 1.786\n",
      "Ep 1 (Step 018870): Train loss 1.326, Val loss 1.786\n",
      "Ep 1 (Step 018875): Train loss 1.222, Val loss 1.786\n",
      "Ep 1 (Step 018880): Train loss 1.112, Val loss 1.784\n",
      "Ep 1 (Step 018885): Train loss 1.070, Val loss 1.781\n",
      "Ep 1 (Step 018890): Train loss 1.147, Val loss 1.780\n",
      "Ep 1 (Step 018895): Train loss 1.073, Val loss 1.779\n",
      "Ep 1 (Step 018900): Train loss 1.201, Val loss 1.780\n",
      "Ep 1 (Step 018905): Train loss 1.071, Val loss 1.781\n",
      "Ep 1 (Step 018910): Train loss 1.247, Val loss 1.780\n",
      "Ep 1 (Step 018915): Train loss 1.217, Val loss 1.777\n",
      "Ep 1 (Step 018920): Train loss 1.205, Val loss 1.775\n",
      "Ep 1 (Step 018925): Train loss 1.183, Val loss 1.772\n",
      "Ep 1 (Step 018930): Train loss 1.153, Val loss 1.770\n",
      "Ep 1 (Step 018935): Train loss 1.099, Val loss 1.770\n",
      "Ep 1 (Step 018940): Train loss 1.303, Val loss 1.765\n",
      "Ep 1 (Step 018945): Train loss 1.105, Val loss 1.761\n",
      "Ep 1 (Step 018950): Train loss 1.218, Val loss 1.761\n",
      "Ep 1 (Step 018955): Train loss 1.300, Val loss 1.761\n",
      "Ep 1 (Step 018960): Train loss 0.945, Val loss 1.762\n",
      "Ep 1 (Step 018965): Train loss 1.182, Val loss 1.763\n",
      "Ep 1 (Step 018970): Train loss 1.233, Val loss 1.764\n",
      "Ep 1 (Step 018975): Train loss 1.541, Val loss 1.765\n",
      "Ep 1 (Step 018980): Train loss 1.173, Val loss 1.766\n",
      "Ep 1 (Step 018985): Train loss 1.318, Val loss 1.765\n",
      "Ep 1 (Step 018990): Train loss 1.024, Val loss 1.765\n",
      "Ep 1 (Step 018995): Train loss 1.299, Val loss 1.764\n",
      "Ep 1 (Step 019000): Train loss 1.423, Val loss 1.764\n",
      "Ep 1 (Step 019005): Train loss 0.979, Val loss 1.765\n",
      "Ep 1 (Step 019010): Train loss 1.191, Val loss 1.766\n",
      "Ep 1 (Step 019015): Train loss 1.174, Val loss 1.768\n",
      "Ep 1 (Step 019020): Train loss 1.502, Val loss 1.772\n",
      "Ep 1 (Step 019025): Train loss 0.958, Val loss 1.776\n",
      "Ep 1 (Step 019030): Train loss 1.210, Val loss 1.778\n",
      "Ep 1 (Step 019035): Train loss 1.125, Val loss 1.778\n",
      "Ep 1 (Step 019040): Train loss 1.081, Val loss 1.778\n",
      "Ep 1 (Step 019045): Train loss 1.084, Val loss 1.776\n",
      "Ep 1 (Step 019050): Train loss 1.184, Val loss 1.775\n",
      "Ep 1 (Step 019055): Train loss 1.209, Val loss 1.774\n",
      "Ep 1 (Step 019060): Train loss 1.162, Val loss 1.773\n",
      "Ep 1 (Step 019065): Train loss 1.314, Val loss 1.772\n",
      "Ep 1 (Step 019070): Train loss 1.299, Val loss 1.771\n",
      "Ep 1 (Step 019075): Train loss 1.249, Val loss 1.771\n",
      "Ep 1 (Step 019080): Train loss 1.117, Val loss 1.771\n",
      "Ep 1 (Step 019085): Train loss 1.186, Val loss 1.770\n",
      "Ep 1 (Step 019090): Train loss 1.491, Val loss 1.767\n",
      "Ep 1 (Step 019095): Train loss 1.165, Val loss 1.764\n",
      "Ep 1 (Step 019100): Train loss 1.030, Val loss 1.764\n",
      "Ep 1 (Step 019105): Train loss 1.363, Val loss 1.764\n",
      "Ep 1 (Step 019110): Train loss 1.511, Val loss 1.765\n",
      "Ep 1 (Step 019115): Train loss 1.182, Val loss 1.766\n",
      "Ep 1 (Step 019120): Train loss 0.940, Val loss 1.767\n",
      "Ep 1 (Step 019125): Train loss 1.250, Val loss 1.769\n",
      "Ep 1 (Step 019130): Train loss 1.486, Val loss 1.768\n",
      "Ep 1 (Step 019135): Train loss 1.243, Val loss 1.766\n",
      "Ep 1 (Step 019140): Train loss 1.190, Val loss 1.764\n",
      "Ep 1 (Step 019145): Train loss 1.166, Val loss 1.764\n",
      "Ep 1 (Step 019150): Train loss 1.376, Val loss 1.763\n",
      "Ep 1 (Step 019155): Train loss 1.051, Val loss 1.761\n",
      "Ep 1 (Step 019160): Train loss 1.470, Val loss 1.763\n",
      "Ep 1 (Step 019165): Train loss 1.238, Val loss 1.763\n",
      "Ep 1 (Step 019170): Train loss 1.403, Val loss 1.762\n",
      "Ep 1 (Step 019175): Train loss 0.867, Val loss 1.761\n",
      "Ep 1 (Step 019180): Train loss 1.282, Val loss 1.761\n",
      "Ep 1 (Step 019185): Train loss 1.517, Val loss 1.760\n",
      "Ep 1 (Step 019190): Train loss 1.074, Val loss 1.759\n",
      "Ep 1 (Step 019195): Train loss 0.994, Val loss 1.760\n",
      "Ep 1 (Step 019200): Train loss 1.123, Val loss 1.760\n",
      "Ep 1 (Step 019205): Train loss 0.934, Val loss 1.760\n",
      "Ep 1 (Step 019210): Train loss 1.268, Val loss 1.762\n",
      "Ep 1 (Step 019215): Train loss 1.156, Val loss 1.763\n",
      "Ep 1 (Step 019220): Train loss 1.087, Val loss 1.764\n",
      "Ep 1 (Step 019225): Train loss 1.184, Val loss 1.763\n",
      "Ep 1 (Step 019230): Train loss 1.643, Val loss 1.761\n",
      "Ep 1 (Step 019235): Train loss 1.369, Val loss 1.761\n",
      "Ep 1 (Step 019240): Train loss 0.812, Val loss 1.761\n",
      "Ep 1 (Step 019245): Train loss 1.048, Val loss 1.761\n",
      "Ep 1 (Step 019250): Train loss 1.154, Val loss 1.758\n",
      "Ep 1 (Step 019255): Train loss 1.501, Val loss 1.758\n",
      "Ep 1 (Step 019260): Train loss 1.158, Val loss 1.760\n",
      "Ep 1 (Step 019265): Train loss 1.060, Val loss 1.762\n",
      "Ep 1 (Step 019270): Train loss 1.211, Val loss 1.762\n",
      "Ep 1 (Step 019275): Train loss 1.366, Val loss 1.762\n",
      "Ep 1 (Step 019280): Train loss 0.782, Val loss 1.761\n",
      "Ep 1 (Step 019285): Train loss 1.268, Val loss 1.761\n",
      "Ep 1 (Step 019290): Train loss 1.065, Val loss 1.760\n",
      "Ep 1 (Step 019295): Train loss 1.333, Val loss 1.760\n",
      "Ep 1 (Step 019300): Train loss 1.512, Val loss 1.760\n",
      "Ep 1 (Step 019305): Train loss 1.012, Val loss 1.760\n",
      "Ep 1 (Step 019310): Train loss 1.166, Val loss 1.762\n",
      "Ep 1 (Step 019315): Train loss 1.074, Val loss 1.765\n",
      "Ep 1 (Step 019320): Train loss 1.183, Val loss 1.767\n",
      "Ep 1 (Step 019325): Train loss 1.192, Val loss 1.770\n",
      "Ep 1 (Step 019330): Train loss 1.196, Val loss 1.771\n",
      "Ep 1 (Step 019335): Train loss 1.100, Val loss 1.773\n",
      "Ep 1 (Step 019340): Train loss 1.244, Val loss 1.773\n",
      "Ep 1 (Step 019345): Train loss 1.181, Val loss 1.772\n",
      "Ep 1 (Step 019350): Train loss 1.108, Val loss 1.773\n",
      "Ep 1 (Step 019355): Train loss 1.484, Val loss 1.774\n",
      "Ep 1 (Step 019360): Train loss 1.179, Val loss 1.775\n",
      "Ep 1 (Step 019365): Train loss 1.225, Val loss 1.777\n",
      "Ep 1 (Step 019370): Train loss 1.396, Val loss 1.779\n",
      "Ep 1 (Step 019375): Train loss 1.282, Val loss 1.782\n",
      "Ep 1 (Step 019380): Train loss 1.102, Val loss 1.782\n",
      "Ep 1 (Step 019385): Train loss 1.047, Val loss 1.782\n",
      "Ep 1 (Step 019390): Train loss 1.210, Val loss 1.782\n",
      "Ep 1 (Step 019395): Train loss 0.969, Val loss 1.783\n",
      "Ep 1 (Step 019400): Train loss 0.889, Val loss 1.784\n",
      "Ep 1 (Step 019405): Train loss 1.252, Val loss 1.785\n",
      "Ep 1 (Step 019410): Train loss 1.170, Val loss 1.787\n",
      "Ep 1 (Step 019415): Train loss 1.465, Val loss 1.789\n",
      "Ep 1 (Step 019420): Train loss 1.161, Val loss 1.788\n",
      "Ep 1 (Step 019425): Train loss 1.441, Val loss 1.787\n",
      "Ep 1 (Step 019430): Train loss 1.154, Val loss 1.786\n",
      "Ep 1 (Step 019435): Train loss 1.075, Val loss 1.787\n",
      "Ep 1 (Step 019440): Train loss 1.364, Val loss 1.788\n",
      "Ep 1 (Step 019445): Train loss 1.140, Val loss 1.788\n",
      "Ep 1 (Step 019450): Train loss 1.184, Val loss 1.786\n",
      "Ep 1 (Step 019455): Train loss 1.171, Val loss 1.783\n",
      "Ep 1 (Step 019460): Train loss 1.170, Val loss 1.779\n",
      "Ep 1 (Step 019465): Train loss 1.355, Val loss 1.776\n",
      "Ep 1 (Step 019470): Train loss 1.323, Val loss 1.774\n",
      "Ep 1 (Step 019475): Train loss 1.299, Val loss 1.773\n",
      "Ep 1 (Step 019480): Train loss 1.393, Val loss 1.773\n",
      "Ep 1 (Step 019485): Train loss 1.425, Val loss 1.774\n",
      "Ep 1 (Step 019490): Train loss 1.555, Val loss 1.775\n",
      "Ep 1 (Step 019495): Train loss 1.062, Val loss 1.777\n",
      "Ep 1 (Step 019500): Train loss 1.214, Val loss 1.780\n",
      "Ep 1 (Step 019505): Train loss 1.267, Val loss 1.782\n",
      "Ep 1 (Step 019510): Train loss 1.343, Val loss 1.780\n",
      "Ep 1 (Step 019515): Train loss 1.048, Val loss 1.778\n",
      "Ep 1 (Step 019520): Train loss 1.176, Val loss 1.776\n",
      "Ep 1 (Step 019525): Train loss 1.275, Val loss 1.775\n",
      "Ep 1 (Step 019530): Train loss 1.209, Val loss 1.775\n",
      "Ep 1 (Step 019535): Train loss 1.213, Val loss 1.776\n",
      "Ep 1 (Step 019540): Train loss 1.220, Val loss 1.777\n",
      "Ep 1 (Step 019545): Train loss 1.233, Val loss 1.778\n",
      "Ep 1 (Step 019550): Train loss 1.523, Val loss 1.779\n",
      "Ep 1 (Step 019555): Train loss 0.938, Val loss 1.779\n",
      "Ep 1 (Step 019560): Train loss 1.319, Val loss 1.780\n",
      "Ep 1 (Step 019565): Train loss 0.916, Val loss 1.781\n",
      "Ep 1 (Step 019570): Train loss 1.248, Val loss 1.781\n",
      "Ep 1 (Step 019575): Train loss 1.122, Val loss 1.780\n",
      "Ep 1 (Step 019580): Train loss 1.113, Val loss 1.779\n",
      "Ep 1 (Step 019585): Train loss 1.064, Val loss 1.778\n",
      "Ep 1 (Step 019590): Train loss 1.137, Val loss 1.777\n",
      "Ep 1 (Step 019595): Train loss 1.136, Val loss 1.777\n",
      "Ep 1 (Step 019600): Train loss 1.428, Val loss 1.778\n",
      "Ep 1 (Step 019605): Train loss 1.141, Val loss 1.780\n",
      "Ep 1 (Step 019610): Train loss 1.082, Val loss 1.781\n",
      "Ep 1 (Step 019615): Train loss 1.125, Val loss 1.781\n",
      "Ep 1 (Step 019620): Train loss 1.196, Val loss 1.779\n",
      "Ep 1 (Step 019625): Train loss 1.292, Val loss 1.776\n",
      "Ep 1 (Step 019630): Train loss 1.248, Val loss 1.774\n",
      "Ep 1 (Step 019635): Train loss 1.282, Val loss 1.771\n",
      "Ep 1 (Step 019640): Train loss 1.386, Val loss 1.770\n",
      "Ep 1 (Step 019645): Train loss 1.390, Val loss 1.768\n",
      "Ep 1 (Step 019650): Train loss 1.351, Val loss 1.767\n",
      "Ep 1 (Step 019655): Train loss 1.130, Val loss 1.766\n",
      "Ep 1 (Step 019660): Train loss 1.096, Val loss 1.765\n",
      "Ep 1 (Step 019665): Train loss 1.154, Val loss 1.764\n",
      "Ep 1 (Step 019670): Train loss 1.190, Val loss 1.766\n",
      "Ep 1 (Step 019675): Train loss 1.493, Val loss 1.768\n",
      "Ep 1 (Step 019680): Train loss 1.005, Val loss 1.769\n",
      "Ep 1 (Step 019685): Train loss 1.166, Val loss 1.770\n",
      "Ep 1 (Step 019690): Train loss 1.172, Val loss 1.770\n",
      "Ep 1 (Step 019695): Train loss 1.279, Val loss 1.771\n",
      "Ep 1 (Step 019700): Train loss 1.275, Val loss 1.771\n",
      "Ep 1 (Step 019705): Train loss 1.367, Val loss 1.770\n",
      "Ep 1 (Step 019710): Train loss 1.388, Val loss 1.772\n",
      "Ep 1 (Step 019715): Train loss 1.351, Val loss 1.772\n",
      "Ep 1 (Step 019720): Train loss 1.073, Val loss 1.771\n",
      "Ep 1 (Step 019725): Train loss 1.105, Val loss 1.770\n",
      "Ep 1 (Step 019730): Train loss 1.201, Val loss 1.771\n",
      "Ep 1 (Step 019735): Train loss 1.045, Val loss 1.771\n",
      "Ep 1 (Step 019740): Train loss 1.085, Val loss 1.772\n",
      "Ep 1 (Step 019745): Train loss 1.199, Val loss 1.774\n",
      "Ep 1 (Step 019750): Train loss 1.008, Val loss 1.775\n",
      "Ep 1 (Step 019755): Train loss 1.296, Val loss 1.775\n",
      "Ep 1 (Step 019760): Train loss 0.826, Val loss 1.776\n",
      "Ep 1 (Step 019765): Train loss 1.501, Val loss 1.775\n",
      "Ep 1 (Step 019770): Train loss 1.250, Val loss 1.775\n",
      "Ep 1 (Step 019775): Train loss 0.977, Val loss 1.774\n",
      "Ep 1 (Step 019780): Train loss 1.137, Val loss 1.774\n",
      "Ep 1 (Step 019785): Train loss 1.205, Val loss 1.775\n",
      "Ep 1 (Step 019790): Train loss 1.298, Val loss 1.776\n",
      "Ep 1 (Step 019795): Train loss 1.098, Val loss 1.777\n",
      "Ep 1 (Step 019800): Train loss 1.504, Val loss 1.776\n",
      "Ep 1 (Step 019805): Train loss 1.401, Val loss 1.776\n",
      "Ep 1 (Step 019810): Train loss 1.173, Val loss 1.774\n",
      "Ep 1 (Step 019815): Train loss 1.119, Val loss 1.773\n",
      "Ep 1 (Step 019820): Train loss 1.112, Val loss 1.773\n",
      "Ep 1 (Step 019825): Train loss 1.127, Val loss 1.774\n",
      "Ep 1 (Step 019830): Train loss 1.290, Val loss 1.774\n",
      "Ep 1 (Step 019835): Train loss 1.245, Val loss 1.773\n",
      "Ep 1 (Step 019840): Train loss 1.030, Val loss 1.772\n",
      "Ep 1 (Step 019845): Train loss 0.986, Val loss 1.772\n",
      "Ep 1 (Step 019850): Train loss 1.340, Val loss 1.773\n",
      "Ep 1 (Step 019855): Train loss 1.419, Val loss 1.773\n",
      "Ep 1 (Step 019860): Train loss 1.134, Val loss 1.776\n",
      "Ep 1 (Step 019865): Train loss 1.229, Val loss 1.776\n",
      "Ep 1 (Step 019870): Train loss 1.393, Val loss 1.777\n",
      "Ep 1 (Step 019875): Train loss 1.400, Val loss 1.779\n",
      "Ep 1 (Step 019880): Train loss 0.933, Val loss 1.781\n",
      "Ep 1 (Step 019885): Train loss 1.086, Val loss 1.783\n",
      "Ep 1 (Step 019890): Train loss 1.145, Val loss 1.784\n",
      "Ep 1 (Step 019895): Train loss 1.213, Val loss 1.784\n",
      "Ep 1 (Step 019900): Train loss 1.251, Val loss 1.783\n",
      "Ep 1 (Step 019905): Train loss 1.635, Val loss 1.783\n",
      "Ep 1 (Step 019910): Train loss 1.469, Val loss 1.784\n",
      "Ep 1 (Step 019915): Train loss 1.201, Val loss 1.787\n",
      "Ep 1 (Step 019920): Train loss 1.238, Val loss 1.789\n",
      "Ep 1 (Step 019925): Train loss 1.165, Val loss 1.787\n",
      "Ep 1 (Step 019930): Train loss 1.262, Val loss 1.785\n",
      "Ep 1 (Step 019935): Train loss 0.991, Val loss 1.782\n",
      "Ep 1 (Step 019940): Train loss 1.329, Val loss 1.780\n",
      "Ep 1 (Step 019945): Train loss 1.559, Val loss 1.779\n",
      "Ep 1 (Step 019950): Train loss 1.098, Val loss 1.779\n",
      "Ep 1 (Step 019955): Train loss 1.362, Val loss 1.780\n",
      "Ep 1 (Step 019960): Train loss 1.027, Val loss 1.780\n",
      "Ep 1 (Step 019965): Train loss 1.401, Val loss 1.779\n",
      "Ep 1 (Step 019970): Train loss 1.194, Val loss 1.777\n",
      "Ep 1 (Step 019975): Train loss 1.157, Val loss 1.775\n",
      "Ep 1 (Step 019980): Train loss 1.256, Val loss 1.775\n",
      "Ep 1 (Step 019985): Train loss 1.371, Val loss 1.775\n",
      "Ep 1 (Step 019990): Train loss 1.130, Val loss 1.775\n",
      "Ep 1 (Step 019995): Train loss 1.227, Val loss 1.776\n",
      "Ep 1 (Step 020000): Train loss 1.243, Val loss 1.775\n",
      "Ep 1 (Step 020005): Train loss 0.943, Val loss 1.774\n",
      "Ep 1 (Step 020010): Train loss 1.229, Val loss 1.775\n",
      "Ep 1 (Step 020015): Train loss 1.246, Val loss 1.775\n",
      "Ep 1 (Step 020020): Train loss 1.318, Val loss 1.775\n",
      "Ep 1 (Step 020025): Train loss 1.243, Val loss 1.774\n",
      "Ep 1 (Step 020030): Train loss 1.375, Val loss 1.772\n",
      "Ep 1 (Step 020035): Train loss 1.559, Val loss 1.772\n",
      "Ep 1 (Step 020040): Train loss 0.954, Val loss 1.774\n",
      "Ep 1 (Step 020045): Train loss 1.435, Val loss 1.775\n",
      "Ep 1 (Step 020050): Train loss 1.015, Val loss 1.776\n",
      "Ep 1 (Step 020055): Train loss 1.320, Val loss 1.776\n",
      "Ep 1 (Step 020060): Train loss 1.170, Val loss 1.775\n",
      "Ep 1 (Step 020065): Train loss 1.006, Val loss 1.777\n",
      "Ep 1 (Step 020070): Train loss 0.948, Val loss 1.780\n",
      "Ep 1 (Step 020075): Train loss 1.189, Val loss 1.782\n",
      "Ep 1 (Step 020080): Train loss 1.226, Val loss 1.783\n",
      "Ep 1 (Step 020085): Train loss 1.346, Val loss 1.784\n",
      "Ep 1 (Step 020090): Train loss 1.143, Val loss 1.785\n",
      "Ep 1 (Step 020095): Train loss 1.328, Val loss 1.786\n",
      "Ep 1 (Step 020100): Train loss 1.219, Val loss 1.787\n",
      "Ep 1 (Step 020105): Train loss 1.064, Val loss 1.787\n",
      "Ep 1 (Step 020110): Train loss 1.177, Val loss 1.786\n",
      "Ep 1 (Step 020115): Train loss 0.976, Val loss 1.785\n",
      "Ep 1 (Step 020120): Train loss 0.938, Val loss 1.786\n",
      "Ep 1 (Step 020125): Train loss 1.115, Val loss 1.788\n",
      "Ep 1 (Step 020130): Train loss 1.077, Val loss 1.790\n",
      "Ep 1 (Step 020135): Train loss 1.258, Val loss 1.791\n",
      "Ep 1 (Step 020140): Train loss 1.446, Val loss 1.791\n",
      "Ep 1 (Step 020145): Train loss 1.318, Val loss 1.790\n",
      "Ep 1 (Step 020150): Train loss 1.206, Val loss 1.785\n",
      "Ep 1 (Step 020155): Train loss 1.286, Val loss 1.779\n",
      "Ep 1 (Step 020160): Train loss 1.257, Val loss 1.776\n",
      "Ep 1 (Step 020165): Train loss 1.364, Val loss 1.775\n",
      "Ep 1 (Step 020170): Train loss 1.208, Val loss 1.775\n",
      "Ep 1 (Step 020175): Train loss 1.163, Val loss 1.775\n",
      "Ep 1 (Step 020180): Train loss 1.171, Val loss 1.775\n",
      "Ep 1 (Step 020185): Train loss 0.916, Val loss 1.776\n",
      "Ep 1 (Step 020190): Train loss 1.396, Val loss 1.775\n",
      "Ep 1 (Step 020195): Train loss 1.637, Val loss 1.774\n",
      "Ep 1 (Step 020200): Train loss 0.853, Val loss 1.772\n",
      "Ep 1 (Step 020205): Train loss 1.014, Val loss 1.771\n",
      "Ep 1 (Step 020210): Train loss 1.208, Val loss 1.769\n",
      "Ep 1 (Step 020215): Train loss 1.400, Val loss 1.769\n",
      "Ep 1 (Step 020220): Train loss 1.553, Val loss 1.768\n",
      "Ep 1 (Step 020225): Train loss 1.510, Val loss 1.766\n",
      "Ep 1 (Step 020230): Train loss 1.044, Val loss 1.764\n",
      "Ep 1 (Step 020235): Train loss 1.028, Val loss 1.763\n",
      "Ep 1 (Step 020240): Train loss 1.042, Val loss 1.762\n",
      "Ep 1 (Step 020245): Train loss 1.345, Val loss 1.761\n",
      "Ep 1 (Step 020250): Train loss 1.416, Val loss 1.761\n",
      "Ep 1 (Step 020255): Train loss 1.075, Val loss 1.761\n",
      "Ep 1 (Step 020260): Train loss 1.170, Val loss 1.762\n",
      "Ep 1 (Step 020265): Train loss 1.240, Val loss 1.762\n",
      "Ep 1 (Step 020270): Train loss 1.118, Val loss 1.762\n",
      "Ep 1 (Step 020275): Train loss 1.151, Val loss 1.764\n",
      "Ep 1 (Step 020280): Train loss 1.071, Val loss 1.766\n",
      "Ep 1 (Step 020285): Train loss 1.019, Val loss 1.769\n",
      "Ep 1 (Step 020290): Train loss 1.013, Val loss 1.771\n",
      "Ep 1 (Step 020295): Train loss 1.201, Val loss 1.773\n",
      "Ep 1 (Step 020300): Train loss 1.213, Val loss 1.775\n",
      "Ep 1 (Step 020305): Train loss 1.228, Val loss 1.777\n",
      "Ep 1 (Step 020310): Train loss 1.179, Val loss 1.779\n",
      "Ep 1 (Step 020315): Train loss 1.061, Val loss 1.780\n",
      "Ep 1 (Step 020320): Train loss 1.299, Val loss 1.778\n",
      "Ep 1 (Step 020325): Train loss 1.478, Val loss 1.775\n",
      "Ep 1 (Step 020330): Train loss 1.154, Val loss 1.773\n",
      "Ep 1 (Step 020335): Train loss 1.309, Val loss 1.773\n",
      "Ep 1 (Step 020340): Train loss 1.130, Val loss 1.772\n",
      "Ep 1 (Step 020345): Train loss 1.254, Val loss 1.770\n",
      "Ep 1 (Step 020350): Train loss 1.031, Val loss 1.767\n",
      "Ep 1 (Step 020355): Train loss 1.003, Val loss 1.768\n",
      "Ep 1 (Step 020360): Train loss 1.432, Val loss 1.769\n",
      "Ep 1 (Step 020365): Train loss 1.193, Val loss 1.768\n",
      "Ep 1 (Step 020370): Train loss 1.358, Val loss 1.768\n",
      "Ep 1 (Step 020375): Train loss 1.043, Val loss 1.769\n",
      "Ep 1 (Step 020380): Train loss 1.213, Val loss 1.770\n",
      "Ep 1 (Step 020385): Train loss 1.319, Val loss 1.769\n",
      "Ep 1 (Step 020390): Train loss 1.195, Val loss 1.768\n",
      "Ep 1 (Step 020395): Train loss 1.073, Val loss 1.770\n",
      "Ep 1 (Step 020400): Train loss 1.299, Val loss 1.773\n",
      "Ep 1 (Step 020405): Train loss 1.175, Val loss 1.775\n",
      "Ep 1 (Step 020410): Train loss 1.063, Val loss 1.775\n",
      "Ep 1 (Step 020415): Train loss 0.990, Val loss 1.776\n",
      "Ep 1 (Step 020420): Train loss 1.489, Val loss 1.775\n",
      "Ep 1 (Step 020425): Train loss 1.380, Val loss 1.774\n",
      "Ep 1 (Step 020430): Train loss 1.193, Val loss 1.775\n",
      "Ep 1 (Step 020435): Train loss 1.249, Val loss 1.776\n",
      "Ep 1 (Step 020440): Train loss 1.274, Val loss 1.776\n",
      "Ep 1 (Step 020445): Train loss 1.107, Val loss 1.776\n",
      "Ep 1 (Step 020450): Train loss 1.084, Val loss 1.774\n",
      "Ep 1 (Step 020455): Train loss 1.416, Val loss 1.772\n",
      "Ep 1 (Step 020460): Train loss 1.206, Val loss 1.772\n",
      "Ep 1 (Step 020465): Train loss 1.303, Val loss 1.773\n",
      "Ep 1 (Step 020470): Train loss 1.112, Val loss 1.775\n",
      "Ep 1 (Step 020475): Train loss 1.002, Val loss 1.777\n",
      "Ep 1 (Step 020480): Train loss 1.142, Val loss 1.779\n",
      "Ep 1 (Step 020485): Train loss 1.081, Val loss 1.779\n",
      "Ep 1 (Step 020490): Train loss 1.086, Val loss 1.779\n",
      "Ep 1 (Step 020495): Train loss 1.523, Val loss 1.780\n",
      "Ep 1 (Step 020500): Train loss 1.477, Val loss 1.780\n",
      "Ep 1 (Step 020505): Train loss 1.447, Val loss 1.780\n",
      "Ep 1 (Step 020510): Train loss 1.014, Val loss 1.778\n",
      "Ep 1 (Step 020515): Train loss 1.290, Val loss 1.777\n",
      "Ep 1 (Step 020520): Train loss 1.054, Val loss 1.777\n",
      "Ep 1 (Step 020525): Train loss 1.092, Val loss 1.776\n",
      "Ep 1 (Step 020530): Train loss 1.203, Val loss 1.775\n",
      "Ep 1 (Step 020535): Train loss 1.046, Val loss 1.774\n",
      "Ep 1 (Step 020540): Train loss 1.269, Val loss 1.774\n",
      "Ep 1 (Step 020545): Train loss 1.220, Val loss 1.773\n",
      "Ep 1 (Step 020550): Train loss 1.453, Val loss 1.772\n",
      "Ep 1 (Step 020555): Train loss 1.296, Val loss 1.772\n",
      "Ep 1 (Step 020560): Train loss 1.272, Val loss 1.774\n",
      "Ep 1 (Step 020565): Train loss 1.307, Val loss 1.776\n",
      "Ep 1 (Step 020570): Train loss 0.992, Val loss 1.778\n",
      "Ep 1 (Step 020575): Train loss 1.301, Val loss 1.780\n",
      "Ep 1 (Step 020580): Train loss 1.325, Val loss 1.781\n",
      "Ep 1 (Step 020585): Train loss 1.170, Val loss 1.783\n",
      "Ep 1 (Step 020590): Train loss 1.641, Val loss 1.785\n",
      "Ep 1 (Step 020595): Train loss 1.069, Val loss 1.786\n",
      "Ep 1 (Step 020600): Train loss 1.127, Val loss 1.787\n",
      "Ep 1 (Step 020605): Train loss 0.898, Val loss 1.788\n",
      "Ep 1 (Step 020610): Train loss 1.072, Val loss 1.789\n",
      "Ep 1 (Step 020615): Train loss 1.083, Val loss 1.787\n",
      "Ep 1 (Step 020620): Train loss 1.034, Val loss 1.787\n",
      "Ep 1 (Step 020625): Train loss 1.143, Val loss 1.787\n",
      "Ep 1 (Step 020630): Train loss 0.954, Val loss 1.787\n",
      "Ep 1 (Step 020635): Train loss 1.060, Val loss 1.787\n",
      "Ep 1 (Step 020640): Train loss 1.342, Val loss 1.787\n",
      "Ep 1 (Step 020645): Train loss 1.373, Val loss 1.787\n",
      "Ep 1 (Step 020650): Train loss 0.992, Val loss 1.787\n",
      "Ep 1 (Step 020655): Train loss 1.474, Val loss 1.785\n",
      "Ep 1 (Step 020660): Train loss 1.324, Val loss 1.783\n",
      "Ep 1 (Step 020665): Train loss 1.119, Val loss 1.781\n",
      "Ep 1 (Step 020670): Train loss 0.966, Val loss 1.780\n",
      "Ep 1 (Step 020675): Train loss 1.419, Val loss 1.780\n",
      "Ep 1 (Step 020680): Train loss 1.090, Val loss 1.781\n",
      "Ep 1 (Step 020685): Train loss 1.396, Val loss 1.780\n",
      "Ep 1 (Step 020690): Train loss 1.220, Val loss 1.780\n",
      "Ep 1 (Step 020695): Train loss 1.375, Val loss 1.780\n",
      "Ep 1 (Step 020700): Train loss 1.459, Val loss 1.779\n",
      "Ep 1 (Step 020705): Train loss 1.360, Val loss 1.779\n",
      "Ep 1 (Step 020710): Train loss 1.243, Val loss 1.779\n",
      "Ep 1 (Step 020715): Train loss 1.339, Val loss 1.779\n",
      "Ep 1 (Step 020720): Train loss 1.290, Val loss 1.777\n",
      "Ep 1 (Step 020725): Train loss 1.187, Val loss 1.777\n",
      "Ep 1 (Step 020730): Train loss 0.831, Val loss 1.775\n",
      "Ep 1 (Step 020735): Train loss 1.088, Val loss 1.774\n",
      "Ep 1 (Step 020740): Train loss 1.039, Val loss 1.772\n",
      "Ep 1 (Step 020745): Train loss 1.048, Val loss 1.771\n",
      "Ep 1 (Step 020750): Train loss 1.132, Val loss 1.770\n",
      "Ep 1 (Step 020755): Train loss 1.373, Val loss 1.768\n",
      "Ep 1 (Step 020760): Train loss 0.932, Val loss 1.766\n",
      "Ep 1 (Step 020765): Train loss 1.150, Val loss 1.766\n",
      "Ep 1 (Step 020770): Train loss 1.173, Val loss 1.766\n",
      "Ep 1 (Step 020775): Train loss 1.021, Val loss 1.766\n",
      "Ep 1 (Step 020780): Train loss 1.255, Val loss 1.768\n",
      "Ep 1 (Step 020785): Train loss 1.751, Val loss 1.771\n",
      "Ep 1 (Step 020790): Train loss 1.152, Val loss 1.773\n",
      "Ep 1 (Step 020795): Train loss 1.323, Val loss 1.773\n",
      "Ep 1 (Step 020800): Train loss 1.331, Val loss 1.774\n",
      "Ep 1 (Step 020805): Train loss 1.356, Val loss 1.775\n",
      "Ep 1 (Step 020810): Train loss 1.171, Val loss 1.777\n",
      "Ep 1 (Step 020815): Train loss 1.252, Val loss 1.779\n",
      "Ep 1 (Step 020820): Train loss 0.987, Val loss 1.781\n",
      "Ep 1 (Step 020825): Train loss 1.367, Val loss 1.783\n",
      "Ep 1 (Step 020830): Train loss 1.291, Val loss 1.784\n",
      "Ep 1 (Step 020835): Train loss 1.119, Val loss 1.784\n",
      "Ep 1 (Step 020840): Train loss 1.334, Val loss 1.784\n",
      "Ep 1 (Step 020845): Train loss 1.098, Val loss 1.785\n",
      "Ep 1 (Step 020850): Train loss 1.098, Val loss 1.786\n",
      "Ep 1 (Step 020855): Train loss 1.411, Val loss 1.786\n",
      "Ep 1 (Step 020860): Train loss 1.147, Val loss 1.786\n",
      "Ep 1 (Step 020865): Train loss 1.067, Val loss 1.788\n",
      "Ep 1 (Step 020870): Train loss 0.960, Val loss 1.790\n",
      "Ep 1 (Step 020875): Train loss 1.164, Val loss 1.791\n",
      "Ep 1 (Step 020880): Train loss 1.188, Val loss 1.791\n",
      "Ep 1 (Step 020885): Train loss 1.125, Val loss 1.792\n",
      "Ep 1 (Step 020890): Train loss 1.233, Val loss 1.793\n",
      "Ep 1 (Step 020895): Train loss 1.311, Val loss 1.791\n",
      "Ep 1 (Step 020900): Train loss 1.042, Val loss 1.787\n",
      "Ep 1 (Step 020905): Train loss 1.105, Val loss 1.782\n",
      "Ep 1 (Step 020910): Train loss 1.179, Val loss 1.778\n",
      "Ep 1 (Step 020915): Train loss 1.176, Val loss 1.776\n",
      "Ep 1 (Step 020920): Train loss 1.536, Val loss 1.776\n",
      "Ep 1 (Step 020925): Train loss 1.204, Val loss 1.775\n",
      "Ep 1 (Step 020930): Train loss 1.451, Val loss 1.775\n",
      "Ep 1 (Step 020935): Train loss 1.391, Val loss 1.776\n",
      "Ep 1 (Step 020940): Train loss 1.141, Val loss 1.778\n",
      "Ep 1 (Step 020945): Train loss 1.177, Val loss 1.780\n",
      "Ep 1 (Step 020950): Train loss 1.111, Val loss 1.782\n",
      "Ep 1 (Step 020955): Train loss 1.209, Val loss 1.782\n",
      "Ep 1 (Step 020960): Train loss 1.192, Val loss 1.781\n",
      "Ep 1 (Step 020965): Train loss 1.168, Val loss 1.781\n",
      "Ep 1 (Step 020970): Train loss 1.145, Val loss 1.779\n",
      "Ep 1 (Step 020975): Train loss 1.228, Val loss 1.779\n",
      "Ep 1 (Step 020980): Train loss 1.281, Val loss 1.780\n",
      "Ep 1 (Step 020985): Train loss 1.082, Val loss 1.782\n",
      "Ep 1 (Step 020990): Train loss 1.391, Val loss 1.784\n",
      "Ep 1 (Step 020995): Train loss 1.368, Val loss 1.785\n",
      "Ep 1 (Step 021000): Train loss 1.408, Val loss 1.785\n",
      "Ep 1 (Step 021005): Train loss 1.021, Val loss 1.786\n",
      "Ep 1 (Step 021010): Train loss 1.176, Val loss 1.788\n",
      "Ep 1 (Step 021015): Train loss 0.986, Val loss 1.789\n",
      "Ep 1 (Step 021020): Train loss 1.490, Val loss 1.791\n",
      "Ep 1 (Step 021025): Train loss 0.877, Val loss 1.792\n",
      "Ep 1 (Step 021030): Train loss 1.132, Val loss 1.791\n",
      "Ep 1 (Step 021035): Train loss 1.330, Val loss 1.789\n",
      "Ep 1 (Step 021040): Train loss 1.352, Val loss 1.785\n",
      "Ep 1 (Step 021045): Train loss 1.399, Val loss 1.782\n",
      "Ep 1 (Step 021050): Train loss 1.178, Val loss 1.781\n",
      "Ep 1 (Step 021055): Train loss 1.226, Val loss 1.781\n",
      "Ep 1 (Step 021060): Train loss 1.024, Val loss 1.781\n",
      "Ep 1 (Step 021065): Train loss 1.041, Val loss 1.780\n",
      "Ep 1 (Step 021070): Train loss 1.010, Val loss 1.780\n",
      "Ep 1 (Step 021075): Train loss 1.082, Val loss 1.779\n",
      "Ep 1 (Step 021080): Train loss 1.400, Val loss 1.778\n",
      "Ep 1 (Step 021085): Train loss 1.005, Val loss 1.776\n",
      "Ep 1 (Step 021090): Train loss 1.195, Val loss 1.776\n",
      "Ep 1 (Step 021095): Train loss 1.084, Val loss 1.777\n",
      "Ep 1 (Step 021100): Train loss 1.022, Val loss 1.778\n",
      "Ep 1 (Step 021105): Train loss 1.488, Val loss 1.779\n",
      "Ep 1 (Step 021110): Train loss 1.175, Val loss 1.780\n",
      "Ep 1 (Step 021115): Train loss 1.099, Val loss 1.780\n",
      "Ep 1 (Step 021120): Train loss 1.093, Val loss 1.781\n",
      "Ep 1 (Step 021125): Train loss 0.973, Val loss 1.783\n",
      "Ep 1 (Step 021130): Train loss 1.133, Val loss 1.782\n",
      "Ep 1 (Step 021135): Train loss 1.269, Val loss 1.782\n",
      "Ep 1 (Step 021140): Train loss 1.190, Val loss 1.782\n",
      "Ep 1 (Step 021145): Train loss 1.286, Val loss 1.782\n",
      "Ep 1 (Step 021150): Train loss 1.406, Val loss 1.784\n",
      "Ep 1 (Step 021155): Train loss 1.068, Val loss 1.787\n",
      "Ep 1 (Step 021160): Train loss 1.177, Val loss 1.789\n",
      "Ep 1 (Step 021165): Train loss 1.074, Val loss 1.789\n",
      "Ep 1 (Step 021170): Train loss 1.281, Val loss 1.787\n",
      "Ep 1 (Step 021175): Train loss 1.405, Val loss 1.783\n",
      "Ep 1 (Step 021180): Train loss 0.873, Val loss 1.780\n",
      "Ep 1 (Step 021185): Train loss 1.260, Val loss 1.777\n",
      "Ep 1 (Step 021190): Train loss 1.456, Val loss 1.776\n",
      "Ep 1 (Step 021195): Train loss 1.299, Val loss 1.776\n",
      "Ep 1 (Step 021200): Train loss 1.377, Val loss 1.777\n",
      "Ep 1 (Step 021205): Train loss 1.428, Val loss 1.777\n",
      "Ep 1 (Step 021210): Train loss 1.261, Val loss 1.778\n",
      "Ep 1 (Step 021215): Train loss 1.259, Val loss 1.779\n",
      "Ep 1 (Step 021220): Train loss 1.185, Val loss 1.779\n",
      "Ep 1 (Step 021225): Train loss 1.156, Val loss 1.780\n",
      "Ep 1 (Step 021230): Train loss 1.167, Val loss 1.780\n",
      "Ep 1 (Step 021235): Train loss 1.200, Val loss 1.778\n",
      "Ep 1 (Step 021240): Train loss 1.108, Val loss 1.776\n",
      "Ep 1 (Step 021245): Train loss 0.994, Val loss 1.774\n",
      "Ep 1 (Step 021250): Train loss 1.058, Val loss 1.771\n",
      "Ep 1 (Step 021255): Train loss 1.228, Val loss 1.770\n",
      "Ep 1 (Step 021260): Train loss 0.880, Val loss 1.771\n",
      "Ep 1 (Step 021265): Train loss 1.322, Val loss 1.772\n",
      "Ep 1 (Step 021270): Train loss 1.088, Val loss 1.772\n",
      "Ep 1 (Step 021275): Train loss 1.279, Val loss 1.771\n",
      "Ep 1 (Step 021280): Train loss 1.125, Val loss 1.770\n",
      "Ep 1 (Step 021285): Train loss 1.256, Val loss 1.771\n",
      "Ep 1 (Step 021290): Train loss 1.491, Val loss 1.770\n",
      "Ep 1 (Step 021295): Train loss 1.384, Val loss 1.772\n",
      "Ep 1 (Step 021300): Train loss 1.079, Val loss 1.774\n",
      "Ep 1 (Step 021305): Train loss 1.363, Val loss 1.775\n",
      "Ep 1 (Step 021310): Train loss 1.070, Val loss 1.774\n",
      "Ep 1 (Step 021315): Train loss 1.199, Val loss 1.773\n",
      "Ep 1 (Step 021320): Train loss 1.145, Val loss 1.772\n",
      "Ep 1 (Step 021325): Train loss 0.924, Val loss 1.774\n",
      "Ep 1 (Step 021330): Train loss 1.198, Val loss 1.773\n",
      "Ep 1 (Step 021335): Train loss 1.246, Val loss 1.772\n",
      "Ep 1 (Step 021340): Train loss 1.387, Val loss 1.771\n",
      "Ep 1 (Step 021345): Train loss 1.106, Val loss 1.771\n",
      "Ep 1 (Step 021350): Train loss 1.265, Val loss 1.770\n",
      "Ep 1 (Step 021355): Train loss 1.156, Val loss 1.771\n",
      "Ep 1 (Step 021360): Train loss 1.170, Val loss 1.773\n",
      "Ep 1 (Step 021365): Train loss 1.167, Val loss 1.774\n",
      "Ep 1 (Step 021370): Train loss 1.297, Val loss 1.774\n",
      "Ep 1 (Step 021375): Train loss 1.082, Val loss 1.773\n",
      "Ep 1 (Step 021380): Train loss 1.391, Val loss 1.773\n",
      "Ep 1 (Step 021385): Train loss 0.975, Val loss 1.772\n",
      "Ep 1 (Step 021390): Train loss 1.305, Val loss 1.771\n",
      "Ep 1 (Step 021395): Train loss 1.187, Val loss 1.771\n",
      "Ep 1 (Step 021400): Train loss 1.204, Val loss 1.770\n",
      "Ep 1 (Step 021405): Train loss 1.285, Val loss 1.771\n",
      "Ep 1 (Step 021410): Train loss 0.994, Val loss 1.772\n",
      "Ep 1 (Step 021415): Train loss 1.266, Val loss 1.771\n",
      "Ep 1 (Step 021420): Train loss 1.497, Val loss 1.771\n",
      "Ep 1 (Step 021425): Train loss 1.259, Val loss 1.772\n",
      "Ep 1 (Step 021430): Train loss 0.998, Val loss 1.774\n",
      "Ep 1 (Step 021435): Train loss 1.343, Val loss 1.776\n",
      "Ep 1 (Step 021440): Train loss 1.418, Val loss 1.779\n",
      "Ep 1 (Step 021445): Train loss 1.306, Val loss 1.780\n",
      "Ep 1 (Step 021450): Train loss 1.205, Val loss 1.782\n",
      "Ep 1 (Step 021455): Train loss 0.900, Val loss 1.783\n",
      "Ep 1 (Step 021460): Train loss 1.183, Val loss 1.785\n",
      "Ep 1 (Step 021465): Train loss 1.122, Val loss 1.784\n",
      "Ep 1 (Step 021470): Train loss 0.796, Val loss 1.784\n",
      "Ep 1 (Step 021475): Train loss 1.170, Val loss 1.782\n",
      "Ep 1 (Step 021480): Train loss 1.331, Val loss 1.782\n",
      "Ep 1 (Step 021485): Train loss 1.293, Val loss 1.783\n",
      "Ep 1 (Step 021490): Train loss 1.268, Val loss 1.782\n",
      "Ep 1 (Step 021495): Train loss 1.027, Val loss 1.780\n",
      "Ep 1 (Step 021500): Train loss 1.180, Val loss 1.777\n",
      "Ep 1 (Step 021505): Train loss 1.203, Val loss 1.775\n",
      "Ep 1 (Step 021510): Train loss 1.384, Val loss 1.774\n",
      "Ep 1 (Step 021515): Train loss 1.338, Val loss 1.774\n",
      "Ep 1 (Step 021520): Train loss 1.261, Val loss 1.773\n",
      "Ep 1 (Step 021525): Train loss 1.085, Val loss 1.774\n",
      "Ep 1 (Step 021530): Train loss 1.185, Val loss 1.776\n",
      "Ep 1 (Step 021535): Train loss 1.057, Val loss 1.778\n",
      "Ep 1 (Step 021540): Train loss 0.771, Val loss 1.779\n",
      "Ep 1 (Step 021545): Train loss 1.080, Val loss 1.781\n",
      "Ep 1 (Step 021550): Train loss 1.066, Val loss 1.782\n",
      "Ep 1 (Step 021555): Train loss 1.017, Val loss 1.783\n",
      "Ep 1 (Step 021560): Train loss 1.378, Val loss 1.784\n",
      "Ep 1 (Step 021565): Train loss 1.356, Val loss 1.784\n",
      "Ep 1 (Step 021570): Train loss 1.429, Val loss 1.783\n",
      "Ep 1 (Step 021575): Train loss 1.123, Val loss 1.782\n",
      "Ep 1 (Step 021580): Train loss 1.356, Val loss 1.783\n",
      "Ep 1 (Step 021585): Train loss 1.216, Val loss 1.783\n",
      "Ep 1 (Step 021590): Train loss 1.246, Val loss 1.782\n",
      "Ep 1 (Step 021595): Train loss 1.328, Val loss 1.781\n",
      "Ep 1 (Step 021600): Train loss 0.950, Val loss 1.781\n",
      "Ep 1 (Step 021605): Train loss 1.083, Val loss 1.780\n",
      "Ep 1 (Step 021610): Train loss 1.127, Val loss 1.780\n",
      "Ep 1 (Step 021615): Train loss 1.182, Val loss 1.780\n",
      "Ep 1 (Step 021620): Train loss 1.300, Val loss 1.782\n",
      "Ep 1 (Step 021625): Train loss 1.256, Val loss 1.784\n",
      "Ep 1 (Step 021630): Train loss 1.120, Val loss 1.784\n",
      "Ep 1 (Step 021635): Train loss 1.083, Val loss 1.783\n",
      "Ep 1 (Step 021640): Train loss 1.230, Val loss 1.780\n",
      "Ep 1 (Step 021645): Train loss 1.205, Val loss 1.779\n",
      "Ep 1 (Step 021650): Train loss 1.151, Val loss 1.780\n",
      "Ep 1 (Step 021655): Train loss 1.348, Val loss 1.781\n",
      "Ep 1 (Step 021660): Train loss 1.284, Val loss 1.782\n",
      "Ep 1 (Step 021665): Train loss 1.267, Val loss 1.783\n",
      "Ep 1 (Step 021670): Train loss 1.228, Val loss 1.783\n",
      "Ep 1 (Step 021675): Train loss 1.208, Val loss 1.783\n",
      "Ep 1 (Step 021680): Train loss 0.862, Val loss 1.783\n",
      "Ep 1 (Step 021685): Train loss 1.107, Val loss 1.783\n",
      "Ep 1 (Step 021690): Train loss 1.020, Val loss 1.783\n",
      "Ep 1 (Step 021695): Train loss 1.091, Val loss 1.781\n",
      "Ep 1 (Step 021700): Train loss 1.167, Val loss 1.781\n",
      "Ep 1 (Step 021705): Train loss 1.180, Val loss 1.780\n",
      "Ep 1 (Step 021710): Train loss 1.448, Val loss 1.779\n",
      "Ep 1 (Step 021715): Train loss 1.067, Val loss 1.779\n",
      "Ep 1 (Step 021720): Train loss 1.101, Val loss 1.779\n",
      "Ep 1 (Step 021725): Train loss 0.877, Val loss 1.779\n",
      "Ep 1 (Step 021730): Train loss 1.197, Val loss 1.779\n",
      "Ep 1 (Step 021735): Train loss 1.110, Val loss 1.779\n",
      "Ep 1 (Step 021740): Train loss 1.205, Val loss 1.778\n",
      "Ep 1 (Step 021745): Train loss 1.100, Val loss 1.778\n",
      "Ep 1 (Step 021750): Train loss 1.188, Val loss 1.779\n",
      "Ep 1 (Step 021755): Train loss 1.272, Val loss 1.780\n",
      "Ep 1 (Step 021760): Train loss 1.242, Val loss 1.781\n",
      "Ep 1 (Step 021765): Train loss 1.036, Val loss 1.780\n",
      "Ep 1 (Step 021770): Train loss 1.205, Val loss 1.778\n",
      "Ep 1 (Step 021775): Train loss 1.319, Val loss 1.777\n",
      "Ep 1 (Step 021780): Train loss 1.061, Val loss 1.777\n",
      "Ep 1 (Step 021785): Train loss 1.268, Val loss 1.776\n",
      "Ep 1 (Step 021790): Train loss 1.172, Val loss 1.775\n",
      "Ep 1 (Step 021795): Train loss 1.248, Val loss 1.774\n",
      "Ep 1 (Step 021800): Train loss 1.117, Val loss 1.774\n",
      "Ep 1 (Step 021805): Train loss 1.250, Val loss 1.773\n",
      "Ep 1 (Step 021810): Train loss 1.221, Val loss 1.773\n",
      "Ep 1 (Step 021815): Train loss 1.302, Val loss 1.773\n",
      "Ep 1 (Step 021820): Train loss 0.872, Val loss 1.773\n",
      "Ep 1 (Step 021825): Train loss 1.145, Val loss 1.773\n",
      "Ep 1 (Step 021830): Train loss 1.289, Val loss 1.774\n",
      "Ep 1 (Step 021835): Train loss 1.080, Val loss 1.775\n",
      "Ep 1 (Step 021840): Train loss 1.323, Val loss 1.776\n",
      "Ep 1 (Step 021845): Train loss 1.285, Val loss 1.774\n",
      "Ep 1 (Step 021850): Train loss 1.315, Val loss 1.772\n",
      "Ep 1 (Step 021855): Train loss 1.294, Val loss 1.773\n",
      "Ep 1 (Step 021860): Train loss 1.275, Val loss 1.774\n",
      "Ep 1 (Step 021865): Train loss 1.407, Val loss 1.774\n",
      "Ep 1 (Step 021870): Train loss 0.916, Val loss 1.774\n",
      "Ep 1 (Step 021875): Train loss 1.031, Val loss 1.774\n",
      "Ep 1 (Step 021880): Train loss 1.064, Val loss 1.774\n",
      "Ep 1 (Step 021885): Train loss 1.235, Val loss 1.771\n",
      "Ep 1 (Step 021890): Train loss 1.067, Val loss 1.768\n",
      "Ep 1 (Step 021895): Train loss 1.315, Val loss 1.767\n",
      "Ep 1 (Step 021900): Train loss 1.235, Val loss 1.766\n",
      "Ep 1 (Step 021905): Train loss 0.988, Val loss 1.765\n",
      "Ep 1 (Step 021910): Train loss 1.455, Val loss 1.763\n",
      "Ep 1 (Step 021915): Train loss 1.109, Val loss 1.762\n",
      "Ep 1 (Step 021920): Train loss 1.232, Val loss 1.763\n",
      "Ep 1 (Step 021925): Train loss 1.277, Val loss 1.764\n",
      "Ep 1 (Step 021930): Train loss 1.159, Val loss 1.765\n",
      "Ep 1 (Step 021935): Train loss 1.133, Val loss 1.767\n",
      "Ep 1 (Step 021940): Train loss 1.365, Val loss 1.770\n",
      "Ep 1 (Step 021945): Train loss 1.307, Val loss 1.771\n",
      "Ep 1 (Step 021950): Train loss 1.290, Val loss 1.771\n",
      "Ep 1 (Step 021955): Train loss 1.040, Val loss 1.772\n",
      "Ep 1 (Step 021960): Train loss 1.377, Val loss 1.775\n",
      "Ep 1 (Step 021965): Train loss 1.214, Val loss 1.776\n",
      "Ep 1 (Step 021970): Train loss 1.566, Val loss 1.779\n",
      "Ep 1 (Step 021975): Train loss 1.143, Val loss 1.780\n",
      "Ep 1 (Step 021980): Train loss 1.319, Val loss 1.781\n",
      "Ep 1 (Step 021985): Train loss 1.189, Val loss 1.782\n",
      "Ep 1 (Step 021990): Train loss 1.160, Val loss 1.783\n",
      "Ep 1 (Step 021995): Train loss 1.310, Val loss 1.784\n",
      "Ep 1 (Step 022000): Train loss 1.387, Val loss 1.783\n",
      "Ep 1 (Step 022005): Train loss 0.927, Val loss 1.783\n",
      "Ep 1 (Step 022010): Train loss 1.086, Val loss 1.781\n",
      "Ep 1 (Step 022015): Train loss 0.902, Val loss 1.779\n",
      "Ep 1 (Step 022020): Train loss 1.314, Val loss 1.776\n",
      "Ep 1 (Step 022025): Train loss 1.088, Val loss 1.774\n",
      "Ep 1 (Step 022030): Train loss 1.272, Val loss 1.771\n",
      "Ep 1 (Step 022035): Train loss 1.226, Val loss 1.770\n",
      "Ep 1 (Step 022040): Train loss 1.274, Val loss 1.769\n",
      "Ep 1 (Step 022045): Train loss 1.550, Val loss 1.768\n",
      "Ep 1 (Step 022050): Train loss 1.176, Val loss 1.768\n",
      "Ep 1 (Step 022055): Train loss 1.104, Val loss 1.766\n",
      "Ep 1 (Step 022060): Train loss 1.221, Val loss 1.766\n",
      "Ep 1 (Step 022065): Train loss 1.177, Val loss 1.767\n",
      "Ep 1 (Step 022070): Train loss 1.196, Val loss 1.769\n",
      "Ep 1 (Step 022075): Train loss 1.164, Val loss 1.773\n",
      "Ep 1 (Step 022080): Train loss 1.166, Val loss 1.775\n",
      "Ep 1 (Step 022085): Train loss 0.972, Val loss 1.774\n",
      "Ep 1 (Step 022090): Train loss 0.994, Val loss 1.774\n",
      "Ep 1 (Step 022095): Train loss 1.189, Val loss 1.773\n",
      "Ep 1 (Step 022100): Train loss 1.298, Val loss 1.772\n",
      "Ep 1 (Step 022105): Train loss 1.118, Val loss 1.772\n",
      "Ep 1 (Step 022110): Train loss 1.160, Val loss 1.773\n",
      "Ep 1 (Step 022115): Train loss 1.182, Val loss 1.774\n",
      "Ep 1 (Step 022120): Train loss 1.015, Val loss 1.775\n",
      "Ep 1 (Step 022125): Train loss 1.131, Val loss 1.777\n",
      "Ep 1 (Step 022130): Train loss 1.319, Val loss 1.779\n",
      "Ep 1 (Step 022135): Train loss 1.313, Val loss 1.780\n",
      "Ep 1 (Step 022140): Train loss 1.133, Val loss 1.779\n",
      "Ep 1 (Step 022145): Train loss 1.076, Val loss 1.779\n",
      "Ep 1 (Step 022150): Train loss 1.174, Val loss 1.780\n",
      "Ep 1 (Step 022155): Train loss 1.273, Val loss 1.781\n",
      "Ep 1 (Step 022160): Train loss 1.108, Val loss 1.781\n",
      "Ep 1 (Step 022165): Train loss 1.112, Val loss 1.783\n",
      "Ep 1 (Step 022170): Train loss 1.330, Val loss 1.784\n",
      "Ep 1 (Step 022175): Train loss 1.452, Val loss 1.783\n",
      "Ep 1 (Step 022180): Train loss 0.994, Val loss 1.785\n",
      "Ep 1 (Step 022185): Train loss 1.313, Val loss 1.786\n",
      "Ep 1 (Step 022190): Train loss 1.421, Val loss 1.787\n",
      "Ep 1 (Step 022195): Train loss 1.314, Val loss 1.786\n",
      "Ep 1 (Step 022200): Train loss 1.241, Val loss 1.786\n",
      "Ep 1 (Step 022205): Train loss 1.328, Val loss 1.784\n",
      "Ep 1 (Step 022210): Train loss 0.962, Val loss 1.783\n",
      "Ep 1 (Step 022215): Train loss 1.054, Val loss 1.780\n",
      "Ep 1 (Step 022220): Train loss 1.079, Val loss 1.778\n",
      "Ep 1 (Step 022225): Train loss 1.339, Val loss 1.775\n",
      "Ep 1 (Step 022230): Train loss 1.053, Val loss 1.774\n",
      "Ep 1 (Step 022235): Train loss 1.048, Val loss 1.774\n",
      "Ep 1 (Step 022240): Train loss 0.930, Val loss 1.774\n",
      "Ep 1 (Step 022245): Train loss 1.413, Val loss 1.774\n",
      "Ep 1 (Step 022250): Train loss 1.091, Val loss 1.774\n",
      "Ep 1 (Step 022255): Train loss 1.146, Val loss 1.773\n",
      "Ep 1 (Step 022260): Train loss 1.117, Val loss 1.774\n",
      "Ep 1 (Step 022265): Train loss 1.450, Val loss 1.776\n",
      "Ep 1 (Step 022270): Train loss 1.245, Val loss 1.776\n",
      "Ep 1 (Step 022275): Train loss 1.097, Val loss 1.776\n",
      "Ep 1 (Step 022280): Train loss 1.103, Val loss 1.777\n",
      "Ep 1 (Step 022285): Train loss 1.109, Val loss 1.778\n",
      "Ep 1 (Step 022290): Train loss 1.165, Val loss 1.778\n",
      "Ep 1 (Step 022295): Train loss 1.225, Val loss 1.779\n",
      "Ep 1 (Step 022300): Train loss 1.332, Val loss 1.778\n",
      "Ep 1 (Step 022305): Train loss 1.076, Val loss 1.776\n",
      "Ep 1 (Step 022310): Train loss 1.316, Val loss 1.775\n",
      "Ep 1 (Step 022315): Train loss 1.051, Val loss 1.775\n",
      "Ep 1 (Step 022320): Train loss 1.113, Val loss 1.775\n",
      "Ep 1 (Step 022325): Train loss 1.107, Val loss 1.775\n",
      "Ep 1 (Step 022330): Train loss 1.328, Val loss 1.775\n",
      "Ep 1 (Step 022335): Train loss 1.109, Val loss 1.775\n",
      "Ep 1 (Step 022340): Train loss 0.893, Val loss 1.775\n",
      "Ep 1 (Step 022345): Train loss 1.277, Val loss 1.774\n",
      "Ep 1 (Step 022350): Train loss 0.919, Val loss 1.772\n",
      "Ep 1 (Step 022355): Train loss 1.357, Val loss 1.770\n",
      "Ep 1 (Step 022360): Train loss 0.943, Val loss 1.771\n",
      "Ep 1 (Step 022365): Train loss 1.445, Val loss 1.773\n",
      "Ep 1 (Step 022370): Train loss 1.475, Val loss 1.774\n",
      "Ep 1 (Step 022375): Train loss 1.159, Val loss 1.775\n",
      "Ep 1 (Step 022380): Train loss 1.013, Val loss 1.774\n",
      "Ep 1 (Step 022385): Train loss 1.532, Val loss 1.773\n",
      "Ep 1 (Step 022390): Train loss 1.233, Val loss 1.772\n",
      "Ep 1 (Step 022395): Train loss 1.272, Val loss 1.773\n",
      "Ep 1 (Step 022400): Train loss 1.259, Val loss 1.773\n",
      "Ep 1 (Step 022405): Train loss 1.114, Val loss 1.772\n",
      "Ep 1 (Step 022410): Train loss 1.032, Val loss 1.771\n",
      "Ep 1 (Step 022415): Train loss 1.053, Val loss 1.771\n",
      "Ep 1 (Step 022420): Train loss 1.188, Val loss 1.773\n",
      "Ep 1 (Step 022425): Train loss 1.288, Val loss 1.775\n",
      "Ep 1 (Step 022430): Train loss 1.249, Val loss 1.777\n",
      "Ep 1 (Step 022435): Train loss 0.990, Val loss 1.779\n",
      "Ep 1 (Step 022440): Train loss 1.105, Val loss 1.779\n",
      "Ep 1 (Step 022445): Train loss 1.113, Val loss 1.778\n",
      "Ep 1 (Step 022450): Train loss 1.340, Val loss 1.776\n",
      "Ep 1 (Step 022455): Train loss 1.363, Val loss 1.774\n",
      "Ep 1 (Step 022460): Train loss 1.190, Val loss 1.773\n",
      "Ep 1 (Step 022465): Train loss 1.287, Val loss 1.772\n",
      "Ep 1 (Step 022470): Train loss 1.509, Val loss 1.772\n",
      "Ep 1 (Step 022475): Train loss 1.193, Val loss 1.773\n",
      "Ep 1 (Step 022480): Train loss 1.339, Val loss 1.774\n",
      "Ep 1 (Step 022485): Train loss 1.113, Val loss 1.774\n",
      "Ep 1 (Step 022490): Train loss 1.348, Val loss 1.775\n",
      "Ep 1 (Step 022495): Train loss 1.194, Val loss 1.773\n",
      "Ep 1 (Step 022500): Train loss 1.128, Val loss 1.773\n",
      "Ep 1 (Step 022505): Train loss 1.177, Val loss 1.775\n",
      "Ep 1 (Step 022510): Train loss 1.132, Val loss 1.776\n",
      "Ep 1 (Step 022515): Train loss 1.179, Val loss 1.776\n",
      "Ep 1 (Step 022520): Train loss 1.079, Val loss 1.775\n",
      "Ep 1 (Step 022525): Train loss 1.407, Val loss 1.775\n",
      "Ep 1 (Step 022530): Train loss 1.039, Val loss 1.776\n",
      "Ep 1 (Step 022535): Train loss 1.399, Val loss 1.777\n",
      "Ep 1 (Step 022540): Train loss 1.225, Val loss 1.777\n",
      "Ep 1 (Step 022545): Train loss 1.374, Val loss 1.777\n",
      "Ep 1 (Step 022550): Train loss 1.078, Val loss 1.775\n",
      "Ep 1 (Step 022555): Train loss 1.028, Val loss 1.772\n",
      "Ep 1 (Step 022560): Train loss 1.731, Val loss 1.771\n",
      "Ep 1 (Step 022565): Train loss 1.456, Val loss 1.772\n",
      "Ep 1 (Step 022570): Train loss 1.215, Val loss 1.773\n",
      "Ep 1 (Step 022575): Train loss 1.065, Val loss 1.774\n",
      "Ep 1 (Step 022580): Train loss 1.185, Val loss 1.774\n",
      "Ep 1 (Step 022585): Train loss 1.408, Val loss 1.772\n",
      "Ep 1 (Step 022590): Train loss 1.174, Val loss 1.769\n",
      "Ep 1 (Step 022595): Train loss 1.259, Val loss 1.768\n",
      "Ep 1 (Step 022600): Train loss 1.153, Val loss 1.769\n",
      "Ep 1 (Step 022605): Train loss 1.325, Val loss 1.769\n",
      "Ep 1 (Step 022610): Train loss 1.367, Val loss 1.769\n",
      "Ep 1 (Step 022615): Train loss 0.975, Val loss 1.769\n",
      "Ep 1 (Step 022620): Train loss 1.187, Val loss 1.770\n",
      "Ep 1 (Step 022625): Train loss 1.164, Val loss 1.770\n",
      "Ep 1 (Step 022630): Train loss 1.038, Val loss 1.767\n",
      "Ep 1 (Step 022635): Train loss 1.066, Val loss 1.765\n",
      "Ep 1 (Step 022640): Train loss 1.321, Val loss 1.763\n",
      "Ep 1 (Step 022645): Train loss 1.280, Val loss 1.764\n",
      "Ep 1 (Step 022650): Train loss 1.312, Val loss 1.765\n",
      "Ep 1 (Step 022655): Train loss 1.329, Val loss 1.766\n",
      "Ep 1 (Step 022660): Train loss 1.380, Val loss 1.767\n",
      "Ep 1 (Step 022665): Train loss 1.319, Val loss 1.768\n",
      "Ep 1 (Step 022670): Train loss 1.388, Val loss 1.769\n",
      "Ep 1 (Step 022675): Train loss 1.148, Val loss 1.770\n",
      "Ep 1 (Step 022680): Train loss 1.101, Val loss 1.769\n",
      "Ep 1 (Step 022685): Train loss 1.365, Val loss 1.769\n",
      "Ep 1 (Step 022690): Train loss 1.157, Val loss 1.769\n",
      "Ep 1 (Step 022695): Train loss 1.104, Val loss 1.771\n",
      "Ep 1 (Step 022700): Train loss 1.238, Val loss 1.772\n",
      "Ep 1 (Step 022705): Train loss 1.097, Val loss 1.772\n",
      "Ep 1 (Step 022710): Train loss 1.294, Val loss 1.773\n",
      "Ep 1 (Step 022715): Train loss 1.013, Val loss 1.774\n",
      "Ep 1 (Step 022720): Train loss 1.004, Val loss 1.772\n",
      "Ep 1 (Step 022725): Train loss 1.182, Val loss 1.771\n",
      "Ep 1 (Step 022730): Train loss 1.363, Val loss 1.771\n",
      "Ep 1 (Step 022735): Train loss 1.131, Val loss 1.772\n",
      "Ep 1 (Step 022740): Train loss 1.355, Val loss 1.772\n",
      "Ep 1 (Step 022745): Train loss 1.242, Val loss 1.771\n",
      "Ep 1 (Step 022750): Train loss 1.183, Val loss 1.769\n",
      "Ep 1 (Step 022755): Train loss 1.445, Val loss 1.767\n",
      "Ep 1 (Step 022760): Train loss 0.929, Val loss 1.766\n",
      "Ep 1 (Step 022765): Train loss 1.356, Val loss 1.767\n",
      "Ep 1 (Step 022770): Train loss 1.311, Val loss 1.768\n",
      "Ep 1 (Step 022775): Train loss 1.177, Val loss 1.771\n",
      "Ep 1 (Step 022780): Train loss 1.404, Val loss 1.771\n",
      "Ep 1 (Step 022785): Train loss 1.241, Val loss 1.769\n",
      "Ep 1 (Step 022790): Train loss 1.173, Val loss 1.767\n",
      "Ep 1 (Step 022795): Train loss 1.409, Val loss 1.767\n",
      "Ep 1 (Step 022800): Train loss 1.130, Val loss 1.767\n",
      "Ep 1 (Step 022805): Train loss 1.304, Val loss 1.769\n",
      "Ep 1 (Step 022810): Train loss 1.277, Val loss 1.771\n",
      "Ep 1 (Step 022815): Train loss 1.021, Val loss 1.775\n",
      "Ep 1 (Step 022820): Train loss 1.018, Val loss 1.778\n",
      "Ep 1 (Step 022825): Train loss 0.861, Val loss 1.779\n",
      "Ep 1 (Step 022830): Train loss 1.215, Val loss 1.779\n",
      "Ep 1 (Step 022835): Train loss 1.198, Val loss 1.776\n",
      "Ep 1 (Step 022840): Train loss 1.200, Val loss 1.774\n",
      "Ep 1 (Step 022845): Train loss 1.277, Val loss 1.773\n",
      "Ep 1 (Step 022850): Train loss 0.824, Val loss 1.772\n",
      "Ep 1 (Step 022855): Train loss 1.299, Val loss 1.771\n",
      "Ep 1 (Step 022860): Train loss 1.221, Val loss 1.770\n",
      "Ep 1 (Step 022865): Train loss 1.273, Val loss 1.769\n",
      "Ep 1 (Step 022870): Train loss 1.125, Val loss 1.769\n",
      "Ep 1 (Step 022875): Train loss 1.401, Val loss 1.769\n",
      "Ep 1 (Step 022880): Train loss 1.336, Val loss 1.768\n",
      "Ep 1 (Step 022885): Train loss 1.332, Val loss 1.768\n",
      "Ep 1 (Step 022890): Train loss 1.066, Val loss 1.767\n",
      "Ep 1 (Step 022895): Train loss 1.324, Val loss 1.767\n",
      "Ep 1 (Step 022900): Train loss 1.223, Val loss 1.768\n",
      "Ep 1 (Step 022905): Train loss 1.179, Val loss 1.768\n",
      "Ep 1 (Step 022910): Train loss 1.406, Val loss 1.768\n",
      "Ep 1 (Step 022915): Train loss 1.223, Val loss 1.768\n",
      "Ep 1 (Step 022920): Train loss 1.163, Val loss 1.768\n",
      "Ep 1 (Step 022925): Train loss 1.155, Val loss 1.766\n",
      "Ep 1 (Step 022930): Train loss 1.013, Val loss 1.765\n",
      "Ep 1 (Step 022935): Train loss 1.112, Val loss 1.764\n",
      "Ep 1 (Step 022940): Train loss 1.002, Val loss 1.765\n",
      "Ep 1 (Step 022945): Train loss 1.040, Val loss 1.766\n",
      "Ep 1 (Step 022950): Train loss 1.235, Val loss 1.767\n",
      "Ep 1 (Step 022955): Train loss 1.374, Val loss 1.769\n",
      "Ep 1 (Step 022960): Train loss 1.466, Val loss 1.772\n",
      "Ep 1 (Step 022965): Train loss 1.385, Val loss 1.774\n",
      "Ep 1 (Step 022970): Train loss 1.440, Val loss 1.774\n",
      "Ep 1 (Step 022975): Train loss 1.300, Val loss 1.774\n",
      "Ep 1 (Step 022980): Train loss 1.224, Val loss 1.775\n",
      "Ep 1 (Step 022985): Train loss 1.013, Val loss 1.774\n",
      "Ep 1 (Step 022990): Train loss 1.102, Val loss 1.771\n",
      "Ep 1 (Step 022995): Train loss 1.126, Val loss 1.769\n",
      "Ep 1 (Step 023000): Train loss 1.132, Val loss 1.767\n",
      "Ep 1 (Step 023005): Train loss 1.165, Val loss 1.769\n",
      "Ep 1 (Step 023010): Train loss 1.167, Val loss 1.770\n",
      "Ep 1 (Step 023015): Train loss 1.145, Val loss 1.772\n",
      "Ep 1 (Step 023020): Train loss 1.284, Val loss 1.773\n",
      "Ep 1 (Step 023025): Train loss 1.214, Val loss 1.773\n",
      "Ep 1 (Step 023030): Train loss 1.101, Val loss 1.773\n",
      "Ep 1 (Step 023035): Train loss 1.401, Val loss 1.774\n",
      "Ep 1 (Step 023040): Train loss 0.830, Val loss 1.776\n",
      "Ep 1 (Step 023045): Train loss 1.203, Val loss 1.778\n",
      "Ep 1 (Step 023050): Train loss 1.215, Val loss 1.781\n",
      "Ep 1 (Step 023055): Train loss 1.098, Val loss 1.784\n",
      "Ep 1 (Step 023060): Train loss 0.940, Val loss 1.786\n",
      "Ep 1 (Step 023065): Train loss 1.090, Val loss 1.786\n",
      "Ep 1 (Step 023070): Train loss 1.051, Val loss 1.786\n",
      "Ep 1 (Step 023075): Train loss 1.018, Val loss 1.783\n",
      "Ep 1 (Step 023080): Train loss 1.110, Val loss 1.781\n",
      "Ep 1 (Step 023085): Train loss 1.158, Val loss 1.779\n",
      "Ep 1 (Step 023090): Train loss 1.421, Val loss 1.779\n",
      "Ep 1 (Step 023095): Train loss 1.201, Val loss 1.779\n",
      "Ep 1 (Step 023100): Train loss 1.204, Val loss 1.777\n",
      "Ep 1 (Step 023105): Train loss 1.088, Val loss 1.776\n",
      "Ep 1 (Step 023110): Train loss 1.091, Val loss 1.776\n",
      "Ep 1 (Step 023115): Train loss 1.131, Val loss 1.776\n",
      "Ep 1 (Step 023120): Train loss 1.313, Val loss 1.774\n",
      "Ep 1 (Step 023125): Train loss 1.519, Val loss 1.772\n",
      "Ep 1 (Step 023130): Train loss 1.105, Val loss 1.770\n",
      "Ep 1 (Step 023135): Train loss 1.233, Val loss 1.770\n",
      "Ep 1 (Step 023140): Train loss 1.002, Val loss 1.769\n",
      "Ep 1 (Step 023145): Train loss 1.204, Val loss 1.769\n",
      "Ep 1 (Step 023150): Train loss 1.184, Val loss 1.770\n",
      "Ep 1 (Step 023155): Train loss 0.996, Val loss 1.773\n",
      "Ep 1 (Step 023160): Train loss 1.286, Val loss 1.775\n",
      "Ep 1 (Step 023165): Train loss 1.358, Val loss 1.775\n",
      "Ep 1 (Step 023170): Train loss 1.475, Val loss 1.774\n",
      "Ep 1 (Step 023175): Train loss 1.237, Val loss 1.774\n",
      "Ep 1 (Step 023180): Train loss 1.264, Val loss 1.774\n",
      "Ep 1 (Step 023185): Train loss 1.087, Val loss 1.775\n",
      "Ep 1 (Step 023190): Train loss 1.329, Val loss 1.776\n",
      "Ep 1 (Step 023195): Train loss 0.991, Val loss 1.775\n",
      "Ep 1 (Step 023200): Train loss 0.866, Val loss 1.773\n",
      "Ep 1 (Step 023205): Train loss 1.446, Val loss 1.772\n",
      "Ep 1 (Step 023210): Train loss 1.231, Val loss 1.769\n",
      "Ep 1 (Step 023215): Train loss 1.146, Val loss 1.768\n",
      "Ep 1 (Step 023220): Train loss 1.036, Val loss 1.768\n",
      "Ep 1 (Step 023225): Train loss 1.149, Val loss 1.770\n",
      "Ep 1 (Step 023230): Train loss 1.428, Val loss 1.771\n",
      "Ep 1 (Step 023235): Train loss 0.975, Val loss 1.774\n",
      "Ep 1 (Step 023240): Train loss 1.138, Val loss 1.775\n",
      "Ep 1 (Step 023245): Train loss 1.094, Val loss 1.777\n",
      "Ep 1 (Step 023250): Train loss 1.340, Val loss 1.777\n",
      "Ep 1 (Step 023255): Train loss 1.254, Val loss 1.778\n",
      "Ep 1 (Step 023260): Train loss 1.152, Val loss 1.780\n",
      "Ep 1 (Step 023265): Train loss 1.066, Val loss 1.782\n",
      "Ep 1 (Step 023270): Train loss 1.101, Val loss 1.784\n",
      "Ep 1 (Step 023275): Train loss 1.427, Val loss 1.786\n",
      "Ep 1 (Step 023280): Train loss 1.230, Val loss 1.787\n",
      "Ep 1 (Step 023285): Train loss 1.187, Val loss 1.788\n",
      "Ep 1 (Step 023290): Train loss 1.153, Val loss 1.786\n",
      "Ep 1 (Step 023295): Train loss 0.915, Val loss 1.784\n",
      "Ep 1 (Step 023300): Train loss 1.199, Val loss 1.780\n",
      "Ep 1 (Step 023305): Train loss 1.442, Val loss 1.777\n",
      "Ep 1 (Step 023310): Train loss 1.282, Val loss 1.772\n",
      "Ep 1 (Step 023315): Train loss 1.016, Val loss 1.769\n",
      "Ep 1 (Step 023320): Train loss 1.049, Val loss 1.768\n",
      "Ep 1 (Step 023325): Train loss 1.331, Val loss 1.767\n",
      "Ep 1 (Step 023330): Train loss 1.272, Val loss 1.766\n",
      "Ep 1 (Step 023335): Train loss 1.420, Val loss 1.765\n",
      "Ep 1 (Step 023340): Train loss 1.162, Val loss 1.766\n",
      "Ep 1 (Step 023345): Train loss 1.175, Val loss 1.768\n",
      "Ep 1 (Step 023350): Train loss 1.350, Val loss 1.771\n",
      "Ep 1 (Step 023355): Train loss 1.167, Val loss 1.774\n",
      "Ep 1 (Step 023360): Train loss 1.367, Val loss 1.776\n",
      "Ep 1 (Step 023365): Train loss 1.059, Val loss 1.776\n",
      "Ep 1 (Step 023370): Train loss 1.269, Val loss 1.775\n",
      "Ep 1 (Step 023375): Train loss 1.162, Val loss 1.773\n",
      "Ep 1 (Step 023380): Train loss 1.104, Val loss 1.773\n",
      "Ep 1 (Step 023385): Train loss 1.153, Val loss 1.773\n",
      "Ep 1 (Step 023390): Train loss 1.359, Val loss 1.774\n",
      "Ep 1 (Step 023395): Train loss 1.186, Val loss 1.775\n",
      "Ep 1 (Step 023400): Train loss 1.234, Val loss 1.774\n",
      "Ep 1 (Step 023405): Train loss 0.931, Val loss 1.771\n",
      "Ep 1 (Step 023410): Train loss 1.137, Val loss 1.767\n",
      "Ep 1 (Step 023415): Train loss 1.141, Val loss 1.765\n",
      "Ep 1 (Step 023420): Train loss 0.961, Val loss 1.763\n",
      "Ep 1 (Step 023425): Train loss 1.397, Val loss 1.761\n",
      "Ep 1 (Step 023430): Train loss 1.183, Val loss 1.761\n",
      "Ep 1 (Step 023435): Train loss 1.245, Val loss 1.760\n",
      "Ep 1 (Step 023440): Train loss 1.333, Val loss 1.758\n",
      "Ep 1 (Step 023445): Train loss 1.033, Val loss 1.757\n",
      "Ep 1 (Step 023450): Train loss 1.206, Val loss 1.756\n",
      "Ep 1 (Step 023455): Train loss 1.284, Val loss 1.755\n",
      "Ep 1 (Step 023460): Train loss 1.080, Val loss 1.756\n",
      "Ep 1 (Step 023465): Train loss 1.241, Val loss 1.757\n",
      "Ep 1 (Step 023470): Train loss 1.473, Val loss 1.757\n",
      "Ep 1 (Step 023475): Train loss 1.271, Val loss 1.757\n",
      "Ep 1 (Step 023480): Train loss 1.268, Val loss 1.758\n",
      "Ep 1 (Step 023485): Train loss 1.253, Val loss 1.758\n",
      "Ep 1 (Step 023490): Train loss 1.137, Val loss 1.758\n",
      "Ep 1 (Step 023495): Train loss 1.354, Val loss 1.758\n",
      "Ep 1 (Step 023500): Train loss 1.275, Val loss 1.759\n",
      "Ep 1 (Step 023505): Train loss 1.106, Val loss 1.759\n",
      "Ep 1 (Step 023510): Train loss 0.946, Val loss 1.758\n",
      "Ep 1 (Step 023515): Train loss 1.262, Val loss 1.757\n",
      "Ep 1 (Step 023520): Train loss 1.161, Val loss 1.758\n",
      "Ep 1 (Step 023525): Train loss 1.172, Val loss 1.758\n",
      "Ep 1 (Step 023530): Train loss 1.165, Val loss 1.758\n",
      "Ep 1 (Step 023535): Train loss 0.895, Val loss 1.759\n",
      "Ep 1 (Step 023540): Train loss 0.926, Val loss 1.760\n",
      "Ep 1 (Step 023545): Train loss 1.212, Val loss 1.762\n",
      "Ep 1 (Step 023550): Train loss 0.984, Val loss 1.766\n",
      "Ep 1 (Step 023555): Train loss 1.034, Val loss 1.770\n",
      "Ep 1 (Step 023560): Train loss 1.145, Val loss 1.774\n",
      "Ep 1 (Step 023565): Train loss 1.297, Val loss 1.775\n",
      "Ep 1 (Step 023570): Train loss 1.063, Val loss 1.775\n",
      "Ep 1 (Step 023575): Train loss 0.959, Val loss 1.772\n",
      "Ep 1 (Step 023580): Train loss 0.892, Val loss 1.772\n",
      "Ep 1 (Step 023585): Train loss 1.397, Val loss 1.771\n",
      "Ep 1 (Step 023590): Train loss 1.176, Val loss 1.770\n",
      "Ep 1 (Step 023595): Train loss 1.363, Val loss 1.768\n",
      "Ep 1 (Step 023600): Train loss 1.261, Val loss 1.767\n",
      "Ep 1 (Step 023605): Train loss 1.354, Val loss 1.767\n",
      "Ep 1 (Step 023610): Train loss 1.200, Val loss 1.764\n",
      "Ep 1 (Step 023615): Train loss 1.005, Val loss 1.762\n",
      "Ep 1 (Step 023620): Train loss 1.331, Val loss 1.761\n",
      "Ep 1 (Step 023625): Train loss 1.127, Val loss 1.762\n",
      "Ep 1 (Step 023630): Train loss 1.058, Val loss 1.764\n",
      "Ep 1 (Step 023635): Train loss 1.122, Val loss 1.766\n",
      "Ep 1 (Step 023640): Train loss 1.184, Val loss 1.767\n",
      "Ep 1 (Step 023645): Train loss 1.234, Val loss 1.767\n",
      "Ep 1 (Step 023650): Train loss 1.443, Val loss 1.765\n",
      "Ep 1 (Step 023655): Train loss 1.170, Val loss 1.765\n",
      "Ep 1 (Step 023660): Train loss 1.160, Val loss 1.764\n",
      "Ep 1 (Step 023665): Train loss 0.995, Val loss 1.762\n",
      "Ep 1 (Step 023670): Train loss 1.100, Val loss 1.760\n",
      "Ep 1 (Step 023675): Train loss 1.533, Val loss 1.759\n",
      "Ep 1 (Step 023680): Train loss 0.839, Val loss 1.761\n",
      "Ep 1 (Step 023685): Train loss 1.261, Val loss 1.763\n",
      "Ep 1 (Step 023690): Train loss 1.294, Val loss 1.764\n",
      "Ep 1 (Step 023695): Train loss 1.060, Val loss 1.766\n",
      "Ep 1 (Step 023700): Train loss 1.569, Val loss 1.767\n",
      "Ep 1 (Step 023705): Train loss 1.159, Val loss 1.768\n",
      "Ep 1 (Step 023710): Train loss 1.034, Val loss 1.770\n",
      "Ep 1 (Step 023715): Train loss 1.052, Val loss 1.769\n",
      "Ep 1 (Step 023720): Train loss 1.140, Val loss 1.767\n",
      "Ep 1 (Step 023725): Train loss 0.981, Val loss 1.765\n",
      "Ep 1 (Step 023730): Train loss 1.078, Val loss 1.764\n",
      "Ep 1 (Step 023735): Train loss 1.118, Val loss 1.763\n",
      "Ep 1 (Step 023740): Train loss 1.145, Val loss 1.764\n",
      "Ep 1 (Step 023745): Train loss 1.052, Val loss 1.763\n",
      "Ep 1 (Step 023750): Train loss 1.403, Val loss 1.763\n",
      "Ep 1 (Step 023755): Train loss 1.183, Val loss 1.763\n",
      "Ep 1 (Step 023760): Train loss 1.123, Val loss 1.762\n",
      "Ep 1 (Step 023765): Train loss 1.017, Val loss 1.762\n",
      "Ep 1 (Step 023770): Train loss 1.211, Val loss 1.764\n",
      "Ep 1 (Step 023775): Train loss 0.867, Val loss 1.769\n",
      "Ep 1 (Step 023780): Train loss 1.003, Val loss 1.771\n",
      "Ep 1 (Step 023785): Train loss 1.257, Val loss 1.767\n",
      "Ep 1 (Step 023790): Train loss 1.168, Val loss 1.764\n",
      "Ep 1 (Step 023795): Train loss 0.990, Val loss 1.762\n",
      "Ep 1 (Step 023800): Train loss 1.362, Val loss 1.762\n",
      "Ep 1 (Step 023805): Train loss 1.105, Val loss 1.764\n",
      "Ep 1 (Step 023810): Train loss 1.379, Val loss 1.766\n",
      "Ep 1 (Step 023815): Train loss 0.903, Val loss 1.767\n",
      "Ep 1 (Step 023820): Train loss 1.209, Val loss 1.766\n",
      "Ep 1 (Step 023825): Train loss 1.470, Val loss 1.766\n",
      "Ep 1 (Step 023830): Train loss 1.234, Val loss 1.767\n",
      "Ep 1 (Step 023835): Train loss 0.963, Val loss 1.768\n",
      "Ep 1 (Step 023840): Train loss 1.157, Val loss 1.769\n",
      "Ep 1 (Step 023845): Train loss 1.517, Val loss 1.769\n",
      "Ep 1 (Step 023850): Train loss 1.154, Val loss 1.769\n",
      "Ep 1 (Step 023855): Train loss 1.102, Val loss 1.771\n",
      "Ep 1 (Step 023860): Train loss 0.876, Val loss 1.772\n",
      "Ep 1 (Step 023865): Train loss 1.174, Val loss 1.770\n",
      "Ep 1 (Step 023870): Train loss 0.805, Val loss 1.769\n",
      "Ep 1 (Step 023875): Train loss 1.304, Val loss 1.769\n",
      "Ep 1 (Step 023880): Train loss 1.272, Val loss 1.768\n",
      "Ep 1 (Step 023885): Train loss 1.431, Val loss 1.768\n",
      "Ep 1 (Step 023890): Train loss 1.001, Val loss 1.769\n",
      "Ep 1 (Step 023895): Train loss 1.105, Val loss 1.771\n",
      "Ep 1 (Step 023900): Train loss 1.303, Val loss 1.770\n",
      "Ep 1 (Step 023905): Train loss 0.963, Val loss 1.770\n",
      "Ep 1 (Step 023910): Train loss 1.340, Val loss 1.770\n",
      "Ep 1 (Step 023915): Train loss 1.033, Val loss 1.770\n",
      "Ep 1 (Step 023920): Train loss 1.065, Val loss 1.768\n",
      "Ep 1 (Step 023925): Train loss 1.117, Val loss 1.768\n",
      "Ep 1 (Step 023930): Train loss 1.027, Val loss 1.767\n",
      "Ep 1 (Step 023935): Train loss 1.073, Val loss 1.768\n",
      "Ep 1 (Step 023940): Train loss 1.078, Val loss 1.767\n",
      "Ep 1 (Step 023945): Train loss 1.290, Val loss 1.764\n",
      "Ep 1 (Step 023950): Train loss 1.156, Val loss 1.761\n",
      "Ep 1 (Step 023955): Train loss 0.980, Val loss 1.760\n",
      "Ep 1 (Step 023960): Train loss 1.165, Val loss 1.759\n",
      "Ep 1 (Step 023965): Train loss 1.069, Val loss 1.757\n",
      "Ep 1 (Step 023970): Train loss 0.956, Val loss 1.758\n",
      "Ep 1 (Step 023975): Train loss 1.142, Val loss 1.758\n",
      "Ep 1 (Step 023980): Train loss 1.226, Val loss 1.758\n",
      "Ep 1 (Step 023985): Train loss 1.153, Val loss 1.758\n",
      "Ep 1 (Step 023990): Train loss 1.288, Val loss 1.758\n",
      "Ep 1 (Step 023995): Train loss 1.370, Val loss 1.759\n",
      "Ep 1 (Step 024000): Train loss 1.042, Val loss 1.761\n",
      "Ep 1 (Step 024005): Train loss 1.319, Val loss 1.762\n",
      "Ep 1 (Step 024010): Train loss 1.213, Val loss 1.764\n",
      "Ep 1 (Step 024015): Train loss 1.261, Val loss 1.765\n",
      "Ep 1 (Step 024020): Train loss 1.286, Val loss 1.766\n",
      "Ep 1 (Step 024025): Train loss 1.100, Val loss 1.766\n",
      "Ep 1 (Step 024030): Train loss 1.116, Val loss 1.766\n",
      "Ep 1 (Step 024035): Train loss 1.293, Val loss 1.765\n",
      "Ep 1 (Step 024040): Train loss 1.181, Val loss 1.764\n",
      "Ep 1 (Step 024045): Train loss 1.151, Val loss 1.762\n",
      "Ep 1 (Step 024050): Train loss 0.888, Val loss 1.760\n",
      "Ep 1 (Step 024055): Train loss 1.240, Val loss 1.759\n",
      "Ep 1 (Step 024060): Train loss 1.169, Val loss 1.758\n",
      "Ep 1 (Step 024065): Train loss 1.155, Val loss 1.758\n",
      "Ep 1 (Step 024070): Train loss 1.363, Val loss 1.756\n",
      "Ep 1 (Step 024075): Train loss 1.323, Val loss 1.754\n",
      "Ep 1 (Step 024080): Train loss 1.290, Val loss 1.753\n",
      "Ep 1 (Step 024085): Train loss 1.055, Val loss 1.751\n",
      "Ep 1 (Step 024090): Train loss 1.281, Val loss 1.750\n",
      "Ep 1 (Step 024095): Train loss 1.335, Val loss 1.751\n",
      "Ep 1 (Step 024100): Train loss 1.205, Val loss 1.751\n",
      "Ep 1 (Step 024105): Train loss 1.014, Val loss 1.752\n",
      "Ep 1 (Step 024110): Train loss 1.302, Val loss 1.753\n",
      "Ep 1 (Step 024115): Train loss 1.050, Val loss 1.754\n",
      "Ep 1 (Step 024120): Train loss 1.269, Val loss 1.754\n",
      "Ep 1 (Step 024125): Train loss 0.973, Val loss 1.754\n",
      "Ep 1 (Step 024130): Train loss 1.198, Val loss 1.755\n",
      "Ep 1 (Step 024135): Train loss 0.925, Val loss 1.754\n",
      "Ep 1 (Step 024140): Train loss 1.275, Val loss 1.754\n",
      "Ep 1 (Step 024145): Train loss 1.105, Val loss 1.753\n",
      "Ep 1 (Step 024150): Train loss 0.881, Val loss 1.754\n",
      "Ep 1 (Step 024155): Train loss 1.205, Val loss 1.754\n",
      "Ep 1 (Step 024160): Train loss 1.149, Val loss 1.755\n",
      "Ep 1 (Step 024165): Train loss 1.405, Val loss 1.756\n",
      "Ep 1 (Step 024170): Train loss 1.264, Val loss 1.756\n",
      "Ep 1 (Step 024175): Train loss 1.213, Val loss 1.757\n",
      "Ep 1 (Step 024180): Train loss 1.008, Val loss 1.757\n",
      "Ep 1 (Step 024185): Train loss 1.213, Val loss 1.759\n",
      "Ep 1 (Step 024190): Train loss 1.484, Val loss 1.761\n",
      "Ep 1 (Step 024195): Train loss 1.266, Val loss 1.763\n",
      "Ep 1 (Step 024200): Train loss 1.140, Val loss 1.765\n",
      "Ep 1 (Step 024205): Train loss 1.199, Val loss 1.766\n",
      "Ep 1 (Step 024210): Train loss 1.190, Val loss 1.767\n",
      "Ep 1 (Step 024215): Train loss 1.204, Val loss 1.766\n",
      "Ep 1 (Step 024220): Train loss 1.098, Val loss 1.765\n",
      "Ep 1 (Step 024225): Train loss 1.180, Val loss 1.763\n",
      "Ep 1 (Step 024230): Train loss 1.349, Val loss 1.762\n",
      "Ep 1 (Step 024235): Train loss 1.086, Val loss 1.762\n",
      "Ep 1 (Step 024240): Train loss 1.229, Val loss 1.762\n",
      "Ep 1 (Step 024245): Train loss 1.188, Val loss 1.762\n",
      "Ep 1 (Step 024250): Train loss 1.140, Val loss 1.765\n",
      "Ep 1 (Step 024255): Train loss 1.293, Val loss 1.768\n",
      "Ep 1 (Step 024260): Train loss 1.186, Val loss 1.769\n",
      "Ep 1 (Step 024265): Train loss 1.520, Val loss 1.768\n",
      "Ep 1 (Step 024270): Train loss 1.253, Val loss 1.767\n",
      "Ep 1 (Step 024275): Train loss 1.162, Val loss 1.765\n",
      "Ep 1 (Step 024280): Train loss 0.959, Val loss 1.763\n",
      "Ep 1 (Step 024285): Train loss 1.323, Val loss 1.764\n",
      "Ep 1 (Step 024290): Train loss 1.063, Val loss 1.762\n",
      "Ep 1 (Step 024295): Train loss 1.197, Val loss 1.761\n",
      "Ep 1 (Step 024300): Train loss 1.036, Val loss 1.761\n",
      "Ep 1 (Step 024305): Train loss 1.359, Val loss 1.762\n",
      "Ep 1 (Step 024310): Train loss 1.138, Val loss 1.763\n",
      "Ep 1 (Step 024315): Train loss 1.248, Val loss 1.764\n",
      "Ep 1 (Step 024320): Train loss 1.341, Val loss 1.764\n",
      "Ep 1 (Step 024325): Train loss 1.046, Val loss 1.763\n",
      "Ep 1 (Step 024330): Train loss 0.915, Val loss 1.763\n",
      "Ep 1 (Step 024335): Train loss 1.137, Val loss 1.762\n",
      "Ep 1 (Step 024340): Train loss 1.284, Val loss 1.762\n",
      "Ep 1 (Step 024345): Train loss 0.774, Val loss 1.762\n",
      "Ep 1 (Step 024350): Train loss 0.791, Val loss 1.762\n",
      "Ep 1 (Step 024355): Train loss 1.031, Val loss 1.763\n",
      "Ep 1 (Step 024360): Train loss 1.248, Val loss 1.764\n",
      "Ep 1 (Step 024365): Train loss 1.075, Val loss 1.766\n",
      "Ep 1 (Step 024370): Train loss 1.243, Val loss 1.767\n",
      "Ep 1 (Step 024375): Train loss 1.075, Val loss 1.768\n",
      "Ep 1 (Step 024380): Train loss 1.284, Val loss 1.766\n",
      "Ep 1 (Step 024385): Train loss 1.264, Val loss 1.766\n",
      "Ep 1 (Step 024390): Train loss 1.363, Val loss 1.765\n",
      "Ep 1 (Step 024395): Train loss 1.184, Val loss 1.765\n",
      "Ep 1 (Step 024400): Train loss 1.118, Val loss 1.764\n",
      "Ep 1 (Step 024405): Train loss 1.362, Val loss 1.763\n",
      "Ep 1 (Step 024410): Train loss 0.991, Val loss 1.764\n",
      "Ep 1 (Step 024415): Train loss 1.199, Val loss 1.767\n",
      "Ep 1 (Step 024420): Train loss 1.174, Val loss 1.769\n",
      "Ep 1 (Step 024425): Train loss 1.267, Val loss 1.769\n",
      "Ep 1 (Step 024430): Train loss 0.966, Val loss 1.770\n",
      "Ep 1 (Step 024435): Train loss 1.026, Val loss 1.771\n",
      "Ep 1 (Step 024440): Train loss 1.109, Val loss 1.773\n",
      "Ep 1 (Step 024445): Train loss 1.264, Val loss 1.773\n",
      "Ep 1 (Step 024450): Train loss 1.219, Val loss 1.773\n",
      "Ep 1 (Step 024455): Train loss 1.282, Val loss 1.772\n",
      "Ep 1 (Step 024460): Train loss 1.147, Val loss 1.773\n",
      "Ep 1 (Step 024465): Train loss 0.991, Val loss 1.775\n",
      "Ep 1 (Step 024470): Train loss 1.048, Val loss 1.775\n",
      "Ep 1 (Step 024475): Train loss 1.129, Val loss 1.774\n",
      "Ep 1 (Step 024480): Train loss 1.088, Val loss 1.773\n",
      "Ep 1 (Step 024485): Train loss 1.211, Val loss 1.771\n",
      "Ep 1 (Step 024490): Train loss 1.361, Val loss 1.769\n",
      "Ep 1 (Step 024495): Train loss 1.095, Val loss 1.766\n",
      "Ep 1 (Step 024500): Train loss 1.433, Val loss 1.764\n",
      "Ep 1 (Step 024505): Train loss 1.414, Val loss 1.763\n",
      "Ep 1 (Step 024510): Train loss 1.063, Val loss 1.761\n",
      "Ep 1 (Step 024515): Train loss 1.265, Val loss 1.759\n",
      "Ep 1 (Step 024520): Train loss 1.156, Val loss 1.756\n",
      "Ep 1 (Step 024525): Train loss 1.519, Val loss 1.752\n",
      "Ep 1 (Step 024530): Train loss 1.319, Val loss 1.750\n",
      "Ep 1 (Step 024535): Train loss 0.961, Val loss 1.749\n",
      "Ep 1 (Step 024540): Train loss 0.919, Val loss 1.747\n",
      "Ep 1 (Step 024545): Train loss 1.013, Val loss 1.747\n",
      "Ep 1 (Step 024550): Train loss 0.866, Val loss 1.746\n",
      "Ep 1 (Step 024555): Train loss 1.354, Val loss 1.746\n",
      "Ep 1 (Step 024560): Train loss 1.095, Val loss 1.747\n",
      "Ep 1 (Step 024565): Train loss 1.126, Val loss 1.748\n",
      "Ep 1 (Step 024570): Train loss 1.374, Val loss 1.749\n",
      "Ep 1 (Step 024575): Train loss 1.194, Val loss 1.749\n",
      "Ep 1 (Step 024580): Train loss 1.273, Val loss 1.748\n",
      "Ep 1 (Step 024585): Train loss 1.144, Val loss 1.748\n",
      "Ep 1 (Step 024590): Train loss 1.363, Val loss 1.748\n",
      "Ep 1 (Step 024595): Train loss 1.091, Val loss 1.749\n",
      "Ep 1 (Step 024600): Train loss 0.813, Val loss 1.750\n",
      "Ep 1 (Step 024605): Train loss 1.321, Val loss 1.752\n",
      "Ep 1 (Step 024610): Train loss 1.426, Val loss 1.754\n",
      "Ep 1 (Step 024615): Train loss 1.279, Val loss 1.756\n",
      "Ep 1 (Step 024620): Train loss 1.293, Val loss 1.755\n",
      "Ep 1 (Step 024625): Train loss 1.101, Val loss 1.755\n",
      "Ep 1 (Step 024630): Train loss 1.305, Val loss 1.755\n",
      "Ep 1 (Step 024635): Train loss 1.045, Val loss 1.755\n",
      "Ep 1 (Step 024640): Train loss 1.013, Val loss 1.756\n",
      "Ep 1 (Step 024645): Train loss 1.248, Val loss 1.756\n",
      "Ep 1 (Step 024650): Train loss 1.275, Val loss 1.755\n",
      "Ep 1 (Step 024655): Train loss 1.091, Val loss 1.754\n",
      "Ep 1 (Step 024660): Train loss 1.215, Val loss 1.752\n",
      "Ep 1 (Step 024665): Train loss 1.257, Val loss 1.751\n",
      "Ep 1 (Step 024670): Train loss 1.254, Val loss 1.751\n",
      "Ep 1 (Step 024675): Train loss 1.218, Val loss 1.752\n",
      "Ep 1 (Step 024680): Train loss 1.573, Val loss 1.753\n",
      "Ep 1 (Step 024685): Train loss 1.343, Val loss 1.755\n",
      "Ep 1 (Step 024690): Train loss 1.172, Val loss 1.755\n",
      "Ep 1 (Step 024695): Train loss 1.101, Val loss 1.755\n",
      "Ep 1 (Step 024700): Train loss 1.353, Val loss 1.756\n",
      "Ep 1 (Step 024705): Train loss 1.273, Val loss 1.757\n",
      "Ep 1 (Step 024710): Train loss 1.221, Val loss 1.758\n",
      "Ep 1 (Step 024715): Train loss 1.197, Val loss 1.759\n",
      "Ep 1 (Step 024720): Train loss 1.176, Val loss 1.761\n",
      "Ep 1 (Step 024725): Train loss 1.101, Val loss 1.762\n",
      "Ep 1 (Step 024730): Train loss 1.336, Val loss 1.765\n",
      "Ep 1 (Step 024735): Train loss 1.312, Val loss 1.767\n",
      "Ep 1 (Step 024740): Train loss 0.954, Val loss 1.766\n",
      "Ep 1 (Step 024745): Train loss 1.393, Val loss 1.764\n",
      "Ep 1 (Step 024750): Train loss 1.306, Val loss 1.762\n",
      "Ep 1 (Step 024755): Train loss 1.313, Val loss 1.761\n",
      "Ep 1 (Step 024760): Train loss 1.052, Val loss 1.762\n",
      "Ep 1 (Step 024765): Train loss 1.038, Val loss 1.764\n",
      "Ep 1 (Step 024770): Train loss 1.288, Val loss 1.764\n",
      "Ep 1 (Step 024775): Train loss 1.244, Val loss 1.766\n",
      "Ep 1 (Step 024780): Train loss 1.247, Val loss 1.768\n",
      "Ep 1 (Step 024785): Train loss 1.072, Val loss 1.769\n",
      "Ep 1 (Step 024790): Train loss 1.401, Val loss 1.768\n",
      "Ep 1 (Step 024795): Train loss 1.357, Val loss 1.767\n",
      "Ep 1 (Step 024800): Train loss 1.038, Val loss 1.766\n",
      "Ep 1 (Step 024805): Train loss 1.092, Val loss 1.765\n",
      "Ep 1 (Step 024810): Train loss 1.143, Val loss 1.766\n",
      "Ep 1 (Step 024815): Train loss 1.245, Val loss 1.765\n",
      "Ep 1 (Step 024820): Train loss 1.125, Val loss 1.765\n",
      "Ep 1 (Step 024825): Train loss 1.106, Val loss 1.763\n",
      "Ep 1 (Step 024830): Train loss 1.017, Val loss 1.762\n",
      "Ep 1 (Step 024835): Train loss 1.203, Val loss 1.761\n",
      "Ep 1 (Step 024840): Train loss 1.064, Val loss 1.761\n",
      "Ep 1 (Step 024845): Train loss 0.913, Val loss 1.761\n",
      "Ep 1 (Step 024850): Train loss 1.117, Val loss 1.760\n",
      "Ep 1 (Step 024855): Train loss 0.981, Val loss 1.759\n",
      "Ep 1 (Step 024860): Train loss 1.061, Val loss 1.759\n",
      "Ep 1 (Step 024865): Train loss 1.234, Val loss 1.759\n",
      "Ep 1 (Step 024870): Train loss 0.936, Val loss 1.759\n",
      "Ep 1 (Step 024875): Train loss 1.407, Val loss 1.761\n",
      "Ep 1 (Step 024880): Train loss 1.239, Val loss 1.763\n",
      "Ep 1 (Step 024885): Train loss 1.073, Val loss 1.764\n",
      "Ep 1 (Step 024890): Train loss 1.314, Val loss 1.765\n",
      "Ep 1 (Step 024895): Train loss 1.081, Val loss 1.766\n",
      "Ep 1 (Step 024900): Train loss 0.895, Val loss 1.765\n",
      "Ep 1 (Step 024905): Train loss 1.356, Val loss 1.765\n",
      "Ep 1 (Step 024910): Train loss 1.028, Val loss 1.765\n",
      "Ep 1 (Step 024915): Train loss 1.169, Val loss 1.764\n",
      "Ep 1 (Step 024920): Train loss 1.029, Val loss 1.763\n",
      "Ep 1 (Step 024925): Train loss 1.501, Val loss 1.762\n",
      "Ep 1 (Step 024930): Train loss 1.459, Val loss 1.761\n",
      "Ep 1 (Step 024935): Train loss 1.207, Val loss 1.762\n",
      "Ep 1 (Step 024940): Train loss 1.259, Val loss 1.763\n",
      "Ep 1 (Step 024945): Train loss 1.154, Val loss 1.762\n",
      "Ep 1 (Step 024950): Train loss 1.204, Val loss 1.761\n",
      "Ep 1 (Step 024955): Train loss 1.262, Val loss 1.760\n",
      "Ep 1 (Step 024960): Train loss 0.916, Val loss 1.759\n",
      "Ep 1 (Step 024965): Train loss 0.937, Val loss 1.757\n",
      "Ep 1 (Step 024970): Train loss 1.230, Val loss 1.758\n",
      "Ep 1 (Step 024975): Train loss 1.277, Val loss 1.759\n",
      "Ep 1 (Step 024980): Train loss 1.016, Val loss 1.759\n",
      "Ep 1 (Step 024985): Train loss 1.106, Val loss 1.759\n",
      "Ep 1 (Step 024990): Train loss 1.108, Val loss 1.760\n",
      "Ep 1 (Step 024995): Train loss 1.093, Val loss 1.761\n",
      "Ep 1 (Step 025000): Train loss 1.220, Val loss 1.762\n",
      "Ep 1 (Step 025005): Train loss 1.062, Val loss 1.762\n",
      "Ep 1 (Step 025010): Train loss 1.072, Val loss 1.763\n",
      "Ep 1 (Step 025015): Train loss 1.145, Val loss 1.765\n",
      "Ep 1 (Step 025020): Train loss 1.325, Val loss 1.765\n",
      "Ep 1 (Step 025025): Train loss 1.225, Val loss 1.765\n",
      "Ep 1 (Step 025030): Train loss 1.266, Val loss 1.765\n",
      "Ep 1 (Step 025035): Train loss 1.307, Val loss 1.764\n",
      "Ep 1 (Step 025040): Train loss 1.308, Val loss 1.763\n",
      "Ep 1 (Step 025045): Train loss 1.187, Val loss 1.763\n",
      "Ep 1 (Step 025050): Train loss 1.278, Val loss 1.762\n",
      "Ep 1 (Step 025055): Train loss 1.075, Val loss 1.762\n",
      "Ep 1 (Step 025060): Train loss 1.404, Val loss 1.762\n",
      "Ep 1 (Step 025065): Train loss 1.466, Val loss 1.762\n",
      "Ep 1 (Step 025070): Train loss 1.295, Val loss 1.762\n",
      "Ep 1 (Step 025075): Train loss 0.980, Val loss 1.762\n",
      "Ep 1 (Step 025080): Train loss 1.552, Val loss 1.760\n",
      "Ep 1 (Step 025085): Train loss 1.284, Val loss 1.761\n",
      "Ep 1 (Step 025090): Train loss 1.211, Val loss 1.760\n",
      "Ep 1 (Step 025095): Train loss 1.042, Val loss 1.760\n",
      "Ep 1 (Step 025100): Train loss 1.297, Val loss 1.759\n",
      "Ep 1 (Step 025105): Train loss 1.068, Val loss 1.760\n",
      "Ep 1 (Step 025110): Train loss 0.977, Val loss 1.759\n",
      "Ep 1 (Step 025115): Train loss 1.367, Val loss 1.757\n",
      "Ep 1 (Step 025120): Train loss 1.227, Val loss 1.757\n",
      "Ep 1 (Step 025125): Train loss 1.061, Val loss 1.757\n",
      "Ep 1 (Step 025130): Train loss 1.165, Val loss 1.759\n",
      "Ep 1 (Step 025135): Train loss 1.146, Val loss 1.760\n",
      "Ep 1 (Step 025140): Train loss 1.176, Val loss 1.761\n",
      "Ep 1 (Step 025145): Train loss 1.133, Val loss 1.762\n",
      "Ep 1 (Step 025150): Train loss 1.044, Val loss 1.762\n",
      "Ep 1 (Step 025155): Train loss 0.954, Val loss 1.762\n",
      "Ep 1 (Step 025160): Train loss 1.167, Val loss 1.761\n",
      "Ep 1 (Step 025165): Train loss 1.013, Val loss 1.760\n",
      "Ep 1 (Step 025170): Train loss 1.302, Val loss 1.761\n",
      "Ep 1 (Step 025175): Train loss 1.196, Val loss 1.762\n",
      "Ep 1 (Step 025180): Train loss 1.440, Val loss 1.763\n",
      "Ep 1 (Step 025185): Train loss 1.425, Val loss 1.765\n",
      "Ep 1 (Step 025190): Train loss 1.155, Val loss 1.767\n",
      "Ep 1 (Step 025195): Train loss 1.147, Val loss 1.766\n",
      "Ep 1 (Step 025200): Train loss 1.102, Val loss 1.764\n",
      "Ep 1 (Step 025205): Train loss 1.074, Val loss 1.763\n",
      "Ep 1 (Step 025210): Train loss 1.263, Val loss 1.763\n",
      "Ep 1 (Step 025215): Train loss 1.031, Val loss 1.763\n",
      "Ep 1 (Step 025220): Train loss 1.080, Val loss 1.762\n",
      "Ep 1 (Step 025225): Train loss 1.133, Val loss 1.761\n",
      "Ep 1 (Step 025230): Train loss 1.071, Val loss 1.760\n",
      "Ep 1 (Step 025235): Train loss 1.108, Val loss 1.758\n",
      "Ep 1 (Step 025240): Train loss 1.395, Val loss 1.756\n",
      "Ep 1 (Step 025245): Train loss 1.274, Val loss 1.756\n",
      "Ep 1 (Step 025250): Train loss 1.154, Val loss 1.755\n",
      "Ep 1 (Step 025255): Train loss 1.323, Val loss 1.754\n",
      "Ep 1 (Step 025260): Train loss 1.361, Val loss 1.752\n",
      "Ep 1 (Step 025265): Train loss 1.102, Val loss 1.752\n",
      "Ep 1 (Step 025270): Train loss 1.126, Val loss 1.750\n",
      "Ep 1 (Step 025275): Train loss 1.006, Val loss 1.750\n",
      "Ep 1 (Step 025280): Train loss 1.188, Val loss 1.749\n",
      "Ep 1 (Step 025285): Train loss 1.331, Val loss 1.749\n",
      "Ep 1 (Step 025290): Train loss 1.157, Val loss 1.749\n",
      "Ep 1 (Step 025295): Train loss 1.445, Val loss 1.750\n",
      "Ep 1 (Step 025300): Train loss 1.025, Val loss 1.751\n",
      "Ep 1 (Step 025305): Train loss 0.939, Val loss 1.754\n",
      "Ep 1 (Step 025310): Train loss 1.239, Val loss 1.756\n",
      "Ep 1 (Step 025315): Train loss 1.095, Val loss 1.756\n",
      "Ep 1 (Step 025320): Train loss 1.140, Val loss 1.755\n",
      "Ep 1 (Step 025325): Train loss 1.098, Val loss 1.754\n",
      "Ep 1 (Step 025330): Train loss 1.025, Val loss 1.753\n",
      "Ep 1 (Step 025335): Train loss 1.012, Val loss 1.751\n",
      "Ep 1 (Step 025340): Train loss 1.405, Val loss 1.749\n",
      "Ep 1 (Step 025345): Train loss 1.091, Val loss 1.748\n",
      "Ep 1 (Step 025350): Train loss 1.091, Val loss 1.747\n",
      "Ep 1 (Step 025355): Train loss 1.067, Val loss 1.747\n",
      "Ep 1 (Step 025360): Train loss 1.117, Val loss 1.747\n",
      "Ep 1 (Step 025365): Train loss 1.138, Val loss 1.749\n",
      "Ep 1 (Step 025370): Train loss 0.987, Val loss 1.750\n",
      "Ep 1 (Step 025375): Train loss 1.115, Val loss 1.751\n",
      "Ep 1 (Step 025380): Train loss 1.146, Val loss 1.751\n",
      "Ep 1 (Step 025385): Train loss 1.178, Val loss 1.752\n",
      "Ep 1 (Step 025390): Train loss 1.258, Val loss 1.752\n",
      "Ep 1 (Step 025395): Train loss 1.181, Val loss 1.752\n",
      "Ep 1 (Step 025400): Train loss 1.014, Val loss 1.752\n",
      "Ep 1 (Step 025405): Train loss 1.624, Val loss 1.752\n",
      "Ep 1 (Step 025410): Train loss 0.999, Val loss 1.752\n",
      "Ep 1 (Step 025415): Train loss 1.107, Val loss 1.752\n",
      "Ep 1 (Step 025420): Train loss 1.024, Val loss 1.751\n",
      "Ep 1 (Step 025425): Train loss 1.132, Val loss 1.751\n",
      "Ep 1 (Step 025430): Train loss 1.055, Val loss 1.750\n",
      "Ep 1 (Step 025435): Train loss 1.112, Val loss 1.749\n",
      "Ep 1 (Step 025440): Train loss 1.172, Val loss 1.749\n",
      "Ep 1 (Step 025445): Train loss 1.103, Val loss 1.748\n",
      "Ep 1 (Step 025450): Train loss 1.216, Val loss 1.747\n",
      "Ep 1 (Step 025455): Train loss 1.143, Val loss 1.746\n",
      "Ep 1 (Step 025460): Train loss 1.220, Val loss 1.746\n",
      "Ep 1 (Step 025465): Train loss 0.811, Val loss 1.746\n",
      "Ep 1 (Step 025470): Train loss 1.190, Val loss 1.746\n",
      "Ep 1 (Step 025475): Train loss 1.499, Val loss 1.744\n",
      "Ep 1 (Step 025480): Train loss 1.390, Val loss 1.742\n",
      "Ep 1 (Step 025485): Train loss 1.435, Val loss 1.741\n",
      "Ep 1 (Step 025490): Train loss 1.387, Val loss 1.741\n",
      "Ep 1 (Step 025495): Train loss 0.914, Val loss 1.742\n",
      "Ep 1 (Step 025500): Train loss 1.152, Val loss 1.744\n",
      "Ep 1 (Step 025505): Train loss 1.361, Val loss 1.745\n",
      "Ep 1 (Step 025510): Train loss 1.168, Val loss 1.745\n",
      "Ep 1 (Step 025515): Train loss 1.094, Val loss 1.745\n",
      "Ep 1 (Step 025520): Train loss 1.213, Val loss 1.744\n",
      "Ep 1 (Step 025525): Train loss 1.181, Val loss 1.743\n",
      "Ep 1 (Step 025530): Train loss 1.229, Val loss 1.742\n",
      "Ep 1 (Step 025535): Train loss 1.141, Val loss 1.743\n",
      "Ep 1 (Step 025540): Train loss 1.100, Val loss 1.745\n",
      "Ep 1 (Step 025545): Train loss 1.079, Val loss 1.746\n",
      "Ep 1 (Step 025550): Train loss 1.244, Val loss 1.747\n",
      "Ep 1 (Step 025555): Train loss 1.000, Val loss 1.748\n",
      "Ep 1 (Step 025560): Train loss 1.408, Val loss 1.748\n",
      "Ep 1 (Step 025565): Train loss 1.145, Val loss 1.748\n",
      "Ep 1 (Step 025570): Train loss 1.050, Val loss 1.748\n",
      "Ep 1 (Step 025575): Train loss 1.250, Val loss 1.748\n",
      "Ep 1 (Step 025580): Train loss 1.152, Val loss 1.749\n",
      "Ep 1 (Step 025585): Train loss 1.131, Val loss 1.749\n",
      "Ep 1 (Step 025590): Train loss 0.929, Val loss 1.748\n",
      "Ep 1 (Step 025595): Train loss 1.034, Val loss 1.748\n",
      "Ep 1 (Step 025600): Train loss 1.171, Val loss 1.749\n",
      "Ep 1 (Step 025605): Train loss 1.200, Val loss 1.750\n",
      "Ep 1 (Step 025610): Train loss 1.446, Val loss 1.752\n",
      "Ep 1 (Step 025615): Train loss 1.213, Val loss 1.754\n",
      "Ep 1 (Step 025620): Train loss 1.130, Val loss 1.754\n",
      "Ep 1 (Step 025625): Train loss 1.155, Val loss 1.753\n",
      "Ep 1 (Step 025630): Train loss 1.309, Val loss 1.750\n",
      "Ep 1 (Step 025635): Train loss 1.354, Val loss 1.747\n",
      "Ep 1 (Step 025640): Train loss 1.140, Val loss 1.746\n",
      "Ep 1 (Step 025645): Train loss 1.185, Val loss 1.746\n",
      "Ep 1 (Step 025650): Train loss 1.103, Val loss 1.747\n",
      "Ep 1 (Step 025655): Train loss 1.037, Val loss 1.747\n",
      "Ep 1 (Step 025660): Train loss 1.088, Val loss 1.747\n",
      "Ep 1 (Step 025665): Train loss 1.377, Val loss 1.748\n",
      "Ep 1 (Step 025670): Train loss 1.155, Val loss 1.750\n",
      "Ep 1 (Step 025675): Train loss 1.483, Val loss 1.750\n",
      "Ep 1 (Step 025680): Train loss 0.983, Val loss 1.750\n",
      "Ep 1 (Step 025685): Train loss 1.255, Val loss 1.751\n",
      "Ep 1 (Step 025690): Train loss 1.225, Val loss 1.753\n",
      "Ep 1 (Step 025695): Train loss 1.708, Val loss 1.755\n",
      "Ep 1 (Step 025700): Train loss 0.966, Val loss 1.756\n",
      "Ep 1 (Step 025705): Train loss 1.435, Val loss 1.757\n",
      "Ep 1 (Step 025710): Train loss 1.207, Val loss 1.757\n",
      "Ep 1 (Step 025715): Train loss 0.952, Val loss 1.756\n",
      "Ep 1 (Step 025720): Train loss 1.386, Val loss 1.755\n",
      "Ep 1 (Step 025725): Train loss 1.219, Val loss 1.753\n",
      "Ep 1 (Step 025730): Train loss 1.273, Val loss 1.753\n",
      "Ep 1 (Step 025735): Train loss 0.822, Val loss 1.751\n",
      "Ep 1 (Step 025740): Train loss 1.123, Val loss 1.751\n",
      "Ep 1 (Step 025745): Train loss 1.200, Val loss 1.752\n",
      "Ep 1 (Step 025750): Train loss 1.196, Val loss 1.754\n",
      "Ep 1 (Step 025755): Train loss 1.031, Val loss 1.755\n",
      "Ep 1 (Step 025760): Train loss 1.224, Val loss 1.757\n",
      "Ep 1 (Step 025765): Train loss 1.151, Val loss 1.760\n",
      "Ep 1 (Step 025770): Train loss 1.230, Val loss 1.761\n",
      "Ep 1 (Step 025775): Train loss 1.538, Val loss 1.761\n",
      "Ep 1 (Step 025780): Train loss 1.002, Val loss 1.761\n",
      "Ep 1 (Step 025785): Train loss 0.944, Val loss 1.760\n",
      "Ep 1 (Step 025790): Train loss 1.258, Val loss 1.758\n",
      "Ep 1 (Step 025795): Train loss 1.168, Val loss 1.756\n",
      "Ep 1 (Step 025800): Train loss 1.200, Val loss 1.756\n",
      "Ep 1 (Step 025805): Train loss 1.081, Val loss 1.755\n",
      "Ep 1 (Step 025810): Train loss 1.322, Val loss 1.754\n",
      "Ep 1 (Step 025815): Train loss 1.132, Val loss 1.754\n",
      "Ep 1 (Step 025820): Train loss 0.982, Val loss 1.754\n",
      "Ep 1 (Step 025825): Train loss 1.163, Val loss 1.753\n",
      "Ep 1 (Step 025830): Train loss 0.921, Val loss 1.752\n",
      "Ep 1 (Step 025835): Train loss 1.122, Val loss 1.753\n",
      "Ep 1 (Step 025840): Train loss 1.333, Val loss 1.754\n",
      "Ep 1 (Step 025845): Train loss 1.353, Val loss 1.756\n",
      "Ep 1 (Step 025850): Train loss 1.067, Val loss 1.757\n",
      "Ep 1 (Step 025855): Train loss 1.124, Val loss 1.757\n",
      "Ep 1 (Step 025860): Train loss 1.220, Val loss 1.757\n",
      "Ep 1 (Step 025865): Train loss 1.309, Val loss 1.759\n",
      "Ep 1 (Step 025870): Train loss 1.554, Val loss 1.760\n",
      "Ep 1 (Step 025875): Train loss 1.058, Val loss 1.762\n",
      "Ep 1 (Step 025880): Train loss 1.075, Val loss 1.763\n",
      "Ep 1 (Step 025885): Train loss 1.114, Val loss 1.762\n",
      "Ep 1 (Step 025890): Train loss 1.211, Val loss 1.761\n",
      "Ep 1 (Step 025895): Train loss 1.170, Val loss 1.761\n",
      "Ep 1 (Step 025900): Train loss 1.202, Val loss 1.761\n",
      "Ep 1 (Step 025905): Train loss 1.197, Val loss 1.761\n",
      "Ep 1 (Step 025910): Train loss 1.185, Val loss 1.762\n",
      "Ep 1 (Step 025915): Train loss 1.188, Val loss 1.763\n",
      "Ep 1 (Step 025920): Train loss 1.257, Val loss 1.764\n",
      "Ep 1 (Step 025925): Train loss 1.246, Val loss 1.765\n",
      "Ep 1 (Step 025930): Train loss 1.516, Val loss 1.768\n",
      "Ep 1 (Step 025935): Train loss 1.061, Val loss 1.769\n",
      "Ep 1 (Step 025940): Train loss 1.085, Val loss 1.769\n",
      "Ep 1 (Step 025945): Train loss 1.309, Val loss 1.769\n",
      "Ep 1 (Step 025950): Train loss 1.126, Val loss 1.768\n",
      "Ep 1 (Step 025955): Train loss 0.971, Val loss 1.764\n",
      "Ep 1 (Step 025960): Train loss 1.182, Val loss 1.761\n",
      "Ep 1 (Step 025965): Train loss 1.317, Val loss 1.759\n",
      "Ep 1 (Step 025970): Train loss 1.418, Val loss 1.759\n",
      "Ep 1 (Step 025975): Train loss 0.968, Val loss 1.759\n",
      "Ep 1 (Step 025980): Train loss 1.258, Val loss 1.760\n",
      "Ep 1 (Step 025985): Train loss 1.276, Val loss 1.760\n",
      "Ep 1 (Step 025990): Train loss 1.270, Val loss 1.760\n",
      "Ep 1 (Step 025995): Train loss 1.756, Val loss 1.760\n",
      "Ep 1 (Step 026000): Train loss 1.108, Val loss 1.760\n",
      "Ep 1 (Step 026005): Train loss 1.368, Val loss 1.761\n",
      "Ep 1 (Step 026010): Train loss 1.129, Val loss 1.761\n",
      "Ep 1 (Step 026015): Train loss 1.124, Val loss 1.763\n",
      "Ep 1 (Step 026020): Train loss 1.213, Val loss 1.766\n",
      "Ep 1 (Step 026025): Train loss 1.126, Val loss 1.767\n",
      "Ep 1 (Step 026030): Train loss 1.025, Val loss 1.769\n",
      "Ep 1 (Step 026035): Train loss 1.108, Val loss 1.770\n",
      "Ep 1 (Step 026040): Train loss 1.131, Val loss 1.769\n",
      "Ep 1 (Step 026045): Train loss 1.174, Val loss 1.768\n",
      "Ep 1 (Step 026050): Train loss 1.126, Val loss 1.766\n",
      "Ep 1 (Step 026055): Train loss 0.957, Val loss 1.765\n",
      "Ep 1 (Step 026060): Train loss 1.191, Val loss 1.764\n",
      "Ep 1 (Step 026065): Train loss 1.396, Val loss 1.764\n",
      "Ep 1 (Step 026070): Train loss 1.128, Val loss 1.763\n",
      "Ep 1 (Step 026075): Train loss 1.315, Val loss 1.763\n",
      "Ep 1 (Step 026080): Train loss 1.165, Val loss 1.763\n",
      "Ep 1 (Step 026085): Train loss 1.326, Val loss 1.764\n",
      "Ep 1 (Step 026090): Train loss 1.178, Val loss 1.766\n",
      "Ep 1 (Step 026095): Train loss 1.139, Val loss 1.767\n",
      "Ep 1 (Step 026100): Train loss 1.503, Val loss 1.767\n",
      "Ep 1 (Step 026105): Train loss 1.290, Val loss 1.766\n",
      "Ep 1 (Step 026110): Train loss 1.155, Val loss 1.766\n",
      "Ep 1 (Step 026115): Train loss 1.000, Val loss 1.764\n",
      "Ep 1 (Step 026120): Train loss 1.135, Val loss 1.763\n",
      "Ep 1 (Step 026125): Train loss 1.106, Val loss 1.764\n",
      "Ep 1 (Step 026130): Train loss 1.246, Val loss 1.766\n",
      "Ep 1 (Step 026135): Train loss 1.037, Val loss 1.768\n",
      "Ep 1 (Step 026140): Train loss 1.314, Val loss 1.770\n",
      "Ep 1 (Step 026145): Train loss 1.185, Val loss 1.771\n",
      "Ep 1 (Step 026150): Train loss 1.446, Val loss 1.771\n",
      "Ep 1 (Step 026155): Train loss 1.228, Val loss 1.772\n",
      "Ep 1 (Step 026160): Train loss 1.112, Val loss 1.771\n",
      "Ep 1 (Step 026165): Train loss 1.081, Val loss 1.770\n",
      "Ep 1 (Step 026170): Train loss 1.359, Val loss 1.769\n",
      "Ep 1 (Step 026175): Train loss 1.170, Val loss 1.767\n",
      "Ep 1 (Step 026180): Train loss 1.398, Val loss 1.765\n",
      "Ep 1 (Step 026185): Train loss 1.068, Val loss 1.764\n",
      "Ep 1 (Step 026190): Train loss 1.067, Val loss 1.762\n",
      "Ep 1 (Step 026195): Train loss 1.229, Val loss 1.762\n",
      "Ep 1 (Step 026200): Train loss 1.278, Val loss 1.762\n",
      "Ep 1 (Step 026205): Train loss 0.911, Val loss 1.762\n",
      "Ep 1 (Step 026210): Train loss 1.294, Val loss 1.762\n",
      "Ep 1 (Step 026215): Train loss 1.177, Val loss 1.762\n",
      "Ep 1 (Step 026220): Train loss 0.983, Val loss 1.762\n",
      "Ep 1 (Step 026225): Train loss 1.114, Val loss 1.762\n",
      "Ep 1 (Step 026230): Train loss 1.138, Val loss 1.761\n",
      "Ep 1 (Step 026235): Train loss 1.223, Val loss 1.761\n",
      "Ep 1 (Step 026240): Train loss 0.967, Val loss 1.761\n",
      "Ep 1 (Step 026245): Train loss 1.332, Val loss 1.761\n",
      "Ep 1 (Step 026250): Train loss 1.066, Val loss 1.762\n",
      "Ep 1 (Step 026255): Train loss 1.423, Val loss 1.764\n",
      "Ep 1 (Step 026260): Train loss 1.109, Val loss 1.766\n",
      "Ep 1 (Step 026265): Train loss 1.041, Val loss 1.767\n",
      "Ep 1 (Step 026270): Train loss 1.430, Val loss 1.768\n",
      "Ep 1 (Step 026275): Train loss 1.092, Val loss 1.768\n",
      "Ep 1 (Step 026280): Train loss 1.388, Val loss 1.767\n",
      "Ep 1 (Step 026285): Train loss 0.984, Val loss 1.768\n",
      "Ep 1 (Step 026290): Train loss 0.934, Val loss 1.770\n",
      "Ep 1 (Step 026295): Train loss 1.434, Val loss 1.772\n",
      "Ep 1 (Step 026300): Train loss 1.323, Val loss 1.771\n",
      "Ep 1 (Step 026305): Train loss 1.195, Val loss 1.771\n",
      "Ep 1 (Step 026310): Train loss 1.045, Val loss 1.771\n",
      "Ep 1 (Step 026315): Train loss 1.171, Val loss 1.772\n",
      "Ep 1 (Step 026320): Train loss 1.196, Val loss 1.774\n",
      "Ep 1 (Step 026325): Train loss 1.011, Val loss 1.774\n",
      "Ep 1 (Step 026330): Train loss 1.112, Val loss 1.775\n",
      "Ep 1 (Step 026335): Train loss 1.428, Val loss 1.774\n",
      "Ep 1 (Step 026340): Train loss 1.273, Val loss 1.773\n",
      "Ep 1 (Step 026345): Train loss 1.203, Val loss 1.772\n",
      "Ep 1 (Step 026350): Train loss 1.411, Val loss 1.771\n",
      "Ep 1 (Step 026355): Train loss 0.991, Val loss 1.771\n",
      "Ep 1 (Step 026360): Train loss 1.017, Val loss 1.770\n",
      "Ep 1 (Step 026365): Train loss 1.291, Val loss 1.768\n",
      "Ep 1 (Step 026370): Train loss 0.997, Val loss 1.766\n",
      "Ep 1 (Step 026375): Train loss 1.471, Val loss 1.764\n",
      "Ep 1 (Step 026380): Train loss 1.147, Val loss 1.762\n",
      "Ep 1 (Step 026385): Train loss 1.268, Val loss 1.761\n",
      "Ep 1 (Step 026390): Train loss 0.969, Val loss 1.761\n",
      "Ep 1 (Step 026395): Train loss 1.107, Val loss 1.760\n",
      "Ep 1 (Step 026400): Train loss 1.070, Val loss 1.760\n",
      "Ep 1 (Step 026405): Train loss 1.257, Val loss 1.760\n",
      "Ep 1 (Step 026410): Train loss 0.944, Val loss 1.761\n",
      "Ep 1 (Step 026415): Train loss 1.044, Val loss 1.762\n",
      "Ep 1 (Step 026420): Train loss 1.198, Val loss 1.763\n",
      "Ep 1 (Step 026425): Train loss 1.071, Val loss 1.765\n",
      "Ep 1 (Step 026430): Train loss 1.074, Val loss 1.767\n",
      "Ep 1 (Step 026435): Train loss 1.143, Val loss 1.767\n",
      "Ep 1 (Step 026440): Train loss 1.165, Val loss 1.767\n",
      "Ep 1 (Step 026445): Train loss 1.201, Val loss 1.767\n",
      "Ep 1 (Step 026450): Train loss 1.197, Val loss 1.766\n",
      "Ep 1 (Step 026455): Train loss 1.305, Val loss 1.764\n",
      "Ep 1 (Step 026460): Train loss 1.205, Val loss 1.762\n",
      "Ep 1 (Step 026465): Train loss 1.016, Val loss 1.760\n",
      "Ep 1 (Step 026470): Train loss 1.028, Val loss 1.759\n",
      "Ep 1 (Step 026475): Train loss 1.249, Val loss 1.759\n",
      "Ep 1 (Step 026480): Train loss 1.072, Val loss 1.759\n",
      "Ep 1 (Step 026485): Train loss 1.248, Val loss 1.759\n",
      "Ep 1 (Step 026490): Train loss 0.955, Val loss 1.757\n",
      "Ep 1 (Step 026495): Train loss 1.315, Val loss 1.755\n",
      "Ep 1 (Step 026500): Train loss 0.903, Val loss 1.754\n",
      "Ep 1 (Step 026505): Train loss 1.101, Val loss 1.754\n",
      "Ep 1 (Step 026510): Train loss 1.418, Val loss 1.754\n",
      "Ep 1 (Step 026515): Train loss 0.913, Val loss 1.755\n",
      "Ep 1 (Step 026520): Train loss 1.209, Val loss 1.755\n",
      "Ep 1 (Step 026525): Train loss 0.905, Val loss 1.755\n",
      "Ep 1 (Step 026530): Train loss 1.018, Val loss 1.754\n",
      "Ep 1 (Step 026535): Train loss 1.132, Val loss 1.753\n",
      "Ep 1 (Step 026540): Train loss 0.804, Val loss 1.754\n",
      "Ep 1 (Step 026545): Train loss 1.164, Val loss 1.756\n",
      "Ep 1 (Step 026550): Train loss 1.345, Val loss 1.756\n",
      "Ep 1 (Step 026555): Train loss 1.297, Val loss 1.757\n",
      "Ep 1 (Step 026560): Train loss 1.264, Val loss 1.758\n",
      "Ep 1 (Step 026565): Train loss 1.129, Val loss 1.759\n",
      "Ep 1 (Step 026570): Train loss 1.218, Val loss 1.758\n",
      "Ep 1 (Step 026575): Train loss 1.319, Val loss 1.757\n",
      "Ep 1 (Step 026580): Train loss 1.045, Val loss 1.755\n",
      "Ep 1 (Step 026585): Train loss 1.122, Val loss 1.754\n",
      "Ep 1 (Step 026590): Train loss 1.044, Val loss 1.752\n",
      "Ep 1 (Step 026595): Train loss 1.339, Val loss 1.753\n",
      "Ep 1 (Step 026600): Train loss 0.974, Val loss 1.752\n",
      "Ep 1 (Step 026605): Train loss 1.494, Val loss 1.751\n",
      "Ep 1 (Step 026610): Train loss 0.862, Val loss 1.750\n",
      "Ep 1 (Step 026615): Train loss 1.143, Val loss 1.750\n",
      "Ep 1 (Step 026620): Train loss 1.085, Val loss 1.750\n",
      "Ep 1 (Step 026625): Train loss 1.174, Val loss 1.749\n",
      "Ep 1 (Step 026630): Train loss 1.304, Val loss 1.749\n",
      "Ep 1 (Step 026635): Train loss 1.106, Val loss 1.748\n",
      "Ep 1 (Step 026640): Train loss 1.101, Val loss 1.749\n",
      "Ep 1 (Step 026645): Train loss 1.369, Val loss 1.750\n",
      "Ep 1 (Step 026650): Train loss 1.412, Val loss 1.750\n",
      "Ep 1 (Step 026655): Train loss 1.245, Val loss 1.751\n",
      "Ep 1 (Step 026660): Train loss 0.965, Val loss 1.751\n",
      "Ep 1 (Step 026665): Train loss 1.095, Val loss 1.752\n",
      "Ep 1 (Step 026670): Train loss 0.968, Val loss 1.752\n",
      "Ep 1 (Step 026675): Train loss 1.063, Val loss 1.752\n",
      "Ep 1 (Step 026680): Train loss 1.333, Val loss 1.752\n",
      "Ep 1 (Step 026685): Train loss 1.203, Val loss 1.753\n",
      "Ep 1 (Step 026690): Train loss 1.008, Val loss 1.752\n",
      "Ep 1 (Step 026695): Train loss 1.576, Val loss 1.754\n",
      "Ep 1 (Step 026700): Train loss 1.324, Val loss 1.755\n",
      "Ep 1 (Step 026705): Train loss 1.051, Val loss 1.756\n",
      "Ep 1 (Step 026710): Train loss 1.192, Val loss 1.756\n",
      "Ep 1 (Step 026715): Train loss 0.914, Val loss 1.755\n",
      "Ep 1 (Step 026720): Train loss 0.966, Val loss 1.754\n",
      "Ep 1 (Step 026725): Train loss 0.944, Val loss 1.753\n",
      "Ep 1 (Step 026730): Train loss 0.983, Val loss 1.753\n",
      "Ep 1 (Step 026735): Train loss 1.441, Val loss 1.752\n",
      "Ep 1 (Step 026740): Train loss 1.076, Val loss 1.751\n",
      "Ep 1 (Step 026745): Train loss 0.984, Val loss 1.751\n",
      "Ep 1 (Step 026750): Train loss 1.345, Val loss 1.752\n",
      "Ep 1 (Step 026755): Train loss 1.047, Val loss 1.752\n",
      "Ep 1 (Step 026760): Train loss 1.383, Val loss 1.752\n",
      "Ep 1 (Step 026765): Train loss 1.009, Val loss 1.750\n",
      "Ep 1 (Step 026770): Train loss 1.362, Val loss 1.750\n",
      "Ep 1 (Step 026775): Train loss 1.369, Val loss 1.751\n",
      "Ep 1 (Step 026780): Train loss 1.197, Val loss 1.754\n",
      "Ep 1 (Step 026785): Train loss 1.207, Val loss 1.753\n",
      "Ep 1 (Step 026790): Train loss 1.401, Val loss 1.751\n",
      "Ep 1 (Step 026795): Train loss 1.001, Val loss 1.749\n",
      "Ep 1 (Step 026800): Train loss 1.112, Val loss 1.748\n",
      "Ep 1 (Step 026805): Train loss 1.114, Val loss 1.748\n",
      "Ep 1 (Step 026810): Train loss 0.954, Val loss 1.748\n",
      "Ep 1 (Step 026815): Train loss 1.201, Val loss 1.749\n",
      "Ep 1 (Step 026820): Train loss 1.124, Val loss 1.750\n",
      "Ep 1 (Step 026825): Train loss 1.176, Val loss 1.751\n",
      "Ep 1 (Step 026830): Train loss 1.236, Val loss 1.751\n",
      "Ep 1 (Step 026835): Train loss 1.382, Val loss 1.751\n",
      "Ep 1 (Step 026840): Train loss 0.971, Val loss 1.753\n",
      "Ep 1 (Step 026845): Train loss 0.761, Val loss 1.752\n",
      "Ep 1 (Step 026850): Train loss 1.114, Val loss 1.750\n",
      "Ep 1 (Step 026855): Train loss 1.233, Val loss 1.748\n",
      "Ep 1 (Step 026860): Train loss 1.140, Val loss 1.748\n",
      "Ep 1 (Step 026865): Train loss 1.075, Val loss 1.748\n",
      "Ep 1 (Step 026870): Train loss 1.388, Val loss 1.748\n",
      "Ep 1 (Step 026875): Train loss 1.216, Val loss 1.748\n",
      "Ep 1 (Step 026880): Train loss 1.167, Val loss 1.747\n",
      "Ep 1 (Step 026885): Train loss 1.222, Val loss 1.747\n",
      "Ep 1 (Step 026890): Train loss 1.015, Val loss 1.747\n",
      "Ep 1 (Step 026895): Train loss 1.288, Val loss 1.748\n",
      "Ep 1 (Step 026900): Train loss 0.942, Val loss 1.749\n",
      "Ep 1 (Step 026905): Train loss 1.075, Val loss 1.749\n",
      "Ep 1 (Step 026910): Train loss 1.217, Val loss 1.750\n",
      "Ep 1 (Step 026915): Train loss 1.438, Val loss 1.752\n",
      "Ep 1 (Step 026920): Train loss 0.716, Val loss 1.753\n",
      "Ep 1 (Step 026925): Train loss 1.490, Val loss 1.753\n",
      "Ep 1 (Step 026930): Train loss 1.302, Val loss 1.752\n",
      "Ep 1 (Step 026935): Train loss 1.201, Val loss 1.752\n",
      "Ep 1 (Step 026940): Train loss 0.955, Val loss 1.753\n",
      "Ep 1 (Step 026945): Train loss 1.179, Val loss 1.753\n",
      "Ep 1 (Step 026950): Train loss 1.224, Val loss 1.754\n",
      "Ep 1 (Step 026955): Train loss 1.422, Val loss 1.756\n",
      "Ep 1 (Step 026960): Train loss 1.068, Val loss 1.757\n",
      "Ep 1 (Step 026965): Train loss 1.361, Val loss 1.758\n",
      "Ep 1 (Step 026970): Train loss 1.373, Val loss 1.760\n",
      "Ep 1 (Step 026975): Train loss 0.951, Val loss 1.761\n",
      "Ep 1 (Step 026980): Train loss 1.190, Val loss 1.761\n",
      "Ep 1 (Step 026985): Train loss 1.234, Val loss 1.761\n",
      "Ep 1 (Step 026990): Train loss 1.014, Val loss 1.762\n",
      "Ep 1 (Step 026995): Train loss 1.281, Val loss 1.763\n",
      "Ep 1 (Step 027000): Train loss 1.106, Val loss 1.766\n",
      "Ep 1 (Step 027005): Train loss 1.209, Val loss 1.767\n",
      "Ep 1 (Step 027010): Train loss 1.152, Val loss 1.767\n",
      "Ep 1 (Step 027015): Train loss 1.002, Val loss 1.766\n",
      "Ep 1 (Step 027020): Train loss 1.055, Val loss 1.766\n",
      "Ep 1 (Step 027025): Train loss 1.202, Val loss 1.765\n",
      "Ep 1 (Step 027030): Train loss 1.238, Val loss 1.764\n",
      "Ep 1 (Step 027035): Train loss 1.199, Val loss 1.766\n",
      "Ep 1 (Step 027040): Train loss 1.468, Val loss 1.766\n",
      "Ep 1 (Step 027045): Train loss 1.227, Val loss 1.766\n",
      "Ep 1 (Step 027050): Train loss 1.225, Val loss 1.768\n",
      "Ep 1 (Step 027055): Train loss 1.263, Val loss 1.769\n",
      "Ep 1 (Step 027060): Train loss 1.030, Val loss 1.770\n",
      "Ep 1 (Step 027065): Train loss 1.386, Val loss 1.770\n",
      "Ep 1 (Step 027070): Train loss 0.796, Val loss 1.771\n",
      "Ep 1 (Step 027075): Train loss 0.898, Val loss 1.773\n",
      "Ep 1 (Step 027080): Train loss 1.131, Val loss 1.773\n",
      "Ep 1 (Step 027085): Train loss 1.042, Val loss 1.773\n",
      "Ep 1 (Step 027090): Train loss 1.216, Val loss 1.773\n",
      "Ep 1 (Step 027095): Train loss 1.217, Val loss 1.772\n",
      "Ep 1 (Step 027100): Train loss 1.266, Val loss 1.771\n",
      "Ep 1 (Step 027105): Train loss 0.941, Val loss 1.770\n",
      "Ep 1 (Step 027110): Train loss 0.854, Val loss 1.769\n",
      "Ep 1 (Step 027115): Train loss 1.084, Val loss 1.770\n",
      "Ep 1 (Step 027120): Train loss 1.230, Val loss 1.771\n",
      "Ep 1 (Step 027125): Train loss 1.247, Val loss 1.773\n",
      "Ep 1 (Step 027130): Train loss 1.335, Val loss 1.775\n",
      "Ep 1 (Step 027135): Train loss 0.949, Val loss 1.774\n",
      "Ep 1 (Step 027140): Train loss 1.347, Val loss 1.773\n",
      "Ep 1 (Step 027145): Train loss 1.288, Val loss 1.772\n",
      "Ep 1 (Step 027150): Train loss 1.073, Val loss 1.770\n",
      "Ep 1 (Step 027155): Train loss 1.024, Val loss 1.769\n",
      "Ep 1 (Step 027160): Train loss 1.254, Val loss 1.768\n",
      "Ep 1 (Step 027165): Train loss 0.940, Val loss 1.767\n",
      "Ep 1 (Step 027170): Train loss 1.267, Val loss 1.766\n",
      "Ep 1 (Step 027175): Train loss 1.233, Val loss 1.764\n",
      "Ep 1 (Step 027180): Train loss 0.815, Val loss 1.762\n",
      "Ep 1 (Step 027185): Train loss 0.978, Val loss 1.762\n",
      "Ep 1 (Step 027190): Train loss 1.144, Val loss 1.763\n",
      "Ep 1 (Step 027195): Train loss 1.066, Val loss 1.765\n",
      "Ep 1 (Step 027200): Train loss 1.354, Val loss 1.768\n",
      "Ep 1 (Step 027205): Train loss 1.273, Val loss 1.769\n",
      "Ep 1 (Step 027210): Train loss 1.080, Val loss 1.771\n",
      "Ep 1 (Step 027215): Train loss 1.240, Val loss 1.773\n",
      "Ep 1 (Step 027220): Train loss 1.136, Val loss 1.775\n",
      "Ep 1 (Step 027225): Train loss 1.138, Val loss 1.776\n",
      "Ep 1 (Step 027230): Train loss 1.064, Val loss 1.776\n",
      "Ep 1 (Step 027235): Train loss 1.261, Val loss 1.776\n",
      "Ep 1 (Step 027240): Train loss 1.198, Val loss 1.773\n",
      "Ep 1 (Step 027245): Train loss 1.210, Val loss 1.771\n",
      "Ep 1 (Step 027250): Train loss 0.989, Val loss 1.771\n",
      "Ep 1 (Step 027255): Train loss 1.244, Val loss 1.771\n",
      "Ep 1 (Step 027260): Train loss 1.128, Val loss 1.770\n",
      "Ep 1 (Step 027265): Train loss 1.133, Val loss 1.770\n",
      "Ep 1 (Step 027270): Train loss 1.201, Val loss 1.769\n",
      "Ep 1 (Step 027275): Train loss 0.934, Val loss 1.770\n",
      "Ep 1 (Step 027280): Train loss 1.246, Val loss 1.770\n",
      "Ep 1 (Step 027285): Train loss 1.094, Val loss 1.771\n",
      "Ep 1 (Step 027290): Train loss 0.963, Val loss 1.770\n",
      "Ep 1 (Step 027295): Train loss 1.128, Val loss 1.771\n",
      "Ep 1 (Step 027300): Train loss 1.214, Val loss 1.771\n",
      "Ep 1 (Step 027305): Train loss 1.015, Val loss 1.771\n",
      "Ep 1 (Step 027310): Train loss 1.274, Val loss 1.770\n",
      "Ep 1 (Step 027315): Train loss 1.174, Val loss 1.770\n",
      "Ep 1 (Step 027320): Train loss 0.864, Val loss 1.770\n",
      "Ep 1 (Step 027325): Train loss 1.251, Val loss 1.769\n",
      "Ep 1 (Step 027330): Train loss 1.095, Val loss 1.768\n",
      "Ep 1 (Step 027335): Train loss 1.197, Val loss 1.768\n",
      "Ep 1 (Step 027340): Train loss 1.080, Val loss 1.769\n",
      "Ep 1 (Step 027345): Train loss 1.085, Val loss 1.769\n",
      "Ep 1 (Step 027350): Train loss 1.358, Val loss 1.768\n",
      "Ep 1 (Step 027355): Train loss 1.333, Val loss 1.766\n",
      "Ep 1 (Step 027360): Train loss 1.211, Val loss 1.766\n",
      "Ep 1 (Step 027365): Train loss 1.011, Val loss 1.765\n",
      "Ep 1 (Step 027370): Train loss 1.550, Val loss 1.763\n",
      "Ep 1 (Step 027375): Train loss 1.192, Val loss 1.762\n",
      "Ep 1 (Step 027380): Train loss 1.199, Val loss 1.760\n",
      "Ep 1 (Step 027385): Train loss 1.337, Val loss 1.758\n",
      "Ep 1 (Step 027390): Train loss 1.303, Val loss 1.757\n",
      "Ep 1 (Step 027395): Train loss 0.890, Val loss 1.757\n",
      "Ep 1 (Step 027400): Train loss 1.440, Val loss 1.758\n",
      "Ep 1 (Step 027405): Train loss 1.032, Val loss 1.759\n",
      "Ep 1 (Step 027410): Train loss 0.982, Val loss 1.760\n",
      "Ep 1 (Step 027415): Train loss 1.177, Val loss 1.761\n",
      "Ep 1 (Step 027420): Train loss 1.180, Val loss 1.761\n",
      "Ep 1 (Step 027425): Train loss 1.403, Val loss 1.760\n",
      "Ep 1 (Step 027430): Train loss 1.446, Val loss 1.760\n",
      "Ep 1 (Step 027435): Train loss 1.065, Val loss 1.759\n",
      "Ep 1 (Step 027440): Train loss 0.977, Val loss 1.756\n",
      "Ep 1 (Step 027445): Train loss 1.087, Val loss 1.755\n",
      "Ep 1 (Step 027450): Train loss 1.156, Val loss 1.752\n",
      "Ep 1 (Step 027455): Train loss 1.284, Val loss 1.750\n",
      "Ep 1 (Step 027460): Train loss 1.489, Val loss 1.747\n",
      "Ep 1 (Step 027465): Train loss 1.092, Val loss 1.744\n",
      "Ep 1 (Step 027470): Train loss 1.430, Val loss 1.742\n",
      "Ep 1 (Step 027475): Train loss 0.914, Val loss 1.742\n",
      "Ep 1 (Step 027480): Train loss 1.070, Val loss 1.743\n",
      "Ep 1 (Step 027485): Train loss 1.316, Val loss 1.743\n",
      "Ep 1 (Step 027490): Train loss 1.070, Val loss 1.744\n",
      "Ep 1 (Step 027495): Train loss 0.823, Val loss 1.744\n",
      "Ep 1 (Step 027500): Train loss 1.106, Val loss 1.744\n",
      "Ep 1 (Step 027505): Train loss 1.686, Val loss 1.745\n",
      "Ep 1 (Step 027510): Train loss 1.049, Val loss 1.745\n",
      "Ep 1 (Step 027515): Train loss 1.294, Val loss 1.746\n",
      "Ep 1 (Step 027520): Train loss 1.278, Val loss 1.746\n",
      "Ep 1 (Step 027525): Train loss 1.247, Val loss 1.747\n",
      "Ep 1 (Step 027530): Train loss 1.322, Val loss 1.747\n",
      "Ep 1 (Step 027535): Train loss 1.109, Val loss 1.749\n",
      "Ep 1 (Step 027540): Train loss 1.202, Val loss 1.749\n",
      "Ep 1 (Step 027545): Train loss 1.034, Val loss 1.748\n",
      "Ep 1 (Step 027550): Train loss 1.056, Val loss 1.749\n",
      "Ep 1 (Step 027555): Train loss 1.253, Val loss 1.750\n",
      "Ep 1 (Step 027560): Train loss 1.185, Val loss 1.751\n",
      "Ep 1 (Step 027565): Train loss 1.230, Val loss 1.752\n",
      "Ep 1 (Step 027570): Train loss 1.105, Val loss 1.752\n",
      "Ep 1 (Step 027575): Train loss 1.018, Val loss 1.755\n",
      "Ep 1 (Step 027580): Train loss 0.832, Val loss 1.756\n",
      "Ep 1 (Step 027585): Train loss 1.330, Val loss 1.757\n",
      "Ep 1 (Step 027590): Train loss 1.119, Val loss 1.759\n",
      "Ep 1 (Step 027595): Train loss 1.072, Val loss 1.759\n",
      "Ep 1 (Step 027600): Train loss 1.357, Val loss 1.759\n",
      "Ep 1 (Step 027605): Train loss 1.255, Val loss 1.759\n",
      "Ep 1 (Step 027610): Train loss 1.070, Val loss 1.758\n",
      "Ep 1 (Step 027615): Train loss 1.561, Val loss 1.759\n",
      "Ep 1 (Step 027620): Train loss 1.281, Val loss 1.758\n",
      "Ep 1 (Step 027625): Train loss 1.438, Val loss 1.757\n",
      "Ep 1 (Step 027630): Train loss 0.972, Val loss 1.757\n",
      "Ep 1 (Step 027635): Train loss 0.924, Val loss 1.759\n",
      "Ep 1 (Step 027640): Train loss 1.115, Val loss 1.760\n",
      "Ep 1 (Step 027645): Train loss 1.369, Val loss 1.759\n",
      "Ep 1 (Step 027650): Train loss 1.105, Val loss 1.756\n",
      "Ep 1 (Step 027655): Train loss 1.287, Val loss 1.755\n",
      "Ep 1 (Step 027660): Train loss 1.201, Val loss 1.755\n",
      "Ep 1 (Step 027665): Train loss 1.242, Val loss 1.755\n",
      "Ep 1 (Step 027670): Train loss 1.125, Val loss 1.755\n",
      "Ep 1 (Step 027675): Train loss 1.234, Val loss 1.754\n",
      "Ep 1 (Step 027680): Train loss 1.108, Val loss 1.754\n",
      "Ep 1 (Step 027685): Train loss 1.184, Val loss 1.754\n",
      "Ep 1 (Step 027690): Train loss 1.122, Val loss 1.755\n",
      "Ep 1 (Step 027695): Train loss 1.109, Val loss 1.756\n",
      "Ep 1 (Step 027700): Train loss 1.087, Val loss 1.757\n",
      "Ep 1 (Step 027705): Train loss 1.192, Val loss 1.756\n",
      "Ep 1 (Step 027710): Train loss 0.906, Val loss 1.756\n",
      "Ep 1 (Step 027715): Train loss 1.036, Val loss 1.756\n",
      "Ep 1 (Step 027720): Train loss 1.289, Val loss 1.757\n",
      "Ep 1 (Step 027725): Train loss 1.245, Val loss 1.756\n",
      "Ep 1 (Step 027730): Train loss 1.427, Val loss 1.756\n",
      "Ep 1 (Step 027735): Train loss 1.275, Val loss 1.757\n",
      "Ep 1 (Step 027740): Train loss 0.913, Val loss 1.758\n",
      "Ep 1 (Step 027745): Train loss 1.124, Val loss 1.756\n",
      "Ep 1 (Step 027750): Train loss 1.585, Val loss 1.754\n",
      "Ep 1 (Step 027755): Train loss 1.179, Val loss 1.753\n",
      "Ep 1 (Step 027760): Train loss 1.490, Val loss 1.754\n",
      "Ep 1 (Step 027765): Train loss 1.526, Val loss 1.754\n",
      "Ep 1 (Step 027770): Train loss 1.122, Val loss 1.753\n",
      "Ep 1 (Step 027775): Train loss 1.042, Val loss 1.754\n",
      "Ep 1 (Step 027780): Train loss 1.098, Val loss 1.755\n",
      "Ep 1 (Step 027785): Train loss 1.586, Val loss 1.754\n",
      "Ep 1 (Step 027790): Train loss 1.367, Val loss 1.755\n",
      "Ep 1 (Step 027795): Train loss 1.339, Val loss 1.756\n",
      "Ep 1 (Step 027800): Train loss 1.174, Val loss 1.756\n",
      "Ep 1 (Step 027805): Train loss 1.189, Val loss 1.756\n",
      "Ep 1 (Step 027810): Train loss 0.929, Val loss 1.755\n",
      "Ep 1 (Step 027815): Train loss 1.313, Val loss 1.753\n",
      "Ep 1 (Step 027820): Train loss 1.296, Val loss 1.753\n",
      "Ep 1 (Step 027825): Train loss 1.219, Val loss 1.753\n",
      "Ep 1 (Step 027830): Train loss 1.191, Val loss 1.753\n",
      "Ep 1 (Step 027835): Train loss 0.968, Val loss 1.752\n",
      "Ep 1 (Step 027840): Train loss 1.219, Val loss 1.753\n",
      "Ep 1 (Step 027845): Train loss 1.067, Val loss 1.755\n",
      "Ep 1 (Step 027850): Train loss 1.093, Val loss 1.755\n",
      "Ep 1 (Step 027855): Train loss 1.202, Val loss 1.755\n",
      "Ep 1 (Step 027860): Train loss 1.097, Val loss 1.756\n",
      "Ep 1 (Step 027865): Train loss 1.214, Val loss 1.754\n",
      "Ep 1 (Step 027870): Train loss 1.214, Val loss 1.754\n",
      "Ep 1 (Step 027875): Train loss 0.905, Val loss 1.753\n",
      "Ep 1 (Step 027880): Train loss 1.088, Val loss 1.752\n",
      "Ep 1 (Step 027885): Train loss 1.100, Val loss 1.751\n",
      "Ep 1 (Step 027890): Train loss 1.078, Val loss 1.751\n",
      "Ep 1 (Step 027895): Train loss 1.167, Val loss 1.751\n",
      "Ep 1 (Step 027900): Train loss 1.483, Val loss 1.751\n",
      "Ep 1 (Step 027905): Train loss 1.204, Val loss 1.751\n",
      "Ep 1 (Step 027910): Train loss 1.097, Val loss 1.749\n",
      "Ep 1 (Step 027915): Train loss 1.070, Val loss 1.747\n",
      "Ep 1 (Step 027920): Train loss 0.930, Val loss 1.745\n",
      "Ep 1 (Step 027925): Train loss 0.910, Val loss 1.743\n",
      "Ep 1 (Step 027930): Train loss 1.150, Val loss 1.742\n",
      "Ep 1 (Step 027935): Train loss 1.209, Val loss 1.741\n",
      "Ep 1 (Step 027940): Train loss 0.954, Val loss 1.742\n",
      "Ep 1 (Step 027945): Train loss 1.436, Val loss 1.745\n",
      "Ep 1 (Step 027950): Train loss 1.028, Val loss 1.746\n",
      "Ep 1 (Step 027955): Train loss 1.082, Val loss 1.748\n",
      "Ep 1 (Step 027960): Train loss 1.093, Val loss 1.750\n",
      "Ep 1 (Step 027965): Train loss 0.983, Val loss 1.751\n",
      "Ep 1 (Step 027970): Train loss 1.295, Val loss 1.753\n",
      "Ep 1 (Step 027975): Train loss 1.351, Val loss 1.754\n",
      "Ep 1 (Step 027980): Train loss 1.223, Val loss 1.753\n",
      "Ep 1 (Step 027985): Train loss 1.263, Val loss 1.753\n",
      "Ep 1 (Step 027990): Train loss 1.230, Val loss 1.753\n",
      "Ep 1 (Step 027995): Train loss 0.976, Val loss 1.753\n",
      "Ep 1 (Step 028000): Train loss 1.236, Val loss 1.754\n",
      "Ep 1 (Step 028005): Train loss 1.297, Val loss 1.755\n",
      "Ep 1 (Step 028010): Train loss 1.475, Val loss 1.755\n",
      "Ep 1 (Step 028015): Train loss 1.259, Val loss 1.754\n",
      "Ep 1 (Step 028020): Train loss 1.164, Val loss 1.755\n",
      "Ep 1 (Step 028025): Train loss 0.848, Val loss 1.755\n",
      "Ep 1 (Step 028030): Train loss 1.174, Val loss 1.755\n",
      "Ep 1 (Step 028035): Train loss 0.904, Val loss 1.755\n",
      "Ep 1 (Step 028040): Train loss 1.124, Val loss 1.755\n",
      "Ep 1 (Step 028045): Train loss 1.264, Val loss 1.756\n",
      "Ep 1 (Step 028050): Train loss 1.011, Val loss 1.757\n",
      "Ep 1 (Step 028055): Train loss 1.198, Val loss 1.757\n",
      "Ep 1 (Step 028060): Train loss 1.500, Val loss 1.756\n",
      "Ep 1 (Step 028065): Train loss 1.208, Val loss 1.756\n",
      "Ep 1 (Step 028070): Train loss 1.275, Val loss 1.756\n",
      "Ep 1 (Step 028075): Train loss 1.082, Val loss 1.756\n",
      "Ep 1 (Step 028080): Train loss 1.182, Val loss 1.756\n",
      "Ep 1 (Step 028085): Train loss 1.395, Val loss 1.756\n",
      "Ep 1 (Step 028090): Train loss 1.079, Val loss 1.756\n",
      "Ep 1 (Step 028095): Train loss 1.507, Val loss 1.755\n",
      "Ep 1 (Step 028100): Train loss 1.258, Val loss 1.754\n",
      "Ep 1 (Step 028105): Train loss 1.222, Val loss 1.754\n",
      "Ep 1 (Step 028110): Train loss 1.075, Val loss 1.750\n",
      "Ep 1 (Step 028115): Train loss 1.292, Val loss 1.749\n",
      "Ep 1 (Step 028120): Train loss 1.590, Val loss 1.748\n",
      "Ep 1 (Step 028125): Train loss 1.080, Val loss 1.746\n",
      "Ep 1 (Step 028130): Train loss 1.212, Val loss 1.745\n",
      "Ep 1 (Step 028135): Train loss 1.389, Val loss 1.744\n",
      "Ep 1 (Step 028140): Train loss 1.032, Val loss 1.744\n",
      "Ep 1 (Step 028145): Train loss 0.921, Val loss 1.744\n",
      "Ep 1 (Step 028150): Train loss 1.396, Val loss 1.744\n",
      "Ep 1 (Step 028155): Train loss 1.248, Val loss 1.747\n",
      "Ep 1 (Step 028160): Train loss 1.021, Val loss 1.749\n",
      "Ep 1 (Step 028165): Train loss 1.498, Val loss 1.752\n",
      "Ep 1 (Step 028170): Train loss 1.034, Val loss 1.754\n",
      "Ep 1 (Step 028175): Train loss 1.095, Val loss 1.755\n",
      "Ep 1 (Step 028180): Train loss 1.111, Val loss 1.755\n",
      "Ep 1 (Step 028185): Train loss 1.280, Val loss 1.755\n",
      "Ep 1 (Step 028190): Train loss 1.310, Val loss 1.756\n",
      "Ep 1 (Step 028195): Train loss 1.217, Val loss 1.756\n",
      "Ep 1 (Step 028200): Train loss 1.202, Val loss 1.756\n",
      "Ep 1 (Step 028205): Train loss 1.278, Val loss 1.755\n",
      "Ep 1 (Step 028210): Train loss 1.074, Val loss 1.753\n",
      "Ep 1 (Step 028215): Train loss 1.369, Val loss 1.753\n",
      "Ep 1 (Step 028220): Train loss 1.116, Val loss 1.754\n",
      "Ep 1 (Step 028225): Train loss 0.971, Val loss 1.755\n",
      "Ep 1 (Step 028230): Train loss 0.898, Val loss 1.756\n",
      "Ep 1 (Step 028235): Train loss 0.941, Val loss 1.756\n",
      "Ep 1 (Step 028240): Train loss 1.401, Val loss 1.755\n",
      "Ep 1 (Step 028245): Train loss 1.017, Val loss 1.753\n",
      "Ep 1 (Step 028250): Train loss 1.031, Val loss 1.753\n",
      "Ep 1 (Step 028255): Train loss 1.292, Val loss 1.754\n",
      "Ep 1 (Step 028260): Train loss 1.499, Val loss 1.753\n",
      "Ep 1 (Step 028265): Train loss 1.588, Val loss 1.753\n",
      "Ep 1 (Step 028270): Train loss 1.199, Val loss 1.756\n",
      "Ep 1 (Step 028275): Train loss 1.185, Val loss 1.758\n",
      "Ep 1 (Step 028280): Train loss 1.228, Val loss 1.758\n",
      "Ep 1 (Step 028285): Train loss 1.237, Val loss 1.757\n",
      "Ep 1 (Step 028290): Train loss 0.996, Val loss 1.757\n",
      "Ep 1 (Step 028295): Train loss 0.951, Val loss 1.758\n",
      "Ep 1 (Step 028300): Train loss 1.517, Val loss 1.758\n",
      "Ep 1 (Step 028305): Train loss 1.251, Val loss 1.758\n",
      "Ep 1 (Step 028310): Train loss 1.393, Val loss 1.755\n",
      "Ep 1 (Step 028315): Train loss 1.056, Val loss 1.753\n",
      "Ep 1 (Step 028320): Train loss 1.109, Val loss 1.752\n",
      "Ep 1 (Step 028325): Train loss 0.846, Val loss 1.753\n",
      "Ep 1 (Step 028330): Train loss 0.952, Val loss 1.754\n",
      "Ep 1 (Step 028335): Train loss 0.803, Val loss 1.755\n",
      "Ep 1 (Step 028340): Train loss 0.968, Val loss 1.754\n",
      "Ep 1 (Step 028345): Train loss 1.086, Val loss 1.752\n",
      "Ep 1 (Step 028350): Train loss 1.331, Val loss 1.749\n",
      "Ep 1 (Step 028355): Train loss 1.094, Val loss 1.747\n",
      "Ep 1 (Step 028360): Train loss 1.200, Val loss 1.748\n",
      "Ep 1 (Step 028365): Train loss 1.283, Val loss 1.749\n",
      "Ep 1 (Step 028370): Train loss 0.891, Val loss 1.749\n",
      "Ep 1 (Step 028375): Train loss 1.235, Val loss 1.748\n",
      "Ep 1 (Step 028380): Train loss 1.437, Val loss 1.748\n",
      "Ep 1 (Step 028385): Train loss 1.214, Val loss 1.748\n",
      "Ep 1 (Step 028390): Train loss 1.157, Val loss 1.748\n",
      "Ep 1 (Step 028395): Train loss 1.237, Val loss 1.747\n",
      "Ep 1 (Step 028400): Train loss 1.105, Val loss 1.746\n",
      "Ep 1 (Step 028405): Train loss 1.146, Val loss 1.746\n",
      "Ep 1 (Step 028410): Train loss 1.112, Val loss 1.745\n",
      "Ep 1 (Step 028415): Train loss 1.030, Val loss 1.743\n",
      "Ep 1 (Step 028420): Train loss 1.341, Val loss 1.742\n",
      "Ep 1 (Step 028425): Train loss 0.821, Val loss 1.742\n",
      "Ep 1 (Step 028430): Train loss 1.244, Val loss 1.742\n",
      "Ep 1 (Step 028435): Train loss 0.982, Val loss 1.742\n",
      "Ep 1 (Step 028440): Train loss 0.929, Val loss 1.743\n",
      "Ep 1 (Step 028445): Train loss 1.356, Val loss 1.745\n",
      "Ep 1 (Step 028450): Train loss 1.410, Val loss 1.745\n",
      "Ep 1 (Step 028455): Train loss 1.377, Val loss 1.746\n",
      "Ep 1 (Step 028460): Train loss 1.137, Val loss 1.746\n",
      "Ep 1 (Step 028465): Train loss 0.980, Val loss 1.746\n",
      "Ep 1 (Step 028470): Train loss 1.237, Val loss 1.745\n",
      "Ep 1 (Step 028475): Train loss 1.266, Val loss 1.745\n",
      "Ep 1 (Step 028480): Train loss 1.088, Val loss 1.744\n",
      "Ep 1 (Step 028485): Train loss 1.024, Val loss 1.745\n",
      "Ep 1 (Step 028490): Train loss 1.311, Val loss 1.747\n",
      "Ep 1 (Step 028495): Train loss 1.157, Val loss 1.747\n",
      "Ep 1 (Step 028500): Train loss 1.204, Val loss 1.748\n",
      "Ep 1 (Step 028505): Train loss 1.015, Val loss 1.747\n",
      "Ep 1 (Step 028510): Train loss 1.123, Val loss 1.746\n",
      "Ep 1 (Step 028515): Train loss 1.487, Val loss 1.744\n",
      "Ep 1 (Step 028520): Train loss 1.059, Val loss 1.744\n",
      "Ep 1 (Step 028525): Train loss 0.883, Val loss 1.743\n",
      "Ep 1 (Step 028530): Train loss 1.334, Val loss 1.743\n",
      "Ep 1 (Step 028535): Train loss 1.325, Val loss 1.742\n",
      "Ep 1 (Step 028540): Train loss 1.466, Val loss 1.742\n",
      "Ep 1 (Step 028545): Train loss 1.399, Val loss 1.742\n",
      "Ep 1 (Step 028550): Train loss 1.214, Val loss 1.741\n",
      "Ep 1 (Step 028555): Train loss 1.205, Val loss 1.742\n",
      "Ep 1 (Step 028560): Train loss 1.302, Val loss 1.743\n",
      "Ep 1 (Step 028565): Train loss 1.036, Val loss 1.745\n",
      "Ep 1 (Step 028570): Train loss 1.099, Val loss 1.745\n",
      "Ep 1 (Step 028575): Train loss 1.453, Val loss 1.745\n",
      "Ep 1 (Step 028580): Train loss 1.342, Val loss 1.744\n",
      "Ep 1 (Step 028585): Train loss 1.198, Val loss 1.742\n",
      "Ep 1 (Step 028590): Train loss 1.029, Val loss 1.741\n",
      "Ep 1 (Step 028595): Train loss 1.003, Val loss 1.740\n",
      "Ep 1 (Step 028600): Train loss 1.129, Val loss 1.740\n",
      "Ep 1 (Step 028605): Train loss 1.208, Val loss 1.741\n",
      "Ep 1 (Step 028610): Train loss 1.508, Val loss 1.742\n",
      "Ep 1 (Step 028615): Train loss 1.199, Val loss 1.743\n",
      "Ep 1 (Step 028620): Train loss 1.193, Val loss 1.744\n",
      "Ep 1 (Step 028625): Train loss 1.167, Val loss 1.746\n",
      "Ep 1 (Step 028630): Train loss 1.245, Val loss 1.746\n",
      "Ep 1 (Step 028635): Train loss 1.074, Val loss 1.746\n",
      "Ep 1 (Step 028640): Train loss 1.409, Val loss 1.745\n",
      "Ep 1 (Step 028645): Train loss 1.063, Val loss 1.741\n",
      "Ep 1 (Step 028650): Train loss 1.270, Val loss 1.739\n",
      "Ep 1 (Step 028655): Train loss 1.325, Val loss 1.736\n",
      "Ep 1 (Step 028660): Train loss 1.200, Val loss 1.735\n",
      "Ep 1 (Step 028665): Train loss 1.350, Val loss 1.735\n",
      "Ep 1 (Step 028670): Train loss 1.152, Val loss 1.735\n",
      "Ep 1 (Step 028675): Train loss 1.147, Val loss 1.734\n",
      "Ep 1 (Step 028680): Train loss 1.051, Val loss 1.733\n",
      "Ep 1 (Step 028685): Train loss 1.449, Val loss 1.733\n",
      "Ep 1 (Step 028690): Train loss 1.111, Val loss 1.734\n",
      "Ep 1 (Step 028695): Train loss 1.059, Val loss 1.736\n",
      "Ep 1 (Step 028700): Train loss 1.120, Val loss 1.736\n",
      "Ep 1 (Step 028705): Train loss 1.064, Val loss 1.736\n",
      "Ep 1 (Step 028710): Train loss 1.186, Val loss 1.737\n",
      "Ep 1 (Step 028715): Train loss 1.094, Val loss 1.738\n",
      "Ep 1 (Step 028720): Train loss 0.915, Val loss 1.739\n",
      "Ep 1 (Step 028725): Train loss 1.106, Val loss 1.737\n",
      "Ep 1 (Step 028730): Train loss 1.299, Val loss 1.736\n",
      "Ep 1 (Step 028735): Train loss 0.998, Val loss 1.735\n",
      "Ep 1 (Step 028740): Train loss 1.029, Val loss 1.732\n",
      "Ep 1 (Step 028745): Train loss 0.960, Val loss 1.730\n",
      "Ep 1 (Step 028750): Train loss 1.249, Val loss 1.730\n",
      "Ep 1 (Step 028755): Train loss 1.133, Val loss 1.729\n",
      "Ep 1 (Step 028760): Train loss 1.130, Val loss 1.729\n",
      "Ep 1 (Step 028765): Train loss 1.066, Val loss 1.731\n",
      "Ep 1 (Step 028770): Train loss 1.241, Val loss 1.733\n",
      "Ep 1 (Step 028775): Train loss 0.990, Val loss 1.734\n",
      "Ep 1 (Step 028780): Train loss 1.162, Val loss 1.737\n",
      "Ep 1 (Step 028785): Train loss 1.210, Val loss 1.738\n",
      "Ep 1 (Step 028790): Train loss 1.079, Val loss 1.740\n",
      "Ep 1 (Step 028795): Train loss 1.125, Val loss 1.740\n",
      "Ep 1 (Step 028800): Train loss 1.241, Val loss 1.740\n",
      "Ep 1 (Step 028805): Train loss 1.173, Val loss 1.740\n",
      "Ep 1 (Step 028810): Train loss 1.219, Val loss 1.740\n",
      "Ep 1 (Step 028815): Train loss 1.108, Val loss 1.738\n",
      "Ep 1 (Step 028820): Train loss 0.904, Val loss 1.738\n",
      "Ep 1 (Step 028825): Train loss 0.960, Val loss 1.738\n",
      "Ep 1 (Step 028830): Train loss 1.190, Val loss 1.736\n",
      "Ep 1 (Step 028835): Train loss 1.029, Val loss 1.735\n",
      "Ep 1 (Step 028840): Train loss 1.306, Val loss 1.734\n",
      "Ep 1 (Step 028845): Train loss 1.143, Val loss 1.735\n",
      "Ep 1 (Step 028850): Train loss 1.205, Val loss 1.737\n",
      "Ep 1 (Step 028855): Train loss 1.401, Val loss 1.738\n",
      "Ep 1 (Step 028860): Train loss 1.364, Val loss 1.738\n",
      "Ep 1 (Step 028865): Train loss 1.192, Val loss 1.738\n",
      "Ep 1 (Step 028870): Train loss 1.171, Val loss 1.737\n",
      "Ep 1 (Step 028875): Train loss 1.264, Val loss 1.736\n",
      "Ep 1 (Step 028880): Train loss 0.845, Val loss 1.737\n",
      "Ep 1 (Step 028885): Train loss 1.142, Val loss 1.738\n",
      "Ep 1 (Step 028890): Train loss 1.744, Val loss 1.739\n",
      "Ep 1 (Step 028895): Train loss 1.294, Val loss 1.738\n",
      "Ep 1 (Step 028900): Train loss 1.481, Val loss 1.737\n",
      "Ep 1 (Step 028905): Train loss 0.887, Val loss 1.735\n",
      "Ep 1 (Step 028910): Train loss 1.201, Val loss 1.735\n",
      "Ep 1 (Step 028915): Train loss 1.122, Val loss 1.735\n",
      "Ep 1 (Step 028920): Train loss 1.190, Val loss 1.736\n",
      "Ep 1 (Step 028925): Train loss 1.208, Val loss 1.736\n",
      "Ep 1 (Step 028930): Train loss 1.034, Val loss 1.736\n",
      "Ep 1 (Step 028935): Train loss 1.218, Val loss 1.737\n",
      "Ep 1 (Step 028940): Train loss 0.889, Val loss 1.739\n",
      "Ep 1 (Step 028945): Train loss 1.415, Val loss 1.742\n",
      "Ep 1 (Step 028950): Train loss 1.194, Val loss 1.741\n",
      "Ep 1 (Step 028955): Train loss 1.168, Val loss 1.741\n",
      "Ep 1 (Step 028960): Train loss 1.117, Val loss 1.741\n",
      "Ep 1 (Step 028965): Train loss 1.383, Val loss 1.742\n",
      "Ep 1 (Step 028970): Train loss 1.079, Val loss 1.741\n",
      "Ep 1 (Step 028975): Train loss 0.932, Val loss 1.740\n",
      "Ep 1 (Step 028980): Train loss 1.181, Val loss 1.740\n",
      "Ep 1 (Step 028985): Train loss 1.234, Val loss 1.739\n",
      "Ep 1 (Step 028990): Train loss 1.103, Val loss 1.737\n",
      "Ep 1 (Step 028995): Train loss 1.168, Val loss 1.735\n",
      "Ep 1 (Step 029000): Train loss 1.044, Val loss 1.734\n",
      "Ep 1 (Step 029005): Train loss 1.243, Val loss 1.734\n",
      "Ep 1 (Step 029010): Train loss 1.098, Val loss 1.733\n",
      "Ep 1 (Step 029015): Train loss 1.291, Val loss 1.734\n",
      "Ep 1 (Step 029020): Train loss 1.202, Val loss 1.735\n",
      "Ep 1 (Step 029025): Train loss 1.318, Val loss 1.735\n",
      "Ep 1 (Step 029030): Train loss 1.067, Val loss 1.734\n",
      "Ep 1 (Step 029035): Train loss 1.151, Val loss 1.734\n",
      "Ep 1 (Step 029040): Train loss 1.260, Val loss 1.734\n",
      "Ep 1 (Step 029045): Train loss 1.137, Val loss 1.736\n",
      "Ep 1 (Step 029050): Train loss 1.327, Val loss 1.738\n",
      "Ep 1 (Step 029055): Train loss 1.201, Val loss 1.741\n",
      "Ep 1 (Step 029060): Train loss 1.310, Val loss 1.744\n",
      "Ep 1 (Step 029065): Train loss 1.101, Val loss 1.745\n",
      "Ep 1 (Step 029070): Train loss 0.942, Val loss 1.745\n",
      "Ep 1 (Step 029075): Train loss 1.134, Val loss 1.744\n",
      "Ep 1 (Step 029080): Train loss 1.122, Val loss 1.743\n",
      "Ep 1 (Step 029085): Train loss 1.364, Val loss 1.743\n",
      "Ep 1 (Step 029090): Train loss 1.155, Val loss 1.742\n",
      "Ep 1 (Step 029095): Train loss 0.968, Val loss 1.742\n",
      "Ep 1 (Step 029100): Train loss 1.281, Val loss 1.741\n",
      "Ep 1 (Step 029105): Train loss 1.340, Val loss 1.740\n",
      "Ep 1 (Step 029110): Train loss 1.160, Val loss 1.739\n",
      "Ep 1 (Step 029115): Train loss 1.137, Val loss 1.738\n",
      "Ep 1 (Step 029120): Train loss 1.523, Val loss 1.739\n",
      "Ep 1 (Step 029125): Train loss 1.272, Val loss 1.741\n",
      "Ep 1 (Step 029130): Train loss 1.123, Val loss 1.744\n",
      "Ep 1 (Step 029135): Train loss 1.319, Val loss 1.746\n",
      "Ep 1 (Step 029140): Train loss 1.041, Val loss 1.748\n",
      "Ep 1 (Step 029145): Train loss 1.176, Val loss 1.750\n",
      "Ep 1 (Step 029150): Train loss 1.063, Val loss 1.751\n",
      "Ep 1 (Step 029155): Train loss 1.248, Val loss 1.754\n",
      "Ep 1 (Step 029160): Train loss 1.332, Val loss 1.755\n",
      "Ep 1 (Step 029165): Train loss 1.089, Val loss 1.755\n",
      "Ep 1 (Step 029170): Train loss 0.851, Val loss 1.754\n",
      "Ep 1 (Step 029175): Train loss 1.324, Val loss 1.754\n",
      "Ep 1 (Step 029180): Train loss 1.312, Val loss 1.753\n",
      "Ep 1 (Step 029185): Train loss 1.023, Val loss 1.752\n",
      "Ep 1 (Step 029190): Train loss 1.209, Val loss 1.750\n",
      "Ep 1 (Step 029195): Train loss 0.913, Val loss 1.749\n",
      "Ep 1 (Step 029200): Train loss 1.147, Val loss 1.748\n",
      "Ep 1 (Step 029205): Train loss 0.977, Val loss 1.745\n",
      "Ep 1 (Step 029210): Train loss 1.196, Val loss 1.742\n",
      "Ep 1 (Step 029215): Train loss 1.300, Val loss 1.741\n",
      "Ep 1 (Step 029220): Train loss 1.045, Val loss 1.740\n",
      "Ep 1 (Step 029225): Train loss 1.102, Val loss 1.740\n",
      "Ep 1 (Step 029230): Train loss 1.382, Val loss 1.739\n",
      "Ep 1 (Step 029235): Train loss 1.173, Val loss 1.737\n",
      "Ep 1 (Step 029240): Train loss 1.119, Val loss 1.737\n",
      "Ep 1 (Step 029245): Train loss 1.232, Val loss 1.735\n",
      "Ep 1 (Step 029250): Train loss 1.393, Val loss 1.733\n",
      "Ep 1 (Step 029255): Train loss 1.150, Val loss 1.732\n",
      "Ep 1 (Step 029260): Train loss 1.210, Val loss 1.731\n",
      "Ep 1 (Step 029265): Train loss 1.429, Val loss 1.730\n",
      "Ep 1 (Step 029270): Train loss 0.877, Val loss 1.728\n",
      "Ep 1 (Step 029275): Train loss 0.932, Val loss 1.726\n",
      "Ep 1 (Step 029280): Train loss 1.510, Val loss 1.726\n",
      "Ep 1 (Step 029285): Train loss 1.218, Val loss 1.727\n",
      "Ep 1 (Step 029290): Train loss 1.160, Val loss 1.728\n",
      "Ep 1 (Step 029295): Train loss 1.161, Val loss 1.728\n",
      "Ep 1 (Step 029300): Train loss 1.214, Val loss 1.728\n",
      "Ep 1 (Step 029305): Train loss 1.115, Val loss 1.729\n",
      "Ep 1 (Step 029310): Train loss 1.148, Val loss 1.729\n",
      "Ep 1 (Step 029315): Train loss 1.114, Val loss 1.730\n",
      "Ep 1 (Step 029320): Train loss 1.307, Val loss 1.731\n",
      "Ep 1 (Step 029325): Train loss 1.162, Val loss 1.732\n",
      "Ep 1 (Step 029330): Train loss 1.304, Val loss 1.733\n",
      "Ep 1 (Step 029335): Train loss 1.067, Val loss 1.733\n",
      "Ep 1 (Step 029340): Train loss 1.132, Val loss 1.733\n",
      "Ep 1 (Step 029345): Train loss 1.041, Val loss 1.734\n",
      "Ep 1 (Step 029350): Train loss 1.296, Val loss 1.734\n",
      "Ep 1 (Step 029355): Train loss 1.365, Val loss 1.734\n",
      "Ep 1 (Step 029360): Train loss 1.045, Val loss 1.733\n",
      "Ep 1 (Step 029365): Train loss 1.081, Val loss 1.733\n",
      "Ep 1 (Step 029370): Train loss 1.008, Val loss 1.733\n",
      "Ep 1 (Step 029375): Train loss 1.113, Val loss 1.732\n",
      "Ep 1 (Step 029380): Train loss 1.112, Val loss 1.732\n",
      "Ep 1 (Step 029385): Train loss 1.205, Val loss 1.733\n",
      "Ep 1 (Step 029390): Train loss 1.033, Val loss 1.733\n",
      "Ep 1 (Step 029395): Train loss 0.988, Val loss 1.734\n",
      "Ep 1 (Step 029400): Train loss 1.190, Val loss 1.734\n",
      "Ep 1 (Step 029405): Train loss 1.251, Val loss 1.733\n",
      "Ep 1 (Step 029410): Train loss 1.164, Val loss 1.732\n",
      "Ep 1 (Step 029415): Train loss 1.381, Val loss 1.731\n",
      "Ep 1 (Step 029420): Train loss 1.237, Val loss 1.732\n",
      "Ep 1 (Step 029425): Train loss 1.327, Val loss 1.733\n",
      "Ep 1 (Step 029430): Train loss 1.158, Val loss 1.735\n",
      "Ep 1 (Step 029435): Train loss 1.173, Val loss 1.738\n",
      "Ep 1 (Step 029440): Train loss 0.901, Val loss 1.740\n",
      "Ep 1 (Step 029445): Train loss 1.284, Val loss 1.742\n",
      "Ep 1 (Step 029450): Train loss 1.012, Val loss 1.743\n",
      "Ep 1 (Step 029455): Train loss 1.172, Val loss 1.744\n",
      "Ep 1 (Step 029460): Train loss 0.873, Val loss 1.746\n",
      "Ep 1 (Step 029465): Train loss 1.261, Val loss 1.747\n",
      "Ep 1 (Step 029470): Train loss 1.272, Val loss 1.747\n",
      "Ep 1 (Step 029475): Train loss 1.348, Val loss 1.747\n",
      "Ep 1 (Step 029480): Train loss 1.200, Val loss 1.747\n",
      "Ep 1 (Step 029485): Train loss 1.061, Val loss 1.746\n",
      "Ep 1 (Step 029490): Train loss 1.199, Val loss 1.745\n",
      "Ep 1 (Step 029495): Train loss 0.966, Val loss 1.745\n",
      "Ep 1 (Step 029500): Train loss 0.941, Val loss 1.746\n",
      "Ep 1 (Step 029505): Train loss 0.847, Val loss 1.747\n",
      "Ep 1 (Step 029510): Train loss 1.279, Val loss 1.747\n",
      "Ep 1 (Step 029515): Train loss 1.284, Val loss 1.748\n",
      "Ep 1 (Step 029520): Train loss 1.111, Val loss 1.748\n",
      "Ep 1 (Step 029525): Train loss 1.219, Val loss 1.747\n",
      "Ep 1 (Step 029530): Train loss 1.002, Val loss 1.746\n",
      "Ep 1 (Step 029535): Train loss 1.186, Val loss 1.745\n",
      "Ep 1 (Step 029540): Train loss 1.126, Val loss 1.744\n",
      "Ep 1 (Step 029545): Train loss 1.279, Val loss 1.743\n",
      "Ep 1 (Step 029550): Train loss 1.474, Val loss 1.743\n",
      "Ep 1 (Step 029555): Train loss 1.306, Val loss 1.743\n",
      "Ep 1 (Step 029560): Train loss 1.057, Val loss 1.743\n",
      "Ep 1 (Step 029565): Train loss 1.066, Val loss 1.742\n",
      "Ep 1 (Step 029570): Train loss 1.231, Val loss 1.740\n",
      "Ep 1 (Step 029575): Train loss 1.163, Val loss 1.740\n",
      "Ep 1 (Step 029580): Train loss 1.292, Val loss 1.741\n",
      "Ep 1 (Step 029585): Train loss 1.224, Val loss 1.742\n",
      "Ep 1 (Step 029590): Train loss 1.157, Val loss 1.741\n",
      "Ep 1 (Step 029595): Train loss 1.192, Val loss 1.739\n",
      "Ep 1 (Step 029600): Train loss 0.921, Val loss 1.738\n",
      "Ep 1 (Step 029605): Train loss 1.104, Val loss 1.738\n",
      "Ep 1 (Step 029610): Train loss 1.259, Val loss 1.738\n",
      "Ep 1 (Step 029615): Train loss 0.991, Val loss 1.738\n",
      "Ep 1 (Step 029620): Train loss 0.952, Val loss 1.739\n",
      "Ep 1 (Step 029625): Train loss 1.506, Val loss 1.740\n",
      "Ep 1 (Step 029630): Train loss 1.101, Val loss 1.741\n",
      "Ep 1 (Step 029635): Train loss 1.257, Val loss 1.740\n",
      "Ep 1 (Step 029640): Train loss 1.168, Val loss 1.739\n",
      "Ep 1 (Step 029645): Train loss 1.215, Val loss 1.736\n",
      "Ep 1 (Step 029650): Train loss 1.277, Val loss 1.734\n",
      "Ep 1 (Step 029655): Train loss 1.070, Val loss 1.732\n",
      "Ep 1 (Step 029660): Train loss 1.075, Val loss 1.730\n",
      "Ep 1 (Step 029665): Train loss 1.273, Val loss 1.730\n",
      "Ep 1 (Step 029670): Train loss 1.127, Val loss 1.728\n",
      "Ep 1 (Step 029675): Train loss 1.187, Val loss 1.727\n",
      "Ep 1 (Step 029680): Train loss 1.137, Val loss 1.728\n",
      "Ep 1 (Step 029685): Train loss 1.159, Val loss 1.730\n",
      "Ep 1 (Step 029690): Train loss 1.102, Val loss 1.732\n",
      "Ep 1 (Step 029695): Train loss 1.103, Val loss 1.735\n",
      "Ep 1 (Step 029700): Train loss 1.128, Val loss 1.738\n",
      "Ep 1 (Step 029705): Train loss 1.005, Val loss 1.740\n",
      "Ep 1 (Step 029710): Train loss 1.028, Val loss 1.743\n",
      "Ep 1 (Step 029715): Train loss 1.100, Val loss 1.744\n",
      "Ep 1 (Step 029720): Train loss 1.060, Val loss 1.744\n",
      "Ep 1 (Step 029725): Train loss 1.106, Val loss 1.743\n",
      "Ep 1 (Step 029730): Train loss 0.999, Val loss 1.742\n",
      "Ep 1 (Step 029735): Train loss 1.074, Val loss 1.740\n",
      "Ep 1 (Step 029740): Train loss 1.088, Val loss 1.739\n",
      "Ep 1 (Step 029745): Train loss 1.225, Val loss 1.738\n",
      "Ep 1 (Step 029750): Train loss 1.355, Val loss 1.737\n",
      "Ep 1 (Step 029755): Train loss 1.046, Val loss 1.736\n",
      "Ep 1 (Step 029760): Train loss 1.224, Val loss 1.736\n",
      "Ep 1 (Step 029765): Train loss 0.942, Val loss 1.736\n",
      "Ep 1 (Step 029770): Train loss 1.177, Val loss 1.735\n",
      "Ep 1 (Step 029775): Train loss 1.063, Val loss 1.734\n",
      "Ep 1 (Step 029780): Train loss 1.252, Val loss 1.733\n",
      "Ep 1 (Step 029785): Train loss 1.252, Val loss 1.733\n",
      "Ep 1 (Step 029790): Train loss 1.385, Val loss 1.734\n",
      "Ep 1 (Step 029795): Train loss 1.356, Val loss 1.735\n",
      "Ep 1 (Step 029800): Train loss 1.194, Val loss 1.737\n",
      "Ep 1 (Step 029805): Train loss 1.267, Val loss 1.737\n",
      "Ep 1 (Step 029810): Train loss 1.290, Val loss 1.736\n",
      "Ep 1 (Step 029815): Train loss 1.307, Val loss 1.736\n",
      "Ep 1 (Step 029820): Train loss 1.069, Val loss 1.736\n",
      "Ep 1 (Step 029825): Train loss 1.135, Val loss 1.734\n",
      "Ep 1 (Step 029830): Train loss 1.349, Val loss 1.734\n",
      "Ep 1 (Step 029835): Train loss 1.110, Val loss 1.735\n",
      "Ep 1 (Step 029840): Train loss 1.102, Val loss 1.737\n",
      "Ep 1 (Step 029845): Train loss 1.066, Val loss 1.739\n",
      "Ep 1 (Step 029850): Train loss 1.448, Val loss 1.739\n",
      "Ep 1 (Step 029855): Train loss 1.296, Val loss 1.739\n",
      "Ep 1 (Step 029860): Train loss 1.212, Val loss 1.738\n",
      "Ep 1 (Step 029865): Train loss 0.919, Val loss 1.739\n",
      "Ep 1 (Step 029870): Train loss 1.103, Val loss 1.739\n",
      "Ep 1 (Step 029875): Train loss 0.987, Val loss 1.740\n",
      "Ep 1 (Step 029880): Train loss 1.037, Val loss 1.742\n",
      "Ep 1 (Step 029885): Train loss 1.070, Val loss 1.744\n",
      "Ep 1 (Step 029890): Train loss 1.506, Val loss 1.744\n",
      "Ep 1 (Step 029895): Train loss 1.191, Val loss 1.745\n",
      "Ep 1 (Step 029900): Train loss 1.065, Val loss 1.746\n",
      "Ep 1 (Step 029905): Train loss 1.068, Val loss 1.747\n",
      "Ep 1 (Step 029910): Train loss 1.205, Val loss 1.746\n",
      "Ep 1 (Step 029915): Train loss 1.461, Val loss 1.746\n",
      "Ep 1 (Step 029920): Train loss 1.403, Val loss 1.745\n",
      "Ep 1 (Step 029925): Train loss 1.050, Val loss 1.746\n",
      "Ep 1 (Step 029930): Train loss 1.320, Val loss 1.747\n",
      "Ep 1 (Step 029935): Train loss 1.331, Val loss 1.747\n",
      "Ep 1 (Step 029940): Train loss 0.936, Val loss 1.749\n",
      "Ep 1 (Step 029945): Train loss 1.199, Val loss 1.749\n",
      "Ep 1 (Step 029950): Train loss 1.165, Val loss 1.749\n",
      "Ep 1 (Step 029955): Train loss 1.463, Val loss 1.749\n",
      "Ep 1 (Step 029960): Train loss 1.217, Val loss 1.748\n",
      "Ep 1 (Step 029965): Train loss 1.144, Val loss 1.746\n",
      "Ep 1 (Step 029970): Train loss 1.283, Val loss 1.746\n",
      "Ep 1 (Step 029975): Train loss 1.253, Val loss 1.745\n",
      "Ep 1 (Step 029980): Train loss 1.009, Val loss 1.744\n",
      "Ep 1 (Step 029985): Train loss 1.042, Val loss 1.744\n",
      "Ep 1 (Step 029990): Train loss 1.133, Val loss 1.745\n",
      "Ep 1 (Step 029995): Train loss 1.084, Val loss 1.746\n",
      "Ep 1 (Step 030000): Train loss 1.475, Val loss 1.746\n",
      "Ep 1 (Step 030005): Train loss 1.207, Val loss 1.747\n",
      "Ep 1 (Step 030010): Train loss 1.104, Val loss 1.748\n",
      "Ep 1 (Step 030015): Train loss 1.169, Val loss 1.747\n",
      "Ep 1 (Step 030020): Train loss 1.171, Val loss 1.747\n",
      "Ep 1 (Step 030025): Train loss 1.059, Val loss 1.746\n",
      "Ep 1 (Step 030030): Train loss 1.132, Val loss 1.747\n",
      "Ep 1 (Step 030035): Train loss 1.207, Val loss 1.748\n",
      "Ep 1 (Step 030040): Train loss 1.015, Val loss 1.748\n",
      "Ep 1 (Step 030045): Train loss 1.422, Val loss 1.747\n",
      "Ep 1 (Step 030050): Train loss 1.300, Val loss 1.746\n",
      "Ep 1 (Step 030055): Train loss 1.069, Val loss 1.745\n",
      "Ep 1 (Step 030060): Train loss 1.157, Val loss 1.743\n",
      "Ep 1 (Step 030065): Train loss 1.033, Val loss 1.742\n",
      "Ep 1 (Step 030070): Train loss 1.291, Val loss 1.742\n",
      "Ep 1 (Step 030075): Train loss 0.960, Val loss 1.742\n",
      "Ep 1 (Step 030080): Train loss 1.441, Val loss 1.743\n",
      "Ep 1 (Step 030085): Train loss 1.255, Val loss 1.744\n",
      "Ep 1 (Step 030090): Train loss 1.019, Val loss 1.746\n",
      "Ep 1 (Step 030095): Train loss 1.131, Val loss 1.747\n",
      "Ep 1 (Step 030100): Train loss 1.098, Val loss 1.750\n",
      "Ep 1 (Step 030105): Train loss 1.377, Val loss 1.752\n",
      "Ep 1 (Step 030110): Train loss 1.184, Val loss 1.754\n",
      "Ep 1 (Step 030115): Train loss 1.344, Val loss 1.755\n",
      "Ep 1 (Step 030120): Train loss 1.113, Val loss 1.754\n",
      "Ep 1 (Step 030125): Train loss 1.003, Val loss 1.754\n",
      "Ep 1 (Step 030130): Train loss 1.046, Val loss 1.754\n",
      "Ep 1 (Step 030135): Train loss 1.106, Val loss 1.754\n",
      "Ep 1 (Step 030140): Train loss 1.011, Val loss 1.753\n",
      "Ep 1 (Step 030145): Train loss 1.221, Val loss 1.752\n",
      "Ep 1 (Step 030150): Train loss 1.127, Val loss 1.752\n",
      "Ep 1 (Step 030155): Train loss 1.251, Val loss 1.751\n",
      "Ep 1 (Step 030160): Train loss 1.047, Val loss 1.749\n",
      "Ep 1 (Step 030165): Train loss 1.234, Val loss 1.747\n",
      "Ep 1 (Step 030170): Train loss 1.278, Val loss 1.747\n",
      "Ep 1 (Step 030175): Train loss 1.134, Val loss 1.747\n",
      "Ep 1 (Step 030180): Train loss 1.039, Val loss 1.746\n",
      "Ep 1 (Step 030185): Train loss 1.190, Val loss 1.746\n",
      "Ep 1 (Step 030190): Train loss 0.902, Val loss 1.744\n",
      "Ep 1 (Step 030195): Train loss 1.131, Val loss 1.743\n",
      "Ep 1 (Step 030200): Train loss 1.125, Val loss 1.742\n",
      "Ep 1 (Step 030205): Train loss 1.022, Val loss 1.742\n",
      "Ep 1 (Step 030210): Train loss 1.251, Val loss 1.742\n",
      "Ep 1 (Step 030215): Train loss 1.172, Val loss 1.743\n",
      "Ep 1 (Step 030220): Train loss 1.137, Val loss 1.745\n",
      "Ep 1 (Step 030225): Train loss 1.332, Val loss 1.746\n",
      "Ep 1 (Step 030230): Train loss 0.989, Val loss 1.748\n",
      "Ep 1 (Step 030235): Train loss 1.202, Val loss 1.750\n",
      "Ep 1 (Step 030240): Train loss 0.979, Val loss 1.750\n",
      "Ep 1 (Step 030245): Train loss 1.282, Val loss 1.750\n",
      "Ep 1 (Step 030250): Train loss 0.855, Val loss 1.750\n",
      "Ep 1 (Step 030255): Train loss 1.128, Val loss 1.751\n",
      "Ep 1 (Step 030260): Train loss 1.131, Val loss 1.750\n",
      "Ep 1 (Step 030265): Train loss 1.167, Val loss 1.750\n",
      "Ep 1 (Step 030270): Train loss 1.102, Val loss 1.751\n",
      "Ep 1 (Step 030275): Train loss 1.184, Val loss 1.752\n",
      "Ep 1 (Step 030280): Train loss 0.956, Val loss 1.754\n",
      "Ep 1 (Step 030285): Train loss 1.101, Val loss 1.755\n",
      "Ep 1 (Step 030290): Train loss 1.267, Val loss 1.755\n",
      "Ep 1 (Step 030295): Train loss 1.061, Val loss 1.753\n",
      "Ep 1 (Step 030300): Train loss 1.082, Val loss 1.752\n",
      "Ep 1 (Step 030305): Train loss 1.268, Val loss 1.750\n",
      "Ep 1 (Step 030310): Train loss 1.155, Val loss 1.748\n",
      "Ep 1 (Step 030315): Train loss 1.142, Val loss 1.748\n",
      "Ep 1 (Step 030320): Train loss 1.362, Val loss 1.748\n",
      "Ep 1 (Step 030325): Train loss 1.141, Val loss 1.748\n",
      "Ep 1 (Step 030330): Train loss 1.398, Val loss 1.749\n",
      "Ep 1 (Step 030335): Train loss 1.301, Val loss 1.750\n",
      "Ep 1 (Step 030340): Train loss 1.145, Val loss 1.750\n",
      "Ep 1 (Step 030345): Train loss 0.958, Val loss 1.750\n",
      "Ep 1 (Step 030350): Train loss 0.985, Val loss 1.749\n",
      "Ep 1 (Step 030355): Train loss 1.047, Val loss 1.748\n",
      "Ep 1 (Step 030360): Train loss 1.001, Val loss 1.747\n",
      "Ep 1 (Step 030365): Train loss 1.207, Val loss 1.747\n",
      "Ep 1 (Step 030370): Train loss 1.091, Val loss 1.748\n",
      "Ep 1 (Step 030375): Train loss 1.047, Val loss 1.748\n",
      "Ep 1 (Step 030380): Train loss 1.456, Val loss 1.749\n",
      "Ep 1 (Step 030385): Train loss 1.229, Val loss 1.748\n",
      "Ep 1 (Step 030390): Train loss 1.315, Val loss 1.748\n",
      "Ep 1 (Step 030395): Train loss 1.350, Val loss 1.749\n",
      "Ep 1 (Step 030400): Train loss 1.281, Val loss 1.749\n",
      "Ep 1 (Step 030405): Train loss 1.215, Val loss 1.751\n",
      "Ep 1 (Step 030410): Train loss 0.966, Val loss 1.752\n",
      "Ep 1 (Step 030415): Train loss 1.128, Val loss 1.751\n",
      "Ep 1 (Step 030420): Train loss 1.045, Val loss 1.751\n",
      "Ep 1 (Step 030425): Train loss 0.956, Val loss 1.752\n",
      "Ep 1 (Step 030430): Train loss 1.111, Val loss 1.752\n",
      "Ep 1 (Step 030435): Train loss 1.126, Val loss 1.750\n",
      "Ep 1 (Step 030440): Train loss 1.405, Val loss 1.749\n",
      "Ep 1 (Step 030445): Train loss 1.035, Val loss 1.748\n",
      "Ep 1 (Step 030450): Train loss 1.193, Val loss 1.748\n",
      "Ep 1 (Step 030455): Train loss 1.029, Val loss 1.748\n",
      "Ep 1 (Step 030460): Train loss 1.135, Val loss 1.748\n",
      "Ep 1 (Step 030465): Train loss 1.041, Val loss 1.750\n",
      "Ep 1 (Step 030470): Train loss 1.115, Val loss 1.752\n",
      "Ep 1 (Step 030475): Train loss 1.395, Val loss 1.753\n",
      "Ep 1 (Step 030480): Train loss 1.143, Val loss 1.751\n",
      "Ep 1 (Step 030485): Train loss 0.975, Val loss 1.750\n",
      "Ep 1 (Step 030490): Train loss 1.275, Val loss 1.749\n",
      "Ep 1 (Step 030495): Train loss 1.008, Val loss 1.751\n",
      "Ep 1 (Step 030500): Train loss 1.149, Val loss 1.752\n",
      "Ep 1 (Step 030505): Train loss 0.884, Val loss 1.752\n",
      "Ep 1 (Step 030510): Train loss 1.381, Val loss 1.751\n",
      "Ep 1 (Step 030515): Train loss 1.133, Val loss 1.751\n",
      "Ep 1 (Step 030520): Train loss 1.081, Val loss 1.752\n",
      "Ep 1 (Step 030525): Train loss 1.145, Val loss 1.751\n",
      "Ep 1 (Step 030530): Train loss 0.956, Val loss 1.750\n",
      "Ep 1 (Step 030535): Train loss 1.242, Val loss 1.752\n",
      "Ep 1 (Step 030540): Train loss 0.940, Val loss 1.752\n",
      "Ep 1 (Step 030545): Train loss 1.286, Val loss 1.753\n",
      "Ep 1 (Step 030550): Train loss 1.149, Val loss 1.753\n",
      "Ep 1 (Step 030555): Train loss 1.198, Val loss 1.754\n",
      "Ep 1 (Step 030560): Train loss 1.324, Val loss 1.754\n",
      "Ep 1 (Step 030565): Train loss 1.174, Val loss 1.755\n",
      "Ep 1 (Step 030570): Train loss 1.043, Val loss 1.757\n",
      "Ep 1 (Step 030575): Train loss 0.906, Val loss 1.759\n",
      "Ep 1 (Step 030580): Train loss 1.262, Val loss 1.761\n",
      "Ep 1 (Step 030585): Train loss 1.040, Val loss 1.762\n",
      "Ep 1 (Step 030590): Train loss 1.161, Val loss 1.762\n",
      "Ep 1 (Step 030595): Train loss 1.035, Val loss 1.763\n",
      "Ep 1 (Step 030600): Train loss 1.213, Val loss 1.764\n",
      "Ep 1 (Step 030605): Train loss 1.140, Val loss 1.765\n",
      "Ep 1 (Step 030610): Train loss 1.258, Val loss 1.765\n",
      "Ep 1 (Step 030615): Train loss 1.075, Val loss 1.764\n",
      "Ep 1 (Step 030620): Train loss 1.486, Val loss 1.763\n",
      "Ep 1 (Step 030625): Train loss 1.052, Val loss 1.762\n",
      "Ep 1 (Step 030630): Train loss 1.025, Val loss 1.762\n",
      "Ep 1 (Step 030635): Train loss 1.079, Val loss 1.763\n",
      "Ep 1 (Step 030640): Train loss 1.135, Val loss 1.762\n",
      "Ep 1 (Step 030645): Train loss 1.132, Val loss 1.761\n",
      "Ep 1 (Step 030650): Train loss 1.012, Val loss 1.761\n",
      "Ep 1 (Step 030655): Train loss 0.888, Val loss 1.762\n",
      "Ep 1 (Step 030660): Train loss 1.273, Val loss 1.762\n",
      "Ep 1 (Step 030665): Train loss 1.110, Val loss 1.761\n",
      "Ep 1 (Step 030670): Train loss 1.114, Val loss 1.760\n",
      "Ep 1 (Step 030675): Train loss 0.909, Val loss 1.759\n",
      "Ep 1 (Step 030680): Train loss 1.286, Val loss 1.759\n",
      "Ep 1 (Step 030685): Train loss 1.328, Val loss 1.759\n",
      "Ep 1 (Step 030690): Train loss 1.047, Val loss 1.758\n",
      "Ep 1 (Step 030695): Train loss 1.346, Val loss 1.756\n",
      "Ep 1 (Step 030700): Train loss 1.162, Val loss 1.754\n",
      "Ep 1 (Step 030705): Train loss 0.797, Val loss 1.754\n",
      "Ep 1 (Step 030710): Train loss 0.975, Val loss 1.754\n",
      "Ep 1 (Step 030715): Train loss 1.175, Val loss 1.754\n",
      "Ep 1 (Step 030720): Train loss 1.482, Val loss 1.754\n",
      "Ep 1 (Step 030725): Train loss 1.122, Val loss 1.753\n",
      "Ep 1 (Step 030730): Train loss 1.269, Val loss 1.753\n",
      "Ep 1 (Step 030735): Train loss 1.144, Val loss 1.752\n",
      "Ep 1 (Step 030740): Train loss 1.065, Val loss 1.751\n",
      "Ep 1 (Step 030745): Train loss 0.981, Val loss 1.750\n",
      "Ep 1 (Step 030750): Train loss 1.218, Val loss 1.750\n",
      "Ep 1 (Step 030755): Train loss 1.050, Val loss 1.751\n",
      "Ep 1 (Step 030760): Train loss 1.231, Val loss 1.753\n",
      "Ep 1 (Step 030765): Train loss 1.238, Val loss 1.753\n",
      "Ep 1 (Step 030770): Train loss 1.082, Val loss 1.753\n",
      "Ep 1 (Step 030775): Train loss 1.068, Val loss 1.753\n",
      "Ep 1 (Step 030780): Train loss 1.038, Val loss 1.752\n",
      "Ep 1 (Step 030785): Train loss 1.069, Val loss 1.751\n",
      "Ep 1 (Step 030790): Train loss 1.229, Val loss 1.751\n",
      "Ep 1 (Step 030795): Train loss 1.284, Val loss 1.752\n",
      "Ep 1 (Step 030800): Train loss 1.192, Val loss 1.753\n",
      "Ep 1 (Step 030805): Train loss 1.138, Val loss 1.754\n",
      "Ep 1 (Step 030810): Train loss 1.280, Val loss 1.754\n",
      "Ep 1 (Step 030815): Train loss 1.111, Val loss 1.754\n",
      "Ep 1 (Step 030820): Train loss 1.185, Val loss 1.753\n",
      "Ep 1 (Step 030825): Train loss 1.183, Val loss 1.755\n",
      "Ep 1 (Step 030830): Train loss 1.219, Val loss 1.756\n",
      "Ep 1 (Step 030835): Train loss 0.923, Val loss 1.757\n",
      "Ep 1 (Step 030840): Train loss 1.214, Val loss 1.757\n",
      "Ep 1 (Step 030845): Train loss 1.049, Val loss 1.757\n",
      "Ep 1 (Step 030850): Train loss 1.008, Val loss 1.756\n",
      "Ep 1 (Step 030855): Train loss 1.058, Val loss 1.755\n",
      "Ep 1 (Step 030860): Train loss 1.291, Val loss 1.755\n",
      "Ep 1 (Step 030865): Train loss 1.177, Val loss 1.755\n",
      "Ep 1 (Step 030870): Train loss 1.170, Val loss 1.754\n",
      "Ep 1 (Step 030875): Train loss 1.167, Val loss 1.753\n",
      "Ep 1 (Step 030880): Train loss 0.951, Val loss 1.753\n",
      "Ep 1 (Step 030885): Train loss 1.079, Val loss 1.755\n",
      "Ep 1 (Step 030890): Train loss 1.277, Val loss 1.757\n",
      "Ep 1 (Step 030895): Train loss 1.331, Val loss 1.761\n",
      "Ep 1 (Step 030900): Train loss 1.219, Val loss 1.763\n",
      "Ep 1 (Step 030905): Train loss 0.975, Val loss 1.764\n",
      "Ep 1 (Step 030910): Train loss 1.283, Val loss 1.764\n",
      "Ep 1 (Step 030915): Train loss 1.527, Val loss 1.764\n",
      "Ep 1 (Step 030920): Train loss 1.272, Val loss 1.763\n",
      "Ep 1 (Step 030925): Train loss 1.391, Val loss 1.763\n",
      "Ep 1 (Step 030930): Train loss 1.056, Val loss 1.762\n",
      "Ep 1 (Step 030935): Train loss 1.321, Val loss 1.762\n",
      "Ep 1 (Step 030940): Train loss 1.247, Val loss 1.762\n",
      "Ep 1 (Step 030945): Train loss 1.163, Val loss 1.761\n",
      "Ep 1 (Step 030950): Train loss 0.972, Val loss 1.760\n",
      "Ep 1 (Step 030955): Train loss 1.006, Val loss 1.758\n",
      "Ep 1 (Step 030960): Train loss 1.296, Val loss 1.757\n",
      "Ep 1 (Step 030965): Train loss 1.019, Val loss 1.754\n",
      "Ep 1 (Step 030970): Train loss 1.113, Val loss 1.753\n",
      "Ep 1 (Step 030975): Train loss 1.173, Val loss 1.753\n",
      "Ep 1 (Step 030980): Train loss 0.979, Val loss 1.752\n",
      "Ep 1 (Step 030985): Train loss 1.268, Val loss 1.753\n",
      "Ep 1 (Step 030990): Train loss 1.416, Val loss 1.755\n",
      "Ep 1 (Step 030995): Train loss 1.353, Val loss 1.755\n",
      "Ep 1 (Step 031000): Train loss 1.171, Val loss 1.756\n",
      "Ep 1 (Step 031005): Train loss 1.005, Val loss 1.757\n",
      "Ep 1 (Step 031010): Train loss 1.269, Val loss 1.758\n",
      "Ep 1 (Step 031015): Train loss 1.192, Val loss 1.759\n",
      "Ep 1 (Step 031020): Train loss 1.188, Val loss 1.760\n",
      "Ep 1 (Step 031025): Train loss 0.948, Val loss 1.760\n",
      "Ep 1 (Step 031030): Train loss 1.383, Val loss 1.760\n",
      "Ep 1 (Step 031035): Train loss 1.361, Val loss 1.761\n",
      "Ep 1 (Step 031040): Train loss 1.160, Val loss 1.760\n",
      "Ep 1 (Step 031045): Train loss 1.240, Val loss 1.760\n",
      "Ep 1 (Step 031050): Train loss 0.916, Val loss 1.760\n",
      "Ep 1 (Step 031055): Train loss 1.152, Val loss 1.761\n",
      "Ep 1 (Step 031060): Train loss 1.332, Val loss 1.762\n",
      "Ep 1 (Step 031065): Train loss 1.245, Val loss 1.763\n",
      "Ep 1 (Step 031070): Train loss 1.228, Val loss 1.763\n",
      "Ep 1 (Step 031075): Train loss 1.237, Val loss 1.763\n",
      "Ep 1 (Step 031080): Train loss 1.276, Val loss 1.762\n",
      "Ep 1 (Step 031085): Train loss 0.989, Val loss 1.761\n",
      "Ep 1 (Step 031090): Train loss 1.036, Val loss 1.762\n",
      "Ep 1 (Step 031095): Train loss 1.289, Val loss 1.761\n",
      "Ep 1 (Step 031100): Train loss 1.478, Val loss 1.762\n",
      "Ep 1 (Step 031105): Train loss 1.100, Val loss 1.764\n",
      "Ep 1 (Step 031110): Train loss 0.856, Val loss 1.767\n",
      "Ep 1 (Step 031115): Train loss 1.156, Val loss 1.771\n",
      "Ep 1 (Step 031120): Train loss 1.161, Val loss 1.773\n",
      "Ep 1 (Step 031125): Train loss 1.246, Val loss 1.773\n",
      "Ep 1 (Step 031130): Train loss 1.353, Val loss 1.772\n",
      "Ep 1 (Step 031135): Train loss 1.145, Val loss 1.770\n",
      "Ep 1 (Step 031140): Train loss 1.181, Val loss 1.769\n",
      "Ep 1 (Step 031145): Train loss 1.287, Val loss 1.767\n",
      "Ep 1 (Step 031150): Train loss 1.393, Val loss 1.767\n",
      "Ep 1 (Step 031155): Train loss 1.124, Val loss 1.768\n",
      "Ep 1 (Step 031160): Train loss 1.170, Val loss 1.767\n",
      "Ep 1 (Step 031165): Train loss 1.058, Val loss 1.764\n",
      "Ep 1 (Step 031170): Train loss 1.127, Val loss 1.764\n",
      "Ep 1 (Step 031175): Train loss 1.166, Val loss 1.764\n",
      "Ep 1 (Step 031180): Train loss 1.078, Val loss 1.765\n",
      "Ep 1 (Step 031185): Train loss 0.947, Val loss 1.765\n",
      "Ep 1 (Step 031190): Train loss 1.433, Val loss 1.765\n",
      "Ep 1 (Step 031195): Train loss 1.056, Val loss 1.765\n",
      "Ep 1 (Step 031200): Train loss 1.321, Val loss 1.763\n",
      "Ep 1 (Step 031205): Train loss 0.915, Val loss 1.762\n",
      "Ep 1 (Step 031210): Train loss 1.257, Val loss 1.761\n",
      "Ep 1 (Step 031215): Train loss 1.349, Val loss 1.760\n",
      "Ep 1 (Step 031220): Train loss 1.069, Val loss 1.760\n",
      "Ep 1 (Step 031225): Train loss 1.174, Val loss 1.760\n",
      "Ep 1 (Step 031230): Train loss 1.095, Val loss 1.761\n",
      "Ep 1 (Step 031235): Train loss 0.922, Val loss 1.763\n",
      "Ep 1 (Step 031240): Train loss 1.165, Val loss 1.765\n",
      "Ep 1 (Step 031245): Train loss 1.127, Val loss 1.765\n",
      "Ep 1 (Step 031250): Train loss 1.100, Val loss 1.764\n",
      "Ep 1 (Step 031255): Train loss 1.138, Val loss 1.763\n",
      "Ep 1 (Step 031260): Train loss 1.048, Val loss 1.763\n",
      "Ep 1 (Step 031265): Train loss 0.944, Val loss 1.763\n",
      "Ep 1 (Step 031270): Train loss 1.168, Val loss 1.762\n",
      "Ep 1 (Step 031275): Train loss 1.041, Val loss 1.760\n",
      "Ep 1 (Step 031280): Train loss 1.107, Val loss 1.758\n",
      "Ep 1 (Step 031285): Train loss 1.051, Val loss 1.758\n",
      "Ep 1 (Step 031290): Train loss 0.822, Val loss 1.758\n",
      "Ep 1 (Step 031295): Train loss 1.230, Val loss 1.759\n",
      "Ep 1 (Step 031300): Train loss 1.251, Val loss 1.759\n",
      "Ep 1 (Step 031305): Train loss 1.211, Val loss 1.760\n",
      "Ep 1 (Step 031310): Train loss 1.302, Val loss 1.760\n",
      "Ep 1 (Step 031315): Train loss 0.920, Val loss 1.761\n",
      "Ep 1 (Step 031320): Train loss 1.051, Val loss 1.761\n",
      "Ep 1 (Step 031325): Train loss 1.143, Val loss 1.759\n",
      "Ep 1 (Step 031330): Train loss 0.938, Val loss 1.758\n",
      "Ep 1 (Step 031335): Train loss 1.015, Val loss 1.757\n",
      "Ep 1 (Step 031340): Train loss 1.379, Val loss 1.757\n",
      "Ep 1 (Step 031345): Train loss 0.896, Val loss 1.757\n",
      "Ep 1 (Step 031350): Train loss 1.274, Val loss 1.756\n",
      "Ep 1 (Step 031355): Train loss 0.917, Val loss 1.755\n",
      "Ep 1 (Step 031360): Train loss 1.235, Val loss 1.755\n",
      "Ep 1 (Step 031365): Train loss 1.086, Val loss 1.755\n",
      "Ep 1 (Step 031370): Train loss 0.901, Val loss 1.755\n",
      "Ep 1 (Step 031375): Train loss 1.008, Val loss 1.754\n",
      "Ep 1 (Step 031380): Train loss 1.169, Val loss 1.754\n",
      "Ep 1 (Step 031385): Train loss 1.012, Val loss 1.754\n",
      "Ep 1 (Step 031390): Train loss 0.967, Val loss 1.754\n",
      "Ep 1 (Step 031395): Train loss 1.243, Val loss 1.753\n",
      "Ep 1 (Step 031400): Train loss 1.115, Val loss 1.754\n",
      "Ep 1 (Step 031405): Train loss 1.433, Val loss 1.754\n",
      "Ep 1 (Step 031410): Train loss 0.966, Val loss 1.754\n",
      "Ep 1 (Step 031415): Train loss 1.246, Val loss 1.755\n",
      "Ep 1 (Step 031420): Train loss 1.342, Val loss 1.758\n",
      "Ep 1 (Step 031425): Train loss 1.160, Val loss 1.760\n",
      "Ep 1 (Step 031430): Train loss 1.353, Val loss 1.762\n",
      "Ep 1 (Step 031435): Train loss 1.156, Val loss 1.764\n",
      "Ep 1 (Step 031440): Train loss 0.889, Val loss 1.765\n",
      "Ep 1 (Step 031445): Train loss 1.037, Val loss 1.766\n",
      "Ep 1 (Step 031450): Train loss 1.058, Val loss 1.770\n",
      "Ep 1 (Step 031455): Train loss 1.123, Val loss 1.771\n",
      "Ep 1 (Step 031460): Train loss 1.004, Val loss 1.771\n",
      "Ep 1 (Step 031465): Train loss 1.360, Val loss 1.771\n",
      "Ep 1 (Step 031470): Train loss 1.198, Val loss 1.769\n",
      "Ep 1 (Step 031475): Train loss 1.070, Val loss 1.767\n",
      "Ep 1 (Step 031480): Train loss 1.093, Val loss 1.765\n",
      "Ep 1 (Step 031485): Train loss 1.435, Val loss 1.762\n",
      "Ep 1 (Step 031490): Train loss 1.082, Val loss 1.760\n",
      "Ep 1 (Step 031495): Train loss 1.379, Val loss 1.759\n",
      "Ep 1 (Step 031500): Train loss 1.151, Val loss 1.758\n",
      "Ep 1 (Step 031505): Train loss 1.062, Val loss 1.757\n",
      "Ep 1 (Step 031510): Train loss 1.188, Val loss 1.757\n",
      "Ep 1 (Step 031515): Train loss 0.993, Val loss 1.757\n",
      "Ep 1 (Step 031520): Train loss 1.058, Val loss 1.758\n",
      "Ep 1 (Step 031525): Train loss 1.092, Val loss 1.758\n",
      "Ep 1 (Step 031530): Train loss 1.083, Val loss 1.759\n",
      "Ep 1 (Step 031535): Train loss 1.140, Val loss 1.761\n",
      "Ep 1 (Step 031540): Train loss 1.516, Val loss 1.762\n",
      "Ep 1 (Step 031545): Train loss 0.997, Val loss 1.763\n",
      "Ep 1 (Step 031550): Train loss 1.061, Val loss 1.762\n",
      "Ep 1 (Step 031555): Train loss 1.293, Val loss 1.762\n",
      "Ep 1 (Step 031560): Train loss 1.284, Val loss 1.762\n",
      "Ep 1 (Step 031565): Train loss 1.260, Val loss 1.762\n",
      "Ep 1 (Step 031570): Train loss 1.219, Val loss 1.762\n",
      "Ep 1 (Step 031575): Train loss 1.186, Val loss 1.764\n",
      "Ep 1 (Step 031580): Train loss 0.880, Val loss 1.765\n",
      "Ep 1 (Step 031585): Train loss 1.123, Val loss 1.764\n",
      "Ep 1 (Step 031590): Train loss 1.452, Val loss 1.764\n",
      "Ep 1 (Step 031595): Train loss 1.210, Val loss 1.764\n",
      "Ep 1 (Step 031600): Train loss 1.458, Val loss 1.763\n",
      "Ep 1 (Step 031605): Train loss 1.150, Val loss 1.761\n",
      "Ep 1 (Step 031610): Train loss 1.267, Val loss 1.760\n",
      "Ep 1 (Step 031615): Train loss 1.233, Val loss 1.759\n",
      "Ep 1 (Step 031620): Train loss 1.006, Val loss 1.759\n",
      "Ep 1 (Step 031625): Train loss 1.046, Val loss 1.760\n",
      "Ep 1 (Step 031630): Train loss 1.017, Val loss 1.760\n",
      "Ep 1 (Step 031635): Train loss 1.414, Val loss 1.761\n",
      "Ep 1 (Step 031640): Train loss 1.039, Val loss 1.763\n",
      "Ep 1 (Step 031645): Train loss 1.036, Val loss 1.765\n",
      "Ep 1 (Step 031650): Train loss 1.217, Val loss 1.767\n",
      "Ep 1 (Step 031655): Train loss 1.023, Val loss 1.768\n",
      "Ep 1 (Step 031660): Train loss 1.164, Val loss 1.769\n",
      "Ep 1 (Step 031665): Train loss 1.245, Val loss 1.771\n",
      "Ep 1 (Step 031670): Train loss 1.082, Val loss 1.772\n",
      "Ep 1 (Step 031675): Train loss 0.964, Val loss 1.773\n",
      "Ep 1 (Step 031680): Train loss 1.083, Val loss 1.771\n",
      "Ep 1 (Step 031685): Train loss 1.138, Val loss 1.769\n",
      "Ep 1 (Step 031690): Train loss 0.868, Val loss 1.767\n",
      "Ep 1 (Step 031695): Train loss 1.258, Val loss 1.766\n",
      "Ep 1 (Step 031700): Train loss 1.346, Val loss 1.764\n",
      "Ep 1 (Step 031705): Train loss 0.849, Val loss 1.763\n",
      "Ep 1 (Step 031710): Train loss 1.014, Val loss 1.763\n",
      "Ep 1 (Step 031715): Train loss 1.101, Val loss 1.763\n",
      "Ep 1 (Step 031720): Train loss 1.022, Val loss 1.763\n",
      "Ep 1 (Step 031725): Train loss 1.068, Val loss 1.764\n",
      "Ep 1 (Step 031730): Train loss 0.948, Val loss 1.763\n",
      "Ep 1 (Step 031735): Train loss 1.208, Val loss 1.762\n",
      "Ep 1 (Step 031740): Train loss 1.046, Val loss 1.761\n",
      "Ep 1 (Step 031745): Train loss 1.372, Val loss 1.761\n",
      "Ep 1 (Step 031750): Train loss 1.271, Val loss 1.760\n",
      "Ep 1 (Step 031755): Train loss 0.985, Val loss 1.759\n",
      "Ep 1 (Step 031760): Train loss 1.021, Val loss 1.759\n",
      "Ep 1 (Step 031765): Train loss 0.777, Val loss 1.758\n",
      "Ep 1 (Step 031770): Train loss 1.026, Val loss 1.759\n",
      "Ep 1 (Step 031775): Train loss 1.102, Val loss 1.759\n",
      "Ep 1 (Step 031780): Train loss 1.261, Val loss 1.760\n",
      "Ep 1 (Step 031785): Train loss 1.487, Val loss 1.760\n",
      "Ep 1 (Step 031790): Train loss 1.352, Val loss 1.760\n",
      "Ep 1 (Step 031795): Train loss 1.407, Val loss 1.762\n",
      "Ep 1 (Step 031800): Train loss 1.369, Val loss 1.763\n",
      "Ep 1 (Step 031805): Train loss 1.140, Val loss 1.763\n",
      "Ep 1 (Step 031810): Train loss 1.382, Val loss 1.761\n",
      "Ep 1 (Step 031815): Train loss 1.465, Val loss 1.759\n",
      "Ep 1 (Step 031820): Train loss 0.977, Val loss 1.757\n",
      "Ep 1 (Step 031825): Train loss 1.255, Val loss 1.754\n",
      "Ep 1 (Step 031830): Train loss 1.099, Val loss 1.753\n",
      "Ep 1 (Step 031835): Train loss 1.135, Val loss 1.754\n",
      "Ep 1 (Step 031840): Train loss 0.827, Val loss 1.753\n",
      "Ep 1 (Step 031845): Train loss 1.220, Val loss 1.753\n",
      "Ep 1 (Step 031850): Train loss 1.138, Val loss 1.752\n",
      "Ep 1 (Step 031855): Train loss 1.313, Val loss 1.753\n",
      "Ep 1 (Step 031860): Train loss 1.172, Val loss 1.754\n",
      "Ep 1 (Step 031865): Train loss 1.187, Val loss 1.756\n",
      "Ep 1 (Step 031870): Train loss 1.095, Val loss 1.757\n",
      "Ep 1 (Step 031875): Train loss 0.925, Val loss 1.758\n",
      "Ep 1 (Step 031880): Train loss 1.332, Val loss 1.758\n",
      "Ep 1 (Step 031885): Train loss 1.066, Val loss 1.757\n",
      "Ep 1 (Step 031890): Train loss 1.395, Val loss 1.755\n",
      "Ep 1 (Step 031895): Train loss 1.207, Val loss 1.753\n",
      "Ep 1 (Step 031900): Train loss 1.111, Val loss 1.753\n",
      "Ep 1 (Step 031905): Train loss 1.305, Val loss 1.752\n",
      "Ep 1 (Step 031910): Train loss 0.886, Val loss 1.752\n",
      "Ep 1 (Step 031915): Train loss 0.962, Val loss 1.751\n",
      "Ep 1 (Step 031920): Train loss 1.064, Val loss 1.750\n",
      "Ep 1 (Step 031925): Train loss 0.965, Val loss 1.749\n",
      "Ep 1 (Step 031930): Train loss 1.244, Val loss 1.750\n",
      "Ep 1 (Step 031935): Train loss 1.433, Val loss 1.751\n",
      "Ep 1 (Step 031940): Train loss 1.335, Val loss 1.753\n",
      "Ep 1 (Step 031945): Train loss 0.864, Val loss 1.754\n",
      "Ep 1 (Step 031950): Train loss 1.125, Val loss 1.756\n",
      "Ep 1 (Step 031955): Train loss 1.287, Val loss 1.759\n",
      "Ep 1 (Step 031960): Train loss 1.237, Val loss 1.761\n",
      "Ep 1 (Step 031965): Train loss 1.277, Val loss 1.762\n",
      "Ep 1 (Step 031970): Train loss 1.075, Val loss 1.761\n",
      "Ep 1 (Step 031975): Train loss 1.287, Val loss 1.760\n",
      "Ep 1 (Step 031980): Train loss 0.934, Val loss 1.760\n",
      "Ep 1 (Step 031985): Train loss 1.195, Val loss 1.760\n",
      "Ep 1 (Step 031990): Train loss 1.173, Val loss 1.761\n",
      "Ep 1 (Step 031995): Train loss 1.250, Val loss 1.761\n",
      "Ep 1 (Step 032000): Train loss 1.246, Val loss 1.762\n",
      "Ep 1 (Step 032005): Train loss 1.237, Val loss 1.763\n",
      "Ep 1 (Step 032010): Train loss 1.108, Val loss 1.762\n",
      "Ep 1 (Step 032015): Train loss 1.002, Val loss 1.761\n",
      "Ep 1 (Step 032020): Train loss 1.117, Val loss 1.761\n",
      "Ep 1 (Step 032025): Train loss 0.995, Val loss 1.760\n",
      "Ep 1 (Step 032030): Train loss 1.241, Val loss 1.758\n",
      "Ep 1 (Step 032035): Train loss 1.112, Val loss 1.757\n",
      "Ep 1 (Step 032040): Train loss 1.158, Val loss 1.758\n",
      "Ep 1 (Step 032045): Train loss 1.242, Val loss 1.757\n",
      "Ep 1 (Step 032050): Train loss 1.092, Val loss 1.757\n",
      "Ep 1 (Step 032055): Train loss 1.187, Val loss 1.757\n",
      "Ep 1 (Step 032060): Train loss 1.013, Val loss 1.758\n",
      "Ep 1 (Step 032065): Train loss 0.882, Val loss 1.758\n",
      "Ep 1 (Step 032070): Train loss 1.182, Val loss 1.761\n",
      "Ep 1 (Step 032075): Train loss 1.138, Val loss 1.763\n",
      "Ep 1 (Step 032080): Train loss 1.236, Val loss 1.765\n",
      "Ep 1 (Step 032085): Train loss 1.332, Val loss 1.766\n",
      "Ep 1 (Step 032090): Train loss 1.122, Val loss 1.767\n",
      "Ep 1 (Step 032095): Train loss 1.265, Val loss 1.765\n",
      "Ep 1 (Step 032100): Train loss 1.121, Val loss 1.765\n",
      "Ep 1 (Step 032105): Train loss 1.011, Val loss 1.765\n",
      "Ep 1 (Step 032110): Train loss 1.305, Val loss 1.766\n",
      "Ep 1 (Step 032115): Train loss 1.373, Val loss 1.766\n",
      "Ep 1 (Step 032120): Train loss 1.198, Val loss 1.765\n",
      "Ep 1 (Step 032125): Train loss 1.017, Val loss 1.765\n",
      "Ep 1 (Step 032130): Train loss 1.261, Val loss 1.764\n",
      "Ep 1 (Step 032135): Train loss 1.222, Val loss 1.762\n",
      "Ep 1 (Step 032140): Train loss 1.381, Val loss 1.762\n",
      "Ep 1 (Step 032145): Train loss 1.116, Val loss 1.761\n",
      "Ep 1 (Step 032150): Train loss 0.889, Val loss 1.759\n",
      "Ep 1 (Step 032155): Train loss 0.871, Val loss 1.759\n",
      "Ep 1 (Step 032160): Train loss 1.073, Val loss 1.760\n",
      "Ep 1 (Step 032165): Train loss 1.247, Val loss 1.760\n",
      "Ep 1 (Step 032170): Train loss 1.126, Val loss 1.759\n",
      "Ep 1 (Step 032175): Train loss 1.363, Val loss 1.758\n",
      "Ep 1 (Step 032180): Train loss 1.284, Val loss 1.758\n",
      "Ep 1 (Step 032185): Train loss 0.915, Val loss 1.758\n",
      "Ep 1 (Step 032190): Train loss 1.112, Val loss 1.758\n",
      "Ep 1 (Step 032195): Train loss 0.898, Val loss 1.759\n",
      "Ep 1 (Step 032200): Train loss 1.118, Val loss 1.759\n",
      "Ep 1 (Step 032205): Train loss 1.293, Val loss 1.760\n",
      "Ep 1 (Step 032210): Train loss 1.063, Val loss 1.760\n",
      "Ep 1 (Step 032215): Train loss 1.152, Val loss 1.759\n",
      "Ep 1 (Step 032220): Train loss 1.514, Val loss 1.760\n",
      "Ep 1 (Step 032225): Train loss 1.208, Val loss 1.763\n",
      "Ep 1 (Step 032230): Train loss 1.191, Val loss 1.764\n",
      "Ep 1 (Step 032235): Train loss 1.183, Val loss 1.766\n",
      "Ep 1 (Step 032240): Train loss 1.182, Val loss 1.767\n",
      "Ep 1 (Step 032245): Train loss 1.314, Val loss 1.768\n",
      "Ep 1 (Step 032250): Train loss 1.251, Val loss 1.768\n",
      "Ep 1 (Step 032255): Train loss 1.136, Val loss 1.767\n",
      "Ep 1 (Step 032260): Train loss 0.975, Val loss 1.765\n",
      "Ep 1 (Step 032265): Train loss 1.345, Val loss 1.762\n",
      "Ep 1 (Step 032270): Train loss 1.203, Val loss 1.761\n",
      "Ep 1 (Step 032275): Train loss 0.970, Val loss 1.761\n",
      "Ep 1 (Step 032280): Train loss 1.141, Val loss 1.762\n",
      "Ep 1 (Step 032285): Train loss 1.437, Val loss 1.762\n",
      "Ep 1 (Step 032290): Train loss 1.111, Val loss 1.764\n",
      "Ep 1 (Step 032295): Train loss 1.020, Val loss 1.768\n",
      "Ep 1 (Step 032300): Train loss 1.160, Val loss 1.771\n",
      "Ep 1 (Step 032305): Train loss 1.004, Val loss 1.772\n",
      "Ep 1 (Step 032310): Train loss 1.282, Val loss 1.775\n",
      "Ep 1 (Step 032315): Train loss 1.104, Val loss 1.775\n",
      "Ep 1 (Step 032320): Train loss 1.012, Val loss 1.773\n",
      "Ep 1 (Step 032325): Train loss 1.072, Val loss 1.771\n",
      "Ep 1 (Step 032330): Train loss 1.120, Val loss 1.770\n",
      "Ep 1 (Step 032335): Train loss 1.176, Val loss 1.768\n",
      "Ep 1 (Step 032340): Train loss 1.377, Val loss 1.767\n",
      "Ep 1 (Step 032345): Train loss 1.504, Val loss 1.766\n",
      "Ep 1 (Step 032350): Train loss 1.185, Val loss 1.766\n",
      "Ep 1 (Step 032355): Train loss 1.270, Val loss 1.765\n",
      "Ep 1 (Step 032360): Train loss 1.153, Val loss 1.765\n",
      "Ep 1 (Step 032365): Train loss 1.143, Val loss 1.767\n",
      "Ep 1 (Step 032370): Train loss 0.957, Val loss 1.768\n",
      "Ep 1 (Step 032375): Train loss 1.195, Val loss 1.768\n",
      "Ep 1 (Step 032380): Train loss 1.162, Val loss 1.767\n",
      "Ep 1 (Step 032385): Train loss 1.291, Val loss 1.767\n",
      "Ep 1 (Step 032390): Train loss 1.168, Val loss 1.767\n",
      "Ep 1 (Step 032395): Train loss 1.161, Val loss 1.766\n",
      "Ep 1 (Step 032400): Train loss 1.086, Val loss 1.767\n",
      "Ep 1 (Step 032405): Train loss 1.017, Val loss 1.768\n",
      "Ep 1 (Step 032410): Train loss 1.143, Val loss 1.768\n",
      "Ep 1 (Step 032415): Train loss 1.008, Val loss 1.768\n",
      "Ep 1 (Step 032420): Train loss 1.166, Val loss 1.768\n",
      "Ep 1 (Step 032425): Train loss 1.269, Val loss 1.767\n",
      "Ep 1 (Step 032430): Train loss 1.088, Val loss 1.767\n",
      "Ep 1 (Step 032435): Train loss 0.997, Val loss 1.768\n",
      "Ep 1 (Step 032440): Train loss 1.078, Val loss 1.769\n",
      "Ep 1 (Step 032445): Train loss 1.064, Val loss 1.769\n",
      "Ep 1 (Step 032450): Train loss 1.287, Val loss 1.767\n",
      "Ep 1 (Step 032455): Train loss 1.468, Val loss 1.765\n",
      "Ep 1 (Step 032460): Train loss 1.110, Val loss 1.764\n",
      "Ep 1 (Step 032465): Train loss 0.994, Val loss 1.763\n",
      "Ep 1 (Step 032470): Train loss 1.083, Val loss 1.760\n",
      "Ep 1 (Step 032475): Train loss 1.221, Val loss 1.757\n",
      "Ep 1 (Step 032480): Train loss 1.376, Val loss 1.754\n",
      "Ep 1 (Step 032485): Train loss 0.968, Val loss 1.753\n",
      "Ep 1 (Step 032490): Train loss 1.256, Val loss 1.753\n",
      "Ep 1 (Step 032495): Train loss 1.014, Val loss 1.753\n",
      "Ep 1 (Step 032500): Train loss 0.957, Val loss 1.754\n",
      "Ep 1 (Step 032505): Train loss 1.167, Val loss 1.753\n",
      "Ep 1 (Step 032510): Train loss 1.076, Val loss 1.752\n",
      "Ep 1 (Step 032515): Train loss 1.057, Val loss 1.751\n",
      "Ep 1 (Step 032520): Train loss 1.346, Val loss 1.751\n",
      "Ep 1 (Step 032525): Train loss 1.097, Val loss 1.751\n",
      "Ep 1 (Step 032530): Train loss 1.292, Val loss 1.752\n",
      "Ep 1 (Step 032535): Train loss 0.960, Val loss 1.753\n",
      "Ep 1 (Step 032540): Train loss 1.200, Val loss 1.753\n",
      "Ep 1 (Step 032545): Train loss 1.117, Val loss 1.753\n",
      "Ep 1 (Step 032550): Train loss 1.057, Val loss 1.750\n",
      "Ep 1 (Step 032555): Train loss 1.210, Val loss 1.749\n",
      "Ep 1 (Step 032560): Train loss 1.309, Val loss 1.750\n",
      "Ep 1 (Step 032565): Train loss 1.051, Val loss 1.750\n",
      "Ep 1 (Step 032570): Train loss 1.390, Val loss 1.750\n",
      "Ep 1 (Step 032575): Train loss 1.327, Val loss 1.752\n",
      "Ep 1 (Step 032580): Train loss 1.357, Val loss 1.753\n",
      "Ep 1 (Step 032585): Train loss 0.872, Val loss 1.754\n",
      "Ep 1 (Step 032590): Train loss 1.042, Val loss 1.755\n",
      "Ep 1 (Step 032595): Train loss 1.011, Val loss 1.755\n",
      "Ep 1 (Step 032600): Train loss 1.140, Val loss 1.756\n",
      "Ep 1 (Step 032605): Train loss 0.968, Val loss 1.757\n",
      "Ep 1 (Step 032610): Train loss 1.046, Val loss 1.759\n",
      "Ep 1 (Step 032615): Train loss 0.856, Val loss 1.760\n",
      "Ep 1 (Step 032620): Train loss 1.351, Val loss 1.763\n",
      "Ep 1 (Step 032625): Train loss 1.138, Val loss 1.762\n",
      "Ep 1 (Step 032630): Train loss 1.026, Val loss 1.762\n",
      "Ep 1 (Step 032635): Train loss 1.362, Val loss 1.762\n",
      "Ep 1 (Step 032640): Train loss 1.305, Val loss 1.762\n",
      "Ep 1 (Step 032645): Train loss 1.167, Val loss 1.762\n",
      "Ep 1 (Step 032650): Train loss 0.939, Val loss 1.761\n",
      "Ep 1 (Step 032655): Train loss 1.107, Val loss 1.761\n",
      "Ep 1 (Step 032660): Train loss 1.283, Val loss 1.763\n",
      "Ep 1 (Step 032665): Train loss 1.028, Val loss 1.763\n",
      "Ep 1 (Step 032670): Train loss 1.214, Val loss 1.762\n",
      "Ep 1 (Step 032675): Train loss 1.071, Val loss 1.762\n",
      "Ep 1 (Step 032680): Train loss 1.240, Val loss 1.762\n",
      "Ep 1 (Step 032685): Train loss 1.375, Val loss 1.762\n",
      "Ep 1 (Step 032690): Train loss 1.329, Val loss 1.764\n",
      "Ep 1 (Step 032695): Train loss 1.396, Val loss 1.763\n",
      "Ep 1 (Step 032700): Train loss 1.186, Val loss 1.762\n",
      "Ep 1 (Step 032705): Train loss 1.053, Val loss 1.761\n",
      "Ep 1 (Step 032710): Train loss 1.174, Val loss 1.761\n",
      "Ep 1 (Step 032715): Train loss 0.991, Val loss 1.762\n",
      "Ep 1 (Step 032720): Train loss 1.312, Val loss 1.762\n",
      "Ep 1 (Step 032725): Train loss 0.976, Val loss 1.761\n",
      "Ep 1 (Step 032730): Train loss 1.080, Val loss 1.761\n",
      "Ep 1 (Step 032735): Train loss 0.984, Val loss 1.761\n",
      "Ep 1 (Step 032740): Train loss 1.207, Val loss 1.763\n",
      "Ep 1 (Step 032745): Train loss 1.047, Val loss 1.766\n",
      "Ep 1 (Step 032750): Train loss 1.180, Val loss 1.768\n",
      "Ep 1 (Step 032755): Train loss 1.122, Val loss 1.768\n",
      "Ep 1 (Step 032760): Train loss 1.021, Val loss 1.767\n",
      "Ep 1 (Step 032765): Train loss 1.180, Val loss 1.764\n",
      "Ep 1 (Step 032770): Train loss 1.028, Val loss 1.762\n",
      "Ep 1 (Step 032775): Train loss 1.245, Val loss 1.761\n",
      "Ep 1 (Step 032780): Train loss 1.026, Val loss 1.760\n",
      "Ep 1 (Step 032785): Train loss 1.015, Val loss 1.760\n",
      "Ep 1 (Step 032790): Train loss 0.908, Val loss 1.760\n",
      "Ep 1 (Step 032795): Train loss 1.143, Val loss 1.760\n",
      "Ep 1 (Step 032800): Train loss 1.327, Val loss 1.760\n",
      "Ep 1 (Step 032805): Train loss 1.222, Val loss 1.759\n",
      "Ep 1 (Step 032810): Train loss 1.176, Val loss 1.757\n",
      "Ep 1 (Step 032815): Train loss 1.155, Val loss 1.756\n",
      "Ep 1 (Step 032820): Train loss 1.207, Val loss 1.755\n",
      "Ep 1 (Step 032825): Train loss 0.940, Val loss 1.755\n",
      "Ep 1 (Step 032830): Train loss 1.100, Val loss 1.754\n",
      "Ep 1 (Step 032835): Train loss 1.134, Val loss 1.754\n",
      "Ep 1 (Step 032840): Train loss 1.003, Val loss 1.753\n",
      "Ep 1 (Step 032845): Train loss 1.213, Val loss 1.752\n",
      "Ep 1 (Step 032850): Train loss 1.295, Val loss 1.751\n",
      "Ep 1 (Step 032855): Train loss 1.146, Val loss 1.751\n",
      "Ep 1 (Step 032860): Train loss 1.200, Val loss 1.752\n",
      "Ep 1 (Step 032865): Train loss 1.293, Val loss 1.752\n",
      "Ep 1 (Step 032870): Train loss 0.877, Val loss 1.752\n",
      "Ep 1 (Step 032875): Train loss 1.137, Val loss 1.753\n",
      "Ep 1 (Step 032880): Train loss 0.937, Val loss 1.754\n",
      "Ep 1 (Step 032885): Train loss 0.938, Val loss 1.756\n",
      "Ep 1 (Step 032890): Train loss 1.309, Val loss 1.756\n",
      "Ep 1 (Step 032895): Train loss 1.180, Val loss 1.757\n",
      "Ep 1 (Step 032900): Train loss 1.227, Val loss 1.758\n",
      "Ep 1 (Step 032905): Train loss 1.417, Val loss 1.760\n",
      "Ep 1 (Step 032910): Train loss 0.906, Val loss 1.761\n",
      "Ep 1 (Step 032915): Train loss 1.435, Val loss 1.761\n",
      "Ep 1 (Step 032920): Train loss 1.092, Val loss 1.761\n",
      "Ep 1 (Step 032925): Train loss 1.070, Val loss 1.761\n",
      "Ep 1 (Step 032930): Train loss 1.356, Val loss 1.761\n",
      "Ep 1 (Step 032935): Train loss 1.315, Val loss 1.761\n",
      "Ep 1 (Step 032940): Train loss 1.047, Val loss 1.761\n",
      "Ep 1 (Step 032945): Train loss 1.059, Val loss 1.761\n",
      "Ep 1 (Step 032950): Train loss 1.182, Val loss 1.760\n",
      "Ep 1 (Step 032955): Train loss 0.949, Val loss 1.757\n",
      "Ep 1 (Step 032960): Train loss 1.013, Val loss 1.755\n",
      "Ep 1 (Step 032965): Train loss 1.130, Val loss 1.753\n",
      "Ep 1 (Step 032970): Train loss 1.108, Val loss 1.754\n",
      "Ep 1 (Step 032975): Train loss 1.176, Val loss 1.756\n",
      "Ep 1 (Step 032980): Train loss 0.864, Val loss 1.758\n",
      "Ep 1 (Step 032985): Train loss 1.230, Val loss 1.759\n",
      "Ep 1 (Step 032990): Train loss 1.237, Val loss 1.760\n",
      "Ep 1 (Step 032995): Train loss 0.854, Val loss 1.760\n",
      "Ep 1 (Step 033000): Train loss 1.110, Val loss 1.760\n",
      "Ep 1 (Step 033005): Train loss 1.364, Val loss 1.758\n",
      "Ep 1 (Step 033010): Train loss 1.243, Val loss 1.757\n",
      "Ep 1 (Step 033015): Train loss 1.178, Val loss 1.756\n",
      "Ep 1 (Step 033020): Train loss 1.091, Val loss 1.753\n",
      "Ep 1 (Step 033025): Train loss 1.117, Val loss 1.752\n",
      "Ep 1 (Step 033030): Train loss 1.260, Val loss 1.750\n",
      "Ep 1 (Step 033035): Train loss 1.404, Val loss 1.750\n",
      "Ep 1 (Step 033040): Train loss 0.793, Val loss 1.750\n",
      "Ep 1 (Step 033045): Train loss 1.486, Val loss 1.750\n",
      "Ep 1 (Step 033050): Train loss 1.100, Val loss 1.750\n",
      "Ep 1 (Step 033055): Train loss 1.307, Val loss 1.751\n",
      "Ep 1 (Step 033060): Train loss 0.996, Val loss 1.751\n",
      "Ep 1 (Step 033065): Train loss 1.332, Val loss 1.751\n",
      "Ep 1 (Step 033070): Train loss 1.192, Val loss 1.751\n",
      "Ep 1 (Step 033075): Train loss 1.436, Val loss 1.751\n",
      "Ep 1 (Step 033080): Train loss 1.182, Val loss 1.751\n",
      "Ep 1 (Step 033085): Train loss 1.292, Val loss 1.751\n",
      "Ep 1 (Step 033090): Train loss 1.077, Val loss 1.750\n",
      "Ep 1 (Step 033095): Train loss 1.409, Val loss 1.750\n",
      "Ep 1 (Step 033100): Train loss 1.012, Val loss 1.749\n",
      "Ep 1 (Step 033105): Train loss 1.105, Val loss 1.748\n",
      "Ep 1 (Step 033110): Train loss 1.133, Val loss 1.748\n",
      "Ep 1 (Step 033115): Train loss 1.031, Val loss 1.748\n",
      "Ep 1 (Step 033120): Train loss 1.187, Val loss 1.748\n",
      "Ep 1 (Step 033125): Train loss 0.919, Val loss 1.749\n",
      "Ep 1 (Step 033130): Train loss 1.175, Val loss 1.751\n",
      "Ep 1 (Step 033135): Train loss 1.003, Val loss 1.752\n",
      "Ep 1 (Step 033140): Train loss 1.091, Val loss 1.752\n",
      "Ep 1 (Step 033145): Train loss 0.979, Val loss 1.754\n",
      "Ep 1 (Step 033150): Train loss 1.065, Val loss 1.754\n",
      "Ep 1 (Step 033155): Train loss 1.268, Val loss 1.754\n",
      "Ep 1 (Step 033160): Train loss 1.385, Val loss 1.755\n",
      "Ep 1 (Step 033165): Train loss 1.039, Val loss 1.755\n",
      "Ep 1 (Step 033170): Train loss 1.008, Val loss 1.754\n",
      "Ep 1 (Step 033175): Train loss 1.341, Val loss 1.752\n",
      "Ep 1 (Step 033180): Train loss 1.329, Val loss 1.750\n",
      "Ep 1 (Step 033185): Train loss 1.199, Val loss 1.750\n",
      "Ep 1 (Step 033190): Train loss 1.306, Val loss 1.749\n",
      "Ep 1 (Step 033195): Train loss 1.059, Val loss 1.748\n",
      "Ep 1 (Step 033200): Train loss 1.461, Val loss 1.747\n",
      "Ep 1 (Step 033205): Train loss 1.573, Val loss 1.747\n",
      "Ep 1 (Step 033210): Train loss 1.270, Val loss 1.747\n",
      "Ep 1 (Step 033215): Train loss 0.914, Val loss 1.747\n",
      "Ep 1 (Step 033220): Train loss 0.955, Val loss 1.747\n",
      "Ep 1 (Step 033225): Train loss 1.082, Val loss 1.745\n",
      "Ep 1 (Step 033230): Train loss 1.279, Val loss 1.744\n",
      "Ep 1 (Step 033235): Train loss 1.036, Val loss 1.744\n",
      "Ep 1 (Step 033240): Train loss 1.117, Val loss 1.745\n",
      "Ep 1 (Step 033245): Train loss 0.963, Val loss 1.746\n",
      "Ep 1 (Step 033250): Train loss 1.088, Val loss 1.748\n",
      "Ep 1 (Step 033255): Train loss 1.086, Val loss 1.748\n",
      "Ep 1 (Step 033260): Train loss 1.082, Val loss 1.748\n",
      "Ep 1 (Step 033265): Train loss 1.473, Val loss 1.749\n",
      "Ep 1 (Step 033270): Train loss 0.977, Val loss 1.751\n",
      "Ep 1 (Step 033275): Train loss 0.927, Val loss 1.753\n",
      "Ep 1 (Step 033280): Train loss 1.123, Val loss 1.753\n",
      "Ep 1 (Step 033285): Train loss 1.237, Val loss 1.754\n",
      "Ep 1 (Step 033290): Train loss 1.375, Val loss 1.753\n",
      "Ep 1 (Step 033295): Train loss 1.124, Val loss 1.752\n",
      "Ep 1 (Step 033300): Train loss 1.222, Val loss 1.750\n",
      "Ep 1 (Step 033305): Train loss 1.209, Val loss 1.750\n",
      "Ep 1 (Step 033310): Train loss 1.380, Val loss 1.749\n",
      "Ep 1 (Step 033315): Train loss 1.202, Val loss 1.749\n",
      "Ep 1 (Step 033320): Train loss 1.113, Val loss 1.749\n",
      "Ep 1 (Step 033325): Train loss 1.177, Val loss 1.750\n",
      "Ep 1 (Step 033330): Train loss 1.200, Val loss 1.750\n",
      "Ep 1 (Step 033335): Train loss 0.867, Val loss 1.750\n",
      "Ep 1 (Step 033340): Train loss 1.267, Val loss 1.751\n",
      "Ep 1 (Step 033345): Train loss 1.275, Val loss 1.753\n",
      "Ep 1 (Step 033350): Train loss 1.151, Val loss 1.753\n",
      "Ep 1 (Step 033355): Train loss 1.108, Val loss 1.754\n",
      "Ep 1 (Step 033360): Train loss 1.172, Val loss 1.754\n",
      "Ep 1 (Step 033365): Train loss 1.012, Val loss 1.755\n",
      "Ep 1 (Step 033370): Train loss 1.114, Val loss 1.756\n",
      "Ep 1 (Step 033375): Train loss 1.085, Val loss 1.757\n",
      "Ep 1 (Step 033380): Train loss 1.016, Val loss 1.757\n",
      "Ep 1 (Step 033385): Train loss 1.244, Val loss 1.758\n",
      "Ep 1 (Step 033390): Train loss 1.297, Val loss 1.758\n",
      "Ep 1 (Step 033395): Train loss 1.271, Val loss 1.759\n",
      "Ep 1 (Step 033400): Train loss 1.109, Val loss 1.759\n",
      "Ep 1 (Step 033405): Train loss 1.333, Val loss 1.759\n",
      "Ep 1 (Step 033410): Train loss 1.254, Val loss 1.757\n",
      "Ep 1 (Step 033415): Train loss 1.081, Val loss 1.756\n",
      "Ep 1 (Step 033420): Train loss 1.177, Val loss 1.757\n",
      "Ep 1 (Step 033425): Train loss 1.163, Val loss 1.756\n",
      "Ep 1 (Step 033430): Train loss 1.182, Val loss 1.755\n",
      "Ep 1 (Step 033435): Train loss 1.602, Val loss 1.756\n",
      "Ep 1 (Step 033440): Train loss 1.149, Val loss 1.756\n",
      "Ep 1 (Step 033445): Train loss 1.030, Val loss 1.756\n",
      "Ep 1 (Step 033450): Train loss 1.095, Val loss 1.757\n",
      "Ep 1 (Step 033455): Train loss 1.172, Val loss 1.758\n",
      "Ep 1 (Step 033460): Train loss 1.079, Val loss 1.759\n",
      "Ep 1 (Step 033465): Train loss 1.007, Val loss 1.759\n",
      "Ep 1 (Step 033470): Train loss 1.461, Val loss 1.758\n",
      "Ep 1 (Step 033475): Train loss 1.043, Val loss 1.757\n",
      "Ep 1 (Step 033480): Train loss 1.249, Val loss 1.756\n",
      "Ep 1 (Step 033485): Train loss 1.168, Val loss 1.755\n",
      "Ep 1 (Step 033490): Train loss 1.012, Val loss 1.754\n",
      "Ep 1 (Step 033495): Train loss 1.041, Val loss 1.753\n",
      "Ep 1 (Step 033500): Train loss 1.281, Val loss 1.753\n",
      "Ep 1 (Step 033505): Train loss 1.088, Val loss 1.753\n",
      "Ep 1 (Step 033510): Train loss 1.364, Val loss 1.753\n",
      "Ep 1 (Step 033515): Train loss 0.993, Val loss 1.754\n",
      "Ep 1 (Step 033520): Train loss 1.215, Val loss 1.755\n",
      "Ep 1 (Step 033525): Train loss 1.075, Val loss 1.756\n",
      "Ep 1 (Step 033530): Train loss 1.051, Val loss 1.757\n",
      "Ep 1 (Step 033535): Train loss 1.055, Val loss 1.758\n",
      "Ep 1 (Step 033540): Train loss 1.009, Val loss 1.759\n",
      "Ep 1 (Step 033545): Train loss 1.152, Val loss 1.760\n",
      "Ep 1 (Step 033550): Train loss 1.016, Val loss 1.760\n",
      "Ep 1 (Step 033555): Train loss 0.922, Val loss 1.760\n",
      "Ep 1 (Step 033560): Train loss 1.123, Val loss 1.760\n",
      "Ep 1 (Step 033565): Train loss 0.946, Val loss 1.760\n",
      "Ep 1 (Step 033570): Train loss 0.999, Val loss 1.761\n",
      "Ep 1 (Step 033575): Train loss 1.239, Val loss 1.760\n",
      "Ep 1 (Step 033580): Train loss 1.058, Val loss 1.758\n",
      "Ep 1 (Step 033585): Train loss 1.186, Val loss 1.757\n",
      "Ep 1 (Step 033590): Train loss 1.492, Val loss 1.757\n",
      "Ep 1 (Step 033595): Train loss 1.281, Val loss 1.758\n",
      "Ep 1 (Step 033600): Train loss 1.429, Val loss 1.758\n",
      "Ep 1 (Step 033605): Train loss 1.109, Val loss 1.758\n",
      "Ep 1 (Step 033610): Train loss 1.094, Val loss 1.757\n",
      "Ep 1 (Step 033615): Train loss 1.200, Val loss 1.755\n",
      "Ep 1 (Step 033620): Train loss 0.939, Val loss 1.755\n",
      "Ep 1 (Step 033625): Train loss 1.083, Val loss 1.754\n",
      "Ep 1 (Step 033630): Train loss 0.971, Val loss 1.753\n",
      "Ep 1 (Step 033635): Train loss 1.054, Val loss 1.753\n",
      "Ep 1 (Step 033640): Train loss 1.248, Val loss 1.754\n",
      "Ep 1 (Step 033645): Train loss 1.386, Val loss 1.755\n",
      "Ep 1 (Step 033650): Train loss 0.925, Val loss 1.757\n",
      "Ep 1 (Step 033655): Train loss 1.174, Val loss 1.758\n",
      "Ep 1 (Step 033660): Train loss 1.318, Val loss 1.760\n",
      "Ep 1 (Step 033665): Train loss 1.209, Val loss 1.761\n",
      "Ep 1 (Step 033670): Train loss 0.996, Val loss 1.761\n",
      "Ep 1 (Step 033675): Train loss 1.090, Val loss 1.760\n",
      "Ep 1 (Step 033680): Train loss 1.018, Val loss 1.761\n",
      "Ep 1 (Step 033685): Train loss 1.145, Val loss 1.763\n",
      "Ep 1 (Step 033690): Train loss 1.054, Val loss 1.764\n",
      "Ep 1 (Step 033695): Train loss 1.171, Val loss 1.766\n",
      "Ep 1 (Step 033700): Train loss 1.164, Val loss 1.767\n",
      "Ep 1 (Step 033705): Train loss 1.367, Val loss 1.770\n",
      "Ep 1 (Step 033710): Train loss 1.048, Val loss 1.771\n",
      "Ep 1 (Step 033715): Train loss 1.202, Val loss 1.773\n",
      "Ep 1 (Step 033720): Train loss 1.333, Val loss 1.773\n",
      "Ep 1 (Step 033725): Train loss 1.072, Val loss 1.773\n",
      "Ep 1 (Step 033730): Train loss 1.109, Val loss 1.773\n",
      "Ep 1 (Step 033735): Train loss 1.046, Val loss 1.773\n",
      "Ep 1 (Step 033740): Train loss 0.871, Val loss 1.770\n",
      "Ep 1 (Step 033745): Train loss 1.083, Val loss 1.769\n",
      "Ep 1 (Step 033750): Train loss 1.197, Val loss 1.768\n",
      "Ep 1 (Step 033755): Train loss 1.059, Val loss 1.767\n",
      "Ep 1 (Step 033760): Train loss 1.095, Val loss 1.766\n",
      "Ep 1 (Step 033765): Train loss 1.236, Val loss 1.766\n",
      "Ep 1 (Step 033770): Train loss 0.985, Val loss 1.767\n",
      "Ep 1 (Step 033775): Train loss 0.995, Val loss 1.768\n",
      "Ep 1 (Step 033780): Train loss 1.049, Val loss 1.768\n",
      "Ep 1 (Step 033785): Train loss 1.202, Val loss 1.769\n",
      "Ep 1 (Step 033790): Train loss 1.144, Val loss 1.770\n",
      "Ep 1 (Step 033795): Train loss 1.011, Val loss 1.770\n",
      "Ep 1 (Step 033800): Train loss 1.190, Val loss 1.770\n",
      "Ep 1 (Step 033805): Train loss 1.227, Val loss 1.771\n",
      "Ep 1 (Step 033810): Train loss 1.109, Val loss 1.771\n",
      "Ep 1 (Step 033815): Train loss 0.819, Val loss 1.771\n",
      "Ep 1 (Step 033820): Train loss 0.946, Val loss 1.770\n",
      "Ep 1 (Step 033825): Train loss 1.379, Val loss 1.770\n",
      "Ep 1 (Step 033830): Train loss 1.312, Val loss 1.767\n",
      "Ep 1 (Step 033835): Train loss 1.117, Val loss 1.764\n",
      "Ep 1 (Step 033840): Train loss 1.135, Val loss 1.763\n",
      "Ep 1 (Step 033845): Train loss 1.088, Val loss 1.762\n",
      "Ep 1 (Step 033850): Train loss 1.124, Val loss 1.760\n",
      "Ep 1 (Step 033855): Train loss 0.852, Val loss 1.760\n",
      "Ep 1 (Step 033860): Train loss 1.159, Val loss 1.760\n",
      "Ep 1 (Step 033865): Train loss 1.118, Val loss 1.760\n",
      "Ep 1 (Step 033870): Train loss 1.088, Val loss 1.761\n",
      "Ep 1 (Step 033875): Train loss 1.138, Val loss 1.763\n",
      "Ep 1 (Step 033880): Train loss 1.511, Val loss 1.765\n",
      "Ep 1 (Step 033885): Train loss 1.181, Val loss 1.766\n",
      "Ep 1 (Step 033890): Train loss 1.314, Val loss 1.764\n",
      "Ep 1 (Step 033895): Train loss 1.275, Val loss 1.760\n",
      "Ep 1 (Step 033900): Train loss 1.226, Val loss 1.758\n",
      "Ep 1 (Step 033905): Train loss 1.017, Val loss 1.759\n",
      "Ep 1 (Step 033910): Train loss 1.243, Val loss 1.759\n",
      "Ep 1 (Step 033915): Train loss 0.949, Val loss 1.760\n",
      "Ep 1 (Step 033920): Train loss 1.273, Val loss 1.762\n",
      "Ep 1 (Step 033925): Train loss 1.156, Val loss 1.762\n",
      "Ep 1 (Step 033930): Train loss 1.458, Val loss 1.762\n",
      "Ep 1 (Step 033935): Train loss 1.092, Val loss 1.761\n",
      "Ep 1 (Step 033940): Train loss 1.107, Val loss 1.760\n",
      "Ep 1 (Step 033945): Train loss 0.995, Val loss 1.761\n",
      "Ep 1 (Step 033950): Train loss 1.208, Val loss 1.762\n",
      "Ep 1 (Step 033955): Train loss 1.119, Val loss 1.763\n",
      "Ep 1 (Step 033960): Train loss 1.162, Val loss 1.762\n",
      "Ep 1 (Step 033965): Train loss 1.059, Val loss 1.761\n",
      "Ep 1 (Step 033970): Train loss 1.131, Val loss 1.760\n",
      "Ep 1 (Step 033975): Train loss 1.042, Val loss 1.760\n",
      "Ep 1 (Step 033980): Train loss 1.270, Val loss 1.759\n",
      "Ep 1 (Step 033985): Train loss 1.109, Val loss 1.758\n",
      "Ep 1 (Step 033990): Train loss 1.245, Val loss 1.757\n",
      "Ep 1 (Step 033995): Train loss 1.160, Val loss 1.756\n",
      "Ep 1 (Step 034000): Train loss 1.109, Val loss 1.755\n",
      "Ep 1 (Step 034005): Train loss 1.302, Val loss 1.753\n",
      "Ep 1 (Step 034010): Train loss 0.974, Val loss 1.752\n",
      "Ep 1 (Step 034015): Train loss 1.045, Val loss 1.752\n",
      "Ep 1 (Step 034020): Train loss 1.206, Val loss 1.752\n",
      "Ep 1 (Step 034025): Train loss 0.882, Val loss 1.752\n",
      "Ep 1 (Step 034030): Train loss 1.298, Val loss 1.750\n",
      "Ep 1 (Step 034035): Train loss 0.811, Val loss 1.748\n",
      "Ep 1 (Step 034040): Train loss 1.082, Val loss 1.747\n",
      "Ep 1 (Step 034045): Train loss 1.101, Val loss 1.746\n",
      "Ep 1 (Step 034050): Train loss 1.320, Val loss 1.746\n",
      "Ep 1 (Step 034055): Train loss 1.293, Val loss 1.746\n",
      "Ep 1 (Step 034060): Train loss 1.210, Val loss 1.747\n",
      "Ep 1 (Step 034065): Train loss 0.979, Val loss 1.748\n",
      "Ep 1 (Step 034070): Train loss 1.314, Val loss 1.747\n",
      "Ep 1 (Step 034075): Train loss 1.078, Val loss 1.747\n",
      "Ep 1 (Step 034080): Train loss 1.257, Val loss 1.748\n",
      "Ep 1 (Step 034085): Train loss 1.077, Val loss 1.749\n",
      "Ep 1 (Step 034090): Train loss 1.353, Val loss 1.749\n",
      "Ep 1 (Step 034095): Train loss 1.218, Val loss 1.752\n",
      "Ep 1 (Step 034100): Train loss 1.084, Val loss 1.753\n",
      "Ep 1 (Step 034105): Train loss 1.168, Val loss 1.752\n",
      "Ep 1 (Step 034110): Train loss 1.274, Val loss 1.751\n",
      "Ep 1 (Step 034115): Train loss 1.159, Val loss 1.751\n",
      "Ep 1 (Step 034120): Train loss 1.131, Val loss 1.751\n",
      "Ep 1 (Step 034125): Train loss 0.951, Val loss 1.752\n",
      "Ep 1 (Step 034130): Train loss 1.266, Val loss 1.753\n",
      "Ep 1 (Step 034135): Train loss 0.999, Val loss 1.755\n",
      "Ep 1 (Step 034140): Train loss 1.193, Val loss 1.756\n",
      "Ep 1 (Step 034145): Train loss 1.461, Val loss 1.756\n",
      "Ep 1 (Step 034150): Train loss 0.976, Val loss 1.757\n",
      "Ep 1 (Step 034155): Train loss 1.313, Val loss 1.758\n",
      "Ep 1 (Step 034160): Train loss 1.299, Val loss 1.758\n",
      "Ep 1 (Step 034165): Train loss 1.087, Val loss 1.758\n",
      "Ep 1 (Step 034170): Train loss 1.434, Val loss 1.759\n",
      "Ep 1 (Step 034175): Train loss 1.106, Val loss 1.759\n",
      "Ep 1 (Step 034180): Train loss 1.246, Val loss 1.759\n",
      "Ep 1 (Step 034185): Train loss 1.259, Val loss 1.759\n",
      "Ep 1 (Step 034190): Train loss 1.106, Val loss 1.759\n",
      "Ep 1 (Step 034195): Train loss 1.263, Val loss 1.759\n",
      "Ep 1 (Step 034200): Train loss 1.202, Val loss 1.759\n",
      "Ep 1 (Step 034205): Train loss 1.038, Val loss 1.761\n",
      "Ep 1 (Step 034210): Train loss 1.256, Val loss 1.760\n",
      "Ep 1 (Step 034215): Train loss 1.010, Val loss 1.759\n",
      "Ep 1 (Step 034220): Train loss 1.288, Val loss 1.759\n",
      "Ep 1 (Step 034225): Train loss 1.426, Val loss 1.757\n",
      "Ep 1 (Step 034230): Train loss 1.073, Val loss 1.754\n",
      "Ep 1 (Step 034235): Train loss 1.018, Val loss 1.752\n",
      "Ep 1 (Step 034240): Train loss 1.328, Val loss 1.750\n",
      "Ep 1 (Step 034245): Train loss 0.917, Val loss 1.749\n",
      "Ep 1 (Step 034250): Train loss 0.941, Val loss 1.749\n",
      "Ep 1 (Step 034255): Train loss 1.122, Val loss 1.749\n",
      "Ep 1 (Step 034260): Train loss 1.238, Val loss 1.750\n",
      "Ep 1 (Step 034265): Train loss 1.344, Val loss 1.750\n",
      "Ep 1 (Step 034270): Train loss 1.097, Val loss 1.751\n",
      "Ep 1 (Step 034275): Train loss 0.893, Val loss 1.750\n",
      "Ep 1 (Step 034280): Train loss 1.116, Val loss 1.750\n",
      "Ep 1 (Step 034285): Train loss 0.989, Val loss 1.750\n",
      "Ep 1 (Step 034290): Train loss 1.040, Val loss 1.751\n",
      "Ep 1 (Step 034295): Train loss 1.388, Val loss 1.751\n",
      "Ep 1 (Step 034300): Train loss 1.064, Val loss 1.752\n",
      "Ep 1 (Step 034305): Train loss 0.943, Val loss 1.754\n",
      "Ep 1 (Step 034310): Train loss 1.343, Val loss 1.754\n",
      "Ep 1 (Step 034315): Train loss 1.351, Val loss 1.754\n",
      "Ep 1 (Step 034320): Train loss 1.226, Val loss 1.753\n",
      "Ep 1 (Step 034325): Train loss 1.021, Val loss 1.752\n",
      "Ep 1 (Step 034330): Train loss 1.022, Val loss 1.752\n",
      "Ep 1 (Step 034335): Train loss 1.198, Val loss 1.753\n",
      "Ep 1 (Step 034340): Train loss 1.015, Val loss 1.754\n",
      "Ep 1 (Step 034345): Train loss 1.119, Val loss 1.755\n",
      "Ep 1 (Step 034350): Train loss 0.960, Val loss 1.755\n",
      "Ep 1 (Step 034355): Train loss 1.049, Val loss 1.755\n",
      "Ep 1 (Step 034360): Train loss 1.120, Val loss 1.756\n",
      "Ep 1 (Step 034365): Train loss 0.964, Val loss 1.756\n",
      "Ep 1 (Step 034370): Train loss 1.087, Val loss 1.755\n",
      "Ep 1 (Step 034375): Train loss 1.258, Val loss 1.755\n",
      "Ep 1 (Step 034380): Train loss 1.152, Val loss 1.757\n",
      "Ep 1 (Step 034385): Train loss 0.775, Val loss 1.758\n",
      "Ep 1 (Step 034390): Train loss 1.132, Val loss 1.759\n",
      "Ep 1 (Step 034395): Train loss 1.124, Val loss 1.758\n",
      "Ep 1 (Step 034400): Train loss 1.231, Val loss 1.758\n",
      "Ep 1 (Step 034405): Train loss 1.319, Val loss 1.759\n",
      "Ep 1 (Step 034410): Train loss 1.088, Val loss 1.759\n",
      "Ep 1 (Step 034415): Train loss 1.090, Val loss 1.758\n",
      "Ep 1 (Step 034420): Train loss 1.082, Val loss 1.756\n",
      "Ep 1 (Step 034425): Train loss 1.194, Val loss 1.756\n",
      "Ep 1 (Step 034430): Train loss 1.324, Val loss 1.757\n",
      "Ep 1 (Step 034435): Train loss 1.147, Val loss 1.758\n",
      "Ep 1 (Step 034440): Train loss 1.240, Val loss 1.756\n",
      "Ep 1 (Step 034445): Train loss 1.462, Val loss 1.753\n",
      "Ep 1 (Step 034450): Train loss 1.274, Val loss 1.753\n",
      "Ep 1 (Step 034455): Train loss 1.321, Val loss 1.754\n",
      "Ep 1 (Step 034460): Train loss 1.414, Val loss 1.755\n",
      "Ep 1 (Step 034465): Train loss 1.176, Val loss 1.759\n",
      "Ep 1 (Step 034470): Train loss 1.108, Val loss 1.761\n",
      "Ep 1 (Step 034475): Train loss 1.107, Val loss 1.762\n",
      "Ep 1 (Step 034480): Train loss 1.131, Val loss 1.762\n",
      "Ep 1 (Step 034485): Train loss 1.010, Val loss 1.761\n",
      "Ep 1 (Step 034490): Train loss 0.993, Val loss 1.761\n",
      "Ep 1 (Step 034495): Train loss 1.155, Val loss 1.760\n",
      "Ep 1 (Step 034500): Train loss 1.103, Val loss 1.758\n",
      "Ep 1 (Step 034505): Train loss 1.080, Val loss 1.755\n",
      "Ep 1 (Step 034510): Train loss 1.251, Val loss 1.753\n",
      "Ep 1 (Step 034515): Train loss 1.013, Val loss 1.751\n",
      "Ep 1 (Step 034520): Train loss 0.938, Val loss 1.751\n",
      "Ep 1 (Step 034525): Train loss 1.236, Val loss 1.750\n",
      "Ep 1 (Step 034530): Train loss 1.106, Val loss 1.750\n",
      "Ep 1 (Step 034535): Train loss 1.075, Val loss 1.750\n",
      "Ep 1 (Step 034540): Train loss 1.281, Val loss 1.750\n",
      "Ep 1 (Step 034545): Train loss 0.894, Val loss 1.749\n",
      "Ep 1 (Step 034550): Train loss 1.394, Val loss 1.748\n",
      "Ep 1 (Step 034555): Train loss 1.129, Val loss 1.748\n",
      "Ep 1 (Step 034560): Train loss 1.032, Val loss 1.750\n",
      "Ep 1 (Step 034565): Train loss 1.269, Val loss 1.752\n",
      "Ep 1 (Step 034570): Train loss 1.206, Val loss 1.753\n",
      "Ep 1 (Step 034575): Train loss 1.056, Val loss 1.755\n",
      "Ep 1 (Step 034580): Train loss 1.205, Val loss 1.757\n",
      "Ep 1 (Step 034585): Train loss 0.925, Val loss 1.759\n",
      "Ep 1 (Step 034590): Train loss 1.055, Val loss 1.759\n",
      "Ep 1 (Step 034595): Train loss 1.043, Val loss 1.758\n",
      "Ep 1 (Step 034600): Train loss 1.359, Val loss 1.757\n",
      "Ep 1 (Step 034605): Train loss 1.303, Val loss 1.757\n",
      "Ep 1 (Step 034610): Train loss 1.252, Val loss 1.758\n",
      "Ep 1 (Step 034615): Train loss 1.098, Val loss 1.758\n",
      "Ep 1 (Step 034620): Train loss 1.182, Val loss 1.757\n",
      "Ep 1 (Step 034625): Train loss 1.221, Val loss 1.756\n",
      "Ep 1 (Step 034630): Train loss 1.118, Val loss 1.755\n",
      "Ep 1 (Step 034635): Train loss 1.004, Val loss 1.754\n",
      "Ep 1 (Step 034640): Train loss 1.314, Val loss 1.753\n",
      "Ep 1 (Step 034645): Train loss 0.975, Val loss 1.751\n",
      "Ep 1 (Step 034650): Train loss 0.885, Val loss 1.751\n",
      "Ep 1 (Step 034655): Train loss 1.064, Val loss 1.751\n",
      "Ep 1 (Step 034660): Train loss 1.242, Val loss 1.750\n",
      "Ep 1 (Step 034665): Train loss 1.053, Val loss 1.750\n",
      "Ep 1 (Step 034670): Train loss 1.216, Val loss 1.750\n",
      "Ep 1 (Step 034675): Train loss 1.063, Val loss 1.750\n",
      "Ep 1 (Step 034680): Train loss 1.299, Val loss 1.751\n",
      "Ep 1 (Step 034685): Train loss 1.107, Val loss 1.753\n",
      "Ep 1 (Step 034690): Train loss 0.798, Val loss 1.753\n",
      "Ep 1 (Step 034695): Train loss 1.283, Val loss 1.754\n",
      "Ep 1 (Step 034700): Train loss 1.084, Val loss 1.754\n",
      "Ep 1 (Step 034705): Train loss 1.059, Val loss 1.754\n",
      "Ep 1 (Step 034710): Train loss 1.058, Val loss 1.754\n",
      "Ep 1 (Step 034715): Train loss 1.126, Val loss 1.755\n",
      "Ep 1 (Step 034720): Train loss 1.056, Val loss 1.754\n",
      "Ep 1 (Step 034725): Train loss 1.197, Val loss 1.755\n",
      "Ep 1 (Step 034730): Train loss 1.285, Val loss 1.755\n",
      "Ep 1 (Step 034735): Train loss 1.465, Val loss 1.755\n",
      "Ep 1 (Step 034740): Train loss 1.027, Val loss 1.753\n",
      "Ep 1 (Step 034745): Train loss 1.149, Val loss 1.753\n",
      "Ep 1 (Step 034750): Train loss 1.110, Val loss 1.754\n",
      "Ep 1 (Step 034755): Train loss 1.044, Val loss 1.755\n",
      "Ep 1 (Step 034760): Train loss 1.104, Val loss 1.756\n",
      "Ep 1 (Step 034765): Train loss 0.959, Val loss 1.755\n",
      "Ep 1 (Step 034770): Train loss 1.119, Val loss 1.755\n",
      "Ep 1 (Step 034775): Train loss 0.993, Val loss 1.754\n",
      "Ep 1 (Step 034780): Train loss 0.880, Val loss 1.753\n",
      "Ep 1 (Step 034785): Train loss 1.190, Val loss 1.754\n",
      "Ep 1 (Step 034790): Train loss 0.847, Val loss 1.755\n",
      "Ep 1 (Step 034795): Train loss 1.232, Val loss 1.755\n",
      "Ep 1 (Step 034800): Train loss 1.168, Val loss 1.754\n",
      "Ep 1 (Step 034805): Train loss 1.015, Val loss 1.754\n",
      "Ep 1 (Step 034810): Train loss 1.245, Val loss 1.754\n",
      "Ep 1 (Step 034815): Train loss 1.104, Val loss 1.754\n",
      "Ep 1 (Step 034820): Train loss 1.227, Val loss 1.755\n",
      "Ep 1 (Step 034825): Train loss 1.373, Val loss 1.755\n",
      "Ep 1 (Step 034830): Train loss 1.306, Val loss 1.755\n",
      "Ep 1 (Step 034835): Train loss 1.055, Val loss 1.754\n",
      "Ep 1 (Step 034840): Train loss 1.183, Val loss 1.754\n",
      "Ep 1 (Step 034845): Train loss 1.098, Val loss 1.755\n",
      "Ep 1 (Step 034850): Train loss 0.939, Val loss 1.754\n",
      "Ep 1 (Step 034855): Train loss 1.051, Val loss 1.753\n",
      "Ep 1 (Step 034860): Train loss 1.052, Val loss 1.751\n",
      "Ep 1 (Step 034865): Train loss 0.818, Val loss 1.750\n",
      "Ep 1 (Step 034870): Train loss 1.379, Val loss 1.750\n",
      "Ep 1 (Step 034875): Train loss 1.261, Val loss 1.752\n",
      "Ep 1 (Step 034880): Train loss 0.949, Val loss 1.753\n",
      "Ep 1 (Step 034885): Train loss 1.158, Val loss 1.754\n",
      "Ep 1 (Step 034890): Train loss 1.390, Val loss 1.753\n",
      "Ep 1 (Step 034895): Train loss 1.069, Val loss 1.753\n",
      "Ep 1 (Step 034900): Train loss 0.966, Val loss 1.752\n",
      "Ep 1 (Step 034905): Train loss 1.108, Val loss 1.752\n",
      "Ep 1 (Step 034910): Train loss 1.198, Val loss 1.751\n",
      "Ep 1 (Step 034915): Train loss 1.186, Val loss 1.750\n",
      "Ep 1 (Step 034920): Train loss 0.924, Val loss 1.747\n",
      "Ep 1 (Step 034925): Train loss 1.209, Val loss 1.746\n",
      "Ep 1 (Step 034930): Train loss 1.229, Val loss 1.746\n",
      "Ep 1 (Step 034935): Train loss 1.134, Val loss 1.748\n",
      "Ep 1 (Step 034940): Train loss 1.142, Val loss 1.748\n",
      "Ep 1 (Step 034945): Train loss 0.955, Val loss 1.749\n",
      "Ep 1 (Step 034950): Train loss 1.477, Val loss 1.750\n",
      "Ep 1 (Step 034955): Train loss 1.040, Val loss 1.750\n",
      "Ep 1 (Step 034960): Train loss 1.153, Val loss 1.750\n",
      "Ep 1 (Step 034965): Train loss 1.146, Val loss 1.749\n",
      "Ep 1 (Step 034970): Train loss 0.968, Val loss 1.747\n",
      "Ep 1 (Step 034975): Train loss 1.162, Val loss 1.747\n",
      "Ep 1 (Step 034980): Train loss 0.992, Val loss 1.747\n",
      "Ep 1 (Step 034985): Train loss 1.093, Val loss 1.748\n",
      "Ep 1 (Step 034990): Train loss 1.045, Val loss 1.748\n",
      "Ep 1 (Step 034995): Train loss 1.108, Val loss 1.748\n",
      "Ep 1 (Step 035000): Train loss 1.129, Val loss 1.748\n",
      "Ep 1 (Step 035005): Train loss 1.218, Val loss 1.748\n",
      "Ep 1 (Step 035010): Train loss 1.067, Val loss 1.748\n",
      "Ep 1 (Step 035015): Train loss 1.401, Val loss 1.748\n",
      "Ep 1 (Step 035020): Train loss 1.079, Val loss 1.748\n",
      "Ep 1 (Step 035025): Train loss 1.196, Val loss 1.749\n",
      "Ep 1 (Step 035030): Train loss 1.277, Val loss 1.749\n",
      "Ep 1 (Step 035035): Train loss 1.525, Val loss 1.751\n",
      "Ep 1 (Step 035040): Train loss 1.197, Val loss 1.752\n",
      "Ep 1 (Step 035045): Train loss 1.124, Val loss 1.752\n",
      "Ep 1 (Step 035050): Train loss 1.042, Val loss 1.753\n",
      "Ep 1 (Step 035055): Train loss 1.188, Val loss 1.753\n",
      "Ep 1 (Step 035060): Train loss 0.952, Val loss 1.753\n",
      "Ep 1 (Step 035065): Train loss 1.312, Val loss 1.753\n",
      "Ep 1 (Step 035070): Train loss 0.958, Val loss 1.753\n",
      "Ep 1 (Step 035075): Train loss 1.212, Val loss 1.753\n",
      "Ep 1 (Step 035080): Train loss 1.178, Val loss 1.754\n",
      "Ep 1 (Step 035085): Train loss 0.923, Val loss 1.753\n",
      "Ep 1 (Step 035090): Train loss 1.251, Val loss 1.752\n",
      "Ep 1 (Step 035095): Train loss 1.258, Val loss 1.752\n",
      "Ep 1 (Step 035100): Train loss 1.420, Val loss 1.751\n",
      "Ep 1 (Step 035105): Train loss 1.228, Val loss 1.750\n",
      "Ep 1 (Step 035110): Train loss 1.137, Val loss 1.750\n",
      "Ep 1 (Step 035115): Train loss 1.074, Val loss 1.751\n",
      "Ep 1 (Step 035120): Train loss 1.054, Val loss 1.752\n",
      "Ep 1 (Step 035125): Train loss 1.077, Val loss 1.753\n",
      "Ep 1 (Step 035130): Train loss 1.225, Val loss 1.754\n",
      "Ep 1 (Step 035135): Train loss 0.913, Val loss 1.755\n",
      "Ep 1 (Step 035140): Train loss 1.162, Val loss 1.757\n",
      "Ep 1 (Step 035145): Train loss 1.273, Val loss 1.758\n",
      "Ep 1 (Step 035150): Train loss 1.206, Val loss 1.758\n",
      "Ep 1 (Step 035155): Train loss 0.997, Val loss 1.758\n",
      "Ep 1 (Step 035160): Train loss 0.949, Val loss 1.756\n",
      "Ep 1 (Step 035165): Train loss 1.325, Val loss 1.755\n",
      "Ep 1 (Step 035170): Train loss 1.359, Val loss 1.755\n",
      "Ep 1 (Step 035175): Train loss 0.853, Val loss 1.754\n",
      "Ep 1 (Step 035180): Train loss 1.154, Val loss 1.754\n",
      "Ep 1 (Step 035185): Train loss 1.210, Val loss 1.753\n",
      "Ep 1 (Step 035190): Train loss 1.114, Val loss 1.754\n",
      "Ep 1 (Step 035195): Train loss 1.080, Val loss 1.754\n",
      "Ep 1 (Step 035200): Train loss 0.817, Val loss 1.754\n",
      "Ep 1 (Step 035205): Train loss 1.344, Val loss 1.756\n",
      "Ep 1 (Step 035210): Train loss 1.234, Val loss 1.758\n",
      "Ep 1 (Step 035215): Train loss 1.336, Val loss 1.759\n",
      "Ep 1 (Step 035220): Train loss 1.151, Val loss 1.760\n",
      "Ep 1 (Step 035225): Train loss 1.172, Val loss 1.760\n",
      "Ep 1 (Step 035230): Train loss 1.010, Val loss 1.760\n",
      "Ep 1 (Step 035235): Train loss 1.208, Val loss 1.759\n",
      "Ep 1 (Step 035240): Train loss 1.291, Val loss 1.760\n",
      "Ep 1 (Step 035245): Train loss 1.053, Val loss 1.761\n",
      "Ep 1 (Step 035250): Train loss 1.155, Val loss 1.762\n",
      "Ep 1 (Step 035255): Train loss 1.157, Val loss 1.761\n",
      "Ep 1 (Step 035260): Train loss 1.117, Val loss 1.760\n",
      "Ep 1 (Step 035265): Train loss 1.248, Val loss 1.760\n",
      "Ep 1 (Step 035270): Train loss 0.996, Val loss 1.760\n",
      "Ep 1 (Step 035275): Train loss 1.128, Val loss 1.759\n",
      "Ep 1 (Step 035280): Train loss 1.301, Val loss 1.758\n",
      "Ep 1 (Step 035285): Train loss 1.047, Val loss 1.758\n",
      "Ep 1 (Step 035290): Train loss 1.150, Val loss 1.757\n",
      "Ep 1 (Step 035295): Train loss 1.141, Val loss 1.757\n",
      "Ep 1 (Step 035300): Train loss 1.204, Val loss 1.756\n",
      "Ep 1 (Step 035305): Train loss 1.316, Val loss 1.756\n",
      "Ep 1 (Step 035310): Train loss 1.147, Val loss 1.757\n",
      "Ep 1 (Step 035315): Train loss 1.371, Val loss 1.758\n",
      "Ep 1 (Step 035320): Train loss 0.791, Val loss 1.759\n",
      "Ep 1 (Step 035325): Train loss 1.069, Val loss 1.759\n",
      "Ep 1 (Step 035330): Train loss 1.106, Val loss 1.760\n",
      "Ep 1 (Step 035335): Train loss 1.305, Val loss 1.762\n",
      "Ep 1 (Step 035340): Train loss 1.096, Val loss 1.763\n",
      "Ep 1 (Step 035345): Train loss 1.293, Val loss 1.762\n",
      "Ep 1 (Step 035350): Train loss 1.516, Val loss 1.761\n",
      "Ep 1 (Step 035355): Train loss 0.881, Val loss 1.758\n",
      "Ep 1 (Step 035360): Train loss 0.889, Val loss 1.755\n",
      "Ep 1 (Step 035365): Train loss 1.339, Val loss 1.754\n",
      "Ep 1 (Step 035370): Train loss 1.116, Val loss 1.754\n",
      "Ep 1 (Step 035375): Train loss 1.149, Val loss 1.752\n",
      "Ep 1 (Step 035380): Train loss 1.004, Val loss 1.751\n",
      "Ep 1 (Step 035385): Train loss 1.212, Val loss 1.751\n",
      "Ep 1 (Step 035390): Train loss 1.001, Val loss 1.752\n",
      "Ep 1 (Step 035395): Train loss 1.084, Val loss 1.753\n",
      "Ep 1 (Step 035400): Train loss 1.541, Val loss 1.753\n",
      "Ep 1 (Step 035405): Train loss 1.412, Val loss 1.754\n",
      "Ep 1 (Step 035410): Train loss 1.149, Val loss 1.755\n",
      "Ep 1 (Step 035415): Train loss 0.908, Val loss 1.755\n",
      "Ep 1 (Step 035420): Train loss 1.354, Val loss 1.755\n",
      "Ep 1 (Step 035425): Train loss 0.899, Val loss 1.755\n",
      "Ep 1 (Step 035430): Train loss 1.261, Val loss 1.755\n",
      "Ep 1 (Step 035435): Train loss 1.321, Val loss 1.755\n",
      "Ep 1 (Step 035440): Train loss 1.419, Val loss 1.755\n",
      "Ep 1 (Step 035445): Train loss 1.305, Val loss 1.757\n",
      "Ep 1 (Step 035450): Train loss 1.244, Val loss 1.758\n",
      "Ep 1 (Step 035455): Train loss 1.044, Val loss 1.760\n",
      "Ep 1 (Step 035460): Train loss 1.158, Val loss 1.762\n",
      "Ep 1 (Step 035465): Train loss 1.105, Val loss 1.763\n",
      "Ep 1 (Step 035470): Train loss 1.374, Val loss 1.762\n",
      "Ep 1 (Step 035475): Train loss 1.245, Val loss 1.762\n",
      "Ep 1 (Step 035480): Train loss 0.884, Val loss 1.762\n",
      "Ep 1 (Step 035485): Train loss 1.284, Val loss 1.761\n",
      "Ep 1 (Step 035490): Train loss 1.362, Val loss 1.761\n",
      "Ep 1 (Step 035495): Train loss 1.193, Val loss 1.760\n",
      "Ep 1 (Step 035500): Train loss 1.061, Val loss 1.760\n",
      "Ep 1 (Step 035505): Train loss 1.154, Val loss 1.760\n",
      "Ep 1 (Step 035510): Train loss 1.015, Val loss 1.761\n",
      "Ep 1 (Step 035515): Train loss 1.016, Val loss 1.761\n",
      "Ep 1 (Step 035520): Train loss 0.929, Val loss 1.761\n",
      "Ep 1 (Step 035525): Train loss 0.944, Val loss 1.762\n",
      "Ep 1 (Step 035530): Train loss 1.318, Val loss 1.763\n",
      "Ep 1 (Step 035535): Train loss 0.950, Val loss 1.764\n",
      "Ep 1 (Step 035540): Train loss 1.016, Val loss 1.764\n",
      "Ep 1 (Step 035545): Train loss 1.143, Val loss 1.765\n",
      "Ep 1 (Step 035550): Train loss 1.215, Val loss 1.765\n",
      "Ep 1 (Step 035555): Train loss 1.300, Val loss 1.764\n",
      "Ep 1 (Step 035560): Train loss 1.224, Val loss 1.762\n",
      "Ep 1 (Step 035565): Train loss 1.348, Val loss 1.761\n",
      "Ep 1 (Step 035570): Train loss 1.138, Val loss 1.759\n",
      "Ep 1 (Step 035575): Train loss 1.236, Val loss 1.758\n",
      "Ep 1 (Step 035580): Train loss 1.403, Val loss 1.757\n",
      "Ep 1 (Step 035585): Train loss 1.444, Val loss 1.758\n",
      "Ep 1 (Step 035590): Train loss 1.240, Val loss 1.757\n",
      "Ep 1 (Step 035595): Train loss 1.074, Val loss 1.757\n",
      "Ep 1 (Step 035600): Train loss 1.041, Val loss 1.757\n",
      "Ep 1 (Step 035605): Train loss 1.053, Val loss 1.757\n",
      "Ep 1 (Step 035610): Train loss 1.210, Val loss 1.757\n",
      "Ep 1 (Step 035615): Train loss 0.979, Val loss 1.757\n",
      "Ep 1 (Step 035620): Train loss 1.126, Val loss 1.758\n",
      "Ep 1 (Step 035625): Train loss 1.106, Val loss 1.759\n",
      "Ep 1 (Step 035630): Train loss 1.054, Val loss 1.760\n",
      "Ep 1 (Step 035635): Train loss 1.065, Val loss 1.761\n",
      "Ep 1 (Step 035640): Train loss 1.103, Val loss 1.762\n",
      "Ep 1 (Step 035645): Train loss 1.252, Val loss 1.764\n",
      "Ep 1 (Step 035650): Train loss 0.979, Val loss 1.764\n",
      "Ep 1 (Step 035655): Train loss 1.347, Val loss 1.764\n",
      "Ep 1 (Step 035660): Train loss 1.081, Val loss 1.764\n",
      "Ep 1 (Step 035665): Train loss 1.326, Val loss 1.763\n",
      "Ep 1 (Step 035670): Train loss 1.169, Val loss 1.762\n",
      "Ep 1 (Step 035675): Train loss 1.003, Val loss 1.761\n",
      "Ep 1 (Step 035680): Train loss 1.120, Val loss 1.760\n",
      "Ep 1 (Step 035685): Train loss 1.093, Val loss 1.759\n",
      "Ep 1 (Step 035690): Train loss 1.037, Val loss 1.760\n",
      "Ep 1 (Step 035695): Train loss 1.139, Val loss 1.761\n",
      "Ep 1 (Step 035700): Train loss 1.175, Val loss 1.762\n",
      "Ep 1 (Step 035705): Train loss 1.306, Val loss 1.762\n",
      "Ep 1 (Step 035710): Train loss 1.043, Val loss 1.762\n",
      "Ep 1 (Step 035715): Train loss 1.175, Val loss 1.762\n",
      "Ep 1 (Step 035720): Train loss 1.142, Val loss 1.761\n",
      "Ep 1 (Step 035725): Train loss 1.190, Val loss 1.758\n",
      "Ep 1 (Step 035730): Train loss 1.023, Val loss 1.756\n",
      "Ep 1 (Step 035735): Train loss 1.232, Val loss 1.754\n",
      "Ep 1 (Step 035740): Train loss 1.249, Val loss 1.753\n",
      "Ep 1 (Step 035745): Train loss 1.259, Val loss 1.753\n",
      "Ep 1 (Step 035750): Train loss 1.025, Val loss 1.754\n",
      "Ep 1 (Step 035755): Train loss 1.077, Val loss 1.755\n",
      "Ep 1 (Step 035760): Train loss 0.814, Val loss 1.755\n",
      "Ep 1 (Step 035765): Train loss 1.307, Val loss 1.756\n",
      "Ep 1 (Step 035770): Train loss 1.106, Val loss 1.759\n",
      "Ep 1 (Step 035775): Train loss 1.085, Val loss 1.761\n",
      "Ep 1 (Step 035780): Train loss 1.116, Val loss 1.762\n",
      "Ep 1 (Step 035785): Train loss 0.870, Val loss 1.764\n",
      "Ep 1 (Step 035790): Train loss 0.984, Val loss 1.765\n",
      "Ep 1 (Step 035795): Train loss 1.121, Val loss 1.768\n",
      "Ep 1 (Step 035800): Train loss 1.056, Val loss 1.771\n",
      "Ep 1 (Step 035805): Train loss 1.260, Val loss 1.773\n",
      "Ep 1 (Step 035810): Train loss 0.969, Val loss 1.774\n",
      "Ep 1 (Step 035815): Train loss 1.085, Val loss 1.775\n",
      "Ep 1 (Step 035820): Train loss 1.005, Val loss 1.774\n",
      "Ep 1 (Step 035825): Train loss 1.345, Val loss 1.774\n",
      "Ep 1 (Step 035830): Train loss 1.150, Val loss 1.772\n",
      "Ep 1 (Step 035835): Train loss 1.485, Val loss 1.772\n",
      "Ep 1 (Step 035840): Train loss 1.073, Val loss 1.773\n",
      "Ep 1 (Step 035845): Train loss 0.975, Val loss 1.772\n",
      "Ep 1 (Step 035850): Train loss 1.367, Val loss 1.773\n",
      "Ep 1 (Step 035855): Train loss 0.948, Val loss 1.775\n",
      "Ep 1 (Step 035860): Train loss 1.054, Val loss 1.776\n",
      "Ep 1 (Step 035865): Train loss 1.138, Val loss 1.776\n",
      "Ep 1 (Step 035870): Train loss 1.023, Val loss 1.774\n",
      "Ep 1 (Step 035875): Train loss 0.958, Val loss 1.773\n",
      "Ep 1 (Step 035880): Train loss 1.098, Val loss 1.772\n",
      "Ep 1 (Step 035885): Train loss 1.244, Val loss 1.772\n",
      "Ep 1 (Step 035890): Train loss 0.816, Val loss 1.770\n",
      "Ep 1 (Step 035895): Train loss 1.028, Val loss 1.769\n",
      "Ep 1 (Step 035900): Train loss 1.066, Val loss 1.769\n",
      "Ep 1 (Step 035905): Train loss 0.968, Val loss 1.769\n",
      "Ep 1 (Step 035910): Train loss 1.069, Val loss 1.770\n",
      "Ep 1 (Step 035915): Train loss 1.195, Val loss 1.771\n",
      "Ep 1 (Step 035920): Train loss 1.070, Val loss 1.772\n",
      "Ep 1 (Step 035925): Train loss 1.098, Val loss 1.773\n",
      "Ep 1 (Step 035930): Train loss 0.990, Val loss 1.773\n",
      "Ep 1 (Step 035935): Train loss 1.122, Val loss 1.773\n",
      "Ep 1 (Step 035940): Train loss 1.093, Val loss 1.774\n",
      "Ep 1 (Step 035945): Train loss 1.090, Val loss 1.775\n",
      "Ep 1 (Step 035950): Train loss 1.225, Val loss 1.775\n",
      "Ep 1 (Step 035955): Train loss 1.459, Val loss 1.774\n",
      "Ep 1 (Step 035960): Train loss 1.131, Val loss 1.772\n",
      "Ep 1 (Step 035965): Train loss 0.804, Val loss 1.770\n",
      "Ep 1 (Step 035970): Train loss 1.327, Val loss 1.769\n",
      "Ep 1 (Step 035975): Train loss 1.197, Val loss 1.769\n",
      "Ep 1 (Step 035980): Train loss 1.086, Val loss 1.769\n",
      "Ep 1 (Step 035985): Train loss 1.076, Val loss 1.769\n",
      "Ep 1 (Step 035990): Train loss 0.916, Val loss 1.768\n",
      "Ep 1 (Step 035995): Train loss 1.018, Val loss 1.769\n",
      "Ep 1 (Step 036000): Train loss 0.936, Val loss 1.768\n",
      "Ep 1 (Step 036005): Train loss 1.062, Val loss 1.768\n",
      "Ep 1 (Step 036010): Train loss 1.106, Val loss 1.767\n",
      "Ep 1 (Step 036015): Train loss 0.989, Val loss 1.767\n",
      "Ep 1 (Step 036020): Train loss 1.040, Val loss 1.766\n",
      "Ep 1 (Step 036025): Train loss 1.165, Val loss 1.767\n",
      "Ep 1 (Step 036030): Train loss 1.001, Val loss 1.768\n",
      "Ep 1 (Step 036035): Train loss 1.118, Val loss 1.769\n",
      "Ep 1 (Step 036040): Train loss 1.396, Val loss 1.769\n",
      "Ep 1 (Step 036045): Train loss 0.972, Val loss 1.767\n",
      "Ep 1 (Step 036050): Train loss 0.858, Val loss 1.764\n",
      "Ep 1 (Step 036055): Train loss 1.155, Val loss 1.763\n",
      "Ep 1 (Step 036060): Train loss 1.169, Val loss 1.761\n",
      "Ep 1 (Step 036065): Train loss 1.024, Val loss 1.761\n",
      "Ep 1 (Step 036070): Train loss 1.055, Val loss 1.761\n",
      "Ep 1 (Step 036075): Train loss 1.271, Val loss 1.763\n",
      "Ep 1 (Step 036080): Train loss 1.204, Val loss 1.764\n",
      "Ep 1 (Step 036085): Train loss 1.078, Val loss 1.768\n",
      "Ep 1 (Step 036090): Train loss 1.061, Val loss 1.772\n",
      "Ep 1 (Step 036095): Train loss 1.084, Val loss 1.773\n",
      "Ep 1 (Step 036100): Train loss 1.044, Val loss 1.772\n",
      "Ep 1 (Step 036105): Train loss 0.932, Val loss 1.773\n",
      "Ep 1 (Step 036110): Train loss 0.991, Val loss 1.773\n",
      "Ep 1 (Step 036115): Train loss 0.901, Val loss 1.772\n",
      "Ep 1 (Step 036120): Train loss 0.834, Val loss 1.773\n",
      "Ep 1 (Step 036125): Train loss 1.275, Val loss 1.774\n",
      "Ep 1 (Step 036130): Train loss 1.258, Val loss 1.774\n",
      "Ep 1 (Step 036135): Train loss 1.002, Val loss 1.772\n",
      "Ep 1 (Step 036140): Train loss 0.835, Val loss 1.770\n",
      "Ep 1 (Step 036145): Train loss 1.176, Val loss 1.769\n",
      "Ep 1 (Step 036150): Train loss 1.279, Val loss 1.769\n",
      "Ep 1 (Step 036155): Train loss 1.109, Val loss 1.769\n",
      "Ep 1 (Step 036160): Train loss 1.142, Val loss 1.769\n",
      "Ep 1 (Step 036165): Train loss 1.146, Val loss 1.771\n",
      "Ep 1 (Step 036170): Train loss 1.154, Val loss 1.771\n",
      "Ep 1 (Step 036175): Train loss 1.068, Val loss 1.772\n",
      "Ep 1 (Step 036180): Train loss 0.949, Val loss 1.774\n",
      "Ep 1 (Step 036185): Train loss 1.228, Val loss 1.773\n",
      "Ep 1 (Step 036190): Train loss 1.092, Val loss 1.774\n",
      "Ep 1 (Step 036195): Train loss 1.023, Val loss 1.775\n",
      "Ep 1 (Step 036200): Train loss 1.352, Val loss 1.775\n",
      "Ep 1 (Step 036205): Train loss 1.107, Val loss 1.774\n",
      "Ep 1 (Step 036210): Train loss 1.152, Val loss 1.774\n",
      "Ep 1 (Step 036215): Train loss 1.268, Val loss 1.774\n",
      "Ep 1 (Step 036220): Train loss 1.046, Val loss 1.774\n",
      "Ep 1 (Step 036225): Train loss 1.202, Val loss 1.775\n",
      "Ep 1 (Step 036230): Train loss 1.135, Val loss 1.776\n",
      "Ep 1 (Step 036235): Train loss 1.112, Val loss 1.777\n",
      "Ep 1 (Step 036240): Train loss 1.093, Val loss 1.777\n",
      "Ep 1 (Step 036245): Train loss 1.244, Val loss 1.775\n",
      "Ep 1 (Step 036250): Train loss 1.412, Val loss 1.775\n",
      "Ep 1 (Step 036255): Train loss 0.880, Val loss 1.775\n",
      "Ep 1 (Step 036260): Train loss 1.062, Val loss 1.774\n",
      "Ep 1 (Step 036265): Train loss 0.989, Val loss 1.773\n",
      "Ep 1 (Step 036270): Train loss 1.133, Val loss 1.772\n",
      "Ep 1 (Step 036275): Train loss 1.048, Val loss 1.771\n",
      "Ep 1 (Step 036280): Train loss 1.069, Val loss 1.771\n",
      "Ep 1 (Step 036285): Train loss 1.172, Val loss 1.771\n",
      "Ep 1 (Step 036290): Train loss 1.144, Val loss 1.770\n",
      "Ep 1 (Step 036295): Train loss 1.140, Val loss 1.769\n",
      "Ep 1 (Step 036300): Train loss 0.951, Val loss 1.770\n",
      "Ep 1 (Step 036305): Train loss 0.873, Val loss 1.770\n",
      "Ep 1 (Step 036310): Train loss 1.237, Val loss 1.772\n",
      "Ep 1 (Step 036315): Train loss 0.955, Val loss 1.773\n",
      "Ep 1 (Step 036320): Train loss 1.184, Val loss 1.774\n",
      "Ep 1 (Step 036325): Train loss 0.881, Val loss 1.775\n",
      "Ep 1 (Step 036330): Train loss 1.403, Val loss 1.774\n",
      "Ep 1 (Step 036335): Train loss 1.279, Val loss 1.772\n",
      "Ep 1 (Step 036340): Train loss 1.388, Val loss 1.771\n",
      "Ep 1 (Step 036345): Train loss 1.350, Val loss 1.771\n",
      "Ep 1 (Step 036350): Train loss 1.012, Val loss 1.771\n",
      "Ep 1 (Step 036355): Train loss 1.394, Val loss 1.773\n",
      "Ep 1 (Step 036360): Train loss 1.380, Val loss 1.773\n",
      "Ep 1 (Step 036365): Train loss 1.483, Val loss 1.773\n",
      "Ep 1 (Step 036370): Train loss 1.130, Val loss 1.774\n",
      "Ep 1 (Step 036375): Train loss 0.906, Val loss 1.772\n",
      "Ep 1 (Step 036380): Train loss 0.965, Val loss 1.771\n",
      "Ep 1 (Step 036385): Train loss 1.265, Val loss 1.769\n",
      "Ep 1 (Step 036390): Train loss 1.085, Val loss 1.768\n",
      "Ep 1 (Step 036395): Train loss 1.349, Val loss 1.766\n",
      "Ep 1 (Step 036400): Train loss 1.295, Val loss 1.765\n",
      "Ep 1 (Step 036405): Train loss 0.827, Val loss 1.765\n",
      "Ep 1 (Step 036410): Train loss 1.254, Val loss 1.764\n",
      "Ep 1 (Step 036415): Train loss 1.324, Val loss 1.765\n",
      "Ep 1 (Step 036420): Train loss 1.140, Val loss 1.764\n",
      "Ep 1 (Step 036425): Train loss 0.983, Val loss 1.763\n",
      "Ep 1 (Step 036430): Train loss 1.037, Val loss 1.762\n",
      "Ep 1 (Step 036435): Train loss 1.124, Val loss 1.760\n",
      "Ep 1 (Step 036440): Train loss 1.059, Val loss 1.759\n",
      "Ep 1 (Step 036445): Train loss 1.263, Val loss 1.760\n",
      "Ep 1 (Step 036450): Train loss 1.003, Val loss 1.759\n",
      "Ep 1 (Step 036455): Train loss 1.244, Val loss 1.758\n",
      "Ep 1 (Step 036460): Train loss 1.215, Val loss 1.758\n",
      "Ep 1 (Step 036465): Train loss 1.263, Val loss 1.757\n",
      "Ep 1 (Step 036470): Train loss 1.118, Val loss 1.758\n",
      "Ep 1 (Step 036475): Train loss 1.050, Val loss 1.761\n",
      "Ep 1 (Step 036480): Train loss 1.311, Val loss 1.761\n",
      "Ep 1 (Step 036485): Train loss 1.048, Val loss 1.762\n",
      "Ep 1 (Step 036490): Train loss 1.094, Val loss 1.761\n",
      "Ep 1 (Step 036495): Train loss 1.145, Val loss 1.759\n",
      "Ep 1 (Step 036500): Train loss 1.244, Val loss 1.756\n",
      "Ep 1 (Step 036505): Train loss 1.174, Val loss 1.753\n",
      "Ep 1 (Step 036510): Train loss 1.247, Val loss 1.750\n",
      "Ep 1 (Step 036515): Train loss 1.382, Val loss 1.748\n",
      "Ep 1 (Step 036520): Train loss 1.347, Val loss 1.746\n",
      "Ep 1 (Step 036525): Train loss 0.971, Val loss 1.745\n",
      "Ep 1 (Step 036530): Train loss 0.980, Val loss 1.746\n",
      "Ep 1 (Step 036535): Train loss 1.013, Val loss 1.747\n",
      "Ep 1 (Step 036540): Train loss 1.058, Val loss 1.746\n",
      "Ep 1 (Step 036545): Train loss 1.292, Val loss 1.745\n",
      "Ep 1 (Step 036550): Train loss 1.297, Val loss 1.743\n",
      "Ep 1 (Step 036555): Train loss 0.998, Val loss 1.743\n",
      "Ep 1 (Step 036560): Train loss 1.162, Val loss 1.744\n",
      "Ep 1 (Step 036565): Train loss 1.467, Val loss 1.743\n",
      "Ep 1 (Step 036570): Train loss 1.502, Val loss 1.742\n",
      "Ep 1 (Step 036575): Train loss 1.366, Val loss 1.741\n",
      "Ep 1 (Step 036580): Train loss 1.212, Val loss 1.741\n",
      "Ep 1 (Step 036585): Train loss 0.866, Val loss 1.737\n",
      "Ep 1 (Step 036590): Train loss 1.241, Val loss 1.736\n",
      "Ep 1 (Step 036595): Train loss 1.295, Val loss 1.735\n",
      "Ep 1 (Step 036600): Train loss 1.286, Val loss 1.735\n",
      "Ep 1 (Step 036605): Train loss 1.208, Val loss 1.735\n",
      "Ep 1 (Step 036610): Train loss 1.099, Val loss 1.736\n",
      "Ep 1 (Step 036615): Train loss 1.152, Val loss 1.739\n",
      "Ep 1 (Step 036620): Train loss 1.365, Val loss 1.742\n",
      "Ep 1 (Step 036625): Train loss 1.266, Val loss 1.743\n",
      "Ep 1 (Step 036630): Train loss 0.903, Val loss 1.744\n",
      "Ep 1 (Step 036635): Train loss 1.177, Val loss 1.744\n",
      "Ep 1 (Step 036640): Train loss 1.138, Val loss 1.742\n",
      "Ep 1 (Step 036645): Train loss 1.361, Val loss 1.741\n",
      "Ep 1 (Step 036650): Train loss 1.189, Val loss 1.740\n",
      "Ep 1 (Step 036655): Train loss 1.128, Val loss 1.739\n",
      "Ep 1 (Step 036660): Train loss 1.052, Val loss 1.739\n",
      "Ep 1 (Step 036665): Train loss 1.301, Val loss 1.740\n",
      "Ep 1 (Step 036670): Train loss 1.108, Val loss 1.740\n",
      "Ep 1 (Step 036675): Train loss 1.247, Val loss 1.739\n",
      "Ep 1 (Step 036680): Train loss 1.329, Val loss 1.738\n",
      "Ep 1 (Step 036685): Train loss 1.080, Val loss 1.738\n",
      "Ep 1 (Step 036690): Train loss 0.969, Val loss 1.740\n",
      "Ep 1 (Step 036695): Train loss 1.022, Val loss 1.741\n",
      "Ep 1 (Step 036700): Train loss 1.342, Val loss 1.743\n",
      "Ep 1 (Step 036705): Train loss 1.177, Val loss 1.745\n",
      "Ep 1 (Step 036710): Train loss 1.317, Val loss 1.746\n",
      "Ep 1 (Step 036715): Train loss 1.186, Val loss 1.746\n",
      "Ep 1 (Step 036720): Train loss 1.365, Val loss 1.744\n",
      "Ep 1 (Step 036725): Train loss 1.364, Val loss 1.741\n",
      "Ep 1 (Step 036730): Train loss 1.275, Val loss 1.739\n",
      "Ep 1 (Step 036735): Train loss 1.260, Val loss 1.737\n",
      "Ep 1 (Step 036740): Train loss 1.282, Val loss 1.733\n",
      "Ep 1 (Step 036745): Train loss 1.179, Val loss 1.731\n",
      "Ep 1 (Step 036750): Train loss 1.505, Val loss 1.730\n",
      "Ep 1 (Step 036755): Train loss 0.889, Val loss 1.729\n",
      "Ep 1 (Step 036760): Train loss 0.884, Val loss 1.729\n",
      "Ep 1 (Step 036765): Train loss 1.229, Val loss 1.731\n",
      "Ep 1 (Step 036770): Train loss 1.223, Val loss 1.733\n",
      "Ep 1 (Step 036775): Train loss 1.269, Val loss 1.734\n",
      "Ep 1 (Step 036780): Train loss 1.052, Val loss 1.733\n",
      "Ep 1 (Step 036785): Train loss 1.027, Val loss 1.734\n",
      "Ep 1 (Step 036790): Train loss 1.054, Val loss 1.734\n",
      "Ep 1 (Step 036795): Train loss 1.251, Val loss 1.735\n",
      "Ep 1 (Step 036800): Train loss 0.975, Val loss 1.736\n",
      "Ep 1 (Step 036805): Train loss 1.218, Val loss 1.737\n",
      "Ep 1 (Step 036810): Train loss 0.991, Val loss 1.736\n",
      "Ep 1 (Step 036815): Train loss 1.730, Val loss 1.736\n",
      "Ep 1 (Step 036820): Train loss 1.016, Val loss 1.735\n",
      "Ep 1 (Step 036825): Train loss 1.184, Val loss 1.735\n",
      "Ep 1 (Step 036830): Train loss 0.969, Val loss 1.736\n",
      "Ep 1 (Step 036835): Train loss 1.293, Val loss 1.736\n",
      "Ep 1 (Step 036840): Train loss 1.126, Val loss 1.735\n",
      "Ep 1 (Step 036845): Train loss 1.167, Val loss 1.736\n",
      "Ep 1 (Step 036850): Train loss 1.055, Val loss 1.736\n",
      "Ep 1 (Step 036855): Train loss 1.391, Val loss 1.737\n",
      "Ep 1 (Step 036860): Train loss 1.380, Val loss 1.738\n",
      "Ep 1 (Step 036865): Train loss 1.044, Val loss 1.738\n",
      "Ep 1 (Step 036870): Train loss 0.947, Val loss 1.738\n",
      "Ep 1 (Step 036875): Train loss 1.170, Val loss 1.736\n",
      "Ep 1 (Step 036880): Train loss 1.121, Val loss 1.737\n",
      "Ep 1 (Step 036885): Train loss 1.272, Val loss 1.737\n",
      "Ep 1 (Step 036890): Train loss 0.998, Val loss 1.736\n",
      "Ep 1 (Step 036895): Train loss 0.855, Val loss 1.736\n",
      "Ep 1 (Step 036900): Train loss 1.027, Val loss 1.736\n",
      "Ep 1 (Step 036905): Train loss 1.006, Val loss 1.736\n",
      "Ep 1 (Step 036910): Train loss 1.062, Val loss 1.736\n",
      "Ep 1 (Step 036915): Train loss 1.025, Val loss 1.737\n",
      "Ep 1 (Step 036920): Train loss 1.426, Val loss 1.739\n",
      "Ep 1 (Step 036925): Train loss 0.951, Val loss 1.740\n",
      "Ep 1 (Step 036930): Train loss 1.409, Val loss 1.742\n",
      "Ep 1 (Step 036935): Train loss 1.261, Val loss 1.742\n",
      "Ep 1 (Step 036940): Train loss 1.108, Val loss 1.742\n",
      "Ep 1 (Step 036945): Train loss 0.928, Val loss 1.741\n",
      "Ep 1 (Step 036950): Train loss 1.219, Val loss 1.742\n",
      "Ep 1 (Step 036955): Train loss 1.134, Val loss 1.743\n",
      "Ep 1 (Step 036960): Train loss 1.052, Val loss 1.744\n",
      "Ep 1 (Step 036965): Train loss 1.474, Val loss 1.744\n",
      "Ep 1 (Step 036970): Train loss 0.955, Val loss 1.745\n",
      "Ep 1 (Step 036975): Train loss 1.130, Val loss 1.746\n",
      "Ep 1 (Step 036980): Train loss 1.242, Val loss 1.747\n",
      "Ep 1 (Step 036985): Train loss 0.989, Val loss 1.745\n",
      "Ep 1 (Step 036990): Train loss 1.276, Val loss 1.744\n",
      "Ep 1 (Step 036995): Train loss 1.429, Val loss 1.744\n",
      "Ep 1 (Step 037000): Train loss 1.128, Val loss 1.744\n",
      "Ep 1 (Step 037005): Train loss 0.933, Val loss 1.744\n",
      "Ep 1 (Step 037010): Train loss 1.277, Val loss 1.743\n",
      "Ep 1 (Step 037015): Train loss 1.090, Val loss 1.741\n",
      "Ep 1 (Step 037020): Train loss 0.813, Val loss 1.738\n",
      "Ep 1 (Step 037025): Train loss 1.272, Val loss 1.737\n",
      "Ep 1 (Step 037030): Train loss 0.926, Val loss 1.736\n",
      "Ep 1 (Step 037035): Train loss 1.237, Val loss 1.737\n",
      "Ep 1 (Step 037040): Train loss 1.109, Val loss 1.738\n",
      "Ep 1 (Step 037045): Train loss 0.957, Val loss 1.740\n",
      "Ep 1 (Step 037050): Train loss 1.144, Val loss 1.743\n",
      "Ep 1 (Step 037055): Train loss 1.227, Val loss 1.743\n",
      "Ep 1 (Step 037060): Train loss 1.168, Val loss 1.742\n",
      "Ep 1 (Step 037065): Train loss 1.052, Val loss 1.742\n",
      "Ep 1 (Step 037070): Train loss 1.404, Val loss 1.742\n",
      "Ep 1 (Step 037075): Train loss 0.965, Val loss 1.742\n",
      "Ep 1 (Step 037080): Train loss 1.150, Val loss 1.742\n",
      "Ep 1 (Step 037085): Train loss 1.119, Val loss 1.743\n",
      "Ep 1 (Step 037090): Train loss 1.278, Val loss 1.744\n",
      "Ep 1 (Step 037095): Train loss 1.235, Val loss 1.744\n",
      "Ep 1 (Step 037100): Train loss 1.193, Val loss 1.742\n",
      "Ep 1 (Step 037105): Train loss 0.921, Val loss 1.742\n",
      "Ep 1 (Step 037110): Train loss 1.059, Val loss 1.742\n",
      "Ep 1 (Step 037115): Train loss 0.975, Val loss 1.741\n",
      "Ep 1 (Step 037120): Train loss 1.147, Val loss 1.740\n",
      "Ep 1 (Step 037125): Train loss 1.129, Val loss 1.741\n",
      "Ep 1 (Step 037130): Train loss 1.103, Val loss 1.743\n",
      "Ep 1 (Step 037135): Train loss 0.936, Val loss 1.745\n",
      "Ep 1 (Step 037140): Train loss 1.043, Val loss 1.746\n",
      "Ep 1 (Step 037145): Train loss 1.125, Val loss 1.747\n",
      "Ep 1 (Step 037150): Train loss 1.169, Val loss 1.747\n",
      "Ep 1 (Step 037155): Train loss 1.018, Val loss 1.747\n",
      "Ep 1 (Step 037160): Train loss 0.926, Val loss 1.747\n",
      "Ep 1 (Step 037165): Train loss 1.326, Val loss 1.746\n",
      "Ep 1 (Step 037170): Train loss 1.250, Val loss 1.745\n",
      "Ep 1 (Step 037175): Train loss 1.080, Val loss 1.745\n",
      "Ep 1 (Step 037180): Train loss 1.067, Val loss 1.744\n",
      "Ep 1 (Step 037185): Train loss 1.069, Val loss 1.744\n",
      "Ep 1 (Step 037190): Train loss 0.993, Val loss 1.744\n",
      "Ep 1 (Step 037195): Train loss 0.927, Val loss 1.743\n",
      "Ep 1 (Step 037200): Train loss 1.046, Val loss 1.742\n",
      "Ep 1 (Step 037205): Train loss 1.008, Val loss 1.741\n",
      "Ep 1 (Step 037210): Train loss 1.121, Val loss 1.741\n",
      "Ep 1 (Step 037215): Train loss 1.098, Val loss 1.740\n",
      "Ep 1 (Step 037220): Train loss 1.229, Val loss 1.742\n",
      "Ep 1 (Step 037225): Train loss 1.250, Val loss 1.743\n",
      "Ep 1 (Step 037230): Train loss 1.004, Val loss 1.744\n",
      "Ep 1 (Step 037235): Train loss 1.247, Val loss 1.746\n",
      "Ep 1 (Step 037240): Train loss 1.298, Val loss 1.748\n",
      "Ep 1 (Step 037245): Train loss 1.219, Val loss 1.750\n",
      "Ep 1 (Step 037250): Train loss 1.026, Val loss 1.749\n",
      "Ep 1 (Step 037255): Train loss 1.409, Val loss 1.749\n",
      "Ep 1 (Step 037260): Train loss 0.989, Val loss 1.750\n",
      "Ep 1 (Step 037265): Train loss 1.498, Val loss 1.751\n",
      "Ep 1 (Step 037270): Train loss 1.132, Val loss 1.751\n",
      "Ep 1 (Step 037275): Train loss 1.057, Val loss 1.752\n",
      "Ep 1 (Step 037280): Train loss 0.977, Val loss 1.753\n",
      "Ep 1 (Step 037285): Train loss 1.348, Val loss 1.754\n",
      "Ep 1 (Step 037290): Train loss 1.100, Val loss 1.755\n",
      "Ep 1 (Step 037295): Train loss 1.240, Val loss 1.756\n",
      "Ep 1 (Step 037300): Train loss 1.269, Val loss 1.756\n",
      "Ep 1 (Step 037305): Train loss 1.096, Val loss 1.758\n",
      "Ep 1 (Step 037310): Train loss 1.277, Val loss 1.759\n",
      "Ep 1 (Step 037315): Train loss 1.192, Val loss 1.760\n",
      "Ep 1 (Step 037320): Train loss 1.335, Val loss 1.760\n",
      "Ep 1 (Step 037325): Train loss 1.300, Val loss 1.759\n",
      "Ep 1 (Step 037330): Train loss 1.142, Val loss 1.760\n",
      "Ep 1 (Step 037335): Train loss 1.391, Val loss 1.761\n",
      "Ep 1 (Step 037340): Train loss 0.975, Val loss 1.761\n",
      "Ep 1 (Step 037345): Train loss 1.013, Val loss 1.760\n",
      "Ep 1 (Step 037350): Train loss 1.180, Val loss 1.760\n",
      "Ep 1 (Step 037355): Train loss 1.228, Val loss 1.759\n",
      "Ep 1 (Step 037360): Train loss 1.245, Val loss 1.756\n",
      "Ep 1 (Step 037365): Train loss 1.162, Val loss 1.754\n",
      "Ep 1 (Step 037370): Train loss 1.203, Val loss 1.754\n",
      "Ep 1 (Step 037375): Train loss 0.868, Val loss 1.755\n",
      "Ep 1 (Step 037380): Train loss 1.131, Val loss 1.755\n",
      "Ep 1 (Step 037385): Train loss 1.084, Val loss 1.755\n",
      "Ep 1 (Step 037390): Train loss 0.926, Val loss 1.754\n",
      "Ep 1 (Step 037395): Train loss 1.164, Val loss 1.755\n",
      "Ep 1 (Step 037400): Train loss 1.036, Val loss 1.756\n",
      "Ep 1 (Step 037405): Train loss 1.178, Val loss 1.758\n",
      "Ep 1 (Step 037410): Train loss 1.239, Val loss 1.758\n",
      "Ep 1 (Step 037415): Train loss 1.236, Val loss 1.759\n",
      "Ep 1 (Step 037420): Train loss 1.075, Val loss 1.759\n",
      "Ep 1 (Step 037425): Train loss 1.028, Val loss 1.758\n",
      "Ep 1 (Step 037430): Train loss 1.068, Val loss 1.756\n",
      "Ep 1 (Step 037435): Train loss 1.448, Val loss 1.756\n",
      "Ep 1 (Step 037440): Train loss 1.006, Val loss 1.755\n",
      "Ep 1 (Step 037445): Train loss 1.319, Val loss 1.754\n",
      "Ep 1 (Step 037450): Train loss 1.248, Val loss 1.755\n",
      "Ep 1 (Step 037455): Train loss 1.411, Val loss 1.755\n",
      "Ep 1 (Step 037460): Train loss 1.140, Val loss 1.756\n",
      "Ep 1 (Step 037465): Train loss 1.030, Val loss 1.755\n",
      "Ep 1 (Step 037470): Train loss 0.827, Val loss 1.756\n",
      "Ep 1 (Step 037475): Train loss 1.378, Val loss 1.757\n",
      "Ep 1 (Step 037480): Train loss 1.183, Val loss 1.759\n",
      "Ep 1 (Step 037485): Train loss 1.004, Val loss 1.761\n",
      "Ep 1 (Step 037490): Train loss 1.206, Val loss 1.762\n",
      "Ep 1 (Step 037495): Train loss 0.974, Val loss 1.762\n",
      "Ep 1 (Step 037500): Train loss 1.384, Val loss 1.761\n",
      "Ep 1 (Step 037505): Train loss 1.147, Val loss 1.760\n",
      "Ep 1 (Step 037510): Train loss 1.270, Val loss 1.760\n",
      "Ep 1 (Step 037515): Train loss 0.914, Val loss 1.760\n",
      "Ep 1 (Step 037520): Train loss 0.887, Val loss 1.761\n",
      "Ep 1 (Step 037525): Train loss 0.997, Val loss 1.762\n",
      "Ep 1 (Step 037530): Train loss 0.925, Val loss 1.762\n",
      "Ep 1 (Step 037535): Train loss 1.177, Val loss 1.761\n",
      "Ep 1 (Step 037540): Train loss 1.279, Val loss 1.760\n",
      "Ep 1 (Step 037545): Train loss 1.221, Val loss 1.759\n",
      "Ep 1 (Step 037550): Train loss 1.123, Val loss 1.757\n",
      "Ep 1 (Step 037555): Train loss 1.102, Val loss 1.757\n",
      "Ep 1 (Step 037560): Train loss 1.160, Val loss 1.758\n",
      "Ep 1 (Step 037565): Train loss 0.832, Val loss 1.759\n",
      "Ep 1 (Step 037570): Train loss 1.051, Val loss 1.760\n",
      "Ep 1 (Step 037575): Train loss 1.210, Val loss 1.761\n",
      "Ep 1 (Step 037580): Train loss 1.002, Val loss 1.761\n",
      "Ep 1 (Step 037585): Train loss 1.166, Val loss 1.761\n",
      "Ep 1 (Step 037590): Train loss 0.903, Val loss 1.761\n",
      "Ep 1 (Step 037595): Train loss 1.159, Val loss 1.761\n",
      "Ep 1 (Step 037600): Train loss 0.900, Val loss 1.762\n",
      "Ep 1 (Step 037605): Train loss 1.387, Val loss 1.763\n",
      "Ep 1 (Step 037610): Train loss 1.096, Val loss 1.764\n",
      "Ep 1 (Step 037615): Train loss 0.908, Val loss 1.763\n",
      "Ep 1 (Step 037620): Train loss 1.346, Val loss 1.764\n",
      "Ep 1 (Step 037625): Train loss 1.202, Val loss 1.765\n",
      "Ep 1 (Step 037630): Train loss 1.435, Val loss 1.763\n",
      "Ep 1 (Step 037635): Train loss 0.913, Val loss 1.760\n",
      "Ep 1 (Step 037640): Train loss 0.967, Val loss 1.757\n",
      "Ep 1 (Step 037645): Train loss 1.225, Val loss 1.755\n",
      "Ep 1 (Step 037650): Train loss 1.260, Val loss 1.755\n",
      "Ep 1 (Step 037655): Train loss 0.966, Val loss 1.756\n",
      "Ep 1 (Step 037660): Train loss 1.365, Val loss 1.757\n",
      "Ep 1 (Step 037665): Train loss 1.014, Val loss 1.757\n",
      "Ep 1 (Step 037670): Train loss 1.278, Val loss 1.757\n",
      "Ep 1 (Step 037675): Train loss 0.911, Val loss 1.758\n",
      "Ep 1 (Step 037680): Train loss 0.959, Val loss 1.760\n",
      "Ep 1 (Step 037685): Train loss 1.127, Val loss 1.762\n",
      "Ep 1 (Step 037690): Train loss 1.053, Val loss 1.763\n",
      "Ep 1 (Step 037695): Train loss 1.167, Val loss 1.765\n",
      "Ep 1 (Step 037700): Train loss 1.138, Val loss 1.766\n",
      "Ep 1 (Step 037705): Train loss 1.003, Val loss 1.766\n",
      "Ep 1 (Step 037710): Train loss 1.059, Val loss 1.766\n",
      "Ep 1 (Step 037715): Train loss 1.330, Val loss 1.766\n",
      "Ep 1 (Step 037720): Train loss 0.890, Val loss 1.767\n",
      "Ep 1 (Step 037725): Train loss 0.953, Val loss 1.768\n",
      "Ep 1 (Step 037730): Train loss 1.270, Val loss 1.767\n",
      "Ep 1 (Step 037735): Train loss 1.208, Val loss 1.766\n",
      "Ep 1 (Step 037740): Train loss 1.197, Val loss 1.766\n",
      "Ep 1 (Step 037745): Train loss 1.160, Val loss 1.765\n",
      "Ep 1 (Step 037750): Train loss 1.122, Val loss 1.765\n",
      "Ep 1 (Step 037755): Train loss 1.236, Val loss 1.764\n",
      "Ep 1 (Step 037760): Train loss 1.354, Val loss 1.762\n",
      "Ep 1 (Step 037765): Train loss 1.097, Val loss 1.762\n",
      "Ep 1 (Step 037770): Train loss 1.051, Val loss 1.762\n",
      "Ep 1 (Step 037775): Train loss 1.204, Val loss 1.761\n",
      "Ep 1 (Step 037780): Train loss 1.242, Val loss 1.760\n",
      "Ep 1 (Step 037785): Train loss 0.841, Val loss 1.760\n",
      "Ep 1 (Step 037790): Train loss 1.180, Val loss 1.760\n",
      "Ep 1 (Step 037795): Train loss 1.257, Val loss 1.760\n",
      "Ep 1 (Step 037800): Train loss 1.062, Val loss 1.760\n",
      "Ep 1 (Step 037805): Train loss 1.122, Val loss 1.760\n",
      "Ep 1 (Step 037810): Train loss 1.243, Val loss 1.760\n",
      "Ep 1 (Step 037815): Train loss 0.884, Val loss 1.761\n",
      "Ep 1 (Step 037820): Train loss 1.333, Val loss 1.760\n",
      "Ep 1 (Step 037825): Train loss 0.772, Val loss 1.760\n",
      "Ep 1 (Step 037830): Train loss 1.155, Val loss 1.761\n",
      "Ep 1 (Step 037835): Train loss 0.963, Val loss 1.761\n",
      "Ep 1 (Step 037840): Train loss 0.974, Val loss 1.761\n",
      "Ep 1 (Step 037845): Train loss 1.030, Val loss 1.760\n",
      "Ep 1 (Step 037850): Train loss 1.300, Val loss 1.760\n",
      "Ep 1 (Step 037855): Train loss 1.253, Val loss 1.761\n",
      "Ep 1 (Step 037860): Train loss 1.003, Val loss 1.762\n",
      "Ep 1 (Step 037865): Train loss 1.175, Val loss 1.762\n",
      "Ep 1 (Step 037870): Train loss 1.234, Val loss 1.762\n",
      "Ep 1 (Step 037875): Train loss 1.329, Val loss 1.759\n",
      "Ep 1 (Step 037880): Train loss 0.979, Val loss 1.758\n",
      "Ep 1 (Step 037885): Train loss 0.918, Val loss 1.757\n",
      "Ep 1 (Step 037890): Train loss 1.136, Val loss 1.757\n",
      "Ep 1 (Step 037895): Train loss 1.179, Val loss 1.756\n",
      "Ep 1 (Step 037900): Train loss 1.027, Val loss 1.755\n",
      "Ep 1 (Step 037905): Train loss 0.992, Val loss 1.755\n",
      "Ep 1 (Step 037910): Train loss 0.929, Val loss 1.756\n",
      "Ep 1 (Step 037915): Train loss 1.151, Val loss 1.756\n",
      "Ep 1 (Step 037920): Train loss 1.174, Val loss 1.756\n",
      "Ep 1 (Step 037925): Train loss 1.074, Val loss 1.755\n",
      "Ep 1 (Step 037930): Train loss 1.333, Val loss 1.756\n",
      "Ep 1 (Step 037935): Train loss 1.047, Val loss 1.755\n",
      "Ep 1 (Step 037940): Train loss 0.829, Val loss 1.754\n",
      "Ep 1 (Step 037945): Train loss 1.016, Val loss 1.753\n",
      "Ep 1 (Step 037950): Train loss 0.950, Val loss 1.753\n",
      "Ep 1 (Step 037955): Train loss 1.114, Val loss 1.753\n",
      "Ep 1 (Step 037960): Train loss 1.152, Val loss 1.754\n",
      "Ep 1 (Step 037965): Train loss 1.167, Val loss 1.754\n",
      "Ep 1 (Step 037970): Train loss 1.077, Val loss 1.755\n",
      "Ep 1 (Step 037975): Train loss 1.048, Val loss 1.756\n",
      "Ep 1 (Step 037980): Train loss 1.137, Val loss 1.756\n",
      "Ep 1 (Step 037985): Train loss 0.778, Val loss 1.756\n",
      "Ep 1 (Step 037990): Train loss 0.899, Val loss 1.757\n",
      "Ep 1 (Step 037995): Train loss 1.278, Val loss 1.759\n",
      "Ep 1 (Step 038000): Train loss 1.343, Val loss 1.760\n",
      "Ep 1 (Step 038005): Train loss 1.126, Val loss 1.760\n",
      "Ep 1 (Step 038010): Train loss 1.134, Val loss 1.760\n",
      "Ep 1 (Step 038015): Train loss 1.588, Val loss 1.759\n",
      "Ep 1 (Step 038020): Train loss 1.007, Val loss 1.758\n",
      "Ep 1 (Step 038025): Train loss 1.061, Val loss 1.757\n",
      "Ep 1 (Step 038030): Train loss 1.283, Val loss 1.756\n",
      "Ep 1 (Step 038035): Train loss 1.044, Val loss 1.755\n",
      "Ep 1 (Step 038040): Train loss 1.059, Val loss 1.755\n",
      "Ep 1 (Step 038045): Train loss 1.065, Val loss 1.754\n",
      "Ep 1 (Step 038050): Train loss 0.821, Val loss 1.753\n",
      "Ep 1 (Step 038055): Train loss 1.263, Val loss 1.753\n",
      "Ep 1 (Step 038060): Train loss 0.948, Val loss 1.753\n",
      "Ep 1 (Step 038065): Train loss 1.078, Val loss 1.753\n",
      "Ep 1 (Step 038070): Train loss 1.270, Val loss 1.753\n",
      "Ep 1 (Step 038075): Train loss 0.977, Val loss 1.753\n",
      "Ep 1 (Step 038080): Train loss 1.260, Val loss 1.753\n",
      "Ep 1 (Step 038085): Train loss 0.949, Val loss 1.754\n",
      "Ep 1 (Step 038090): Train loss 0.967, Val loss 1.755\n",
      "Ep 1 (Step 038095): Train loss 1.081, Val loss 1.757\n",
      "Ep 1 (Step 038100): Train loss 1.313, Val loss 1.760\n",
      "Ep 1 (Step 038105): Train loss 1.115, Val loss 1.762\n",
      "Ep 1 (Step 038110): Train loss 1.297, Val loss 1.762\n",
      "Ep 1 (Step 038115): Train loss 0.969, Val loss 1.761\n",
      "Ep 1 (Step 038120): Train loss 0.965, Val loss 1.760\n",
      "Ep 1 (Step 038125): Train loss 1.458, Val loss 1.761\n",
      "Ep 1 (Step 038130): Train loss 0.900, Val loss 1.762\n",
      "Ep 1 (Step 038135): Train loss 0.867, Val loss 1.763\n",
      "Ep 1 (Step 038140): Train loss 0.937, Val loss 1.763\n",
      "Ep 1 (Step 038145): Train loss 1.096, Val loss 1.763\n",
      "Ep 1 (Step 038150): Train loss 0.890, Val loss 1.763\n",
      "Ep 1 (Step 038155): Train loss 1.461, Val loss 1.763\n",
      "Ep 1 (Step 038160): Train loss 1.203, Val loss 1.762\n",
      "Ep 1 (Step 038165): Train loss 1.094, Val loss 1.761\n",
      "Ep 1 (Step 038170): Train loss 1.190, Val loss 1.759\n",
      "Ep 1 (Step 038175): Train loss 1.115, Val loss 1.757\n",
      "Ep 1 (Step 038180): Train loss 0.997, Val loss 1.756\n",
      "Ep 1 (Step 038185): Train loss 0.978, Val loss 1.756\n",
      "Ep 1 (Step 038190): Train loss 1.273, Val loss 1.756\n",
      "Ep 1 (Step 038195): Train loss 1.183, Val loss 1.756\n",
      "Ep 1 (Step 038200): Train loss 0.911, Val loss 1.754\n",
      "Ep 1 (Step 038205): Train loss 1.081, Val loss 1.754\n",
      "Ep 1 (Step 038210): Train loss 0.701, Val loss 1.755\n",
      "Ep 1 (Step 038215): Train loss 1.239, Val loss 1.755\n",
      "Ep 1 (Step 038220): Train loss 0.941, Val loss 1.755\n",
      "Ep 1 (Step 038225): Train loss 0.958, Val loss 1.755\n",
      "Ep 1 (Step 038230): Train loss 0.706, Val loss 1.753\n",
      "Ep 1 (Step 038235): Train loss 0.917, Val loss 1.752\n",
      "Ep 1 (Step 038240): Train loss 1.180, Val loss 1.752\n",
      "Ep 1 (Step 038245): Train loss 0.954, Val loss 1.750\n",
      "Ep 1 (Step 038250): Train loss 1.210, Val loss 1.748\n",
      "Ep 1 (Step 038255): Train loss 1.103, Val loss 1.747\n",
      "Ep 1 (Step 038260): Train loss 1.019, Val loss 1.746\n",
      "Ep 1 (Step 038265): Train loss 0.965, Val loss 1.744\n",
      "Ep 1 (Step 038270): Train loss 1.278, Val loss 1.744\n",
      "Ep 1 (Step 038275): Train loss 0.967, Val loss 1.745\n",
      "Ep 1 (Step 038280): Train loss 1.185, Val loss 1.744\n",
      "Ep 1 (Step 038285): Train loss 1.055, Val loss 1.743\n",
      "Ep 1 (Step 038290): Train loss 1.038, Val loss 1.743\n",
      "Ep 1 (Step 038295): Train loss 1.376, Val loss 1.744\n",
      "Ep 1 (Step 038300): Train loss 1.252, Val loss 1.743\n",
      "Ep 1 (Step 038305): Train loss 1.116, Val loss 1.742\n",
      "Ep 1 (Step 038310): Train loss 1.050, Val loss 1.742\n",
      "Ep 1 (Step 038315): Train loss 1.241, Val loss 1.742\n",
      "Ep 1 (Step 038320): Train loss 1.363, Val loss 1.743\n",
      "Ep 1 (Step 038325): Train loss 1.395, Val loss 1.744\n",
      "Ep 1 (Step 038330): Train loss 0.986, Val loss 1.742\n",
      "Ep 1 (Step 038335): Train loss 1.336, Val loss 1.741\n",
      "Ep 1 (Step 038340): Train loss 1.030, Val loss 1.741\n",
      "Ep 1 (Step 038345): Train loss 1.350, Val loss 1.742\n",
      "Ep 1 (Step 038350): Train loss 1.003, Val loss 1.742\n",
      "Ep 1 (Step 038355): Train loss 0.867, Val loss 1.742\n",
      "Ep 1 (Step 038360): Train loss 1.210, Val loss 1.742\n",
      "Ep 1 (Step 038365): Train loss 1.014, Val loss 1.743\n",
      "Ep 1 (Step 038370): Train loss 1.239, Val loss 1.743\n",
      "Ep 1 (Step 038375): Train loss 1.144, Val loss 1.743\n",
      "Ep 1 (Step 038380): Train loss 1.221, Val loss 1.743\n",
      "Ep 1 (Step 038385): Train loss 1.098, Val loss 1.744\n",
      "Ep 1 (Step 038390): Train loss 1.214, Val loss 1.746\n",
      "Ep 1 (Step 038395): Train loss 1.104, Val loss 1.747\n",
      "Ep 1 (Step 038400): Train loss 1.298, Val loss 1.749\n",
      "Ep 1 (Step 038405): Train loss 1.336, Val loss 1.749\n",
      "Ep 1 (Step 038410): Train loss 0.992, Val loss 1.749\n",
      "Ep 1 (Step 038415): Train loss 1.231, Val loss 1.750\n",
      "Ep 1 (Step 038420): Train loss 1.122, Val loss 1.749\n",
      "Ep 1 (Step 038425): Train loss 1.348, Val loss 1.747\n",
      "Ep 1 (Step 038430): Train loss 0.942, Val loss 1.747\n",
      "Ep 1 (Step 038435): Train loss 1.075, Val loss 1.746\n",
      "Ep 1 (Step 038440): Train loss 1.099, Val loss 1.746\n",
      "Ep 1 (Step 038445): Train loss 1.058, Val loss 1.746\n",
      "Ep 1 (Step 038450): Train loss 1.018, Val loss 1.745\n",
      "Ep 1 (Step 038455): Train loss 1.208, Val loss 1.746\n",
      "Ep 1 (Step 038460): Train loss 0.856, Val loss 1.746\n",
      "Ep 1 (Step 038465): Train loss 1.055, Val loss 1.746\n",
      "Ep 1 (Step 038470): Train loss 1.113, Val loss 1.746\n",
      "Ep 1 (Step 038475): Train loss 1.380, Val loss 1.746\n",
      "Ep 1 (Step 038480): Train loss 0.871, Val loss 1.745\n",
      "Ep 1 (Step 038485): Train loss 1.311, Val loss 1.746\n",
      "Ep 1 (Step 038490): Train loss 1.252, Val loss 1.747\n",
      "Ep 1 (Step 038495): Train loss 1.453, Val loss 1.747\n",
      "Ep 1 (Step 038500): Train loss 1.164, Val loss 1.747\n",
      "Ep 1 (Step 038505): Train loss 1.381, Val loss 1.747\n",
      "Ep 1 (Step 038510): Train loss 1.284, Val loss 1.746\n",
      "Ep 1 (Step 038515): Train loss 1.154, Val loss 1.746\n",
      "Ep 1 (Step 038520): Train loss 0.893, Val loss 1.747\n",
      "Ep 1 (Step 038525): Train loss 1.250, Val loss 1.748\n",
      "Ep 1 (Step 038530): Train loss 1.434, Val loss 1.749\n",
      "Ep 1 (Step 038535): Train loss 1.092, Val loss 1.749\n",
      "Ep 1 (Step 038540): Train loss 0.999, Val loss 1.750\n",
      "Ep 1 (Step 038545): Train loss 1.040, Val loss 1.752\n",
      "Ep 1 (Step 038550): Train loss 0.962, Val loss 1.753\n",
      "Ep 1 (Step 038555): Train loss 1.151, Val loss 1.754\n",
      "Ep 1 (Step 038560): Train loss 1.120, Val loss 1.754\n",
      "Ep 1 (Step 038565): Train loss 1.226, Val loss 1.754\n",
      "Ep 1 (Step 038570): Train loss 1.210, Val loss 1.754\n",
      "Ep 1 (Step 038575): Train loss 0.793, Val loss 1.755\n",
      "Ep 1 (Step 038580): Train loss 1.427, Val loss 1.753\n",
      "Ep 1 (Step 038585): Train loss 1.303, Val loss 1.751\n",
      "Ep 1 (Step 038590): Train loss 1.320, Val loss 1.750\n",
      "Ep 1 (Step 038595): Train loss 0.881, Val loss 1.750\n",
      "Ep 1 (Step 038600): Train loss 1.173, Val loss 1.750\n",
      "Ep 1 (Step 038605): Train loss 1.085, Val loss 1.752\n",
      "Ep 1 (Step 038610): Train loss 1.162, Val loss 1.753\n",
      "Ep 1 (Step 038615): Train loss 1.113, Val loss 1.754\n",
      "Ep 1 (Step 038620): Train loss 1.116, Val loss 1.754\n",
      "Ep 1 (Step 038625): Train loss 1.028, Val loss 1.754\n",
      "Ep 1 (Step 038630): Train loss 1.328, Val loss 1.753\n",
      "Ep 1 (Step 038635): Train loss 0.948, Val loss 1.752\n",
      "Ep 1 (Step 038640): Train loss 0.884, Val loss 1.753\n",
      "Ep 1 (Step 038645): Train loss 0.984, Val loss 1.753\n",
      "Ep 1 (Step 038650): Train loss 1.008, Val loss 1.753\n",
      "Ep 1 (Step 038655): Train loss 0.819, Val loss 1.753\n",
      "Ep 1 (Step 038660): Train loss 0.845, Val loss 1.754\n",
      "Ep 1 (Step 038665): Train loss 1.117, Val loss 1.755\n",
      "Ep 1 (Step 038670): Train loss 0.862, Val loss 1.756\n",
      "Ep 1 (Step 038675): Train loss 1.375, Val loss 1.757\n",
      "Ep 1 (Step 038680): Train loss 0.860, Val loss 1.757\n",
      "Ep 1 (Step 038685): Train loss 1.021, Val loss 1.757\n",
      "Ep 1 (Step 038690): Train loss 1.274, Val loss 1.756\n",
      "Ep 1 (Step 038695): Train loss 1.222, Val loss 1.755\n",
      "Ep 1 (Step 038700): Train loss 1.303, Val loss 1.754\n",
      "Ep 1 (Step 038705): Train loss 1.207, Val loss 1.754\n",
      "Ep 1 (Step 038710): Train loss 1.142, Val loss 1.753\n",
      "Ep 1 (Step 038715): Train loss 1.523, Val loss 1.753\n",
      "Ep 1 (Step 038720): Train loss 1.107, Val loss 1.753\n",
      "Ep 1 (Step 038725): Train loss 1.110, Val loss 1.753\n",
      "Ep 1 (Step 038730): Train loss 1.089, Val loss 1.752\n",
      "Ep 1 (Step 038735): Train loss 1.107, Val loss 1.750\n",
      "Ep 1 (Step 038740): Train loss 1.209, Val loss 1.749\n",
      "Ep 1 (Step 038745): Train loss 1.111, Val loss 1.749\n",
      "Ep 1 (Step 038750): Train loss 1.189, Val loss 1.747\n",
      "Ep 1 (Step 038755): Train loss 1.139, Val loss 1.747\n",
      "Ep 1 (Step 038760): Train loss 1.062, Val loss 1.747\n",
      "Ep 1 (Step 038765): Train loss 1.373, Val loss 1.748\n",
      "Ep 1 (Step 038770): Train loss 0.807, Val loss 1.749\n",
      "Ep 1 (Step 038775): Train loss 1.229, Val loss 1.751\n",
      "Ep 1 (Step 038780): Train loss 1.214, Val loss 1.753\n",
      "Ep 1 (Step 038785): Train loss 1.440, Val loss 1.753\n",
      "Ep 1 (Step 038790): Train loss 1.021, Val loss 1.752\n",
      "Ep 1 (Step 038795): Train loss 1.310, Val loss 1.751\n",
      "Ep 1 (Step 038800): Train loss 1.414, Val loss 1.751\n",
      "Ep 1 (Step 038805): Train loss 1.156, Val loss 1.749\n",
      "Ep 1 (Step 038810): Train loss 1.253, Val loss 1.748\n",
      "Ep 1 (Step 038815): Train loss 1.304, Val loss 1.748\n",
      "Ep 1 (Step 038820): Train loss 1.194, Val loss 1.749\n",
      "Ep 1 (Step 038825): Train loss 1.203, Val loss 1.749\n",
      "Ep 1 (Step 038830): Train loss 1.007, Val loss 1.749\n",
      "Ep 1 (Step 038835): Train loss 1.273, Val loss 1.749\n",
      "Ep 1 (Step 038840): Train loss 1.093, Val loss 1.750\n",
      "Ep 1 (Step 038845): Train loss 0.875, Val loss 1.751\n",
      "Ep 1 (Step 038850): Train loss 1.654, Val loss 1.753\n",
      "Ep 1 (Step 038855): Train loss 1.317, Val loss 1.753\n",
      "Ep 1 (Step 038860): Train loss 0.929, Val loss 1.754\n",
      "Ep 1 (Step 038865): Train loss 1.187, Val loss 1.754\n",
      "Ep 1 (Step 038870): Train loss 1.146, Val loss 1.755\n",
      "Ep 1 (Step 038875): Train loss 1.193, Val loss 1.756\n",
      "Ep 1 (Step 038880): Train loss 1.029, Val loss 1.756\n",
      "Ep 1 (Step 038885): Train loss 1.011, Val loss 1.757\n",
      "Ep 1 (Step 038890): Train loss 1.145, Val loss 1.756\n",
      "Ep 1 (Step 038895): Train loss 1.209, Val loss 1.755\n",
      "Ep 1 (Step 038900): Train loss 1.043, Val loss 1.754\n",
      "Ep 1 (Step 038905): Train loss 1.216, Val loss 1.755\n",
      "Ep 1 (Step 038910): Train loss 1.201, Val loss 1.756\n",
      "Ep 1 (Step 038915): Train loss 1.192, Val loss 1.756\n",
      "Ep 1 (Step 038920): Train loss 1.266, Val loss 1.757\n",
      "Ep 1 (Step 038925): Train loss 1.272, Val loss 1.756\n",
      "Ep 1 (Step 038930): Train loss 1.108, Val loss 1.754\n",
      "Ep 1 (Step 038935): Train loss 1.324, Val loss 1.753\n",
      "Ep 1 (Step 038940): Train loss 0.953, Val loss 1.752\n",
      "Ep 1 (Step 038945): Train loss 1.199, Val loss 1.753\n",
      "Ep 1 (Step 038950): Train loss 1.301, Val loss 1.753\n",
      "Ep 1 (Step 038955): Train loss 1.080, Val loss 1.755\n",
      "Ep 1 (Step 038960): Train loss 1.058, Val loss 1.757\n",
      "Ep 1 (Step 038965): Train loss 0.948, Val loss 1.758\n",
      "Ep 1 (Step 038970): Train loss 1.296, Val loss 1.757\n",
      "Ep 1 (Step 038975): Train loss 1.084, Val loss 1.757\n",
      "Ep 1 (Step 038980): Train loss 0.984, Val loss 1.758\n",
      "Ep 1 (Step 038985): Train loss 0.980, Val loss 1.758\n",
      "Ep 1 (Step 038990): Train loss 1.161, Val loss 1.757\n",
      "Ep 1 (Step 038995): Train loss 0.851, Val loss 1.758\n",
      "Ep 1 (Step 039000): Train loss 1.402, Val loss 1.759\n",
      "Ep 1 (Step 039005): Train loss 1.156, Val loss 1.759\n",
      "Ep 1 (Step 039010): Train loss 1.434, Val loss 1.758\n",
      "Ep 1 (Step 039015): Train loss 1.354, Val loss 1.755\n",
      "Ep 1 (Step 039020): Train loss 1.367, Val loss 1.753\n",
      "Ep 1 (Step 039025): Train loss 1.127, Val loss 1.752\n",
      "Ep 1 (Step 039030): Train loss 1.255, Val loss 1.752\n",
      "Ep 1 (Step 039035): Train loss 1.027, Val loss 1.750\n",
      "Ep 1 (Step 039040): Train loss 0.888, Val loss 1.750\n",
      "Ep 1 (Step 039045): Train loss 1.158, Val loss 1.750\n",
      "Ep 1 (Step 039050): Train loss 1.085, Val loss 1.750\n",
      "Ep 1 (Step 039055): Train loss 1.379, Val loss 1.749\n",
      "Ep 1 (Step 039060): Train loss 1.010, Val loss 1.750\n",
      "Ep 1 (Step 039065): Train loss 1.505, Val loss 1.751\n",
      "Ep 1 (Step 039070): Train loss 1.206, Val loss 1.754\n",
      "Ep 1 (Step 039075): Train loss 1.213, Val loss 1.756\n",
      "Ep 1 (Step 039080): Train loss 1.382, Val loss 1.757\n",
      "Ep 1 (Step 039085): Train loss 1.284, Val loss 1.757\n",
      "Ep 1 (Step 039090): Train loss 1.258, Val loss 1.757\n",
      "Ep 1 (Step 039095): Train loss 1.240, Val loss 1.757\n",
      "Ep 1 (Step 039100): Train loss 0.920, Val loss 1.758\n",
      "Ep 1 (Step 039105): Train loss 0.900, Val loss 1.759\n",
      "Ep 1 (Step 039110): Train loss 0.896, Val loss 1.760\n",
      "Ep 1 (Step 039115): Train loss 1.031, Val loss 1.761\n",
      "Ep 1 (Step 039120): Train loss 1.311, Val loss 1.764\n",
      "Ep 1 (Step 039125): Train loss 1.184, Val loss 1.765\n",
      "Ep 1 (Step 039130): Train loss 1.116, Val loss 1.766\n",
      "Ep 1 (Step 039135): Train loss 1.031, Val loss 1.767\n",
      "Ep 1 (Step 039140): Train loss 1.135, Val loss 1.768\n",
      "Ep 1 (Step 039145): Train loss 1.186, Val loss 1.767\n",
      "Ep 1 (Step 039150): Train loss 0.833, Val loss 1.767\n",
      "Ep 1 (Step 039155): Train loss 1.246, Val loss 1.766\n",
      "Ep 1 (Step 039160): Train loss 1.048, Val loss 1.766\n",
      "Ep 1 (Step 039165): Train loss 1.172, Val loss 1.765\n",
      "Ep 1 (Step 039170): Train loss 1.080, Val loss 1.764\n",
      "Ep 1 (Step 039175): Train loss 1.166, Val loss 1.760\n",
      "Ep 1 (Step 039180): Train loss 1.344, Val loss 1.757\n",
      "Ep 1 (Step 039185): Train loss 1.172, Val loss 1.755\n",
      "Ep 1 (Step 039190): Train loss 1.191, Val loss 1.755\n",
      "Ep 1 (Step 039195): Train loss 1.145, Val loss 1.755\n",
      "Ep 1 (Step 039200): Train loss 1.126, Val loss 1.755\n",
      "Ep 1 (Step 039205): Train loss 0.942, Val loss 1.755\n",
      "Ep 1 (Step 039210): Train loss 1.053, Val loss 1.755\n",
      "Ep 1 (Step 039215): Train loss 1.121, Val loss 1.753\n",
      "Ep 1 (Step 039220): Train loss 1.085, Val loss 1.754\n",
      "Ep 1 (Step 039225): Train loss 1.038, Val loss 1.756\n",
      "Ep 1 (Step 039230): Train loss 1.101, Val loss 1.756\n",
      "Ep 1 (Step 039235): Train loss 1.278, Val loss 1.757\n",
      "Ep 1 (Step 039240): Train loss 1.286, Val loss 1.756\n",
      "Ep 1 (Step 039245): Train loss 1.141, Val loss 1.756\n",
      "Ep 1 (Step 039250): Train loss 1.148, Val loss 1.755\n",
      "Ep 1 (Step 039255): Train loss 1.023, Val loss 1.755\n",
      "Ep 1 (Step 039260): Train loss 1.576, Val loss 1.753\n",
      "Ep 1 (Step 039265): Train loss 1.074, Val loss 1.752\n",
      "Ep 1 (Step 039270): Train loss 1.179, Val loss 1.751\n",
      "Ep 1 (Step 039275): Train loss 0.985, Val loss 1.750\n",
      "Ep 1 (Step 039280): Train loss 1.167, Val loss 1.749\n",
      "Ep 1 (Step 039285): Train loss 1.270, Val loss 1.748\n",
      "Ep 1 (Step 039290): Train loss 1.026, Val loss 1.748\n",
      "Ep 1 (Step 039295): Train loss 1.551, Val loss 1.748\n",
      "Ep 1 (Step 039300): Train loss 1.019, Val loss 1.749\n",
      "Ep 1 (Step 039305): Train loss 0.937, Val loss 1.749\n",
      "Ep 1 (Step 039310): Train loss 1.013, Val loss 1.749\n",
      "Ep 1 (Step 039315): Train loss 0.962, Val loss 1.749\n",
      "Ep 1 (Step 039320): Train loss 1.220, Val loss 1.749\n",
      "Ep 1 (Step 039325): Train loss 1.112, Val loss 1.750\n",
      "Ep 1 (Step 039330): Train loss 1.312, Val loss 1.751\n",
      "Ep 1 (Step 039335): Train loss 1.062, Val loss 1.751\n",
      "Ep 1 (Step 039340): Train loss 0.859, Val loss 1.752\n",
      "Ep 1 (Step 039345): Train loss 1.145, Val loss 1.751\n",
      "Ep 1 (Step 039350): Train loss 1.302, Val loss 1.751\n",
      "Ep 1 (Step 039355): Train loss 1.113, Val loss 1.752\n",
      "Ep 1 (Step 039360): Train loss 1.173, Val loss 1.754\n",
      "Ep 1 (Step 039365): Train loss 1.159, Val loss 1.756\n",
      "Ep 1 (Step 039370): Train loss 1.262, Val loss 1.757\n",
      "Ep 1 (Step 039375): Train loss 1.024, Val loss 1.759\n",
      "Ep 1 (Step 039380): Train loss 0.927, Val loss 1.760\n",
      "Ep 1 (Step 039385): Train loss 0.982, Val loss 1.761\n",
      "Ep 1 (Step 039390): Train loss 0.898, Val loss 1.760\n",
      "Ep 1 (Step 039395): Train loss 0.935, Val loss 1.759\n",
      "Ep 1 (Step 039400): Train loss 1.018, Val loss 1.760\n",
      "Ep 1 (Step 039405): Train loss 0.949, Val loss 1.760\n",
      "Ep 1 (Step 039410): Train loss 0.961, Val loss 1.760\n",
      "Ep 1 (Step 039415): Train loss 0.935, Val loss 1.760\n",
      "Ep 1 (Step 039420): Train loss 1.337, Val loss 1.759\n",
      "Ep 1 (Step 039425): Train loss 1.203, Val loss 1.757\n",
      "Ep 1 (Step 039430): Train loss 1.162, Val loss 1.756\n",
      "Ep 1 (Step 039435): Train loss 1.153, Val loss 1.755\n",
      "Ep 1 (Step 039440): Train loss 1.044, Val loss 1.756\n",
      "Ep 1 (Step 039445): Train loss 1.165, Val loss 1.756\n",
      "Ep 1 (Step 039450): Train loss 0.830, Val loss 1.756\n",
      "Ep 1 (Step 039455): Train loss 0.859, Val loss 1.756\n",
      "Ep 1 (Step 039460): Train loss 1.015, Val loss 1.756\n",
      "Ep 1 (Step 039465): Train loss 0.999, Val loss 1.757\n",
      "Ep 1 (Step 039470): Train loss 1.124, Val loss 1.759\n",
      "Ep 1 (Step 039475): Train loss 0.904, Val loss 1.760\n",
      "Ep 1 (Step 039480): Train loss 1.237, Val loss 1.761\n",
      "Ep 1 (Step 039485): Train loss 1.271, Val loss 1.761\n",
      "Ep 1 (Step 039490): Train loss 1.031, Val loss 1.760\n",
      "Ep 1 (Step 039495): Train loss 0.805, Val loss 1.760\n",
      "Ep 1 (Step 039500): Train loss 1.225, Val loss 1.760\n",
      "Ep 1 (Step 039505): Train loss 0.972, Val loss 1.762\n",
      "Ep 1 (Step 039510): Train loss 0.998, Val loss 1.763\n",
      "Ep 1 (Step 039515): Train loss 1.186, Val loss 1.763\n",
      "Ep 1 (Step 039520): Train loss 1.053, Val loss 1.764\n",
      "Ep 1 (Step 039525): Train loss 1.126, Val loss 1.764\n",
      "Ep 1 (Step 039530): Train loss 1.141, Val loss 1.765\n",
      "Ep 1 (Step 039535): Train loss 1.079, Val loss 1.764\n",
      "Ep 1 (Step 039540): Train loss 1.192, Val loss 1.764\n",
      "Ep 1 (Step 039545): Train loss 0.935, Val loss 1.764\n",
      "Ep 1 (Step 039550): Train loss 1.229, Val loss 1.764\n",
      "Ep 1 (Step 039555): Train loss 1.222, Val loss 1.763\n",
      "Ep 1 (Step 039560): Train loss 1.006, Val loss 1.762\n",
      "Ep 1 (Step 039565): Train loss 1.331, Val loss 1.762\n",
      "Ep 1 (Step 039570): Train loss 1.028, Val loss 1.761\n",
      "Ep 1 (Step 039575): Train loss 1.066, Val loss 1.760\n",
      "Ep 1 (Step 039580): Train loss 1.186, Val loss 1.761\n",
      "Ep 1 (Step 039585): Train loss 0.972, Val loss 1.761\n",
      "Ep 1 (Step 039590): Train loss 1.105, Val loss 1.761\n",
      "Ep 1 (Step 039595): Train loss 1.221, Val loss 1.760\n",
      "Ep 1 (Step 039600): Train loss 0.933, Val loss 1.760\n",
      "Ep 1 (Step 039605): Train loss 0.872, Val loss 1.760\n",
      "Ep 1 (Step 039610): Train loss 0.943, Val loss 1.760\n",
      "Ep 1 (Step 039615): Train loss 1.134, Val loss 1.761\n",
      "Ep 1 (Step 039620): Train loss 1.027, Val loss 1.761\n",
      "Ep 1 (Step 039625): Train loss 0.971, Val loss 1.761\n",
      "Ep 1 (Step 039630): Train loss 1.171, Val loss 1.760\n",
      "Ep 1 (Step 039635): Train loss 1.305, Val loss 1.760\n",
      "Ep 1 (Step 039640): Train loss 1.160, Val loss 1.760\n",
      "Ep 1 (Step 039645): Train loss 1.324, Val loss 1.759\n",
      "Ep 1 (Step 039650): Train loss 1.059, Val loss 1.758\n",
      "Ep 1 (Step 039655): Train loss 1.194, Val loss 1.758\n",
      "Ep 1 (Step 039660): Train loss 0.776, Val loss 1.758\n",
      "Ep 1 (Step 039665): Train loss 1.278, Val loss 1.755\n",
      "Ep 1 (Step 039670): Train loss 1.137, Val loss 1.756\n",
      "Ep 1 (Step 039675): Train loss 1.305, Val loss 1.758\n",
      "Ep 1 (Step 039680): Train loss 1.172, Val loss 1.761\n",
      "Ep 1 (Step 039685): Train loss 0.917, Val loss 1.763\n",
      "Ep 1 (Step 039690): Train loss 0.947, Val loss 1.763\n",
      "Ep 1 (Step 039695): Train loss 1.255, Val loss 1.764\n",
      "Ep 1 (Step 039700): Train loss 1.037, Val loss 1.766\n",
      "Ep 1 (Step 039705): Train loss 0.945, Val loss 1.769\n",
      "Ep 1 (Step 039710): Train loss 0.899, Val loss 1.771\n",
      "Ep 1 (Step 039715): Train loss 1.122, Val loss 1.770\n",
      "Ep 1 (Step 039720): Train loss 0.910, Val loss 1.768\n",
      "Ep 1 (Step 039725): Train loss 1.392, Val loss 1.766\n",
      "Ep 1 (Step 039730): Train loss 1.043, Val loss 1.765\n",
      "Ep 1 (Step 039735): Train loss 1.078, Val loss 1.762\n",
      "Ep 1 (Step 039740): Train loss 1.345, Val loss 1.759\n",
      "Ep 1 (Step 039745): Train loss 1.229, Val loss 1.757\n",
      "Ep 1 (Step 039750): Train loss 1.226, Val loss 1.757\n",
      "Ep 1 (Step 039755): Train loss 1.292, Val loss 1.757\n",
      "Ep 1 (Step 039760): Train loss 1.237, Val loss 1.756\n",
      "Ep 1 (Step 039765): Train loss 1.132, Val loss 1.755\n",
      "Ep 1 (Step 039770): Train loss 1.161, Val loss 1.755\n",
      "Ep 1 (Step 039775): Train loss 0.953, Val loss 1.757\n",
      "Ep 1 (Step 039780): Train loss 0.955, Val loss 1.759\n",
      "Ep 1 (Step 039785): Train loss 0.978, Val loss 1.761\n",
      "Ep 1 (Step 039790): Train loss 1.145, Val loss 1.763\n",
      "Ep 1 (Step 039795): Train loss 1.251, Val loss 1.766\n",
      "Ep 1 (Step 039800): Train loss 1.011, Val loss 1.767\n",
      "Ep 1 (Step 039805): Train loss 1.050, Val loss 1.767\n",
      "Ep 1 (Step 039810): Train loss 1.280, Val loss 1.765\n",
      "Ep 1 (Step 039815): Train loss 1.117, Val loss 1.762\n",
      "Ep 1 (Step 039820): Train loss 1.336, Val loss 1.760\n",
      "Ep 1 (Step 039825): Train loss 1.134, Val loss 1.759\n",
      "Ep 1 (Step 039830): Train loss 0.906, Val loss 1.759\n",
      "Ep 1 (Step 039835): Train loss 1.132, Val loss 1.758\n",
      "Ep 1 (Step 039840): Train loss 1.302, Val loss 1.756\n",
      "Ep 1 (Step 039845): Train loss 1.175, Val loss 1.754\n",
      "Ep 1 (Step 039850): Train loss 1.142, Val loss 1.752\n",
      "Ep 1 (Step 039855): Train loss 1.331, Val loss 1.753\n",
      "Ep 1 (Step 039860): Train loss 1.253, Val loss 1.753\n",
      "Ep 1 (Step 039865): Train loss 1.149, Val loss 1.753\n",
      "Ep 1 (Step 039870): Train loss 1.231, Val loss 1.753\n",
      "Ep 1 (Step 039875): Train loss 1.157, Val loss 1.753\n",
      "Ep 1 (Step 039880): Train loss 1.199, Val loss 1.753\n",
      "Ep 1 (Step 039885): Train loss 1.308, Val loss 1.753\n",
      "Ep 1 (Step 039890): Train loss 0.861, Val loss 1.753\n",
      "Ep 1 (Step 039895): Train loss 1.155, Val loss 1.753\n",
      "Ep 1 (Step 039900): Train loss 1.188, Val loss 1.752\n",
      "Ep 1 (Step 039905): Train loss 1.323, Val loss 1.750\n",
      "Ep 1 (Step 039910): Train loss 1.247, Val loss 1.749\n",
      "Ep 1 (Step 039915): Train loss 1.064, Val loss 1.749\n",
      "Ep 1 (Step 039920): Train loss 0.975, Val loss 1.751\n",
      "Ep 1 (Step 039925): Train loss 1.202, Val loss 1.752\n",
      "Ep 1 (Step 039930): Train loss 1.294, Val loss 1.753\n",
      "Ep 1 (Step 039935): Train loss 1.049, Val loss 1.752\n",
      "Ep 1 (Step 039940): Train loss 1.055, Val loss 1.751\n",
      "Ep 1 (Step 039945): Train loss 1.232, Val loss 1.749\n",
      "Ep 1 (Step 039950): Train loss 1.145, Val loss 1.749\n",
      "Ep 1 (Step 039955): Train loss 0.927, Val loss 1.749\n",
      "Ep 1 (Step 039960): Train loss 1.242, Val loss 1.749\n",
      "Ep 1 (Step 039965): Train loss 1.173, Val loss 1.748\n",
      "Ep 1 (Step 039970): Train loss 1.046, Val loss 1.748\n",
      "Ep 1 (Step 039975): Train loss 1.289, Val loss 1.747\n",
      "Ep 1 (Step 039980): Train loss 1.154, Val loss 1.748\n",
      "Ep 1 (Step 039985): Train loss 1.368, Val loss 1.749\n",
      "Ep 1 (Step 039990): Train loss 1.202, Val loss 1.751\n",
      "Ep 1 (Step 039995): Train loss 1.154, Val loss 1.752\n",
      "Ep 1 (Step 040000): Train loss 1.296, Val loss 1.753\n",
      "Ep 1 (Step 040005): Train loss 1.410, Val loss 1.753\n",
      "Ep 1 (Step 040010): Train loss 0.805, Val loss 1.753\n",
      "Ep 1 (Step 040015): Train loss 1.280, Val loss 1.752\n",
      "Ep 1 (Step 040020): Train loss 1.362, Val loss 1.752\n",
      "Ep 1 (Step 040025): Train loss 1.151, Val loss 1.752\n",
      "Ep 1 (Step 040030): Train loss 1.143, Val loss 1.751\n",
      "Ep 1 (Step 040035): Train loss 1.014, Val loss 1.749\n",
      "Ep 1 (Step 040040): Train loss 1.190, Val loss 1.748\n",
      "Ep 1 (Step 040045): Train loss 1.047, Val loss 1.748\n",
      "Ep 1 (Step 040050): Train loss 1.110, Val loss 1.748\n",
      "Ep 1 (Step 040055): Train loss 1.348, Val loss 1.748\n",
      "Ep 1 (Step 040060): Train loss 0.948, Val loss 1.749\n",
      "Ep 1 (Step 040065): Train loss 0.991, Val loss 1.749\n",
      "Ep 1 (Step 040070): Train loss 1.148, Val loss 1.750\n",
      "Ep 1 (Step 040075): Train loss 1.020, Val loss 1.752\n",
      "Ep 1 (Step 040080): Train loss 1.126, Val loss 1.755\n",
      "Ep 1 (Step 040085): Train loss 1.066, Val loss 1.758\n",
      "Ep 1 (Step 040090): Train loss 1.203, Val loss 1.759\n",
      "Ep 1 (Step 040095): Train loss 1.050, Val loss 1.760\n",
      "Ep 1 (Step 040100): Train loss 0.881, Val loss 1.760\n",
      "Ep 1 (Step 040105): Train loss 1.130, Val loss 1.761\n",
      "Ep 1 (Step 040110): Train loss 1.093, Val loss 1.761\n",
      "Ep 1 (Step 040115): Train loss 0.796, Val loss 1.762\n",
      "Ep 1 (Step 040120): Train loss 1.055, Val loss 1.762\n",
      "Ep 1 (Step 040125): Train loss 1.274, Val loss 1.762\n",
      "Ep 1 (Step 040130): Train loss 1.123, Val loss 1.763\n",
      "Ep 1 (Step 040135): Train loss 0.958, Val loss 1.763\n",
      "Ep 1 (Step 040140): Train loss 1.133, Val loss 1.764\n",
      "Ep 1 (Step 040145): Train loss 0.996, Val loss 1.764\n",
      "Ep 1 (Step 040150): Train loss 1.254, Val loss 1.763\n",
      "Ep 1 (Step 040155): Train loss 1.365, Val loss 1.762\n",
      "Ep 1 (Step 040160): Train loss 1.202, Val loss 1.760\n",
      "Ep 1 (Step 040165): Train loss 1.276, Val loss 1.759\n",
      "Ep 1 (Step 040170): Train loss 1.146, Val loss 1.758\n",
      "Ep 1 (Step 040175): Train loss 1.294, Val loss 1.759\n",
      "Ep 1 (Step 040180): Train loss 1.144, Val loss 1.760\n",
      "Ep 1 (Step 040185): Train loss 1.053, Val loss 1.760\n",
      "Ep 1 (Step 040190): Train loss 1.016, Val loss 1.759\n",
      "Ep 1 (Step 040195): Train loss 1.118, Val loss 1.757\n",
      "Ep 1 (Step 040200): Train loss 1.332, Val loss 1.755\n",
      "Ep 1 (Step 040205): Train loss 1.174, Val loss 1.754\n",
      "Ep 1 (Step 040210): Train loss 1.094, Val loss 1.752\n",
      "Ep 1 (Step 040215): Train loss 0.895, Val loss 1.752\n",
      "Ep 1 (Step 040220): Train loss 1.308, Val loss 1.753\n",
      "Ep 1 (Step 040225): Train loss 1.128, Val loss 1.753\n",
      "Ep 1 (Step 040230): Train loss 1.247, Val loss 1.755\n",
      "Ep 1 (Step 040235): Train loss 1.100, Val loss 1.757\n",
      "Ep 1 (Step 040240): Train loss 1.244, Val loss 1.759\n",
      "Ep 1 (Step 040245): Train loss 1.474, Val loss 1.760\n",
      "Ep 1 (Step 040250): Train loss 0.810, Val loss 1.760\n",
      "Ep 1 (Step 040255): Train loss 1.138, Val loss 1.758\n",
      "Ep 1 (Step 040260): Train loss 1.372, Val loss 1.755\n",
      "Ep 1 (Step 040265): Train loss 0.879, Val loss 1.755\n",
      "Ep 1 (Step 040270): Train loss 1.067, Val loss 1.755\n",
      "Ep 1 (Step 040275): Train loss 1.211, Val loss 1.755\n",
      "Ep 1 (Step 040280): Train loss 1.065, Val loss 1.756\n",
      "Ep 1 (Step 040285): Train loss 1.141, Val loss 1.758\n",
      "Ep 1 (Step 040290): Train loss 1.029, Val loss 1.758\n",
      "Ep 1 (Step 040295): Train loss 0.883, Val loss 1.759\n",
      "Ep 1 (Step 040300): Train loss 1.093, Val loss 1.760\n",
      "Ep 1 (Step 040305): Train loss 1.041, Val loss 1.761\n",
      "Ep 1 (Step 040310): Train loss 1.348, Val loss 1.761\n",
      "Ep 1 (Step 040315): Train loss 1.086, Val loss 1.761\n",
      "Ep 1 (Step 040320): Train loss 1.467, Val loss 1.762\n",
      "Ep 1 (Step 040325): Train loss 0.893, Val loss 1.761\n",
      "Ep 1 (Step 040330): Train loss 1.285, Val loss 1.760\n",
      "Ep 1 (Step 040335): Train loss 1.096, Val loss 1.759\n",
      "Ep 1 (Step 040340): Train loss 1.176, Val loss 1.760\n",
      "Ep 1 (Step 040345): Train loss 1.162, Val loss 1.759\n",
      "Ep 1 (Step 040350): Train loss 0.956, Val loss 1.759\n",
      "Ep 1 (Step 040355): Train loss 1.092, Val loss 1.760\n",
      "Ep 1 (Step 040360): Train loss 1.147, Val loss 1.761\n",
      "Ep 1 (Step 040365): Train loss 1.017, Val loss 1.761\n",
      "Ep 1 (Step 040370): Train loss 1.103, Val loss 1.762\n",
      "Ep 1 (Step 040375): Train loss 1.273, Val loss 1.762\n",
      "Ep 1 (Step 040380): Train loss 0.999, Val loss 1.761\n",
      "Ep 1 (Step 040385): Train loss 1.207, Val loss 1.760\n",
      "Ep 1 (Step 040390): Train loss 1.024, Val loss 1.759\n",
      "Ep 1 (Step 040395): Train loss 1.119, Val loss 1.758\n",
      "Ep 1 (Step 040400): Train loss 1.118, Val loss 1.757\n",
      "Ep 1 (Step 040405): Train loss 0.986, Val loss 1.757\n",
      "Ep 1 (Step 040410): Train loss 1.117, Val loss 1.759\n",
      "Ep 1 (Step 040415): Train loss 0.995, Val loss 1.761\n",
      "Ep 1 (Step 040420): Train loss 0.971, Val loss 1.763\n",
      "Ep 1 (Step 040425): Train loss 1.086, Val loss 1.764\n",
      "Ep 1 (Step 040430): Train loss 1.058, Val loss 1.766\n",
      "Ep 1 (Step 040435): Train loss 0.954, Val loss 1.765\n",
      "Ep 1 (Step 040440): Train loss 1.323, Val loss 1.764\n",
      "Ep 1 (Step 040445): Train loss 1.039, Val loss 1.763\n",
      "Ep 1 (Step 040450): Train loss 1.316, Val loss 1.763\n",
      "Ep 1 (Step 040455): Train loss 1.116, Val loss 1.763\n",
      "Ep 1 (Step 040460): Train loss 1.493, Val loss 1.764\n",
      "Ep 1 (Step 040465): Train loss 1.354, Val loss 1.765\n",
      "Ep 1 (Step 040470): Train loss 1.231, Val loss 1.765\n",
      "Ep 1 (Step 040475): Train loss 0.979, Val loss 1.765\n",
      "Ep 1 (Step 040480): Train loss 1.048, Val loss 1.764\n",
      "Ep 1 (Step 040485): Train loss 1.060, Val loss 1.764\n",
      "Ep 1 (Step 040490): Train loss 1.049, Val loss 1.765\n",
      "Ep 1 (Step 040495): Train loss 1.403, Val loss 1.765\n",
      "Ep 1 (Step 040500): Train loss 1.084, Val loss 1.766\n",
      "Ep 1 (Step 040505): Train loss 1.022, Val loss 1.768\n",
      "Ep 1 (Step 040510): Train loss 1.136, Val loss 1.770\n",
      "Ep 1 (Step 040515): Train loss 1.287, Val loss 1.772\n",
      "Ep 1 (Step 040520): Train loss 1.091, Val loss 1.774\n",
      "Ep 1 (Step 040525): Train loss 0.735, Val loss 1.776\n",
      "Ep 1 (Step 040530): Train loss 1.296, Val loss 1.775\n",
      "Ep 1 (Step 040535): Train loss 1.121, Val loss 1.773\n",
      "Ep 1 (Step 040540): Train loss 0.958, Val loss 1.772\n",
      "Ep 1 (Step 040545): Train loss 0.923, Val loss 1.771\n",
      "Ep 1 (Step 040550): Train loss 1.051, Val loss 1.771\n",
      "Ep 1 (Step 040555): Train loss 1.060, Val loss 1.772\n",
      "Ep 1 (Step 040560): Train loss 1.125, Val loss 1.770\n",
      "Ep 1 (Step 040565): Train loss 1.074, Val loss 1.769\n",
      "Ep 1 (Step 040570): Train loss 1.080, Val loss 1.768\n",
      "Ep 1 (Step 040575): Train loss 1.252, Val loss 1.768\n",
      "Ep 1 (Step 040580): Train loss 1.020, Val loss 1.767\n",
      "Ep 1 (Step 040585): Train loss 0.978, Val loss 1.767\n",
      "Ep 1 (Step 040590): Train loss 1.008, Val loss 1.767\n",
      "Ep 1 (Step 040595): Train loss 0.931, Val loss 1.767\n",
      "Ep 1 (Step 040600): Train loss 1.104, Val loss 1.768\n",
      "Ep 1 (Step 040605): Train loss 1.022, Val loss 1.769\n",
      "Ep 1 (Step 040610): Train loss 1.170, Val loss 1.769\n",
      "Ep 1 (Step 040615): Train loss 1.064, Val loss 1.769\n",
      "Ep 1 (Step 040620): Train loss 1.140, Val loss 1.768\n",
      "Ep 1 (Step 040625): Train loss 1.001, Val loss 1.766\n",
      "Ep 1 (Step 040630): Train loss 1.180, Val loss 1.765\n",
      "Ep 1 (Step 040635): Train loss 1.147, Val loss 1.764\n",
      "Ep 1 (Step 040640): Train loss 1.278, Val loss 1.764\n",
      "Ep 1 (Step 040645): Train loss 1.058, Val loss 1.764\n",
      "Ep 1 (Step 040650): Train loss 1.220, Val loss 1.763\n",
      "Ep 1 (Step 040655): Train loss 0.984, Val loss 1.763\n",
      "Ep 1 (Step 040660): Train loss 1.274, Val loss 1.762\n",
      "Ep 1 (Step 040665): Train loss 1.023, Val loss 1.763\n",
      "Ep 1 (Step 040670): Train loss 0.974, Val loss 1.762\n",
      "Ep 1 (Step 040675): Train loss 1.068, Val loss 1.762\n",
      "Ep 1 (Step 040680): Train loss 1.120, Val loss 1.762\n",
      "Ep 1 (Step 040685): Train loss 1.394, Val loss 1.763\n",
      "Ep 1 (Step 040690): Train loss 1.454, Val loss 1.764\n",
      "Ep 1 (Step 040695): Train loss 1.460, Val loss 1.764\n",
      "Ep 1 (Step 040700): Train loss 0.946, Val loss 1.765\n",
      "Ep 1 (Step 040705): Train loss 1.021, Val loss 1.765\n",
      "Ep 1 (Step 040710): Train loss 1.088, Val loss 1.764\n",
      "Ep 1 (Step 040715): Train loss 1.572, Val loss 1.765\n",
      "Ep 1 (Step 040720): Train loss 1.164, Val loss 1.765\n",
      "Ep 1 (Step 040725): Train loss 1.107, Val loss 1.766\n",
      "Ep 1 (Step 040730): Train loss 1.129, Val loss 1.768\n",
      "Ep 1 (Step 040735): Train loss 1.109, Val loss 1.770\n",
      "Ep 1 (Step 040740): Train loss 1.170, Val loss 1.770\n",
      "Ep 1 (Step 040745): Train loss 1.068, Val loss 1.770\n",
      "Ep 1 (Step 040750): Train loss 1.256, Val loss 1.770\n",
      "Ep 1 (Step 040755): Train loss 1.107, Val loss 1.770\n",
      "Ep 1 (Step 040760): Train loss 1.068, Val loss 1.770\n",
      "Ep 1 (Step 040765): Train loss 1.125, Val loss 1.771\n",
      "Ep 1 (Step 040770): Train loss 1.187, Val loss 1.772\n",
      "Ep 1 (Step 040775): Train loss 0.971, Val loss 1.773\n",
      "Ep 1 (Step 040780): Train loss 1.252, Val loss 1.773\n",
      "Ep 1 (Step 040785): Train loss 0.996, Val loss 1.773\n",
      "Ep 1 (Step 040790): Train loss 1.326, Val loss 1.773\n",
      "Ep 1 (Step 040795): Train loss 1.054, Val loss 1.774\n",
      "Ep 1 (Step 040800): Train loss 1.013, Val loss 1.775\n",
      "Ep 1 (Step 040805): Train loss 1.263, Val loss 1.777\n",
      "Ep 1 (Step 040810): Train loss 0.900, Val loss 1.778\n",
      "Ep 1 (Step 040815): Train loss 1.099, Val loss 1.777\n",
      "Ep 1 (Step 040820): Train loss 1.106, Val loss 1.777\n",
      "Ep 1 (Step 040825): Train loss 1.030, Val loss 1.777\n",
      "Ep 1 (Step 040830): Train loss 1.109, Val loss 1.778\n",
      "Ep 1 (Step 040835): Train loss 1.084, Val loss 1.778\n",
      "Ep 1 (Step 040840): Train loss 1.023, Val loss 1.779\n",
      "Ep 1 (Step 040845): Train loss 1.045, Val loss 1.779\n",
      "Ep 1 (Step 040850): Train loss 0.840, Val loss 1.777\n",
      "Ep 1 (Step 040855): Train loss 1.033, Val loss 1.776\n",
      "Ep 1 (Step 040860): Train loss 1.091, Val loss 1.775\n",
      "Ep 1 (Step 040865): Train loss 1.263, Val loss 1.773\n",
      "Ep 1 (Step 040870): Train loss 1.033, Val loss 1.773\n",
      "Ep 1 (Step 040875): Train loss 1.386, Val loss 1.774\n",
      "Ep 1 (Step 040880): Train loss 1.083, Val loss 1.775\n",
      "Ep 1 (Step 040885): Train loss 1.190, Val loss 1.776\n",
      "Ep 1 (Step 040890): Train loss 1.254, Val loss 1.776\n",
      "Ep 1 (Step 040895): Train loss 1.173, Val loss 1.776\n",
      "Ep 1 (Step 040900): Train loss 1.109, Val loss 1.775\n",
      "Ep 1 (Step 040905): Train loss 1.084, Val loss 1.773\n",
      "Ep 1 (Step 040910): Train loss 1.277, Val loss 1.773\n",
      "Ep 1 (Step 040915): Train loss 1.143, Val loss 1.773\n",
      "Ep 1 (Step 040920): Train loss 1.070, Val loss 1.773\n",
      "Ep 1 (Step 040925): Train loss 1.330, Val loss 1.772\n",
      "Ep 1 (Step 040930): Train loss 1.335, Val loss 1.771\n",
      "Ep 1 (Step 040935): Train loss 1.298, Val loss 1.772\n",
      "Ep 1 (Step 040940): Train loss 1.239, Val loss 1.774\n",
      "Ep 1 (Step 040945): Train loss 1.351, Val loss 1.774\n",
      "Ep 1 (Step 040950): Train loss 1.104, Val loss 1.775\n",
      "Ep 1 (Step 040955): Train loss 1.274, Val loss 1.775\n",
      "Ep 1 (Step 040960): Train loss 0.983, Val loss 1.776\n",
      "Ep 1 (Step 040965): Train loss 0.970, Val loss 1.776\n",
      "Ep 1 (Step 040970): Train loss 1.126, Val loss 1.775\n",
      "Ep 1 (Step 040975): Train loss 1.082, Val loss 1.774\n",
      "Ep 1 (Step 040980): Train loss 1.078, Val loss 1.774\n",
      "Ep 1 (Step 040985): Train loss 0.892, Val loss 1.774\n",
      "Ep 1 (Step 040990): Train loss 1.070, Val loss 1.773\n",
      "Ep 1 (Step 040995): Train loss 1.147, Val loss 1.772\n",
      "Ep 1 (Step 041000): Train loss 1.078, Val loss 1.770\n",
      "Ep 1 (Step 041005): Train loss 1.375, Val loss 1.770\n",
      "Ep 1 (Step 041010): Train loss 0.932, Val loss 1.770\n",
      "Ep 1 (Step 041015): Train loss 1.098, Val loss 1.771\n",
      "Ep 1 (Step 041020): Train loss 0.948, Val loss 1.770\n",
      "Ep 1 (Step 041025): Train loss 1.290, Val loss 1.770\n",
      "Ep 1 (Step 041030): Train loss 0.977, Val loss 1.770\n",
      "Ep 1 (Step 041035): Train loss 0.946, Val loss 1.769\n",
      "Ep 1 (Step 041040): Train loss 1.212, Val loss 1.769\n",
      "Ep 1 (Step 041045): Train loss 0.994, Val loss 1.769\n",
      "Ep 1 (Step 041050): Train loss 0.887, Val loss 1.768\n",
      "Ep 1 (Step 041055): Train loss 1.068, Val loss 1.767\n",
      "Ep 1 (Step 041060): Train loss 1.341, Val loss 1.765\n",
      "Ep 1 (Step 041065): Train loss 1.045, Val loss 1.766\n",
      "Ep 1 (Step 041070): Train loss 1.209, Val loss 1.766\n",
      "Ep 1 (Step 041075): Train loss 1.178, Val loss 1.766\n",
      "Ep 1 (Step 041080): Train loss 1.525, Val loss 1.765\n",
      "Ep 1 (Step 041085): Train loss 1.088, Val loss 1.763\n",
      "Ep 1 (Step 041090): Train loss 1.404, Val loss 1.762\n",
      "Ep 1 (Step 041095): Train loss 1.075, Val loss 1.762\n",
      "Ep 1 (Step 041100): Train loss 1.165, Val loss 1.762\n",
      "Ep 1 (Step 041105): Train loss 1.242, Val loss 1.763\n",
      "Ep 1 (Step 041110): Train loss 1.022, Val loss 1.764\n",
      "Ep 1 (Step 041115): Train loss 0.844, Val loss 1.764\n",
      "Ep 1 (Step 041120): Train loss 1.100, Val loss 1.764\n",
      "Ep 1 (Step 041125): Train loss 1.272, Val loss 1.765\n",
      "Ep 1 (Step 041130): Train loss 0.863, Val loss 1.766\n",
      "Ep 1 (Step 041135): Train loss 1.195, Val loss 1.766\n",
      "Ep 1 (Step 041140): Train loss 1.257, Val loss 1.766\n",
      "Ep 1 (Step 041145): Train loss 1.233, Val loss 1.766\n",
      "Ep 1 (Step 041150): Train loss 1.162, Val loss 1.766\n",
      "Ep 1 (Step 041155): Train loss 0.964, Val loss 1.765\n",
      "Ep 1 (Step 041160): Train loss 1.200, Val loss 1.763\n",
      "Ep 1 (Step 041165): Train loss 1.297, Val loss 1.763\n",
      "Ep 1 (Step 041170): Train loss 1.260, Val loss 1.763\n",
      "Ep 1 (Step 041175): Train loss 0.999, Val loss 1.764\n",
      "Ep 1 (Step 041180): Train loss 1.160, Val loss 1.764\n",
      "Ep 1 (Step 041185): Train loss 0.993, Val loss 1.763\n",
      "Ep 1 (Step 041190): Train loss 0.982, Val loss 1.762\n",
      "Ep 1 (Step 041195): Train loss 1.212, Val loss 1.760\n",
      "Ep 1 (Step 041200): Train loss 0.999, Val loss 1.758\n",
      "Ep 1 (Step 041205): Train loss 0.925, Val loss 1.758\n",
      "Ep 1 (Step 041210): Train loss 1.062, Val loss 1.758\n",
      "Ep 1 (Step 041215): Train loss 1.036, Val loss 1.758\n",
      "Ep 1 (Step 041220): Train loss 1.012, Val loss 1.759\n",
      "Ep 1 (Step 041225): Train loss 1.178, Val loss 1.759\n",
      "Ep 1 (Step 041230): Train loss 1.073, Val loss 1.760\n",
      "Ep 1 (Step 041235): Train loss 1.199, Val loss 1.760\n",
      "Ep 1 (Step 041240): Train loss 1.041, Val loss 1.759\n",
      "Ep 1 (Step 041245): Train loss 0.882, Val loss 1.758\n",
      "Ep 1 (Step 041250): Train loss 0.959, Val loss 1.758\n",
      "Ep 1 (Step 041255): Train loss 1.187, Val loss 1.759\n",
      "Ep 1 (Step 041260): Train loss 1.401, Val loss 1.759\n",
      "Ep 1 (Step 041265): Train loss 0.987, Val loss 1.759\n",
      "Ep 1 (Step 041270): Train loss 1.455, Val loss 1.759\n",
      "Ep 1 (Step 041275): Train loss 1.209, Val loss 1.761\n",
      "Ep 1 (Step 041280): Train loss 1.155, Val loss 1.762\n",
      "Ep 1 (Step 041285): Train loss 0.849, Val loss 1.763\n",
      "Ep 1 (Step 041290): Train loss 1.381, Val loss 1.762\n",
      "Ep 1 (Step 041295): Train loss 0.911, Val loss 1.763\n",
      "Ep 1 (Step 041300): Train loss 1.120, Val loss 1.765\n",
      "Ep 1 (Step 041305): Train loss 1.055, Val loss 1.766\n",
      "Ep 1 (Step 041310): Train loss 1.118, Val loss 1.765\n",
      "Ep 1 (Step 041315): Train loss 0.932, Val loss 1.764\n",
      "Ep 1 (Step 041320): Train loss 1.312, Val loss 1.762\n",
      "Ep 1 (Step 041325): Train loss 1.298, Val loss 1.760\n",
      "Ep 1 (Step 041330): Train loss 0.964, Val loss 1.759\n",
      "Ep 1 (Step 041335): Train loss 1.184, Val loss 1.758\n",
      "Ep 1 (Step 041340): Train loss 1.042, Val loss 1.756\n",
      "Ep 1 (Step 041345): Train loss 0.995, Val loss 1.755\n",
      "Ep 1 (Step 041350): Train loss 1.015, Val loss 1.755\n",
      "Ep 1 (Step 041355): Train loss 1.189, Val loss 1.755\n",
      "Ep 1 (Step 041360): Train loss 1.190, Val loss 1.755\n",
      "Ep 1 (Step 041365): Train loss 1.045, Val loss 1.754\n",
      "Ep 1 (Step 041370): Train loss 1.038, Val loss 1.754\n",
      "Ep 1 (Step 041375): Train loss 1.158, Val loss 1.755\n",
      "Ep 1 (Step 041380): Train loss 0.819, Val loss 1.757\n",
      "Ep 1 (Step 041385): Train loss 1.139, Val loss 1.757\n",
      "Ep 1 (Step 041390): Train loss 0.941, Val loss 1.757\n",
      "Ep 1 (Step 041395): Train loss 1.277, Val loss 1.757\n",
      "Ep 1 (Step 041400): Train loss 1.245, Val loss 1.757\n",
      "Ep 1 (Step 041405): Train loss 1.242, Val loss 1.756\n",
      "Ep 1 (Step 041410): Train loss 1.251, Val loss 1.755\n",
      "Ep 1 (Step 041415): Train loss 1.061, Val loss 1.756\n",
      "Ep 1 (Step 041420): Train loss 1.033, Val loss 1.755\n",
      "Ep 1 (Step 041425): Train loss 1.251, Val loss 1.755\n",
      "Ep 1 (Step 041430): Train loss 0.917, Val loss 1.755\n",
      "Ep 1 (Step 041435): Train loss 0.866, Val loss 1.756\n",
      "Ep 1 (Step 041440): Train loss 1.196, Val loss 1.756\n",
      "Ep 1 (Step 041445): Train loss 1.256, Val loss 1.755\n",
      "Ep 1 (Step 041450): Train loss 1.303, Val loss 1.755\n",
      "Ep 1 (Step 041455): Train loss 1.283, Val loss 1.756\n",
      "Ep 1 (Step 041460): Train loss 1.209, Val loss 1.756\n",
      "Ep 1 (Step 041465): Train loss 0.996, Val loss 1.757\n",
      "Ep 1 (Step 041470): Train loss 1.188, Val loss 1.758\n",
      "Ep 1 (Step 041475): Train loss 1.181, Val loss 1.759\n",
      "Ep 1 (Step 041480): Train loss 1.118, Val loss 1.761\n",
      "Ep 1 (Step 041485): Train loss 1.453, Val loss 1.763\n",
      "Ep 1 (Step 041490): Train loss 1.044, Val loss 1.765\n",
      "Ep 1 (Step 041495): Train loss 0.884, Val loss 1.766\n",
      "Ep 1 (Step 041500): Train loss 1.256, Val loss 1.767\n",
      "Ep 1 (Step 041505): Train loss 1.116, Val loss 1.766\n",
      "Ep 1 (Step 041510): Train loss 1.074, Val loss 1.766\n",
      "Ep 1 (Step 041515): Train loss 0.860, Val loss 1.765\n",
      "Ep 1 (Step 041520): Train loss 0.994, Val loss 1.763\n",
      "Ep 1 (Step 041525): Train loss 1.073, Val loss 1.761\n",
      "Ep 1 (Step 041530): Train loss 0.945, Val loss 1.760\n",
      "Ep 1 (Step 041535): Train loss 0.972, Val loss 1.760\n",
      "Ep 1 (Step 041540): Train loss 1.023, Val loss 1.759\n",
      "Ep 1 (Step 041545): Train loss 1.232, Val loss 1.759\n",
      "Ep 1 (Step 041550): Train loss 1.073, Val loss 1.760\n",
      "Ep 1 (Step 041555): Train loss 1.179, Val loss 1.761\n",
      "Ep 1 (Step 041560): Train loss 1.313, Val loss 1.761\n",
      "Ep 1 (Step 041565): Train loss 1.124, Val loss 1.761\n",
      "Ep 1 (Step 041570): Train loss 1.085, Val loss 1.762\n",
      "Ep 1 (Step 041575): Train loss 0.958, Val loss 1.762\n",
      "Ep 1 (Step 041580): Train loss 1.315, Val loss 1.762\n",
      "Ep 1 (Step 041585): Train loss 1.323, Val loss 1.762\n",
      "Ep 1 (Step 041590): Train loss 1.137, Val loss 1.761\n",
      "Ep 1 (Step 041595): Train loss 1.201, Val loss 1.760\n",
      "Ep 1 (Step 041600): Train loss 1.184, Val loss 1.759\n",
      "Ep 1 (Step 041605): Train loss 0.877, Val loss 1.757\n",
      "Ep 1 (Step 041610): Train loss 1.213, Val loss 1.756\n",
      "Ep 1 (Step 041615): Train loss 1.233, Val loss 1.755\n",
      "Ep 1 (Step 041620): Train loss 1.138, Val loss 1.757\n",
      "Ep 1 (Step 041625): Train loss 1.208, Val loss 1.758\n",
      "Ep 1 (Step 041630): Train loss 1.152, Val loss 1.759\n",
      "Ep 1 (Step 041635): Train loss 1.162, Val loss 1.759\n",
      "Ep 1 (Step 041640): Train loss 0.934, Val loss 1.759\n",
      "Ep 1 (Step 041645): Train loss 0.919, Val loss 1.760\n",
      "Ep 1 (Step 041650): Train loss 1.124, Val loss 1.761\n",
      "Ep 1 (Step 041655): Train loss 0.936, Val loss 1.761\n",
      "Ep 1 (Step 041660): Train loss 1.345, Val loss 1.760\n",
      "Ep 1 (Step 041665): Train loss 1.194, Val loss 1.757\n",
      "Ep 1 (Step 041670): Train loss 0.860, Val loss 1.754\n",
      "Ep 1 (Step 041675): Train loss 1.114, Val loss 1.752\n",
      "Ep 1 (Step 041680): Train loss 1.017, Val loss 1.749\n",
      "Ep 1 (Step 041685): Train loss 1.361, Val loss 1.748\n",
      "Ep 1 (Step 041690): Train loss 0.919, Val loss 1.747\n",
      "Ep 1 (Step 041695): Train loss 1.351, Val loss 1.744\n",
      "Ep 1 (Step 041700): Train loss 1.063, Val loss 1.743\n",
      "Ep 1 (Step 041705): Train loss 0.966, Val loss 1.743\n",
      "Ep 1 (Step 041710): Train loss 1.464, Val loss 1.743\n",
      "Ep 1 (Step 041715): Train loss 0.910, Val loss 1.745\n",
      "Ep 1 (Step 041720): Train loss 1.240, Val loss 1.746\n",
      "Ep 1 (Step 041725): Train loss 1.170, Val loss 1.747\n",
      "Ep 1 (Step 041730): Train loss 0.796, Val loss 1.748\n",
      "Ep 1 (Step 041735): Train loss 0.953, Val loss 1.749\n",
      "Ep 1 (Step 041740): Train loss 1.124, Val loss 1.748\n",
      "Ep 1 (Step 041745): Train loss 1.059, Val loss 1.749\n",
      "Ep 1 (Step 041750): Train loss 1.261, Val loss 1.749\n",
      "Ep 1 (Step 041755): Train loss 1.183, Val loss 1.749\n",
      "Ep 1 (Step 041760): Train loss 0.911, Val loss 1.749\n",
      "Ep 1 (Step 041765): Train loss 0.880, Val loss 1.751\n",
      "Ep 1 (Step 041770): Train loss 1.108, Val loss 1.751\n",
      "Ep 1 (Step 041775): Train loss 0.931, Val loss 1.751\n",
      "Ep 1 (Step 041780): Train loss 1.197, Val loss 1.750\n",
      "Ep 1 (Step 041785): Train loss 1.262, Val loss 1.748\n",
      "Ep 1 (Step 041790): Train loss 0.958, Val loss 1.746\n",
      "Ep 1 (Step 041795): Train loss 0.949, Val loss 1.746\n",
      "Ep 1 (Step 041800): Train loss 1.168, Val loss 1.745\n",
      "Ep 1 (Step 041805): Train loss 1.287, Val loss 1.745\n",
      "Ep 1 (Step 041810): Train loss 0.986, Val loss 1.745\n",
      "Ep 1 (Step 041815): Train loss 1.119, Val loss 1.745\n",
      "Ep 1 (Step 041820): Train loss 1.013, Val loss 1.745\n",
      "Ep 1 (Step 041825): Train loss 1.265, Val loss 1.746\n",
      "Ep 1 (Step 041830): Train loss 1.038, Val loss 1.746\n",
      "Ep 1 (Step 041835): Train loss 1.155, Val loss 1.747\n",
      "Ep 1 (Step 041840): Train loss 1.062, Val loss 1.746\n",
      "Ep 1 (Step 041845): Train loss 1.508, Val loss 1.746\n",
      "Ep 1 (Step 041850): Train loss 0.952, Val loss 1.745\n",
      "Ep 1 (Step 041855): Train loss 1.154, Val loss 1.745\n",
      "Ep 1 (Step 041860): Train loss 1.055, Val loss 1.745\n",
      "Ep 1 (Step 041865): Train loss 1.269, Val loss 1.746\n",
      "Ep 1 (Step 041870): Train loss 1.368, Val loss 1.747\n",
      "Ep 1 (Step 041875): Train loss 1.137, Val loss 1.747\n",
      "Ep 1 (Step 041880): Train loss 0.984, Val loss 1.746\n",
      "Ep 1 (Step 041885): Train loss 1.035, Val loss 1.746\n",
      "Ep 1 (Step 041890): Train loss 1.206, Val loss 1.745\n",
      "Ep 1 (Step 041895): Train loss 0.989, Val loss 1.746\n",
      "Ep 1 (Step 041900): Train loss 1.376, Val loss 1.746\n",
      "Ep 1 (Step 041905): Train loss 1.139, Val loss 1.746\n",
      "Ep 1 (Step 041910): Train loss 1.174, Val loss 1.749\n",
      "Ep 1 (Step 041915): Train loss 1.088, Val loss 1.751\n",
      "Ep 1 (Step 041920): Train loss 1.132, Val loss 1.752\n",
      "Ep 1 (Step 041925): Train loss 1.135, Val loss 1.753\n",
      "Ep 1 (Step 041930): Train loss 1.062, Val loss 1.753\n",
      "Ep 1 (Step 041935): Train loss 0.807, Val loss 1.752\n",
      "Ep 1 (Step 041940): Train loss 1.092, Val loss 1.752\n",
      "Ep 1 (Step 041945): Train loss 1.094, Val loss 1.752\n",
      "Ep 1 (Step 041950): Train loss 1.164, Val loss 1.752\n",
      "Ep 1 (Step 041955): Train loss 1.364, Val loss 1.751\n",
      "Ep 1 (Step 041960): Train loss 1.183, Val loss 1.750\n",
      "Ep 1 (Step 041965): Train loss 1.275, Val loss 1.750\n",
      "Ep 1 (Step 041970): Train loss 0.913, Val loss 1.751\n",
      "Ep 1 (Step 041975): Train loss 1.148, Val loss 1.751\n",
      "Ep 1 (Step 041980): Train loss 1.072, Val loss 1.750\n",
      "Ep 1 (Step 041985): Train loss 1.013, Val loss 1.751\n",
      "Ep 1 (Step 041990): Train loss 0.921, Val loss 1.751\n",
      "Ep 1 (Step 041995): Train loss 0.891, Val loss 1.751\n",
      "Ep 1 (Step 042000): Train loss 1.020, Val loss 1.751\n",
      "Ep 1 (Step 042005): Train loss 1.197, Val loss 1.752\n",
      "Ep 1 (Step 042010): Train loss 1.091, Val loss 1.753\n",
      "Ep 1 (Step 042015): Train loss 1.114, Val loss 1.754\n",
      "Ep 1 (Step 042020): Train loss 0.948, Val loss 1.755\n",
      "Ep 1 (Step 042025): Train loss 0.984, Val loss 1.755\n",
      "Ep 1 (Step 042030): Train loss 0.876, Val loss 1.755\n",
      "Ep 1 (Step 042035): Train loss 0.862, Val loss 1.755\n",
      "Ep 1 (Step 042040): Train loss 1.175, Val loss 1.755\n",
      "Ep 1 (Step 042045): Train loss 1.102, Val loss 1.755\n",
      "Ep 1 (Step 042050): Train loss 1.123, Val loss 1.754\n",
      "Ep 1 (Step 042055): Train loss 1.224, Val loss 1.754\n",
      "Ep 1 (Step 042060): Train loss 1.428, Val loss 1.755\n",
      "Ep 1 (Step 042065): Train loss 1.106, Val loss 1.755\n",
      "Ep 1 (Step 042070): Train loss 1.068, Val loss 1.754\n",
      "Ep 1 (Step 042075): Train loss 1.014, Val loss 1.754\n",
      "Ep 1 (Step 042080): Train loss 1.205, Val loss 1.756\n",
      "Ep 1 (Step 042085): Train loss 0.930, Val loss 1.756\n",
      "Ep 1 (Step 042090): Train loss 1.320, Val loss 1.755\n",
      "Ep 1 (Step 042095): Train loss 1.054, Val loss 1.755\n",
      "Ep 1 (Step 042100): Train loss 1.125, Val loss 1.754\n",
      "Ep 1 (Step 042105): Train loss 0.745, Val loss 1.753\n",
      "Ep 1 (Step 042110): Train loss 0.947, Val loss 1.753\n",
      "Ep 1 (Step 042115): Train loss 1.135, Val loss 1.752\n",
      "Ep 1 (Step 042120): Train loss 0.950, Val loss 1.753\n",
      "Ep 1 (Step 042125): Train loss 1.240, Val loss 1.755\n",
      "Ep 1 (Step 042130): Train loss 1.200, Val loss 1.756\n",
      "Ep 1 (Step 042135): Train loss 1.074, Val loss 1.756\n",
      "Ep 1 (Step 042140): Train loss 1.357, Val loss 1.756\n",
      "Ep 1 (Step 042145): Train loss 1.291, Val loss 1.756\n",
      "Ep 1 (Step 042150): Train loss 1.030, Val loss 1.756\n",
      "Ep 1 (Step 042155): Train loss 1.437, Val loss 1.756\n",
      "Ep 1 (Step 042160): Train loss 1.318, Val loss 1.756\n",
      "Ep 1 (Step 042165): Train loss 1.017, Val loss 1.757\n",
      "Ep 1 (Step 042170): Train loss 1.154, Val loss 1.758\n",
      "Ep 1 (Step 042175): Train loss 1.139, Val loss 1.759\n",
      "Ep 1 (Step 042180): Train loss 1.228, Val loss 1.759\n",
      "Ep 1 (Step 042185): Train loss 1.162, Val loss 1.761\n",
      "Ep 1 (Step 042190): Train loss 0.855, Val loss 1.761\n",
      "Ep 1 (Step 042195): Train loss 1.192, Val loss 1.761\n",
      "Ep 1 (Step 042200): Train loss 1.083, Val loss 1.760\n",
      "Ep 1 (Step 042205): Train loss 1.133, Val loss 1.760\n",
      "Ep 1 (Step 042210): Train loss 1.002, Val loss 1.760\n",
      "Ep 1 (Step 042215): Train loss 1.160, Val loss 1.761\n",
      "Ep 1 (Step 042220): Train loss 1.194, Val loss 1.763\n",
      "Ep 1 (Step 042225): Train loss 1.227, Val loss 1.763\n",
      "Ep 1 (Step 042230): Train loss 1.127, Val loss 1.761\n",
      "Ep 1 (Step 042235): Train loss 0.911, Val loss 1.759\n",
      "Ep 1 (Step 042240): Train loss 0.859, Val loss 1.756\n",
      "Ep 1 (Step 042245): Train loss 1.235, Val loss 1.754\n",
      "Ep 1 (Step 042250): Train loss 1.112, Val loss 1.753\n",
      "Ep 1 (Step 042255): Train loss 1.138, Val loss 1.754\n",
      "Ep 1 (Step 042260): Train loss 1.228, Val loss 1.754\n",
      "Ep 1 (Step 042265): Train loss 1.015, Val loss 1.755\n",
      "Ep 1 (Step 042270): Train loss 0.930, Val loss 1.755\n",
      "Ep 1 (Step 042275): Train loss 1.025, Val loss 1.756\n",
      "Ep 1 (Step 042280): Train loss 0.777, Val loss 1.756\n",
      "Ep 1 (Step 042285): Train loss 1.200, Val loss 1.754\n",
      "Ep 1 (Step 042290): Train loss 0.839, Val loss 1.752\n",
      "Ep 1 (Step 042295): Train loss 1.199, Val loss 1.752\n",
      "Ep 1 (Step 042300): Train loss 1.206, Val loss 1.753\n",
      "Ep 1 (Step 042305): Train loss 1.144, Val loss 1.754\n",
      "Ep 1 (Step 042310): Train loss 1.249, Val loss 1.756\n",
      "Ep 1 (Step 042315): Train loss 1.024, Val loss 1.756\n",
      "Ep 1 (Step 042320): Train loss 0.942, Val loss 1.755\n",
      "Ep 1 (Step 042325): Train loss 1.230, Val loss 1.756\n",
      "Ep 1 (Step 042330): Train loss 1.248, Val loss 1.757\n",
      "Ep 1 (Step 042335): Train loss 0.960, Val loss 1.758\n",
      "Ep 1 (Step 042340): Train loss 1.187, Val loss 1.758\n",
      "Ep 1 (Step 042345): Train loss 0.962, Val loss 1.758\n",
      "Ep 1 (Step 042350): Train loss 1.018, Val loss 1.757\n",
      "Ep 1 (Step 042355): Train loss 1.354, Val loss 1.757\n",
      "Ep 1 (Step 042360): Train loss 1.158, Val loss 1.757\n",
      "Ep 1 (Step 042365): Train loss 0.937, Val loss 1.758\n",
      "Ep 1 (Step 042370): Train loss 1.251, Val loss 1.760\n",
      "Ep 1 (Step 042375): Train loss 0.806, Val loss 1.762\n",
      "Ep 1 (Step 042380): Train loss 1.245, Val loss 1.763\n",
      "Ep 1 (Step 042385): Train loss 1.393, Val loss 1.764\n",
      "Ep 1 (Step 042390): Train loss 1.156, Val loss 1.764\n",
      "Ep 1 (Step 042395): Train loss 0.960, Val loss 1.764\n",
      "Ep 1 (Step 042400): Train loss 1.045, Val loss 1.765\n",
      "Ep 1 (Step 042405): Train loss 1.042, Val loss 1.767\n",
      "Ep 1 (Step 042410): Train loss 1.011, Val loss 1.767\n",
      "Ep 1 (Step 042415): Train loss 1.110, Val loss 1.766\n",
      "Ep 1 (Step 042420): Train loss 1.252, Val loss 1.765\n",
      "Ep 1 (Step 042425): Train loss 1.159, Val loss 1.764\n",
      "Ep 1 (Step 042430): Train loss 1.142, Val loss 1.764\n",
      "Ep 1 (Step 042435): Train loss 1.102, Val loss 1.760\n",
      "Ep 1 (Step 042440): Train loss 1.254, Val loss 1.758\n",
      "Ep 1 (Step 042445): Train loss 1.047, Val loss 1.758\n",
      "Ep 1 (Step 042450): Train loss 1.059, Val loss 1.758\n",
      "Ep 1 (Step 042455): Train loss 0.835, Val loss 1.758\n",
      "Ep 1 (Step 042460): Train loss 1.100, Val loss 1.759\n",
      "Ep 1 (Step 042465): Train loss 1.022, Val loss 1.759\n",
      "Ep 1 (Step 042470): Train loss 1.030, Val loss 1.760\n",
      "Ep 1 (Step 042475): Train loss 1.283, Val loss 1.762\n",
      "Ep 1 (Step 042480): Train loss 0.855, Val loss 1.764\n",
      "Ep 1 (Step 042485): Train loss 1.293, Val loss 1.766\n",
      "Ep 1 (Step 042490): Train loss 0.832, Val loss 1.767\n",
      "Ep 1 (Step 042495): Train loss 1.087, Val loss 1.769\n",
      "Ep 1 (Step 042500): Train loss 1.041, Val loss 1.771\n",
      "Ep 1 (Step 042505): Train loss 1.284, Val loss 1.772\n",
      "Ep 1 (Step 042510): Train loss 1.360, Val loss 1.771\n",
      "Ep 1 (Step 042515): Train loss 1.083, Val loss 1.770\n",
      "Ep 1 (Step 042520): Train loss 0.900, Val loss 1.769\n",
      "Ep 1 (Step 042525): Train loss 0.950, Val loss 1.769\n",
      "Ep 1 (Step 042530): Train loss 1.202, Val loss 1.770\n",
      "Ep 1 (Step 042535): Train loss 1.493, Val loss 1.770\n",
      "Ep 1 (Step 042540): Train loss 1.309, Val loss 1.771\n",
      "Ep 1 (Step 042545): Train loss 1.141, Val loss 1.770\n",
      "Ep 1 (Step 042550): Train loss 1.057, Val loss 1.769\n",
      "Ep 1 (Step 042555): Train loss 1.037, Val loss 1.767\n",
      "Ep 1 (Step 042560): Train loss 1.127, Val loss 1.765\n",
      "Ep 1 (Step 042565): Train loss 1.484, Val loss 1.763\n",
      "Ep 1 (Step 042570): Train loss 1.230, Val loss 1.762\n",
      "Ep 1 (Step 042575): Train loss 1.063, Val loss 1.761\n",
      "Ep 1 (Step 042580): Train loss 1.185, Val loss 1.761\n",
      "Ep 1 (Step 042585): Train loss 0.926, Val loss 1.759\n",
      "Ep 1 (Step 042590): Train loss 1.138, Val loss 1.759\n",
      "Ep 1 (Step 042595): Train loss 1.142, Val loss 1.760\n",
      "Ep 1 (Step 042600): Train loss 1.027, Val loss 1.760\n",
      "Ep 1 (Step 042605): Train loss 1.027, Val loss 1.760\n",
      "Ep 1 (Step 042610): Train loss 1.405, Val loss 1.760\n",
      "Ep 1 (Step 042615): Train loss 1.074, Val loss 1.761\n",
      "Ep 1 (Step 042620): Train loss 0.920, Val loss 1.763\n",
      "Ep 1 (Step 042625): Train loss 0.995, Val loss 1.764\n",
      "Ep 1 (Step 042630): Train loss 1.392, Val loss 1.765\n",
      "Ep 1 (Step 042635): Train loss 1.134, Val loss 1.765\n",
      "Ep 1 (Step 042640): Train loss 1.095, Val loss 1.766\n",
      "Ep 1 (Step 042645): Train loss 1.051, Val loss 1.767\n",
      "Ep 1 (Step 042650): Train loss 1.121, Val loss 1.767\n",
      "Ep 1 (Step 042655): Train loss 0.859, Val loss 1.767\n",
      "Ep 1 (Step 042660): Train loss 1.199, Val loss 1.765\n",
      "Ep 1 (Step 042665): Train loss 0.967, Val loss 1.764\n",
      "Ep 1 (Step 042670): Train loss 1.095, Val loss 1.763\n",
      "Ep 1 (Step 042675): Train loss 1.282, Val loss 1.761\n",
      "Ep 1 (Step 042680): Train loss 1.165, Val loss 1.760\n",
      "Ep 1 (Step 042685): Train loss 0.968, Val loss 1.759\n",
      "Ep 1 (Step 042690): Train loss 1.029, Val loss 1.759\n",
      "Ep 1 (Step 042695): Train loss 1.167, Val loss 1.758\n",
      "Ep 1 (Step 042700): Train loss 1.026, Val loss 1.757\n",
      "Ep 1 (Step 042705): Train loss 0.987, Val loss 1.757\n",
      "Ep 1 (Step 042710): Train loss 1.168, Val loss 1.756\n",
      "Ep 1 (Step 042715): Train loss 0.989, Val loss 1.754\n",
      "Ep 1 (Step 042720): Train loss 1.122, Val loss 1.753\n",
      "Ep 1 (Step 042725): Train loss 0.875, Val loss 1.754\n",
      "Ep 1 (Step 042730): Train loss 1.351, Val loss 1.756\n",
      "Ep 1 (Step 042735): Train loss 0.999, Val loss 1.758\n",
      "Ep 1 (Step 042740): Train loss 0.886, Val loss 1.763\n",
      "Ep 1 (Step 042745): Train loss 1.219, Val loss 1.766\n",
      "Ep 1 (Step 042750): Train loss 1.102, Val loss 1.768\n",
      "Ep 1 (Step 042755): Train loss 1.260, Val loss 1.769\n",
      "Ep 1 (Step 042760): Train loss 1.072, Val loss 1.771\n",
      "Ep 1 (Step 042765): Train loss 0.987, Val loss 1.771\n",
      "Ep 1 (Step 042770): Train loss 1.186, Val loss 1.770\n",
      "Ep 1 (Step 042775): Train loss 0.974, Val loss 1.769\n",
      "Ep 1 (Step 042780): Train loss 1.110, Val loss 1.768\n",
      "Ep 1 (Step 042785): Train loss 1.252, Val loss 1.767\n",
      "Ep 1 (Step 042790): Train loss 1.093, Val loss 1.766\n",
      "Ep 1 (Step 042795): Train loss 1.240, Val loss 1.763\n",
      "Ep 1 (Step 042800): Train loss 1.155, Val loss 1.761\n",
      "Ep 1 (Step 042805): Train loss 1.018, Val loss 1.758\n",
      "Ep 1 (Step 042810): Train loss 1.065, Val loss 1.755\n",
      "Ep 1 (Step 042815): Train loss 0.979, Val loss 1.754\n",
      "Ep 1 (Step 042820): Train loss 0.929, Val loss 1.754\n",
      "Ep 1 (Step 042825): Train loss 0.996, Val loss 1.753\n",
      "Ep 1 (Step 042830): Train loss 1.063, Val loss 1.752\n",
      "Ep 1 (Step 042835): Train loss 1.155, Val loss 1.753\n",
      "Ep 1 (Step 042840): Train loss 1.101, Val loss 1.754\n",
      "Ep 1 (Step 042845): Train loss 1.085, Val loss 1.756\n",
      "Ep 1 (Step 042850): Train loss 0.975, Val loss 1.758\n",
      "Ep 1 (Step 042855): Train loss 1.131, Val loss 1.759\n",
      "Ep 1 (Step 042860): Train loss 1.087, Val loss 1.760\n",
      "Ep 1 (Step 042865): Train loss 1.113, Val loss 1.762\n",
      "Ep 1 (Step 042870): Train loss 1.072, Val loss 1.763\n",
      "Ep 1 (Step 042875): Train loss 1.408, Val loss 1.765\n",
      "Ep 1 (Step 042880): Train loss 0.966, Val loss 1.765\n",
      "Ep 1 (Step 042885): Train loss 1.064, Val loss 1.765\n",
      "Ep 1 (Step 042890): Train loss 0.940, Val loss 1.762\n",
      "Ep 1 (Step 042895): Train loss 1.220, Val loss 1.759\n",
      "Ep 1 (Step 042900): Train loss 1.361, Val loss 1.758\n",
      "Ep 1 (Step 042905): Train loss 1.112, Val loss 1.759\n",
      "Ep 1 (Step 042910): Train loss 1.242, Val loss 1.759\n",
      "Ep 1 (Step 042915): Train loss 1.187, Val loss 1.758\n",
      "Ep 1 (Step 042920): Train loss 0.905, Val loss 1.758\n",
      "Ep 1 (Step 042925): Train loss 1.021, Val loss 1.759\n",
      "Ep 1 (Step 042930): Train loss 0.902, Val loss 1.760\n",
      "Ep 1 (Step 042935): Train loss 0.911, Val loss 1.761\n",
      "Ep 1 (Step 042940): Train loss 1.100, Val loss 1.762\n",
      "Ep 1 (Step 042945): Train loss 1.172, Val loss 1.763\n",
      "Ep 1 (Step 042950): Train loss 1.109, Val loss 1.766\n",
      "Ep 1 (Step 042955): Train loss 0.909, Val loss 1.767\n",
      "Ep 1 (Step 042960): Train loss 1.178, Val loss 1.769\n",
      "Ep 1 (Step 042965): Train loss 1.135, Val loss 1.770\n",
      "Ep 1 (Step 042970): Train loss 1.070, Val loss 1.770\n",
      "Ep 1 (Step 042975): Train loss 1.132, Val loss 1.770\n",
      "Ep 1 (Step 042980): Train loss 1.196, Val loss 1.770\n",
      "Ep 1 (Step 042985): Train loss 1.048, Val loss 1.768\n",
      "Ep 1 (Step 042990): Train loss 0.803, Val loss 1.767\n",
      "Ep 1 (Step 042995): Train loss 1.147, Val loss 1.767\n",
      "Ep 1 (Step 043000): Train loss 0.921, Val loss 1.767\n",
      "Ep 1 (Step 043005): Train loss 0.904, Val loss 1.768\n",
      "Ep 1 (Step 043010): Train loss 1.109, Val loss 1.769\n",
      "Ep 1 (Step 043015): Train loss 1.168, Val loss 1.769\n",
      "Ep 1 (Step 043020): Train loss 1.027, Val loss 1.768\n",
      "Ep 1 (Step 043025): Train loss 1.128, Val loss 1.767\n",
      "Ep 1 (Step 043030): Train loss 1.586, Val loss 1.766\n",
      "Ep 1 (Step 043035): Train loss 1.203, Val loss 1.767\n",
      "Ep 1 (Step 043040): Train loss 1.297, Val loss 1.768\n",
      "Ep 1 (Step 043045): Train loss 1.262, Val loss 1.767\n",
      "Ep 1 (Step 043050): Train loss 1.058, Val loss 1.767\n",
      "Ep 1 (Step 043055): Train loss 0.721, Val loss 1.767\n",
      "Ep 1 (Step 043060): Train loss 1.095, Val loss 1.766\n",
      "Ep 1 (Step 043065): Train loss 0.961, Val loss 1.766\n",
      "Ep 1 (Step 043070): Train loss 1.128, Val loss 1.767\n",
      "Ep 1 (Step 043075): Train loss 1.042, Val loss 1.768\n",
      "Ep 1 (Step 043080): Train loss 1.031, Val loss 1.767\n",
      "Ep 1 (Step 043085): Train loss 1.068, Val loss 1.765\n",
      "Ep 1 (Step 043090): Train loss 1.144, Val loss 1.765\n",
      "Ep 1 (Step 043095): Train loss 1.102, Val loss 1.766\n",
      "Ep 1 (Step 043100): Train loss 1.137, Val loss 1.767\n",
      "Ep 1 (Step 043105): Train loss 0.812, Val loss 1.767\n",
      "Ep 1 (Step 043110): Train loss 1.353, Val loss 1.768\n",
      "Ep 1 (Step 043115): Train loss 1.128, Val loss 1.768\n",
      "Ep 1 (Step 043120): Train loss 1.141, Val loss 1.768\n",
      "Ep 1 (Step 043125): Train loss 1.589, Val loss 1.769\n",
      "Ep 1 (Step 043130): Train loss 1.194, Val loss 1.768\n",
      "Ep 1 (Step 043135): Train loss 1.056, Val loss 1.768\n",
      "Ep 1 (Step 043140): Train loss 1.104, Val loss 1.767\n",
      "Ep 1 (Step 043145): Train loss 1.134, Val loss 1.765\n",
      "Ep 1 (Step 043150): Train loss 0.981, Val loss 1.764\n",
      "Ep 1 (Step 043155): Train loss 1.202, Val loss 1.764\n",
      "Ep 1 (Step 043160): Train loss 1.117, Val loss 1.764\n",
      "Ep 1 (Step 043165): Train loss 1.103, Val loss 1.764\n",
      "Ep 1 (Step 043170): Train loss 0.854, Val loss 1.765\n",
      "Ep 1 (Step 043175): Train loss 1.094, Val loss 1.766\n",
      "Ep 1 (Step 043180): Train loss 0.905, Val loss 1.767\n",
      "Ep 1 (Step 043185): Train loss 1.394, Val loss 1.769\n",
      "Ep 1 (Step 043190): Train loss 0.920, Val loss 1.773\n",
      "Ep 1 (Step 043195): Train loss 1.140, Val loss 1.774\n",
      "Ep 1 (Step 043200): Train loss 1.327, Val loss 1.775\n",
      "Ep 1 (Step 043205): Train loss 0.913, Val loss 1.776\n",
      "Ep 1 (Step 043210): Train loss 0.991, Val loss 1.776\n",
      "Ep 1 (Step 043215): Train loss 1.178, Val loss 1.775\n",
      "Ep 1 (Step 043220): Train loss 1.035, Val loss 1.775\n",
      "Ep 1 (Step 043225): Train loss 1.072, Val loss 1.776\n",
      "Ep 1 (Step 043230): Train loss 1.221, Val loss 1.779\n",
      "Ep 1 (Step 043235): Train loss 1.129, Val loss 1.780\n",
      "Ep 1 (Step 043240): Train loss 1.108, Val loss 1.780\n",
      "Ep 1 (Step 043245): Train loss 1.013, Val loss 1.781\n",
      "Ep 1 (Step 043250): Train loss 1.077, Val loss 1.782\n",
      "Ep 1 (Step 043255): Train loss 1.122, Val loss 1.782\n",
      "Ep 1 (Step 043260): Train loss 0.924, Val loss 1.783\n",
      "Ep 1 (Step 043265): Train loss 1.082, Val loss 1.785\n",
      "Ep 1 (Step 043270): Train loss 1.052, Val loss 1.785\n",
      "Ep 1 (Step 043275): Train loss 1.127, Val loss 1.785\n",
      "Ep 1 (Step 043280): Train loss 0.858, Val loss 1.784\n",
      "Ep 1 (Step 043285): Train loss 1.041, Val loss 1.782\n",
      "Ep 1 (Step 043290): Train loss 1.070, Val loss 1.781\n",
      "Ep 1 (Step 043295): Train loss 1.173, Val loss 1.778\n",
      "Ep 1 (Step 043300): Train loss 0.749, Val loss 1.776\n",
      "Ep 1 (Step 043305): Train loss 0.979, Val loss 1.775\n",
      "Ep 1 (Step 043310): Train loss 1.299, Val loss 1.774\n",
      "Ep 1 (Step 043315): Train loss 0.882, Val loss 1.771\n",
      "Ep 1 (Step 043320): Train loss 1.038, Val loss 1.768\n",
      "Ep 1 (Step 043325): Train loss 1.286, Val loss 1.768\n",
      "Ep 1 (Step 043330): Train loss 1.038, Val loss 1.767\n",
      "Ep 1 (Step 043335): Train loss 1.070, Val loss 1.769\n",
      "Ep 1 (Step 043340): Train loss 0.864, Val loss 1.769\n",
      "Ep 1 (Step 043345): Train loss 1.110, Val loss 1.769\n",
      "Ep 1 (Step 043350): Train loss 1.099, Val loss 1.769\n",
      "Ep 1 (Step 043355): Train loss 1.328, Val loss 1.768\n",
      "Ep 1 (Step 043360): Train loss 0.980, Val loss 1.768\n",
      "Ep 1 (Step 043365): Train loss 1.064, Val loss 1.767\n",
      "Ep 1 (Step 043370): Train loss 1.359, Val loss 1.766\n",
      "Ep 1 (Step 043375): Train loss 1.299, Val loss 1.767\n",
      "Ep 1 (Step 043380): Train loss 1.365, Val loss 1.767\n",
      "Ep 1 (Step 043385): Train loss 0.923, Val loss 1.768\n",
      "Ep 1 (Step 043390): Train loss 1.130, Val loss 1.769\n",
      "Ep 1 (Step 043395): Train loss 0.954, Val loss 1.770\n",
      "Ep 1 (Step 043400): Train loss 0.865, Val loss 1.770\n",
      "Ep 1 (Step 043405): Train loss 0.881, Val loss 1.771\n",
      "Ep 1 (Step 043410): Train loss 0.809, Val loss 1.771\n",
      "Ep 1 (Step 043415): Train loss 1.181, Val loss 1.770\n",
      "Ep 1 (Step 043420): Train loss 1.242, Val loss 1.769\n",
      "Ep 1 (Step 043425): Train loss 1.237, Val loss 1.769\n",
      "Ep 1 (Step 043430): Train loss 1.461, Val loss 1.768\n",
      "Ep 1 (Step 043435): Train loss 1.171, Val loss 1.768\n",
      "Ep 1 (Step 043440): Train loss 1.292, Val loss 1.769\n",
      "Ep 1 (Step 043445): Train loss 0.977, Val loss 1.771\n",
      "Ep 1 (Step 043450): Train loss 1.087, Val loss 1.772\n",
      "Ep 1 (Step 043455): Train loss 1.125, Val loss 1.773\n",
      "Ep 1 (Step 043460): Train loss 1.072, Val loss 1.775\n",
      "Ep 1 (Step 043465): Train loss 1.206, Val loss 1.775\n",
      "Ep 1 (Step 043470): Train loss 1.178, Val loss 1.775\n",
      "Ep 1 (Step 043475): Train loss 0.916, Val loss 1.774\n",
      "Ep 1 (Step 043480): Train loss 0.907, Val loss 1.774\n",
      "Ep 1 (Step 043485): Train loss 0.744, Val loss 1.774\n",
      "Ep 1 (Step 043490): Train loss 1.127, Val loss 1.774\n",
      "Ep 1 (Step 043495): Train loss 1.199, Val loss 1.773\n",
      "Ep 1 (Step 043500): Train loss 0.851, Val loss 1.773\n",
      "Ep 1 (Step 043505): Train loss 1.046, Val loss 1.772\n",
      "Ep 1 (Step 043510): Train loss 1.114, Val loss 1.771\n",
      "Ep 1 (Step 043515): Train loss 1.135, Val loss 1.771\n",
      "Ep 1 (Step 043520): Train loss 1.424, Val loss 1.771\n",
      "Ep 1 (Step 043525): Train loss 0.955, Val loss 1.771\n",
      "Ep 1 (Step 043530): Train loss 0.994, Val loss 1.771\n",
      "Ep 1 (Step 043535): Train loss 0.910, Val loss 1.770\n",
      "Ep 1 (Step 043540): Train loss 1.111, Val loss 1.771\n",
      "Ep 1 (Step 043545): Train loss 1.106, Val loss 1.772\n",
      "Ep 1 (Step 043550): Train loss 1.251, Val loss 1.775\n",
      "Ep 1 (Step 043555): Train loss 0.969, Val loss 1.777\n",
      "Ep 1 (Step 043560): Train loss 1.089, Val loss 1.777\n",
      "Ep 1 (Step 043565): Train loss 0.904, Val loss 1.776\n",
      "Ep 1 (Step 043570): Train loss 1.100, Val loss 1.774\n",
      "Ep 1 (Step 043575): Train loss 1.226, Val loss 1.773\n",
      "Ep 1 (Step 043580): Train loss 1.204, Val loss 1.771\n",
      "Ep 1 (Step 043585): Train loss 0.811, Val loss 1.771\n",
      "Ep 1 (Step 043590): Train loss 1.162, Val loss 1.771\n",
      "Ep 1 (Step 043595): Train loss 1.069, Val loss 1.771\n",
      "Ep 1 (Step 043600): Train loss 1.241, Val loss 1.772\n",
      "Ep 1 (Step 043605): Train loss 1.281, Val loss 1.772\n",
      "Ep 1 (Step 043610): Train loss 1.234, Val loss 1.772\n",
      "Ep 1 (Step 043615): Train loss 0.993, Val loss 1.772\n",
      "Ep 1 (Step 043620): Train loss 0.901, Val loss 1.772\n",
      "Ep 1 (Step 043625): Train loss 1.370, Val loss 1.771\n",
      "Ep 1 (Step 043630): Train loss 1.064, Val loss 1.770\n",
      "Ep 1 (Step 043635): Train loss 1.124, Val loss 1.769\n",
      "Ep 1 (Step 043640): Train loss 1.069, Val loss 1.768\n",
      "Ep 1 (Step 043645): Train loss 1.125, Val loss 1.766\n",
      "Ep 1 (Step 043650): Train loss 1.167, Val loss 1.764\n",
      "Ep 1 (Step 043655): Train loss 1.049, Val loss 1.763\n",
      "Ep 1 (Step 043660): Train loss 0.801, Val loss 1.762\n",
      "Ep 1 (Step 043665): Train loss 0.983, Val loss 1.762\n",
      "Ep 1 (Step 043670): Train loss 1.065, Val loss 1.763\n",
      "Ep 1 (Step 043675): Train loss 1.199, Val loss 1.762\n",
      "Ep 1 (Step 043680): Train loss 0.946, Val loss 1.762\n",
      "Ep 1 (Step 043685): Train loss 1.231, Val loss 1.763\n",
      "Ep 1 (Step 043690): Train loss 0.904, Val loss 1.764\n",
      "Ep 1 (Step 043695): Train loss 1.240, Val loss 1.764\n",
      "Ep 1 (Step 043700): Train loss 1.055, Val loss 1.765\n",
      "Ep 1 (Step 043705): Train loss 1.450, Val loss 1.766\n",
      "Ep 1 (Step 043710): Train loss 0.995, Val loss 1.766\n",
      "Ep 1 (Step 043715): Train loss 1.158, Val loss 1.764\n",
      "Ep 1 (Step 043720): Train loss 0.961, Val loss 1.763\n",
      "Ep 1 (Step 043725): Train loss 1.216, Val loss 1.763\n",
      "Ep 1 (Step 043730): Train loss 1.397, Val loss 1.764\n",
      "Ep 1 (Step 043735): Train loss 1.074, Val loss 1.764\n",
      "Ep 1 (Step 043740): Train loss 1.322, Val loss 1.763\n",
      "Ep 1 (Step 043745): Train loss 1.115, Val loss 1.761\n",
      "Ep 1 (Step 043750): Train loss 1.122, Val loss 1.757\n",
      "Ep 1 (Step 043755): Train loss 0.850, Val loss 1.755\n",
      "Ep 1 (Step 043760): Train loss 0.960, Val loss 1.754\n",
      "Ep 1 (Step 043765): Train loss 1.015, Val loss 1.755\n",
      "Ep 1 (Step 043770): Train loss 1.123, Val loss 1.755\n",
      "Ep 1 (Step 043775): Train loss 1.148, Val loss 1.755\n",
      "Ep 1 (Step 043780): Train loss 1.011, Val loss 1.756\n",
      "Ep 1 (Step 043785): Train loss 1.187, Val loss 1.756\n",
      "Ep 1 (Step 043790): Train loss 0.914, Val loss 1.754\n",
      "Ep 1 (Step 043795): Train loss 1.098, Val loss 1.753\n",
      "Ep 1 (Step 043800): Train loss 1.402, Val loss 1.753\n",
      "Ep 1 (Step 043805): Train loss 1.128, Val loss 1.754\n",
      "Ep 1 (Step 043810): Train loss 1.067, Val loss 1.754\n",
      "Ep 1 (Step 043815): Train loss 0.853, Val loss 1.754\n",
      "Ep 1 (Step 043820): Train loss 1.170, Val loss 1.755\n",
      "Ep 1 (Step 043825): Train loss 0.924, Val loss 1.756\n",
      "Ep 1 (Step 043830): Train loss 1.380, Val loss 1.758\n",
      "Ep 1 (Step 043835): Train loss 1.033, Val loss 1.759\n",
      "Ep 1 (Step 043840): Train loss 1.190, Val loss 1.759\n",
      "Ep 1 (Step 043845): Train loss 1.191, Val loss 1.759\n",
      "Ep 1 (Step 043850): Train loss 1.219, Val loss 1.759\n",
      "Ep 1 (Step 043855): Train loss 0.963, Val loss 1.759\n",
      "Ep 1 (Step 043860): Train loss 1.260, Val loss 1.759\n",
      "Ep 1 (Step 043865): Train loss 0.971, Val loss 1.760\n",
      "Ep 1 (Step 043870): Train loss 1.124, Val loss 1.760\n",
      "Ep 1 (Step 043875): Train loss 1.029, Val loss 1.762\n",
      "Ep 1 (Step 043880): Train loss 1.017, Val loss 1.762\n",
      "Ep 1 (Step 043885): Train loss 1.139, Val loss 1.763\n",
      "Ep 1 (Step 043890): Train loss 1.243, Val loss 1.764\n",
      "Ep 1 (Step 043895): Train loss 1.175, Val loss 1.766\n",
      "Ep 1 (Step 043900): Train loss 1.254, Val loss 1.768\n",
      "Ep 1 (Step 043905): Train loss 1.147, Val loss 1.769\n",
      "Ep 1 (Step 043910): Train loss 1.307, Val loss 1.769\n",
      "Ep 1 (Step 043915): Train loss 0.991, Val loss 1.769\n",
      "Ep 1 (Step 043920): Train loss 1.144, Val loss 1.769\n",
      "Ep 1 (Step 043925): Train loss 1.166, Val loss 1.766\n",
      "Ep 1 (Step 043930): Train loss 1.317, Val loss 1.764\n",
      "Ep 1 (Step 043935): Train loss 1.078, Val loss 1.762\n",
      "Ep 1 (Step 043940): Train loss 1.154, Val loss 1.761\n",
      "Ep 1 (Step 043945): Train loss 1.068, Val loss 1.760\n",
      "Ep 1 (Step 043950): Train loss 0.984, Val loss 1.760\n",
      "Ep 1 (Step 043955): Train loss 1.109, Val loss 1.760\n",
      "Ep 1 (Step 043960): Train loss 0.977, Val loss 1.759\n",
      "Ep 1 (Step 043965): Train loss 1.051, Val loss 1.758\n",
      "Ep 1 (Step 043970): Train loss 0.831, Val loss 1.757\n",
      "Ep 1 (Step 043975): Train loss 0.972, Val loss 1.756\n",
      "Ep 1 (Step 043980): Train loss 1.502, Val loss 1.757\n",
      "Ep 1 (Step 043985): Train loss 1.151, Val loss 1.759\n",
      "Ep 1 (Step 043990): Train loss 0.863, Val loss 1.759\n",
      "Ep 1 (Step 043995): Train loss 1.023, Val loss 1.760\n",
      "Ep 1 (Step 044000): Train loss 1.219, Val loss 1.760\n",
      "Ep 1 (Step 044005): Train loss 0.758, Val loss 1.760\n",
      "Ep 1 (Step 044010): Train loss 1.521, Val loss 1.761\n",
      "Ep 1 (Step 044015): Train loss 1.055, Val loss 1.762\n",
      "Ep 1 (Step 044020): Train loss 1.244, Val loss 1.763\n",
      "Ep 1 (Step 044025): Train loss 1.333, Val loss 1.764\n",
      "Ep 1 (Step 044030): Train loss 1.106, Val loss 1.764\n",
      "Ep 1 (Step 044035): Train loss 1.251, Val loss 1.766\n",
      "Ep 1 (Step 044040): Train loss 1.459, Val loss 1.767\n",
      "Ep 1 (Step 044045): Train loss 1.084, Val loss 1.767\n",
      "Ep 1 (Step 044050): Train loss 1.057, Val loss 1.768\n",
      "Ep 1 (Step 044055): Train loss 0.915, Val loss 1.768\n",
      "Ep 1 (Step 044060): Train loss 1.069, Val loss 1.769\n",
      "Ep 1 (Step 044065): Train loss 1.161, Val loss 1.769\n",
      "Ep 1 (Step 044070): Train loss 0.826, Val loss 1.769\n",
      "Ep 1 (Step 044075): Train loss 1.083, Val loss 1.770\n",
      "Ep 1 (Step 044080): Train loss 1.147, Val loss 1.770\n",
      "Ep 1 (Step 044085): Train loss 1.074, Val loss 1.768\n",
      "Ep 1 (Step 044090): Train loss 0.892, Val loss 1.767\n",
      "Ep 1 (Step 044095): Train loss 1.168, Val loss 1.767\n",
      "Ep 1 (Step 044100): Train loss 0.998, Val loss 1.767\n",
      "Ep 1 (Step 044105): Train loss 1.151, Val loss 1.768\n",
      "Ep 1 (Step 044110): Train loss 1.051, Val loss 1.769\n",
      "Ep 1 (Step 044115): Train loss 1.124, Val loss 1.770\n",
      "Ep 1 (Step 044120): Train loss 1.075, Val loss 1.771\n",
      "Ep 1 (Step 044125): Train loss 1.146, Val loss 1.771\n",
      "Ep 1 (Step 044130): Train loss 1.199, Val loss 1.770\n",
      "Ep 1 (Step 044135): Train loss 1.093, Val loss 1.768\n",
      "Ep 1 (Step 044140): Train loss 0.996, Val loss 1.766\n",
      "Ep 1 (Step 044145): Train loss 1.106, Val loss 1.765\n",
      "Ep 1 (Step 044150): Train loss 1.161, Val loss 1.765\n",
      "Ep 1 (Step 044155): Train loss 1.152, Val loss 1.765\n",
      "Ep 1 (Step 044160): Train loss 1.039, Val loss 1.766\n",
      "Ep 1 (Step 044165): Train loss 1.198, Val loss 1.767\n",
      "Ep 1 (Step 044170): Train loss 0.943, Val loss 1.767\n",
      "Ep 1 (Step 044175): Train loss 0.917, Val loss 1.765\n",
      "Ep 1 (Step 044180): Train loss 0.990, Val loss 1.764\n",
      "Ep 1 (Step 044185): Train loss 1.656, Val loss 1.763\n",
      "Ep 1 (Step 044190): Train loss 0.973, Val loss 1.762\n",
      "Ep 1 (Step 044195): Train loss 1.071, Val loss 1.760\n",
      "Ep 1 (Step 044200): Train loss 0.845, Val loss 1.759\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Explain how using transitional words help in writing  ### Input: \"<noinput>\"  ### Response: Transitional words help in writing by allowing for a more natural transition between different parts of a sentence. For example, \"I\" can be used to transition from the subject to the verb, \"I\" can be used\n",
      "Ep 2 (Step 044205): Train loss 1.174, Val loss 1.758\n",
      "Ep 2 (Step 044210): Train loss 0.848, Val loss 1.760\n",
      "Ep 2 (Step 044215): Train loss 1.154, Val loss 1.761\n",
      "Ep 2 (Step 044220): Train loss 1.252, Val loss 1.763\n",
      "Ep 2 (Step 044225): Train loss 1.060, Val loss 1.764\n",
      "Ep 2 (Step 044230): Train loss 1.241, Val loss 1.765\n",
      "Ep 2 (Step 044235): Train loss 1.286, Val loss 1.767\n",
      "Ep 2 (Step 044240): Train loss 0.943, Val loss 1.770\n",
      "Ep 2 (Step 044245): Train loss 1.083, Val loss 1.771\n",
      "Ep 2 (Step 044250): Train loss 0.987, Val loss 1.772\n",
      "Ep 2 (Step 044255): Train loss 1.035, Val loss 1.772\n",
      "Ep 2 (Step 044260): Train loss 0.883, Val loss 1.771\n",
      "Ep 2 (Step 044265): Train loss 1.129, Val loss 1.769\n",
      "Ep 2 (Step 044270): Train loss 1.300, Val loss 1.768\n",
      "Ep 2 (Step 044275): Train loss 1.253, Val loss 1.768\n",
      "Ep 2 (Step 044280): Train loss 0.958, Val loss 1.770\n",
      "Ep 2 (Step 044285): Train loss 1.056, Val loss 1.772\n",
      "Ep 2 (Step 044290): Train loss 0.899, Val loss 1.772\n",
      "Ep 2 (Step 044295): Train loss 1.168, Val loss 1.772\n",
      "Ep 2 (Step 044300): Train loss 1.113, Val loss 1.772\n",
      "Ep 2 (Step 044305): Train loss 1.473, Val loss 1.773\n",
      "Ep 2 (Step 044310): Train loss 1.060, Val loss 1.776\n",
      "Ep 2 (Step 044315): Train loss 1.130, Val loss 1.777\n",
      "Ep 2 (Step 044320): Train loss 1.228, Val loss 1.778\n",
      "Ep 2 (Step 044325): Train loss 1.216, Val loss 1.778\n",
      "Ep 2 (Step 044330): Train loss 1.231, Val loss 1.779\n",
      "Ep 2 (Step 044335): Train loss 0.964, Val loss 1.779\n",
      "Ep 2 (Step 044340): Train loss 0.900, Val loss 1.780\n",
      "Ep 2 (Step 044345): Train loss 0.861, Val loss 1.781\n",
      "Ep 2 (Step 044350): Train loss 1.358, Val loss 1.780\n",
      "Ep 2 (Step 044355): Train loss 1.078, Val loss 1.778\n",
      "Ep 2 (Step 044360): Train loss 1.360, Val loss 1.775\n",
      "Ep 2 (Step 044365): Train loss 1.139, Val loss 1.775\n",
      "Ep 2 (Step 044370): Train loss 0.981, Val loss 1.774\n",
      "Ep 2 (Step 044375): Train loss 1.095, Val loss 1.773\n",
      "Ep 2 (Step 044380): Train loss 1.058, Val loss 1.772\n",
      "Ep 2 (Step 044385): Train loss 1.004, Val loss 1.772\n",
      "Ep 2 (Step 044390): Train loss 1.076, Val loss 1.772\n",
      "Ep 2 (Step 044395): Train loss 1.491, Val loss 1.771\n",
      "Ep 2 (Step 044400): Train loss 1.192, Val loss 1.771\n",
      "Ep 2 (Step 044405): Train loss 0.976, Val loss 1.772\n",
      "Ep 2 (Step 044410): Train loss 0.919, Val loss 1.773\n",
      "Ep 2 (Step 044415): Train loss 1.458, Val loss 1.775\n",
      "Ep 2 (Step 044420): Train loss 1.029, Val loss 1.775\n",
      "Ep 2 (Step 044425): Train loss 1.080, Val loss 1.775\n",
      "Ep 2 (Step 044430): Train loss 1.249, Val loss 1.774\n",
      "Ep 2 (Step 044435): Train loss 0.793, Val loss 1.774\n",
      "Ep 2 (Step 044440): Train loss 0.845, Val loss 1.774\n",
      "Ep 2 (Step 044445): Train loss 1.142, Val loss 1.774\n",
      "Ep 2 (Step 044450): Train loss 1.208, Val loss 1.774\n",
      "Ep 2 (Step 044455): Train loss 1.089, Val loss 1.773\n",
      "Ep 2 (Step 044460): Train loss 1.116, Val loss 1.772\n",
      "Ep 2 (Step 044465): Train loss 1.249, Val loss 1.773\n",
      "Ep 2 (Step 044470): Train loss 1.107, Val loss 1.774\n",
      "Ep 2 (Step 044475): Train loss 1.285, Val loss 1.776\n",
      "Ep 2 (Step 044480): Train loss 1.182, Val loss 1.776\n",
      "Ep 2 (Step 044485): Train loss 1.238, Val loss 1.777\n",
      "Ep 2 (Step 044490): Train loss 1.125, Val loss 1.776\n",
      "Ep 2 (Step 044495): Train loss 1.025, Val loss 1.775\n",
      "Ep 2 (Step 044500): Train loss 1.255, Val loss 1.773\n",
      "Ep 2 (Step 044505): Train loss 1.029, Val loss 1.770\n",
      "Ep 2 (Step 044510): Train loss 1.049, Val loss 1.768\n",
      "Ep 2 (Step 044515): Train loss 1.242, Val loss 1.767\n",
      "Ep 2 (Step 044520): Train loss 1.191, Val loss 1.767\n",
      "Ep 2 (Step 044525): Train loss 1.209, Val loss 1.769\n",
      "Ep 2 (Step 044530): Train loss 1.251, Val loss 1.770\n",
      "Ep 2 (Step 044535): Train loss 1.014, Val loss 1.770\n",
      "Ep 2 (Step 044540): Train loss 1.070, Val loss 1.769\n",
      "Ep 2 (Step 044545): Train loss 1.212, Val loss 1.767\n",
      "Ep 2 (Step 044550): Train loss 1.072, Val loss 1.768\n",
      "Ep 2 (Step 044555): Train loss 0.863, Val loss 1.768\n",
      "Ep 2 (Step 044560): Train loss 0.973, Val loss 1.769\n",
      "Ep 2 (Step 044565): Train loss 1.035, Val loss 1.769\n",
      "Ep 2 (Step 044570): Train loss 0.965, Val loss 1.769\n",
      "Ep 2 (Step 044575): Train loss 1.282, Val loss 1.770\n",
      "Ep 2 (Step 044580): Train loss 1.195, Val loss 1.772\n",
      "Ep 2 (Step 044585): Train loss 1.057, Val loss 1.773\n",
      "Ep 2 (Step 044590): Train loss 1.114, Val loss 1.773\n",
      "Ep 2 (Step 044595): Train loss 0.968, Val loss 1.773\n",
      "Ep 2 (Step 044600): Train loss 1.065, Val loss 1.772\n",
      "Ep 2 (Step 044605): Train loss 1.096, Val loss 1.772\n",
      "Ep 2 (Step 044610): Train loss 0.927, Val loss 1.773\n",
      "Ep 2 (Step 044615): Train loss 0.791, Val loss 1.772\n",
      "Ep 2 (Step 044620): Train loss 1.022, Val loss 1.771\n",
      "Ep 2 (Step 044625): Train loss 1.152, Val loss 1.770\n",
      "Ep 2 (Step 044630): Train loss 1.228, Val loss 1.769\n",
      "Ep 2 (Step 044635): Train loss 1.284, Val loss 1.768\n",
      "Ep 2 (Step 044640): Train loss 1.311, Val loss 1.768\n",
      "Ep 2 (Step 044645): Train loss 0.847, Val loss 1.767\n",
      "Ep 2 (Step 044650): Train loss 1.022, Val loss 1.767\n",
      "Ep 2 (Step 044655): Train loss 0.957, Val loss 1.767\n",
      "Ep 2 (Step 044660): Train loss 1.021, Val loss 1.769\n",
      "Ep 2 (Step 044665): Train loss 0.990, Val loss 1.772\n",
      "Ep 2 (Step 044670): Train loss 1.251, Val loss 1.775\n",
      "Ep 2 (Step 044675): Train loss 1.197, Val loss 1.777\n",
      "Ep 2 (Step 044680): Train loss 1.142, Val loss 1.779\n",
      "Ep 2 (Step 044685): Train loss 1.305, Val loss 1.779\n",
      "Ep 2 (Step 044690): Train loss 0.923, Val loss 1.778\n",
      "Ep 2 (Step 044695): Train loss 0.936, Val loss 1.776\n",
      "Ep 2 (Step 044700): Train loss 1.110, Val loss 1.775\n",
      "Ep 2 (Step 044705): Train loss 1.394, Val loss 1.775\n",
      "Ep 2 (Step 044710): Train loss 1.046, Val loss 1.775\n",
      "Ep 2 (Step 044715): Train loss 1.152, Val loss 1.775\n",
      "Ep 2 (Step 044720): Train loss 1.191, Val loss 1.777\n",
      "Ep 2 (Step 044725): Train loss 1.003, Val loss 1.776\n",
      "Ep 2 (Step 044730): Train loss 1.390, Val loss 1.776\n",
      "Ep 2 (Step 044735): Train loss 1.183, Val loss 1.776\n",
      "Ep 2 (Step 044740): Train loss 0.975, Val loss 1.776\n",
      "Ep 2 (Step 044745): Train loss 1.042, Val loss 1.776\n",
      "Ep 2 (Step 044750): Train loss 1.216, Val loss 1.776\n",
      "Ep 2 (Step 044755): Train loss 1.083, Val loss 1.776\n",
      "Ep 2 (Step 044760): Train loss 1.248, Val loss 1.775\n",
      "Ep 2 (Step 044765): Train loss 1.049, Val loss 1.773\n",
      "Ep 2 (Step 044770): Train loss 1.325, Val loss 1.773\n",
      "Ep 2 (Step 044775): Train loss 0.856, Val loss 1.773\n",
      "Ep 2 (Step 044780): Train loss 1.045, Val loss 1.773\n",
      "Ep 2 (Step 044785): Train loss 1.103, Val loss 1.773\n",
      "Ep 2 (Step 044790): Train loss 1.207, Val loss 1.775\n",
      "Ep 2 (Step 044795): Train loss 1.435, Val loss 1.776\n",
      "Ep 2 (Step 044800): Train loss 1.172, Val loss 1.777\n",
      "Ep 2 (Step 044805): Train loss 1.124, Val loss 1.779\n",
      "Ep 2 (Step 044810): Train loss 1.029, Val loss 1.781\n",
      "Ep 2 (Step 044815): Train loss 1.013, Val loss 1.783\n",
      "Ep 2 (Step 044820): Train loss 1.205, Val loss 1.785\n",
      "Ep 2 (Step 044825): Train loss 0.836, Val loss 1.783\n",
      "Ep 2 (Step 044830): Train loss 1.235, Val loss 1.782\n",
      "Ep 2 (Step 044835): Train loss 1.356, Val loss 1.782\n",
      "Ep 2 (Step 044840): Train loss 1.342, Val loss 1.782\n",
      "Ep 2 (Step 044845): Train loss 1.047, Val loss 1.782\n",
      "Ep 2 (Step 044850): Train loss 0.932, Val loss 1.783\n",
      "Ep 2 (Step 044855): Train loss 1.192, Val loss 1.784\n",
      "Ep 2 (Step 044860): Train loss 0.880, Val loss 1.784\n",
      "Ep 2 (Step 044865): Train loss 1.279, Val loss 1.785\n",
      "Ep 2 (Step 044870): Train loss 1.103, Val loss 1.786\n",
      "Ep 2 (Step 044875): Train loss 1.112, Val loss 1.788\n",
      "Ep 2 (Step 044880): Train loss 1.119, Val loss 1.788\n",
      "Ep 2 (Step 044885): Train loss 0.865, Val loss 1.787\n",
      "Ep 2 (Step 044890): Train loss 1.480, Val loss 1.786\n",
      "Ep 2 (Step 044895): Train loss 1.267, Val loss 1.786\n",
      "Ep 2 (Step 044900): Train loss 0.996, Val loss 1.786\n",
      "Ep 2 (Step 044905): Train loss 1.381, Val loss 1.786\n",
      "Ep 2 (Step 044910): Train loss 0.866, Val loss 1.784\n",
      "Ep 2 (Step 044915): Train loss 1.027, Val loss 1.784\n",
      "Ep 2 (Step 044920): Train loss 1.240, Val loss 1.783\n",
      "Ep 2 (Step 044925): Train loss 0.880, Val loss 1.782\n",
      "Ep 2 (Step 044930): Train loss 1.088, Val loss 1.783\n",
      "Ep 2 (Step 044935): Train loss 0.982, Val loss 1.782\n",
      "Ep 2 (Step 044940): Train loss 1.051, Val loss 1.780\n",
      "Ep 2 (Step 044945): Train loss 1.038, Val loss 1.779\n",
      "Ep 2 (Step 044950): Train loss 0.887, Val loss 1.779\n",
      "Ep 2 (Step 044955): Train loss 1.006, Val loss 1.779\n",
      "Ep 2 (Step 044960): Train loss 1.005, Val loss 1.779\n",
      "Ep 2 (Step 044965): Train loss 1.053, Val loss 1.781\n",
      "Ep 2 (Step 044970): Train loss 1.236, Val loss 1.784\n",
      "Ep 2 (Step 044975): Train loss 1.136, Val loss 1.786\n",
      "Ep 2 (Step 044980): Train loss 1.002, Val loss 1.786\n",
      "Ep 2 (Step 044985): Train loss 1.023, Val loss 1.786\n",
      "Ep 2 (Step 044990): Train loss 1.311, Val loss 1.787\n",
      "Ep 2 (Step 044995): Train loss 1.266, Val loss 1.788\n",
      "Ep 2 (Step 045000): Train loss 0.955, Val loss 1.787\n",
      "Ep 2 (Step 045005): Train loss 1.498, Val loss 1.786\n",
      "Ep 2 (Step 045010): Train loss 1.181, Val loss 1.784\n",
      "Ep 2 (Step 045015): Train loss 1.026, Val loss 1.781\n",
      "Ep 2 (Step 045020): Train loss 0.909, Val loss 1.778\n",
      "Ep 2 (Step 045025): Train loss 1.283, Val loss 1.775\n",
      "Ep 2 (Step 045030): Train loss 1.052, Val loss 1.774\n",
      "Ep 2 (Step 045035): Train loss 1.202, Val loss 1.774\n",
      "Ep 2 (Step 045040): Train loss 1.215, Val loss 1.775\n",
      "Ep 2 (Step 045045): Train loss 1.053, Val loss 1.776\n",
      "Ep 2 (Step 045050): Train loss 1.175, Val loss 1.777\n",
      "Ep 2 (Step 045055): Train loss 1.084, Val loss 1.778\n",
      "Ep 2 (Step 045060): Train loss 0.925, Val loss 1.778\n",
      "Ep 2 (Step 045065): Train loss 1.096, Val loss 1.779\n",
      "Ep 2 (Step 045070): Train loss 0.984, Val loss 1.781\n",
      "Ep 2 (Step 045075): Train loss 1.253, Val loss 1.782\n",
      "Ep 2 (Step 045080): Train loss 1.233, Val loss 1.782\n",
      "Ep 2 (Step 045085): Train loss 0.998, Val loss 1.783\n",
      "Ep 2 (Step 045090): Train loss 1.048, Val loss 1.783\n",
      "Ep 2 (Step 045095): Train loss 1.137, Val loss 1.786\n",
      "Ep 2 (Step 045100): Train loss 0.935, Val loss 1.788\n",
      "Ep 2 (Step 045105): Train loss 0.972, Val loss 1.788\n",
      "Ep 2 (Step 045110): Train loss 1.129, Val loss 1.789\n",
      "Ep 2 (Step 045115): Train loss 1.191, Val loss 1.790\n",
      "Ep 2 (Step 045120): Train loss 0.828, Val loss 1.792\n",
      "Ep 2 (Step 045125): Train loss 1.249, Val loss 1.792\n",
      "Ep 2 (Step 045130): Train loss 0.772, Val loss 1.791\n",
      "Ep 2 (Step 045135): Train loss 1.028, Val loss 1.791\n",
      "Ep 2 (Step 045140): Train loss 0.905, Val loss 1.791\n",
      "Ep 2 (Step 045145): Train loss 0.919, Val loss 1.793\n",
      "Ep 2 (Step 045150): Train loss 0.996, Val loss 1.794\n",
      "Ep 2 (Step 045155): Train loss 1.126, Val loss 1.793\n",
      "Ep 2 (Step 045160): Train loss 1.218, Val loss 1.793\n",
      "Ep 2 (Step 045165): Train loss 0.996, Val loss 1.793\n",
      "Ep 2 (Step 045170): Train loss 0.925, Val loss 1.793\n",
      "Ep 2 (Step 045175): Train loss 1.236, Val loss 1.791\n",
      "Ep 2 (Step 045180): Train loss 1.154, Val loss 1.790\n",
      "Ep 2 (Step 045185): Train loss 0.847, Val loss 1.788\n",
      "Ep 2 (Step 045190): Train loss 1.092, Val loss 1.788\n",
      "Ep 2 (Step 045195): Train loss 1.107, Val loss 1.787\n",
      "Ep 2 (Step 045200): Train loss 1.071, Val loss 1.787\n",
      "Ep 2 (Step 045205): Train loss 0.666, Val loss 1.786\n",
      "Ep 2 (Step 045210): Train loss 0.929, Val loss 1.784\n",
      "Ep 2 (Step 045215): Train loss 1.131, Val loss 1.781\n",
      "Ep 2 (Step 045220): Train loss 1.411, Val loss 1.779\n",
      "Ep 2 (Step 045225): Train loss 1.125, Val loss 1.777\n",
      "Ep 2 (Step 045230): Train loss 1.166, Val loss 1.778\n",
      "Ep 2 (Step 045235): Train loss 1.323, Val loss 1.778\n",
      "Ep 2 (Step 045240): Train loss 1.105, Val loss 1.779\n",
      "Ep 2 (Step 045245): Train loss 1.090, Val loss 1.779\n",
      "Ep 2 (Step 045250): Train loss 1.038, Val loss 1.780\n",
      "Ep 2 (Step 045255): Train loss 0.937, Val loss 1.780\n",
      "Ep 2 (Step 045260): Train loss 1.007, Val loss 1.779\n",
      "Ep 2 (Step 045265): Train loss 1.276, Val loss 1.778\n",
      "Ep 2 (Step 045270): Train loss 1.128, Val loss 1.778\n",
      "Ep 2 (Step 045275): Train loss 1.228, Val loss 1.778\n",
      "Ep 2 (Step 045280): Train loss 1.126, Val loss 1.779\n",
      "Ep 2 (Step 045285): Train loss 1.065, Val loss 1.779\n",
      "Ep 2 (Step 045290): Train loss 1.372, Val loss 1.780\n",
      "Ep 2 (Step 045295): Train loss 1.158, Val loss 1.780\n",
      "Ep 2 (Step 045300): Train loss 0.918, Val loss 1.782\n",
      "Ep 2 (Step 045305): Train loss 1.104, Val loss 1.783\n",
      "Ep 2 (Step 045310): Train loss 1.094, Val loss 1.783\n",
      "Ep 2 (Step 045315): Train loss 0.994, Val loss 1.782\n",
      "Ep 2 (Step 045320): Train loss 1.129, Val loss 1.783\n",
      "Ep 2 (Step 045325): Train loss 1.051, Val loss 1.784\n",
      "Ep 2 (Step 045330): Train loss 1.331, Val loss 1.785\n",
      "Ep 2 (Step 045335): Train loss 0.895, Val loss 1.786\n",
      "Ep 2 (Step 045340): Train loss 1.236, Val loss 1.786\n",
      "Ep 2 (Step 045345): Train loss 1.151, Val loss 1.786\n",
      "Ep 2 (Step 045350): Train loss 0.954, Val loss 1.784\n",
      "Ep 2 (Step 045355): Train loss 1.139, Val loss 1.782\n",
      "Ep 2 (Step 045360): Train loss 1.319, Val loss 1.779\n",
      "Ep 2 (Step 045365): Train loss 0.972, Val loss 1.779\n",
      "Ep 2 (Step 045370): Train loss 1.156, Val loss 1.780\n",
      "Ep 2 (Step 045375): Train loss 0.983, Val loss 1.782\n",
      "Ep 2 (Step 045380): Train loss 0.697, Val loss 1.785\n",
      "Ep 2 (Step 045385): Train loss 1.370, Val loss 1.785\n",
      "Ep 2 (Step 045390): Train loss 0.901, Val loss 1.786\n",
      "Ep 2 (Step 045395): Train loss 1.204, Val loss 1.786\n",
      "Ep 2 (Step 045400): Train loss 0.950, Val loss 1.786\n",
      "Ep 2 (Step 045405): Train loss 1.371, Val loss 1.786\n",
      "Ep 2 (Step 045410): Train loss 1.081, Val loss 1.785\n",
      "Ep 2 (Step 045415): Train loss 1.120, Val loss 1.784\n",
      "Ep 2 (Step 045420): Train loss 0.996, Val loss 1.784\n",
      "Ep 2 (Step 045425): Train loss 1.092, Val loss 1.784\n",
      "Ep 2 (Step 045430): Train loss 1.202, Val loss 1.783\n",
      "Ep 2 (Step 045435): Train loss 1.199, Val loss 1.782\n",
      "Ep 2 (Step 045440): Train loss 0.997, Val loss 1.783\n",
      "Ep 2 (Step 045445): Train loss 1.217, Val loss 1.783\n",
      "Ep 2 (Step 045450): Train loss 1.096, Val loss 1.783\n",
      "Ep 2 (Step 045455): Train loss 0.995, Val loss 1.783\n",
      "Ep 2 (Step 045460): Train loss 1.182, Val loss 1.782\n",
      "Ep 2 (Step 045465): Train loss 1.292, Val loss 1.781\n",
      "Ep 2 (Step 045470): Train loss 0.861, Val loss 1.780\n",
      "Ep 2 (Step 045475): Train loss 1.287, Val loss 1.780\n",
      "Ep 2 (Step 045480): Train loss 1.105, Val loss 1.780\n",
      "Ep 2 (Step 045485): Train loss 1.089, Val loss 1.777\n",
      "Ep 2 (Step 045490): Train loss 1.373, Val loss 1.777\n",
      "Ep 2 (Step 045495): Train loss 1.050, Val loss 1.776\n",
      "Ep 2 (Step 045500): Train loss 1.175, Val loss 1.777\n",
      "Ep 2 (Step 045505): Train loss 1.106, Val loss 1.779\n",
      "Ep 2 (Step 045510): Train loss 1.245, Val loss 1.780\n",
      "Ep 2 (Step 045515): Train loss 0.912, Val loss 1.781\n",
      "Ep 2 (Step 045520): Train loss 1.122, Val loss 1.781\n",
      "Ep 2 (Step 045525): Train loss 1.286, Val loss 1.781\n",
      "Ep 2 (Step 045530): Train loss 1.290, Val loss 1.781\n",
      "Ep 2 (Step 045535): Train loss 0.957, Val loss 1.780\n",
      "Ep 2 (Step 045540): Train loss 0.962, Val loss 1.780\n",
      "Ep 2 (Step 045545): Train loss 0.955, Val loss 1.782\n",
      "Ep 2 (Step 045550): Train loss 0.976, Val loss 1.784\n",
      "Ep 2 (Step 045555): Train loss 0.924, Val loss 1.785\n",
      "Ep 2 (Step 045560): Train loss 1.026, Val loss 1.786\n",
      "Ep 2 (Step 045565): Train loss 1.151, Val loss 1.787\n",
      "Ep 2 (Step 045570): Train loss 1.267, Val loss 1.787\n",
      "Ep 2 (Step 045575): Train loss 1.169, Val loss 1.787\n",
      "Ep 2 (Step 045580): Train loss 0.976, Val loss 1.788\n",
      "Ep 2 (Step 045585): Train loss 1.365, Val loss 1.790\n",
      "Ep 2 (Step 045590): Train loss 1.232, Val loss 1.791\n",
      "Ep 2 (Step 045595): Train loss 1.199, Val loss 1.791\n",
      "Ep 2 (Step 045600): Train loss 1.090, Val loss 1.788\n",
      "Ep 2 (Step 045605): Train loss 1.285, Val loss 1.785\n",
      "Ep 2 (Step 045610): Train loss 0.991, Val loss 1.784\n",
      "Ep 2 (Step 045615): Train loss 1.274, Val loss 1.784\n",
      "Ep 2 (Step 045620): Train loss 1.060, Val loss 1.786\n",
      "Ep 2 (Step 045625): Train loss 0.970, Val loss 1.788\n",
      "Ep 2 (Step 045630): Train loss 0.960, Val loss 1.789\n",
      "Ep 2 (Step 045635): Train loss 1.164, Val loss 1.789\n",
      "Ep 2 (Step 045640): Train loss 0.889, Val loss 1.788\n",
      "Ep 2 (Step 045645): Train loss 1.135, Val loss 1.786\n",
      "Ep 2 (Step 045650): Train loss 1.079, Val loss 1.786\n",
      "Ep 2 (Step 045655): Train loss 1.193, Val loss 1.787\n",
      "Ep 2 (Step 045660): Train loss 0.947, Val loss 1.787\n",
      "Ep 2 (Step 045665): Train loss 1.181, Val loss 1.788\n",
      "Ep 2 (Step 045670): Train loss 1.143, Val loss 1.790\n",
      "Ep 2 (Step 045675): Train loss 1.237, Val loss 1.792\n",
      "Ep 2 (Step 045680): Train loss 0.945, Val loss 1.793\n",
      "Ep 2 (Step 045685): Train loss 1.278, Val loss 1.792\n",
      "Ep 2 (Step 045690): Train loss 1.186, Val loss 1.792\n",
      "Ep 2 (Step 045695): Train loss 0.961, Val loss 1.791\n",
      "Ep 2 (Step 045700): Train loss 0.919, Val loss 1.790\n",
      "Ep 2 (Step 045705): Train loss 0.925, Val loss 1.789\n",
      "Ep 2 (Step 045710): Train loss 1.145, Val loss 1.791\n",
      "Ep 2 (Step 045715): Train loss 0.836, Val loss 1.792\n",
      "Ep 2 (Step 045720): Train loss 1.048, Val loss 1.794\n",
      "Ep 2 (Step 045725): Train loss 0.914, Val loss 1.796\n",
      "Ep 2 (Step 045730): Train loss 1.438, Val loss 1.798\n",
      "Ep 2 (Step 045735): Train loss 1.066, Val loss 1.798\n",
      "Ep 2 (Step 045740): Train loss 0.805, Val loss 1.800\n",
      "Ep 2 (Step 045745): Train loss 1.347, Val loss 1.800\n",
      "Ep 2 (Step 045750): Train loss 1.061, Val loss 1.803\n",
      "Ep 2 (Step 045755): Train loss 0.935, Val loss 1.806\n",
      "Ep 2 (Step 045760): Train loss 0.905, Val loss 1.809\n",
      "Ep 2 (Step 045765): Train loss 1.180, Val loss 1.811\n",
      "Ep 2 (Step 045770): Train loss 1.210, Val loss 1.812\n",
      "Ep 2 (Step 045775): Train loss 1.001, Val loss 1.812\n",
      "Ep 2 (Step 045780): Train loss 1.143, Val loss 1.811\n",
      "Ep 2 (Step 045785): Train loss 1.149, Val loss 1.808\n",
      "Ep 2 (Step 045790): Train loss 0.832, Val loss 1.804\n",
      "Ep 2 (Step 045795): Train loss 1.434, Val loss 1.802\n",
      "Ep 2 (Step 045800): Train loss 1.048, Val loss 1.800\n",
      "Ep 2 (Step 045805): Train loss 1.012, Val loss 1.798\n",
      "Ep 2 (Step 045810): Train loss 0.928, Val loss 1.797\n",
      "Ep 2 (Step 045815): Train loss 0.810, Val loss 1.797\n",
      "Ep 2 (Step 045820): Train loss 1.227, Val loss 1.796\n",
      "Ep 2 (Step 045825): Train loss 1.231, Val loss 1.795\n",
      "Ep 2 (Step 045830): Train loss 1.372, Val loss 1.793\n",
      "Ep 2 (Step 045835): Train loss 0.957, Val loss 1.791\n",
      "Ep 2 (Step 045840): Train loss 1.004, Val loss 1.787\n",
      "Ep 2 (Step 045845): Train loss 1.031, Val loss 1.785\n",
      "Ep 2 (Step 045850): Train loss 0.848, Val loss 1.783\n",
      "Ep 2 (Step 045855): Train loss 1.275, Val loss 1.782\n",
      "Ep 2 (Step 045860): Train loss 1.255, Val loss 1.780\n",
      "Ep 2 (Step 045865): Train loss 0.936, Val loss 1.780\n",
      "Ep 2 (Step 045870): Train loss 0.880, Val loss 1.782\n",
      "Ep 2 (Step 045875): Train loss 0.868, Val loss 1.783\n",
      "Ep 2 (Step 045880): Train loss 1.221, Val loss 1.784\n",
      "Ep 2 (Step 045885): Train loss 1.058, Val loss 1.786\n",
      "Ep 2 (Step 045890): Train loss 0.913, Val loss 1.787\n",
      "Ep 2 (Step 045895): Train loss 0.976, Val loss 1.786\n",
      "Ep 2 (Step 045900): Train loss 1.006, Val loss 1.784\n",
      "Ep 2 (Step 045905): Train loss 1.225, Val loss 1.784\n",
      "Ep 2 (Step 045910): Train loss 0.903, Val loss 1.782\n",
      "Ep 2 (Step 045915): Train loss 1.168, Val loss 1.782\n",
      "Ep 2 (Step 045920): Train loss 1.257, Val loss 1.782\n",
      "Ep 2 (Step 045925): Train loss 1.049, Val loss 1.782\n",
      "Ep 2 (Step 045930): Train loss 1.253, Val loss 1.781\n",
      "Ep 2 (Step 045935): Train loss 0.974, Val loss 1.782\n",
      "Ep 2 (Step 045940): Train loss 0.983, Val loss 1.783\n",
      "Ep 2 (Step 045945): Train loss 1.243, Val loss 1.783\n",
      "Ep 2 (Step 045950): Train loss 1.198, Val loss 1.784\n",
      "Ep 2 (Step 045955): Train loss 0.985, Val loss 1.784\n",
      "Ep 2 (Step 045960): Train loss 1.211, Val loss 1.786\n",
      "Ep 2 (Step 045965): Train loss 0.825, Val loss 1.788\n",
      "Ep 2 (Step 045970): Train loss 1.156, Val loss 1.790\n",
      "Ep 2 (Step 045975): Train loss 1.162, Val loss 1.790\n",
      "Ep 2 (Step 045980): Train loss 1.093, Val loss 1.790\n",
      "Ep 2 (Step 045985): Train loss 1.009, Val loss 1.790\n",
      "Ep 2 (Step 045990): Train loss 0.972, Val loss 1.790\n",
      "Ep 2 (Step 045995): Train loss 0.990, Val loss 1.790\n",
      "Ep 2 (Step 046000): Train loss 1.237, Val loss 1.791\n",
      "Ep 2 (Step 046005): Train loss 1.160, Val loss 1.791\n",
      "Ep 2 (Step 046010): Train loss 1.020, Val loss 1.791\n",
      "Ep 2 (Step 046015): Train loss 1.307, Val loss 1.791\n",
      "Ep 2 (Step 046020): Train loss 1.249, Val loss 1.790\n",
      "Ep 2 (Step 046025): Train loss 0.888, Val loss 1.791\n",
      "Ep 2 (Step 046030): Train loss 0.836, Val loss 1.791\n",
      "Ep 2 (Step 046035): Train loss 0.850, Val loss 1.791\n",
      "Ep 2 (Step 046040): Train loss 1.256, Val loss 1.791\n",
      "Ep 2 (Step 046045): Train loss 1.153, Val loss 1.792\n",
      "Ep 2 (Step 046050): Train loss 0.993, Val loss 1.792\n",
      "Ep 2 (Step 046055): Train loss 1.085, Val loss 1.791\n",
      "Ep 2 (Step 046060): Train loss 1.490, Val loss 1.790\n",
      "Ep 2 (Step 046065): Train loss 1.029, Val loss 1.789\n",
      "Ep 2 (Step 046070): Train loss 0.923, Val loss 1.788\n",
      "Ep 2 (Step 046075): Train loss 1.125, Val loss 1.786\n",
      "Ep 2 (Step 046080): Train loss 1.021, Val loss 1.785\n",
      "Ep 2 (Step 046085): Train loss 1.275, Val loss 1.784\n",
      "Ep 2 (Step 046090): Train loss 1.184, Val loss 1.782\n",
      "Ep 2 (Step 046095): Train loss 0.912, Val loss 1.781\n",
      "Ep 2 (Step 046100): Train loss 1.019, Val loss 1.782\n",
      "Ep 2 (Step 046105): Train loss 0.971, Val loss 1.782\n",
      "Ep 2 (Step 046110): Train loss 1.229, Val loss 1.781\n",
      "Ep 2 (Step 046115): Train loss 1.206, Val loss 1.782\n",
      "Ep 2 (Step 046120): Train loss 1.205, Val loss 1.784\n",
      "Ep 2 (Step 046125): Train loss 1.052, Val loss 1.786\n",
      "Ep 2 (Step 046130): Train loss 1.081, Val loss 1.787\n",
      "Ep 2 (Step 046135): Train loss 1.194, Val loss 1.788\n",
      "Ep 2 (Step 046140): Train loss 1.087, Val loss 1.788\n",
      "Ep 2 (Step 046145): Train loss 0.967, Val loss 1.789\n",
      "Ep 2 (Step 046150): Train loss 1.058, Val loss 1.791\n",
      "Ep 2 (Step 046155): Train loss 0.968, Val loss 1.793\n",
      "Ep 2 (Step 046160): Train loss 1.025, Val loss 1.795\n",
      "Ep 2 (Step 046165): Train loss 0.902, Val loss 1.796\n",
      "Ep 2 (Step 046170): Train loss 1.075, Val loss 1.799\n",
      "Ep 2 (Step 046175): Train loss 1.065, Val loss 1.803\n",
      "Ep 2 (Step 046180): Train loss 1.045, Val loss 1.805\n",
      "Ep 2 (Step 046185): Train loss 0.934, Val loss 1.804\n",
      "Ep 2 (Step 046190): Train loss 1.253, Val loss 1.803\n",
      "Ep 2 (Step 046195): Train loss 1.250, Val loss 1.801\n",
      "Ep 2 (Step 046200): Train loss 1.124, Val loss 1.799\n",
      "Ep 2 (Step 046205): Train loss 1.074, Val loss 1.797\n",
      "Ep 2 (Step 046210): Train loss 1.079, Val loss 1.796\n",
      "Ep 2 (Step 046215): Train loss 1.046, Val loss 1.798\n",
      "Ep 2 (Step 046220): Train loss 0.958, Val loss 1.801\n",
      "Ep 2 (Step 046225): Train loss 0.974, Val loss 1.803\n",
      "Ep 2 (Step 046230): Train loss 1.212, Val loss 1.803\n",
      "Ep 2 (Step 046235): Train loss 0.843, Val loss 1.801\n",
      "Ep 2 (Step 046240): Train loss 1.187, Val loss 1.799\n",
      "Ep 2 (Step 046245): Train loss 1.216, Val loss 1.796\n",
      "Ep 2 (Step 046250): Train loss 1.128, Val loss 1.796\n",
      "Ep 2 (Step 046255): Train loss 1.322, Val loss 1.796\n",
      "Ep 2 (Step 046260): Train loss 0.843, Val loss 1.797\n",
      "Ep 2 (Step 046265): Train loss 1.090, Val loss 1.798\n",
      "Ep 2 (Step 046270): Train loss 1.274, Val loss 1.797\n",
      "Ep 2 (Step 046275): Train loss 1.121, Val loss 1.796\n",
      "Ep 2 (Step 046280): Train loss 1.165, Val loss 1.795\n",
      "Ep 2 (Step 046285): Train loss 0.944, Val loss 1.795\n",
      "Ep 2 (Step 046290): Train loss 1.034, Val loss 1.798\n",
      "Ep 2 (Step 046295): Train loss 1.073, Val loss 1.801\n",
      "Ep 2 (Step 046300): Train loss 1.251, Val loss 1.804\n",
      "Ep 2 (Step 046305): Train loss 1.082, Val loss 1.808\n",
      "Ep 2 (Step 046310): Train loss 0.979, Val loss 1.812\n",
      "Ep 2 (Step 046315): Train loss 1.116, Val loss 1.815\n",
      "Ep 2 (Step 046320): Train loss 1.199, Val loss 1.819\n",
      "Ep 2 (Step 046325): Train loss 0.979, Val loss 1.820\n",
      "Ep 2 (Step 046330): Train loss 0.865, Val loss 1.819\n",
      "Ep 2 (Step 046335): Train loss 1.064, Val loss 1.817\n",
      "Ep 2 (Step 046340): Train loss 1.132, Val loss 1.816\n",
      "Ep 2 (Step 046345): Train loss 1.203, Val loss 1.814\n",
      "Ep 2 (Step 046350): Train loss 1.151, Val loss 1.813\n",
      "Ep 2 (Step 046355): Train loss 1.091, Val loss 1.810\n",
      "Ep 2 (Step 046360): Train loss 1.163, Val loss 1.807\n",
      "Ep 2 (Step 046365): Train loss 1.426, Val loss 1.803\n",
      "Ep 2 (Step 046370): Train loss 1.109, Val loss 1.801\n",
      "Ep 2 (Step 046375): Train loss 1.076, Val loss 1.798\n",
      "Ep 2 (Step 046380): Train loss 1.082, Val loss 1.795\n",
      "Ep 2 (Step 046385): Train loss 1.267, Val loss 1.794\n",
      "Ep 2 (Step 046390): Train loss 1.145, Val loss 1.793\n",
      "Ep 2 (Step 046395): Train loss 1.156, Val loss 1.791\n",
      "Ep 2 (Step 046400): Train loss 1.034, Val loss 1.789\n",
      "Ep 2 (Step 046405): Train loss 1.084, Val loss 1.788\n",
      "Ep 2 (Step 046410): Train loss 1.147, Val loss 1.788\n",
      "Ep 2 (Step 046415): Train loss 1.097, Val loss 1.788\n",
      "Ep 2 (Step 046420): Train loss 0.986, Val loss 1.786\n",
      "Ep 2 (Step 046425): Train loss 1.193, Val loss 1.785\n",
      "Ep 2 (Step 046430): Train loss 1.144, Val loss 1.787\n",
      "Ep 2 (Step 046435): Train loss 1.148, Val loss 1.786\n",
      "Ep 2 (Step 046440): Train loss 0.924, Val loss 1.786\n",
      "Ep 2 (Step 046445): Train loss 1.036, Val loss 1.786\n",
      "Ep 2 (Step 046450): Train loss 1.122, Val loss 1.786\n",
      "Ep 2 (Step 046455): Train loss 0.799, Val loss 1.786\n",
      "Ep 2 (Step 046460): Train loss 1.001, Val loss 1.787\n",
      "Ep 2 (Step 046465): Train loss 1.261, Val loss 1.787\n",
      "Ep 2 (Step 046470): Train loss 0.999, Val loss 1.786\n",
      "Ep 2 (Step 046475): Train loss 0.980, Val loss 1.784\n",
      "Ep 2 (Step 046480): Train loss 1.009, Val loss 1.783\n",
      "Ep 2 (Step 046485): Train loss 1.138, Val loss 1.784\n",
      "Ep 2 (Step 046490): Train loss 0.903, Val loss 1.785\n",
      "Ep 2 (Step 046495): Train loss 1.267, Val loss 1.787\n",
      "Ep 2 (Step 046500): Train loss 1.061, Val loss 1.786\n",
      "Ep 2 (Step 046505): Train loss 1.079, Val loss 1.784\n",
      "Ep 2 (Step 046510): Train loss 1.273, Val loss 1.784\n",
      "Ep 2 (Step 046515): Train loss 1.204, Val loss 1.782\n",
      "Ep 2 (Step 046520): Train loss 1.319, Val loss 1.782\n",
      "Ep 2 (Step 046525): Train loss 0.851, Val loss 1.781\n",
      "Ep 2 (Step 046530): Train loss 0.930, Val loss 1.780\n",
      "Ep 2 (Step 046535): Train loss 1.346, Val loss 1.779\n",
      "Ep 2 (Step 046540): Train loss 1.143, Val loss 1.779\n",
      "Ep 2 (Step 046545): Train loss 1.158, Val loss 1.779\n",
      "Ep 2 (Step 046550): Train loss 1.015, Val loss 1.780\n",
      "Ep 2 (Step 046555): Train loss 1.278, Val loss 1.780\n",
      "Ep 2 (Step 046560): Train loss 1.155, Val loss 1.781\n",
      "Ep 2 (Step 046565): Train loss 1.260, Val loss 1.782\n",
      "Ep 2 (Step 046570): Train loss 1.195, Val loss 1.782\n",
      "Ep 2 (Step 046575): Train loss 0.844, Val loss 1.781\n",
      "Ep 2 (Step 046580): Train loss 0.972, Val loss 1.781\n",
      "Ep 2 (Step 046585): Train loss 1.198, Val loss 1.782\n",
      "Ep 2 (Step 046590): Train loss 0.897, Val loss 1.783\n",
      "Ep 2 (Step 046595): Train loss 0.887, Val loss 1.781\n",
      "Ep 2 (Step 046600): Train loss 1.205, Val loss 1.780\n",
      "Ep 2 (Step 046605): Train loss 1.351, Val loss 1.780\n",
      "Ep 2 (Step 046610): Train loss 1.154, Val loss 1.780\n",
      "Ep 2 (Step 046615): Train loss 1.097, Val loss 1.780\n",
      "Ep 2 (Step 046620): Train loss 0.798, Val loss 1.780\n",
      "Ep 2 (Step 046625): Train loss 1.094, Val loss 1.778\n",
      "Ep 2 (Step 046630): Train loss 0.789, Val loss 1.774\n",
      "Ep 2 (Step 046635): Train loss 1.117, Val loss 1.772\n",
      "Ep 2 (Step 046640): Train loss 1.174, Val loss 1.770\n",
      "Ep 2 (Step 046645): Train loss 1.314, Val loss 1.769\n",
      "Ep 2 (Step 046650): Train loss 0.857, Val loss 1.768\n",
      "Ep 2 (Step 046655): Train loss 1.383, Val loss 1.768\n",
      "Ep 2 (Step 046660): Train loss 1.285, Val loss 1.769\n",
      "Ep 2 (Step 046665): Train loss 0.972, Val loss 1.768\n",
      "Ep 2 (Step 046670): Train loss 1.488, Val loss 1.768\n",
      "Ep 2 (Step 046675): Train loss 1.298, Val loss 1.767\n",
      "Ep 2 (Step 046680): Train loss 1.270, Val loss 1.768\n",
      "Ep 2 (Step 046685): Train loss 1.088, Val loss 1.769\n",
      "Ep 2 (Step 046690): Train loss 0.897, Val loss 1.771\n",
      "Ep 2 (Step 046695): Train loss 0.947, Val loss 1.773\n",
      "Ep 2 (Step 046700): Train loss 0.947, Val loss 1.774\n",
      "Ep 2 (Step 046705): Train loss 1.007, Val loss 1.775\n",
      "Ep 2 (Step 046710): Train loss 1.082, Val loss 1.775\n",
      "Ep 2 (Step 046715): Train loss 1.172, Val loss 1.775\n",
      "Ep 2 (Step 046720): Train loss 1.033, Val loss 1.775\n",
      "Ep 2 (Step 046725): Train loss 1.206, Val loss 1.775\n",
      "Ep 2 (Step 046730): Train loss 1.258, Val loss 1.777\n",
      "Ep 2 (Step 046735): Train loss 1.088, Val loss 1.779\n",
      "Ep 2 (Step 046740): Train loss 1.208, Val loss 1.781\n",
      "Ep 2 (Step 046745): Train loss 0.831, Val loss 1.782\n",
      "Ep 2 (Step 046750): Train loss 1.238, Val loss 1.782\n",
      "Ep 2 (Step 046755): Train loss 0.776, Val loss 1.782\n",
      "Ep 2 (Step 046760): Train loss 0.818, Val loss 1.781\n",
      "Ep 2 (Step 046765): Train loss 0.903, Val loss 1.783\n",
      "Ep 2 (Step 046770): Train loss 0.846, Val loss 1.784\n",
      "Ep 2 (Step 046775): Train loss 1.074, Val loss 1.784\n",
      "Ep 2 (Step 046780): Train loss 0.993, Val loss 1.784\n",
      "Ep 2 (Step 046785): Train loss 1.110, Val loss 1.784\n",
      "Ep 2 (Step 046790): Train loss 1.178, Val loss 1.786\n",
      "Ep 2 (Step 046795): Train loss 1.316, Val loss 1.786\n",
      "Ep 2 (Step 046800): Train loss 1.228, Val loss 1.786\n",
      "Ep 2 (Step 046805): Train loss 0.856, Val loss 1.787\n",
      "Ep 2 (Step 046810): Train loss 1.325, Val loss 1.788\n",
      "Ep 2 (Step 046815): Train loss 1.471, Val loss 1.787\n",
      "Ep 2 (Step 046820): Train loss 0.887, Val loss 1.787\n",
      "Ep 2 (Step 046825): Train loss 1.219, Val loss 1.786\n",
      "Ep 2 (Step 046830): Train loss 0.961, Val loss 1.787\n",
      "Ep 2 (Step 046835): Train loss 0.924, Val loss 1.788\n",
      "Ep 2 (Step 046840): Train loss 1.085, Val loss 1.788\n",
      "Ep 2 (Step 046845): Train loss 1.338, Val loss 1.788\n",
      "Ep 2 (Step 046850): Train loss 0.976, Val loss 1.786\n",
      "Ep 2 (Step 046855): Train loss 0.952, Val loss 1.784\n",
      "Ep 2 (Step 046860): Train loss 1.198, Val loss 1.783\n",
      "Ep 2 (Step 046865): Train loss 1.112, Val loss 1.782\n",
      "Ep 2 (Step 046870): Train loss 1.096, Val loss 1.780\n",
      "Ep 2 (Step 046875): Train loss 1.090, Val loss 1.779\n",
      "Ep 2 (Step 046880): Train loss 1.130, Val loss 1.777\n",
      "Ep 2 (Step 046885): Train loss 1.139, Val loss 1.777\n",
      "Ep 2 (Step 046890): Train loss 1.345, Val loss 1.776\n",
      "Ep 2 (Step 046895): Train loss 1.102, Val loss 1.776\n",
      "Ep 2 (Step 046900): Train loss 0.987, Val loss 1.776\n",
      "Ep 2 (Step 046905): Train loss 0.906, Val loss 1.775\n",
      "Ep 2 (Step 046910): Train loss 0.974, Val loss 1.774\n",
      "Ep 2 (Step 046915): Train loss 0.946, Val loss 1.772\n",
      "Ep 2 (Step 046920): Train loss 0.990, Val loss 1.771\n",
      "Ep 2 (Step 046925): Train loss 1.027, Val loss 1.770\n",
      "Ep 2 (Step 046930): Train loss 0.907, Val loss 1.771\n",
      "Ep 2 (Step 046935): Train loss 0.802, Val loss 1.774\n",
      "Ep 2 (Step 046940): Train loss 1.118, Val loss 1.777\n",
      "Ep 2 (Step 046945): Train loss 1.021, Val loss 1.779\n",
      "Ep 2 (Step 046950): Train loss 1.005, Val loss 1.780\n",
      "Ep 2 (Step 046955): Train loss 1.013, Val loss 1.780\n",
      "Ep 2 (Step 046960): Train loss 1.351, Val loss 1.781\n",
      "Ep 2 (Step 046965): Train loss 1.144, Val loss 1.783\n",
      "Ep 2 (Step 046970): Train loss 1.089, Val loss 1.787\n",
      "Ep 2 (Step 046975): Train loss 0.865, Val loss 1.788\n",
      "Ep 2 (Step 046980): Train loss 1.289, Val loss 1.788\n",
      "Ep 2 (Step 046985): Train loss 1.106, Val loss 1.787\n",
      "Ep 2 (Step 046990): Train loss 1.046, Val loss 1.786\n",
      "Ep 2 (Step 046995): Train loss 1.191, Val loss 1.784\n",
      "Ep 2 (Step 047000): Train loss 1.385, Val loss 1.784\n",
      "Ep 2 (Step 047005): Train loss 1.062, Val loss 1.784\n",
      "Ep 2 (Step 047010): Train loss 1.126, Val loss 1.782\n",
      "Ep 2 (Step 047015): Train loss 1.062, Val loss 1.782\n",
      "Ep 2 (Step 047020): Train loss 1.144, Val loss 1.783\n",
      "Ep 2 (Step 047025): Train loss 1.016, Val loss 1.783\n",
      "Ep 2 (Step 047030): Train loss 1.016, Val loss 1.784\n",
      "Ep 2 (Step 047035): Train loss 1.149, Val loss 1.785\n",
      "Ep 2 (Step 047040): Train loss 1.226, Val loss 1.786\n",
      "Ep 2 (Step 047045): Train loss 1.210, Val loss 1.784\n",
      "Ep 2 (Step 047050): Train loss 1.213, Val loss 1.781\n",
      "Ep 2 (Step 047055): Train loss 0.915, Val loss 1.779\n",
      "Ep 2 (Step 047060): Train loss 1.100, Val loss 1.778\n",
      "Ep 2 (Step 047065): Train loss 1.025, Val loss 1.778\n",
      "Ep 2 (Step 047070): Train loss 1.105, Val loss 1.779\n",
      "Ep 2 (Step 047075): Train loss 1.076, Val loss 1.781\n",
      "Ep 2 (Step 047080): Train loss 1.515, Val loss 1.782\n",
      "Ep 2 (Step 047085): Train loss 1.130, Val loss 1.783\n",
      "Ep 2 (Step 047090): Train loss 1.299, Val loss 1.783\n",
      "Ep 2 (Step 047095): Train loss 1.080, Val loss 1.784\n",
      "Ep 2 (Step 047100): Train loss 1.162, Val loss 1.784\n",
      "Ep 2 (Step 047105): Train loss 1.090, Val loss 1.784\n",
      "Ep 2 (Step 047110): Train loss 1.266, Val loss 1.784\n",
      "Ep 2 (Step 047115): Train loss 1.074, Val loss 1.782\n",
      "Ep 2 (Step 047120): Train loss 0.890, Val loss 1.783\n",
      "Ep 2 (Step 047125): Train loss 1.092, Val loss 1.784\n",
      "Ep 2 (Step 047130): Train loss 1.108, Val loss 1.785\n",
      "Ep 2 (Step 047135): Train loss 1.056, Val loss 1.787\n",
      "Ep 2 (Step 047140): Train loss 1.036, Val loss 1.790\n",
      "Ep 2 (Step 047145): Train loss 1.187, Val loss 1.791\n",
      "Ep 2 (Step 047150): Train loss 1.150, Val loss 1.792\n",
      "Ep 2 (Step 047155): Train loss 1.244, Val loss 1.793\n",
      "Ep 2 (Step 047160): Train loss 1.204, Val loss 1.794\n",
      "Ep 2 (Step 047165): Train loss 1.027, Val loss 1.794\n",
      "Ep 2 (Step 047170): Train loss 1.057, Val loss 1.793\n",
      "Ep 2 (Step 047175): Train loss 1.099, Val loss 1.793\n",
      "Ep 2 (Step 047180): Train loss 1.342, Val loss 1.793\n",
      "Ep 2 (Step 047185): Train loss 1.134, Val loss 1.794\n",
      "Ep 2 (Step 047190): Train loss 1.094, Val loss 1.794\n",
      "Ep 2 (Step 047195): Train loss 1.128, Val loss 1.794\n",
      "Ep 2 (Step 047200): Train loss 0.973, Val loss 1.792\n",
      "Ep 2 (Step 047205): Train loss 1.067, Val loss 1.791\n",
      "Ep 2 (Step 047210): Train loss 0.945, Val loss 1.789\n",
      "Ep 2 (Step 047215): Train loss 1.105, Val loss 1.786\n",
      "Ep 2 (Step 047220): Train loss 0.964, Val loss 1.783\n",
      "Ep 2 (Step 047225): Train loss 1.042, Val loss 1.781\n",
      "Ep 2 (Step 047230): Train loss 1.012, Val loss 1.778\n",
      "Ep 2 (Step 047235): Train loss 1.000, Val loss 1.775\n",
      "Ep 2 (Step 047240): Train loss 1.305, Val loss 1.774\n",
      "Ep 2 (Step 047245): Train loss 0.940, Val loss 1.773\n",
      "Ep 2 (Step 047250): Train loss 0.987, Val loss 1.773\n",
      "Ep 2 (Step 047255): Train loss 1.265, Val loss 1.774\n",
      "Ep 2 (Step 047260): Train loss 0.936, Val loss 1.776\n",
      "Ep 2 (Step 047265): Train loss 1.080, Val loss 1.777\n",
      "Ep 2 (Step 047270): Train loss 1.367, Val loss 1.777\n",
      "Ep 2 (Step 047275): Train loss 0.816, Val loss 1.776\n",
      "Ep 2 (Step 047280): Train loss 1.170, Val loss 1.776\n",
      "Ep 2 (Step 047285): Train loss 0.852, Val loss 1.779\n",
      "Ep 2 (Step 047290): Train loss 1.292, Val loss 1.782\n",
      "Ep 2 (Step 047295): Train loss 0.902, Val loss 1.782\n",
      "Ep 2 (Step 047300): Train loss 1.167, Val loss 1.783\n",
      "Ep 2 (Step 047305): Train loss 1.057, Val loss 1.783\n",
      "Ep 2 (Step 047310): Train loss 1.099, Val loss 1.783\n",
      "Ep 2 (Step 047315): Train loss 1.069, Val loss 1.784\n",
      "Ep 2 (Step 047320): Train loss 1.161, Val loss 1.786\n",
      "Ep 2 (Step 047325): Train loss 1.096, Val loss 1.788\n",
      "Ep 2 (Step 047330): Train loss 1.128, Val loss 1.790\n",
      "Ep 2 (Step 047335): Train loss 0.836, Val loss 1.791\n",
      "Ep 2 (Step 047340): Train loss 1.033, Val loss 1.791\n",
      "Ep 2 (Step 047345): Train loss 1.203, Val loss 1.791\n",
      "Ep 2 (Step 047350): Train loss 1.270, Val loss 1.790\n",
      "Ep 2 (Step 047355): Train loss 1.233, Val loss 1.790\n",
      "Ep 2 (Step 047360): Train loss 1.238, Val loss 1.790\n",
      "Ep 2 (Step 047365): Train loss 1.243, Val loss 1.789\n",
      "Ep 2 (Step 047370): Train loss 1.123, Val loss 1.788\n",
      "Ep 2 (Step 047375): Train loss 1.225, Val loss 1.787\n",
      "Ep 2 (Step 047380): Train loss 1.105, Val loss 1.783\n",
      "Ep 2 (Step 047385): Train loss 1.091, Val loss 1.780\n",
      "Ep 2 (Step 047390): Train loss 1.085, Val loss 1.778\n",
      "Ep 2 (Step 047395): Train loss 1.322, Val loss 1.779\n",
      "Ep 2 (Step 047400): Train loss 1.096, Val loss 1.780\n",
      "Ep 2 (Step 047405): Train loss 1.317, Val loss 1.780\n",
      "Ep 2 (Step 047410): Train loss 1.163, Val loss 1.780\n",
      "Ep 2 (Step 047415): Train loss 1.078, Val loss 1.781\n",
      "Ep 2 (Step 047420): Train loss 1.062, Val loss 1.780\n",
      "Ep 2 (Step 047425): Train loss 1.219, Val loss 1.780\n",
      "Ep 2 (Step 047430): Train loss 1.119, Val loss 1.780\n",
      "Ep 2 (Step 047435): Train loss 1.316, Val loss 1.780\n",
      "Ep 2 (Step 047440): Train loss 0.998, Val loss 1.779\n",
      "Ep 2 (Step 047445): Train loss 1.303, Val loss 1.778\n",
      "Ep 2 (Step 047450): Train loss 1.275, Val loss 1.779\n",
      "Ep 2 (Step 047455): Train loss 1.105, Val loss 1.780\n",
      "Ep 2 (Step 047460): Train loss 1.154, Val loss 1.781\n",
      "Ep 2 (Step 047465): Train loss 1.093, Val loss 1.782\n",
      "Ep 2 (Step 047470): Train loss 1.245, Val loss 1.783\n",
      "Ep 2 (Step 047475): Train loss 1.188, Val loss 1.783\n",
      "Ep 2 (Step 047480): Train loss 0.973, Val loss 1.784\n",
      "Ep 2 (Step 047485): Train loss 0.800, Val loss 1.783\n",
      "Ep 2 (Step 047490): Train loss 1.080, Val loss 1.784\n",
      "Ep 2 (Step 047495): Train loss 1.108, Val loss 1.786\n",
      "Ep 2 (Step 047500): Train loss 1.063, Val loss 1.787\n",
      "Ep 2 (Step 047505): Train loss 1.022, Val loss 1.787\n",
      "Ep 2 (Step 047510): Train loss 1.241, Val loss 1.787\n",
      "Ep 2 (Step 047515): Train loss 0.751, Val loss 1.785\n",
      "Ep 2 (Step 047520): Train loss 1.235, Val loss 1.783\n",
      "Ep 2 (Step 047525): Train loss 1.082, Val loss 1.781\n",
      "Ep 2 (Step 047530): Train loss 1.383, Val loss 1.780\n",
      "Ep 2 (Step 047535): Train loss 1.367, Val loss 1.780\n",
      "Ep 2 (Step 047540): Train loss 1.075, Val loss 1.782\n",
      "Ep 2 (Step 047545): Train loss 1.186, Val loss 1.784\n",
      "Ep 2 (Step 047550): Train loss 1.041, Val loss 1.785\n",
      "Ep 2 (Step 047555): Train loss 1.283, Val loss 1.787\n",
      "Ep 2 (Step 047560): Train loss 0.896, Val loss 1.788\n",
      "Ep 2 (Step 047565): Train loss 1.081, Val loss 1.790\n",
      "Ep 2 (Step 047570): Train loss 0.649, Val loss 1.791\n",
      "Ep 2 (Step 047575): Train loss 1.025, Val loss 1.792\n",
      "Ep 2 (Step 047580): Train loss 1.022, Val loss 1.791\n",
      "Ep 2 (Step 047585): Train loss 1.119, Val loss 1.791\n",
      "Ep 2 (Step 047590): Train loss 0.895, Val loss 1.791\n",
      "Ep 2 (Step 047595): Train loss 1.244, Val loss 1.791\n",
      "Ep 2 (Step 047600): Train loss 1.245, Val loss 1.793\n",
      "Ep 2 (Step 047605): Train loss 1.331, Val loss 1.793\n",
      "Ep 2 (Step 047610): Train loss 0.892, Val loss 1.791\n",
      "Ep 2 (Step 047615): Train loss 1.171, Val loss 1.786\n",
      "Ep 2 (Step 047620): Train loss 0.880, Val loss 1.784\n",
      "Ep 2 (Step 047625): Train loss 0.993, Val loss 1.784\n",
      "Ep 2 (Step 047630): Train loss 1.077, Val loss 1.782\n",
      "Ep 2 (Step 047635): Train loss 1.341, Val loss 1.781\n",
      "Ep 2 (Step 047640): Train loss 1.400, Val loss 1.780\n",
      "Ep 2 (Step 047645): Train loss 0.725, Val loss 1.778\n",
      "Ep 2 (Step 047650): Train loss 0.951, Val loss 1.779\n",
      "Ep 2 (Step 047655): Train loss 1.026, Val loss 1.779\n",
      "Ep 2 (Step 047660): Train loss 0.999, Val loss 1.779\n",
      "Ep 2 (Step 047665): Train loss 1.032, Val loss 1.778\n",
      "Ep 2 (Step 047670): Train loss 0.824, Val loss 1.778\n",
      "Ep 2 (Step 047675): Train loss 1.107, Val loss 1.777\n",
      "Ep 2 (Step 047680): Train loss 1.165, Val loss 1.779\n",
      "Ep 2 (Step 047685): Train loss 1.126, Val loss 1.780\n",
      "Ep 2 (Step 047690): Train loss 1.052, Val loss 1.781\n",
      "Ep 2 (Step 047695): Train loss 0.991, Val loss 1.783\n",
      "Ep 2 (Step 047700): Train loss 0.986, Val loss 1.783\n",
      "Ep 2 (Step 047705): Train loss 1.186, Val loss 1.783\n",
      "Ep 2 (Step 047710): Train loss 0.950, Val loss 1.783\n",
      "Ep 2 (Step 047715): Train loss 0.868, Val loss 1.784\n",
      "Ep 2 (Step 047720): Train loss 1.182, Val loss 1.783\n",
      "Ep 2 (Step 047725): Train loss 1.114, Val loss 1.782\n",
      "Ep 2 (Step 047730): Train loss 1.001, Val loss 1.780\n",
      "Ep 2 (Step 047735): Train loss 1.133, Val loss 1.780\n",
      "Ep 2 (Step 047740): Train loss 0.967, Val loss 1.780\n",
      "Ep 2 (Step 047745): Train loss 1.022, Val loss 1.781\n",
      "Ep 2 (Step 047750): Train loss 0.921, Val loss 1.781\n",
      "Ep 2 (Step 047755): Train loss 1.067, Val loss 1.780\n",
      "Ep 2 (Step 047760): Train loss 1.067, Val loss 1.779\n",
      "Ep 2 (Step 047765): Train loss 1.127, Val loss 1.777\n",
      "Ep 2 (Step 047770): Train loss 0.888, Val loss 1.777\n",
      "Ep 2 (Step 047775): Train loss 1.066, Val loss 1.776\n",
      "Ep 2 (Step 047780): Train loss 0.804, Val loss 1.777\n",
      "Ep 2 (Step 047785): Train loss 0.976, Val loss 1.779\n",
      "Ep 2 (Step 047790): Train loss 1.104, Val loss 1.780\n",
      "Ep 2 (Step 047795): Train loss 0.979, Val loss 1.782\n",
      "Ep 2 (Step 047800): Train loss 1.123, Val loss 1.780\n",
      "Ep 2 (Step 047805): Train loss 0.987, Val loss 1.780\n",
      "Ep 2 (Step 047810): Train loss 0.966, Val loss 1.780\n",
      "Ep 2 (Step 047815): Train loss 1.176, Val loss 1.780\n",
      "Ep 2 (Step 047820): Train loss 1.264, Val loss 1.779\n",
      "Ep 2 (Step 047825): Train loss 0.808, Val loss 1.776\n",
      "Ep 2 (Step 047830): Train loss 1.063, Val loss 1.775\n",
      "Ep 2 (Step 047835): Train loss 1.131, Val loss 1.772\n",
      "Ep 2 (Step 047840): Train loss 1.189, Val loss 1.771\n",
      "Ep 2 (Step 047845): Train loss 1.122, Val loss 1.770\n",
      "Ep 2 (Step 047850): Train loss 1.068, Val loss 1.769\n",
      "Ep 2 (Step 047855): Train loss 0.890, Val loss 1.767\n",
      "Ep 2 (Step 047860): Train loss 1.336, Val loss 1.767\n",
      "Ep 2 (Step 047865): Train loss 1.063, Val loss 1.767\n",
      "Ep 2 (Step 047870): Train loss 1.384, Val loss 1.769\n",
      "Ep 2 (Step 047875): Train loss 1.200, Val loss 1.769\n",
      "Ep 2 (Step 047880): Train loss 1.219, Val loss 1.770\n",
      "Ep 2 (Step 047885): Train loss 1.264, Val loss 1.770\n",
      "Ep 2 (Step 047890): Train loss 1.437, Val loss 1.770\n",
      "Ep 2 (Step 047895): Train loss 1.131, Val loss 1.769\n",
      "Ep 2 (Step 047900): Train loss 0.945, Val loss 1.769\n",
      "Ep 2 (Step 047905): Train loss 1.019, Val loss 1.770\n",
      "Ep 2 (Step 047910): Train loss 1.118, Val loss 1.771\n",
      "Ep 2 (Step 047915): Train loss 0.933, Val loss 1.772\n",
      "Ep 2 (Step 047920): Train loss 0.992, Val loss 1.772\n",
      "Ep 2 (Step 047925): Train loss 0.832, Val loss 1.773\n",
      "Ep 2 (Step 047930): Train loss 1.061, Val loss 1.774\n",
      "Ep 2 (Step 047935): Train loss 1.128, Val loss 1.775\n",
      "Ep 2 (Step 047940): Train loss 0.992, Val loss 1.776\n",
      "Ep 2 (Step 047945): Train loss 1.013, Val loss 1.776\n",
      "Ep 2 (Step 047950): Train loss 1.082, Val loss 1.777\n",
      "Ep 2 (Step 047955): Train loss 1.314, Val loss 1.779\n",
      "Ep 2 (Step 047960): Train loss 1.380, Val loss 1.779\n",
      "Ep 2 (Step 047965): Train loss 1.086, Val loss 1.779\n",
      "Ep 2 (Step 047970): Train loss 0.932, Val loss 1.780\n",
      "Ep 2 (Step 047975): Train loss 1.211, Val loss 1.780\n",
      "Ep 2 (Step 047980): Train loss 1.063, Val loss 1.779\n",
      "Ep 2 (Step 047985): Train loss 0.869, Val loss 1.778\n",
      "Ep 2 (Step 047990): Train loss 1.134, Val loss 1.776\n",
      "Ep 2 (Step 047995): Train loss 1.099, Val loss 1.777\n",
      "Ep 2 (Step 048000): Train loss 1.119, Val loss 1.778\n",
      "Ep 2 (Step 048005): Train loss 1.380, Val loss 1.778\n",
      "Ep 2 (Step 048010): Train loss 1.220, Val loss 1.778\n",
      "Ep 2 (Step 048015): Train loss 1.387, Val loss 1.777\n",
      "Ep 2 (Step 048020): Train loss 1.146, Val loss 1.778\n",
      "Ep 2 (Step 048025): Train loss 0.823, Val loss 1.779\n",
      "Ep 2 (Step 048030): Train loss 1.067, Val loss 1.778\n",
      "Ep 2 (Step 048035): Train loss 1.054, Val loss 1.776\n",
      "Ep 2 (Step 048040): Train loss 1.307, Val loss 1.777\n",
      "Ep 2 (Step 048045): Train loss 1.030, Val loss 1.777\n",
      "Ep 2 (Step 048050): Train loss 0.781, Val loss 1.778\n",
      "Ep 2 (Step 048055): Train loss 1.114, Val loss 1.777\n",
      "Ep 2 (Step 048060): Train loss 0.966, Val loss 1.774\n",
      "Ep 2 (Step 048065): Train loss 1.072, Val loss 1.773\n",
      "Ep 2 (Step 048070): Train loss 1.172, Val loss 1.772\n",
      "Ep 2 (Step 048075): Train loss 0.781, Val loss 1.771\n",
      "Ep 2 (Step 048080): Train loss 1.167, Val loss 1.771\n",
      "Ep 2 (Step 048085): Train loss 0.988, Val loss 1.772\n",
      "Ep 2 (Step 048090): Train loss 1.149, Val loss 1.773\n",
      "Ep 2 (Step 048095): Train loss 1.216, Val loss 1.774\n",
      "Ep 2 (Step 048100): Train loss 0.718, Val loss 1.775\n",
      "Ep 2 (Step 048105): Train loss 0.854, Val loss 1.776\n",
      "Ep 2 (Step 048110): Train loss 0.985, Val loss 1.776\n",
      "Ep 2 (Step 048115): Train loss 0.995, Val loss 1.778\n",
      "Ep 2 (Step 048120): Train loss 1.120, Val loss 1.779\n",
      "Ep 2 (Step 048125): Train loss 0.839, Val loss 1.780\n",
      "Ep 2 (Step 048130): Train loss 1.245, Val loss 1.780\n",
      "Ep 2 (Step 048135): Train loss 0.944, Val loss 1.780\n",
      "Ep 2 (Step 048140): Train loss 1.196, Val loss 1.779\n",
      "Ep 2 (Step 048145): Train loss 0.980, Val loss 1.779\n",
      "Ep 2 (Step 048150): Train loss 1.269, Val loss 1.778\n",
      "Ep 2 (Step 048155): Train loss 1.148, Val loss 1.778\n",
      "Ep 2 (Step 048160): Train loss 1.022, Val loss 1.776\n",
      "Ep 2 (Step 048165): Train loss 1.238, Val loss 1.774\n",
      "Ep 2 (Step 048170): Train loss 1.224, Val loss 1.774\n",
      "Ep 2 (Step 048175): Train loss 1.255, Val loss 1.775\n",
      "Ep 2 (Step 048180): Train loss 1.097, Val loss 1.777\n",
      "Ep 2 (Step 048185): Train loss 0.962, Val loss 1.778\n",
      "Ep 2 (Step 048190): Train loss 1.095, Val loss 1.778\n",
      "Ep 2 (Step 048195): Train loss 1.390, Val loss 1.781\n",
      "Ep 2 (Step 048200): Train loss 1.261, Val loss 1.783\n",
      "Ep 2 (Step 048205): Train loss 1.025, Val loss 1.784\n",
      "Ep 2 (Step 048210): Train loss 0.890, Val loss 1.784\n",
      "Ep 2 (Step 048215): Train loss 1.156, Val loss 1.787\n",
      "Ep 2 (Step 048220): Train loss 1.095, Val loss 1.788\n",
      "Ep 2 (Step 048225): Train loss 1.320, Val loss 1.788\n",
      "Ep 2 (Step 048230): Train loss 1.113, Val loss 1.790\n",
      "Ep 2 (Step 048235): Train loss 1.115, Val loss 1.791\n",
      "Ep 2 (Step 048240): Train loss 0.938, Val loss 1.793\n",
      "Ep 2 (Step 048245): Train loss 1.256, Val loss 1.794\n",
      "Ep 2 (Step 048250): Train loss 1.155, Val loss 1.794\n",
      "Ep 2 (Step 048255): Train loss 1.026, Val loss 1.791\n",
      "Ep 2 (Step 048260): Train loss 0.734, Val loss 1.791\n",
      "Ep 2 (Step 048265): Train loss 0.976, Val loss 1.790\n",
      "Ep 2 (Step 048270): Train loss 1.202, Val loss 1.790\n",
      "Ep 2 (Step 048275): Train loss 1.111, Val loss 1.789\n",
      "Ep 2 (Step 048280): Train loss 0.943, Val loss 1.788\n",
      "Ep 2 (Step 048285): Train loss 1.003, Val loss 1.786\n",
      "Ep 2 (Step 048290): Train loss 0.925, Val loss 1.787\n",
      "Ep 2 (Step 048295): Train loss 1.379, Val loss 1.787\n",
      "Ep 2 (Step 048300): Train loss 1.084, Val loss 1.786\n",
      "Ep 2 (Step 048305): Train loss 1.001, Val loss 1.786\n",
      "Ep 2 (Step 048310): Train loss 1.192, Val loss 1.786\n",
      "Ep 2 (Step 048315): Train loss 1.232, Val loss 1.785\n",
      "Ep 2 (Step 048320): Train loss 1.183, Val loss 1.784\n",
      "Ep 2 (Step 048325): Train loss 1.004, Val loss 1.781\n",
      "Ep 2 (Step 048330): Train loss 0.937, Val loss 1.779\n",
      "Ep 2 (Step 048335): Train loss 1.113, Val loss 1.778\n",
      "Ep 2 (Step 048340): Train loss 0.835, Val loss 1.777\n",
      "Ep 2 (Step 048345): Train loss 0.843, Val loss 1.777\n",
      "Ep 2 (Step 048350): Train loss 1.042, Val loss 1.777\n",
      "Ep 2 (Step 048355): Train loss 1.151, Val loss 1.779\n",
      "Ep 2 (Step 048360): Train loss 0.718, Val loss 1.782\n",
      "Ep 2 (Step 048365): Train loss 0.971, Val loss 1.784\n",
      "Ep 2 (Step 048370): Train loss 1.260, Val loss 1.785\n",
      "Ep 2 (Step 048375): Train loss 1.219, Val loss 1.784\n",
      "Ep 2 (Step 048380): Train loss 1.018, Val loss 1.783\n",
      "Ep 2 (Step 048385): Train loss 0.979, Val loss 1.782\n",
      "Ep 2 (Step 048390): Train loss 0.904, Val loss 1.780\n",
      "Ep 2 (Step 048395): Train loss 1.142, Val loss 1.779\n",
      "Ep 2 (Step 048400): Train loss 1.119, Val loss 1.777\n",
      "Ep 2 (Step 048405): Train loss 0.878, Val loss 1.776\n",
      "Ep 2 (Step 048410): Train loss 1.131, Val loss 1.776\n",
      "Ep 2 (Step 048415): Train loss 1.274, Val loss 1.776\n",
      "Ep 2 (Step 048420): Train loss 1.102, Val loss 1.774\n",
      "Ep 2 (Step 048425): Train loss 0.761, Val loss 1.774\n",
      "Ep 2 (Step 048430): Train loss 1.128, Val loss 1.775\n",
      "Ep 2 (Step 048435): Train loss 1.222, Val loss 1.773\n",
      "Ep 2 (Step 048440): Train loss 1.038, Val loss 1.772\n",
      "Ep 2 (Step 048445): Train loss 0.866, Val loss 1.773\n",
      "Ep 2 (Step 048450): Train loss 1.418, Val loss 1.773\n",
      "Ep 2 (Step 048455): Train loss 0.966, Val loss 1.773\n",
      "Ep 2 (Step 048460): Train loss 1.077, Val loss 1.775\n",
      "Ep 2 (Step 048465): Train loss 1.215, Val loss 1.775\n",
      "Ep 2 (Step 048470): Train loss 1.112, Val loss 1.774\n",
      "Ep 2 (Step 048475): Train loss 1.214, Val loss 1.773\n",
      "Ep 2 (Step 048480): Train loss 1.262, Val loss 1.774\n",
      "Ep 2 (Step 048485): Train loss 1.088, Val loss 1.776\n",
      "Ep 2 (Step 048490): Train loss 0.888, Val loss 1.774\n",
      "Ep 2 (Step 048495): Train loss 1.039, Val loss 1.773\n",
      "Ep 2 (Step 048500): Train loss 0.912, Val loss 1.772\n",
      "Ep 2 (Step 048505): Train loss 1.085, Val loss 1.771\n",
      "Ep 2 (Step 048510): Train loss 1.238, Val loss 1.770\n",
      "Ep 2 (Step 048515): Train loss 1.135, Val loss 1.771\n",
      "Ep 2 (Step 048520): Train loss 1.251, Val loss 1.771\n",
      "Ep 2 (Step 048525): Train loss 1.131, Val loss 1.772\n",
      "Ep 2 (Step 048530): Train loss 1.095, Val loss 1.774\n",
      "Ep 2 (Step 048535): Train loss 1.193, Val loss 1.775\n",
      "Ep 2 (Step 048540): Train loss 1.127, Val loss 1.775\n",
      "Ep 2 (Step 048545): Train loss 1.258, Val loss 1.775\n",
      "Ep 2 (Step 048550): Train loss 1.063, Val loss 1.775\n",
      "Ep 2 (Step 048555): Train loss 1.374, Val loss 1.776\n",
      "Ep 2 (Step 048560): Train loss 0.986, Val loss 1.777\n",
      "Ep 2 (Step 048565): Train loss 0.746, Val loss 1.778\n",
      "Ep 2 (Step 048570): Train loss 1.080, Val loss 1.778\n",
      "Ep 2 (Step 048575): Train loss 0.788, Val loss 1.779\n",
      "Ep 2 (Step 048580): Train loss 0.929, Val loss 1.779\n",
      "Ep 2 (Step 048585): Train loss 1.066, Val loss 1.779\n",
      "Ep 2 (Step 048590): Train loss 1.183, Val loss 1.779\n",
      "Ep 2 (Step 048595): Train loss 1.106, Val loss 1.779\n",
      "Ep 2 (Step 048600): Train loss 1.289, Val loss 1.780\n",
      "Ep 2 (Step 048605): Train loss 1.124, Val loss 1.782\n",
      "Ep 2 (Step 048610): Train loss 1.077, Val loss 1.783\n",
      "Ep 2 (Step 048615): Train loss 1.082, Val loss 1.784\n",
      "Ep 2 (Step 048620): Train loss 1.053, Val loss 1.784\n",
      "Ep 2 (Step 048625): Train loss 1.117, Val loss 1.782\n",
      "Ep 2 (Step 048630): Train loss 1.262, Val loss 1.782\n",
      "Ep 2 (Step 048635): Train loss 1.018, Val loss 1.782\n",
      "Ep 2 (Step 048640): Train loss 1.173, Val loss 1.780\n",
      "Ep 2 (Step 048645): Train loss 0.862, Val loss 1.779\n",
      "Ep 2 (Step 048650): Train loss 1.071, Val loss 1.780\n",
      "Ep 2 (Step 048655): Train loss 1.083, Val loss 1.781\n",
      "Ep 2 (Step 048660): Train loss 1.086, Val loss 1.783\n",
      "Ep 2 (Step 048665): Train loss 1.038, Val loss 1.784\n",
      "Ep 2 (Step 048670): Train loss 0.762, Val loss 1.783\n",
      "Ep 2 (Step 048675): Train loss 1.162, Val loss 1.783\n",
      "Ep 2 (Step 048680): Train loss 0.684, Val loss 1.783\n",
      "Ep 2 (Step 048685): Train loss 0.885, Val loss 1.781\n",
      "Ep 2 (Step 048690): Train loss 0.929, Val loss 1.780\n",
      "Ep 2 (Step 048695): Train loss 0.968, Val loss 1.781\n",
      "Ep 2 (Step 048700): Train loss 1.072, Val loss 1.782\n",
      "Ep 2 (Step 048705): Train loss 1.148, Val loss 1.783\n",
      "Ep 2 (Step 048710): Train loss 1.056, Val loss 1.786\n",
      "Ep 2 (Step 048715): Train loss 0.950, Val loss 1.786\n",
      "Ep 2 (Step 048720): Train loss 1.262, Val loss 1.786\n",
      "Ep 2 (Step 048725): Train loss 1.122, Val loss 1.787\n",
      "Ep 2 (Step 048730): Train loss 1.255, Val loss 1.786\n",
      "Ep 2 (Step 048735): Train loss 1.372, Val loss 1.786\n",
      "Ep 2 (Step 048740): Train loss 1.009, Val loss 1.785\n",
      "Ep 2 (Step 048745): Train loss 1.232, Val loss 1.783\n",
      "Ep 2 (Step 048750): Train loss 1.002, Val loss 1.781\n",
      "Ep 2 (Step 048755): Train loss 1.066, Val loss 1.781\n",
      "Ep 2 (Step 048760): Train loss 0.896, Val loss 1.783\n",
      "Ep 2 (Step 048765): Train loss 1.048, Val loss 1.784\n",
      "Ep 2 (Step 048770): Train loss 1.155, Val loss 1.784\n",
      "Ep 2 (Step 048775): Train loss 1.062, Val loss 1.785\n",
      "Ep 2 (Step 048780): Train loss 1.098, Val loss 1.786\n",
      "Ep 2 (Step 048785): Train loss 0.863, Val loss 1.787\n",
      "Ep 2 (Step 048790): Train loss 1.206, Val loss 1.788\n",
      "Ep 2 (Step 048795): Train loss 1.279, Val loss 1.789\n",
      "Ep 2 (Step 048800): Train loss 1.205, Val loss 1.788\n",
      "Ep 2 (Step 048805): Train loss 1.044, Val loss 1.789\n",
      "Ep 2 (Step 048810): Train loss 1.264, Val loss 1.787\n",
      "Ep 2 (Step 048815): Train loss 0.790, Val loss 1.785\n",
      "Ep 2 (Step 048820): Train loss 0.833, Val loss 1.783\n",
      "Ep 2 (Step 048825): Train loss 0.979, Val loss 1.782\n",
      "Ep 2 (Step 048830): Train loss 1.369, Val loss 1.782\n",
      "Ep 2 (Step 048835): Train loss 1.024, Val loss 1.782\n",
      "Ep 2 (Step 048840): Train loss 0.982, Val loss 1.782\n",
      "Ep 2 (Step 048845): Train loss 1.100, Val loss 1.784\n",
      "Ep 2 (Step 048850): Train loss 1.190, Val loss 1.785\n",
      "Ep 2 (Step 048855): Train loss 1.068, Val loss 1.787\n",
      "Ep 2 (Step 048860): Train loss 1.030, Val loss 1.786\n",
      "Ep 2 (Step 048865): Train loss 0.956, Val loss 1.785\n",
      "Ep 2 (Step 048870): Train loss 1.059, Val loss 1.786\n",
      "Ep 2 (Step 048875): Train loss 0.980, Val loss 1.785\n",
      "Ep 2 (Step 048880): Train loss 1.219, Val loss 1.785\n",
      "Ep 2 (Step 048885): Train loss 1.097, Val loss 1.787\n",
      "Ep 2 (Step 048890): Train loss 1.429, Val loss 1.788\n",
      "Ep 2 (Step 048895): Train loss 0.835, Val loss 1.790\n",
      "Ep 2 (Step 048900): Train loss 0.814, Val loss 1.792\n",
      "Ep 2 (Step 048905): Train loss 1.091, Val loss 1.794\n",
      "Ep 2 (Step 048910): Train loss 0.956, Val loss 1.795\n",
      "Ep 2 (Step 048915): Train loss 1.172, Val loss 1.794\n",
      "Ep 2 (Step 048920): Train loss 1.331, Val loss 1.793\n",
      "Ep 2 (Step 048925): Train loss 0.872, Val loss 1.791\n",
      "Ep 2 (Step 048930): Train loss 1.337, Val loss 1.790\n",
      "Ep 2 (Step 048935): Train loss 1.286, Val loss 1.790\n",
      "Ep 2 (Step 048940): Train loss 1.051, Val loss 1.790\n",
      "Ep 2 (Step 048945): Train loss 1.076, Val loss 1.789\n",
      "Ep 2 (Step 048950): Train loss 1.193, Val loss 1.786\n",
      "Ep 2 (Step 048955): Train loss 1.115, Val loss 1.787\n",
      "Ep 2 (Step 048960): Train loss 1.306, Val loss 1.787\n",
      "Ep 2 (Step 048965): Train loss 0.841, Val loss 1.787\n",
      "Ep 2 (Step 048970): Train loss 1.178, Val loss 1.787\n",
      "Ep 2 (Step 048975): Train loss 1.156, Val loss 1.788\n",
      "Ep 2 (Step 048980): Train loss 1.436, Val loss 1.787\n",
      "Ep 2 (Step 048985): Train loss 0.847, Val loss 1.788\n",
      "Ep 2 (Step 048990): Train loss 0.932, Val loss 1.789\n",
      "Ep 2 (Step 048995): Train loss 1.190, Val loss 1.787\n",
      "Ep 2 (Step 049000): Train loss 1.150, Val loss 1.785\n",
      "Ep 2 (Step 049005): Train loss 1.502, Val loss 1.784\n",
      "Ep 2 (Step 049010): Train loss 1.183, Val loss 1.784\n",
      "Ep 2 (Step 049015): Train loss 1.161, Val loss 1.785\n",
      "Ep 2 (Step 049020): Train loss 1.185, Val loss 1.786\n",
      "Ep 2 (Step 049025): Train loss 1.231, Val loss 1.787\n",
      "Ep 2 (Step 049030): Train loss 0.910, Val loss 1.788\n",
      "Ep 2 (Step 049035): Train loss 1.002, Val loss 1.789\n",
      "Ep 2 (Step 049040): Train loss 1.119, Val loss 1.789\n",
      "Ep 2 (Step 049045): Train loss 0.996, Val loss 1.790\n",
      "Ep 2 (Step 049050): Train loss 1.298, Val loss 1.790\n",
      "Ep 2 (Step 049055): Train loss 1.224, Val loss 1.789\n",
      "Ep 2 (Step 049060): Train loss 1.058, Val loss 1.787\n",
      "Ep 2 (Step 049065): Train loss 1.167, Val loss 1.786\n",
      "Ep 2 (Step 049070): Train loss 1.101, Val loss 1.786\n",
      "Ep 2 (Step 049075): Train loss 1.106, Val loss 1.785\n",
      "Ep 2 (Step 049080): Train loss 1.044, Val loss 1.785\n",
      "Ep 2 (Step 049085): Train loss 0.889, Val loss 1.785\n",
      "Ep 2 (Step 049090): Train loss 0.993, Val loss 1.786\n",
      "Ep 2 (Step 049095): Train loss 1.219, Val loss 1.787\n",
      "Ep 2 (Step 049100): Train loss 1.113, Val loss 1.788\n",
      "Ep 2 (Step 049105): Train loss 1.125, Val loss 1.788\n",
      "Ep 2 (Step 049110): Train loss 0.881, Val loss 1.789\n",
      "Ep 2 (Step 049115): Train loss 1.185, Val loss 1.789\n",
      "Ep 2 (Step 049120): Train loss 1.170, Val loss 1.789\n",
      "Ep 2 (Step 049125): Train loss 1.017, Val loss 1.788\n",
      "Ep 2 (Step 049130): Train loss 0.898, Val loss 1.788\n",
      "Ep 2 (Step 049135): Train loss 1.220, Val loss 1.789\n",
      "Ep 2 (Step 049140): Train loss 1.020, Val loss 1.790\n",
      "Ep 2 (Step 049145): Train loss 0.920, Val loss 1.791\n",
      "Ep 2 (Step 049150): Train loss 1.185, Val loss 1.792\n",
      "Ep 2 (Step 049155): Train loss 1.292, Val loss 1.791\n",
      "Ep 2 (Step 049160): Train loss 0.992, Val loss 1.790\n",
      "Ep 2 (Step 049165): Train loss 1.038, Val loss 1.789\n",
      "Ep 2 (Step 049170): Train loss 1.213, Val loss 1.790\n",
      "Ep 2 (Step 049175): Train loss 1.075, Val loss 1.791\n",
      "Ep 2 (Step 049180): Train loss 1.231, Val loss 1.790\n",
      "Ep 2 (Step 049185): Train loss 1.068, Val loss 1.790\n",
      "Ep 2 (Step 049190): Train loss 0.970, Val loss 1.790\n",
      "Ep 2 (Step 049195): Train loss 1.140, Val loss 1.788\n",
      "Ep 2 (Step 049200): Train loss 0.963, Val loss 1.786\n",
      "Ep 2 (Step 049205): Train loss 1.119, Val loss 1.786\n",
      "Ep 2 (Step 049210): Train loss 1.220, Val loss 1.786\n",
      "Ep 2 (Step 049215): Train loss 1.176, Val loss 1.785\n",
      "Ep 2 (Step 049220): Train loss 0.983, Val loss 1.784\n",
      "Ep 2 (Step 049225): Train loss 1.164, Val loss 1.783\n",
      "Ep 2 (Step 049230): Train loss 1.099, Val loss 1.783\n",
      "Ep 2 (Step 049235): Train loss 0.909, Val loss 1.783\n",
      "Ep 2 (Step 049240): Train loss 1.310, Val loss 1.781\n",
      "Ep 2 (Step 049245): Train loss 0.865, Val loss 1.780\n",
      "Ep 2 (Step 049250): Train loss 1.088, Val loss 1.780\n",
      "Ep 2 (Step 049255): Train loss 1.111, Val loss 1.780\n",
      "Ep 2 (Step 049260): Train loss 1.203, Val loss 1.779\n",
      "Ep 2 (Step 049265): Train loss 1.048, Val loss 1.778\n",
      "Ep 2 (Step 049270): Train loss 1.023, Val loss 1.777\n",
      "Ep 2 (Step 049275): Train loss 0.988, Val loss 1.776\n",
      "Ep 2 (Step 049280): Train loss 1.015, Val loss 1.773\n",
      "Ep 2 (Step 049285): Train loss 0.945, Val loss 1.770\n",
      "Ep 2 (Step 049290): Train loss 1.030, Val loss 1.770\n",
      "Ep 2 (Step 049295): Train loss 0.964, Val loss 1.771\n",
      "Ep 2 (Step 049300): Train loss 1.181, Val loss 1.771\n",
      "Ep 2 (Step 049305): Train loss 1.130, Val loss 1.772\n",
      "Ep 2 (Step 049310): Train loss 0.890, Val loss 1.772\n",
      "Ep 2 (Step 049315): Train loss 1.291, Val loss 1.773\n",
      "Ep 2 (Step 049320): Train loss 0.921, Val loss 1.774\n",
      "Ep 2 (Step 049325): Train loss 0.943, Val loss 1.776\n",
      "Ep 2 (Step 049330): Train loss 1.572, Val loss 1.777\n",
      "Ep 2 (Step 049335): Train loss 0.931, Val loss 1.778\n",
      "Ep 2 (Step 049340): Train loss 0.949, Val loss 1.776\n",
      "Ep 2 (Step 049345): Train loss 1.237, Val loss 1.776\n",
      "Ep 2 (Step 049350): Train loss 1.082, Val loss 1.778\n",
      "Ep 2 (Step 049355): Train loss 1.194, Val loss 1.780\n",
      "Ep 2 (Step 049360): Train loss 1.027, Val loss 1.783\n",
      "Ep 2 (Step 049365): Train loss 1.205, Val loss 1.783\n",
      "Ep 2 (Step 049370): Train loss 1.373, Val loss 1.783\n",
      "Ep 2 (Step 049375): Train loss 1.167, Val loss 1.782\n",
      "Ep 2 (Step 049380): Train loss 0.861, Val loss 1.784\n",
      "Ep 2 (Step 049385): Train loss 1.064, Val loss 1.786\n",
      "Ep 2 (Step 049390): Train loss 1.162, Val loss 1.787\n",
      "Ep 2 (Step 049395): Train loss 1.257, Val loss 1.787\n",
      "Ep 2 (Step 049400): Train loss 0.929, Val loss 1.787\n",
      "Ep 2 (Step 049405): Train loss 1.113, Val loss 1.786\n",
      "Ep 2 (Step 049410): Train loss 1.055, Val loss 1.786\n",
      "Ep 2 (Step 049415): Train loss 0.915, Val loss 1.787\n",
      "Ep 2 (Step 049420): Train loss 1.069, Val loss 1.787\n",
      "Ep 2 (Step 049425): Train loss 0.874, Val loss 1.787\n",
      "Ep 2 (Step 049430): Train loss 1.253, Val loss 1.787\n",
      "Ep 2 (Step 049435): Train loss 1.127, Val loss 1.787\n",
      "Ep 2 (Step 049440): Train loss 1.277, Val loss 1.785\n",
      "Ep 2 (Step 049445): Train loss 1.286, Val loss 1.784\n",
      "Ep 2 (Step 049450): Train loss 1.294, Val loss 1.785\n",
      "Ep 2 (Step 049455): Train loss 0.955, Val loss 1.785\n",
      "Ep 2 (Step 049460): Train loss 0.860, Val loss 1.784\n",
      "Ep 2 (Step 049465): Train loss 1.064, Val loss 1.781\n",
      "Ep 2 (Step 049470): Train loss 1.271, Val loss 1.779\n",
      "Ep 2 (Step 049475): Train loss 1.108, Val loss 1.778\n",
      "Ep 2 (Step 049480): Train loss 1.148, Val loss 1.779\n",
      "Ep 2 (Step 049485): Train loss 1.207, Val loss 1.780\n",
      "Ep 2 (Step 049490): Train loss 0.799, Val loss 1.779\n",
      "Ep 2 (Step 049495): Train loss 1.013, Val loss 1.778\n",
      "Ep 2 (Step 049500): Train loss 1.058, Val loss 1.775\n",
      "Ep 2 (Step 049505): Train loss 0.956, Val loss 1.774\n",
      "Ep 2 (Step 049510): Train loss 1.356, Val loss 1.773\n",
      "Ep 2 (Step 049515): Train loss 0.943, Val loss 1.773\n",
      "Ep 2 (Step 049520): Train loss 0.947, Val loss 1.773\n",
      "Ep 2 (Step 049525): Train loss 1.108, Val loss 1.773\n",
      "Ep 2 (Step 049530): Train loss 1.240, Val loss 1.771\n",
      "Ep 2 (Step 049535): Train loss 1.030, Val loss 1.769\n",
      "Ep 2 (Step 049540): Train loss 1.103, Val loss 1.769\n",
      "Ep 2 (Step 049545): Train loss 1.210, Val loss 1.770\n",
      "Ep 2 (Step 049550): Train loss 1.416, Val loss 1.770\n",
      "Ep 2 (Step 049555): Train loss 0.807, Val loss 1.770\n",
      "Ep 2 (Step 049560): Train loss 1.339, Val loss 1.770\n",
      "Ep 2 (Step 049565): Train loss 1.255, Val loss 1.771\n",
      "Ep 2 (Step 049570): Train loss 1.215, Val loss 1.771\n",
      "Ep 2 (Step 049575): Train loss 1.015, Val loss 1.771\n",
      "Ep 2 (Step 049580): Train loss 0.936, Val loss 1.770\n",
      "Ep 2 (Step 049585): Train loss 0.993, Val loss 1.770\n",
      "Ep 2 (Step 049590): Train loss 0.879, Val loss 1.770\n",
      "Ep 2 (Step 049595): Train loss 0.656, Val loss 1.770\n",
      "Ep 2 (Step 049600): Train loss 1.126, Val loss 1.770\n",
      "Ep 2 (Step 049605): Train loss 1.100, Val loss 1.769\n",
      "Ep 2 (Step 049610): Train loss 1.138, Val loss 1.768\n",
      "Ep 2 (Step 049615): Train loss 0.964, Val loss 1.767\n",
      "Ep 2 (Step 049620): Train loss 1.140, Val loss 1.766\n",
      "Ep 2 (Step 049625): Train loss 1.003, Val loss 1.765\n",
      "Ep 2 (Step 049630): Train loss 0.997, Val loss 1.764\n",
      "Ep 2 (Step 049635): Train loss 0.861, Val loss 1.763\n",
      "Ep 2 (Step 049640): Train loss 1.677, Val loss 1.763\n",
      "Ep 2 (Step 049645): Train loss 0.990, Val loss 1.763\n",
      "Ep 2 (Step 049650): Train loss 1.236, Val loss 1.762\n",
      "Ep 2 (Step 049655): Train loss 1.082, Val loss 1.763\n",
      "Ep 2 (Step 049660): Train loss 1.506, Val loss 1.764\n",
      "Ep 2 (Step 049665): Train loss 0.777, Val loss 1.765\n",
      "Ep 2 (Step 049670): Train loss 1.027, Val loss 1.766\n",
      "Ep 2 (Step 049675): Train loss 1.361, Val loss 1.766\n",
      "Ep 2 (Step 049680): Train loss 1.292, Val loss 1.765\n",
      "Ep 2 (Step 049685): Train loss 1.206, Val loss 1.765\n",
      "Ep 2 (Step 049690): Train loss 1.073, Val loss 1.765\n",
      "Ep 2 (Step 049695): Train loss 1.452, Val loss 1.764\n",
      "Ep 2 (Step 049700): Train loss 1.150, Val loss 1.763\n",
      "Ep 2 (Step 049705): Train loss 1.176, Val loss 1.763\n",
      "Ep 2 (Step 049710): Train loss 0.914, Val loss 1.764\n",
      "Ep 2 (Step 049715): Train loss 1.290, Val loss 1.763\n",
      "Ep 2 (Step 049720): Train loss 1.226, Val loss 1.762\n",
      "Ep 2 (Step 049725): Train loss 0.947, Val loss 1.763\n",
      "Ep 2 (Step 049730): Train loss 0.835, Val loss 1.764\n",
      "Ep 2 (Step 049735): Train loss 0.977, Val loss 1.766\n",
      "Ep 2 (Step 049740): Train loss 1.037, Val loss 1.766\n",
      "Ep 2 (Step 049745): Train loss 1.201, Val loss 1.767\n",
      "Ep 2 (Step 049750): Train loss 1.318, Val loss 1.769\n",
      "Ep 2 (Step 049755): Train loss 0.884, Val loss 1.769\n",
      "Ep 2 (Step 049760): Train loss 1.075, Val loss 1.769\n",
      "Ep 2 (Step 049765): Train loss 1.154, Val loss 1.768\n",
      "Ep 2 (Step 049770): Train loss 0.958, Val loss 1.769\n",
      "Ep 2 (Step 049775): Train loss 0.948, Val loss 1.771\n",
      "Ep 2 (Step 049780): Train loss 1.142, Val loss 1.773\n",
      "Ep 2 (Step 049785): Train loss 1.042, Val loss 1.776\n",
      "Ep 2 (Step 049790): Train loss 0.958, Val loss 1.777\n",
      "Ep 2 (Step 049795): Train loss 1.094, Val loss 1.776\n",
      "Ep 2 (Step 049800): Train loss 0.974, Val loss 1.778\n",
      "Ep 2 (Step 049805): Train loss 1.006, Val loss 1.778\n",
      "Ep 2 (Step 049810): Train loss 0.885, Val loss 1.778\n",
      "Ep 2 (Step 049815): Train loss 1.324, Val loss 1.778\n",
      "Ep 2 (Step 049820): Train loss 1.066, Val loss 1.777\n",
      "Ep 2 (Step 049825): Train loss 1.288, Val loss 1.777\n",
      "Ep 2 (Step 049830): Train loss 1.054, Val loss 1.776\n",
      "Ep 2 (Step 049835): Train loss 0.922, Val loss 1.777\n",
      "Ep 2 (Step 049840): Train loss 1.257, Val loss 1.777\n",
      "Ep 2 (Step 049845): Train loss 1.006, Val loss 1.777\n",
      "Ep 2 (Step 049850): Train loss 1.252, Val loss 1.777\n",
      "Ep 2 (Step 049855): Train loss 1.206, Val loss 1.778\n",
      "Ep 2 (Step 049860): Train loss 1.483, Val loss 1.779\n",
      "Ep 2 (Step 049865): Train loss 1.187, Val loss 1.779\n",
      "Ep 2 (Step 049870): Train loss 1.250, Val loss 1.779\n",
      "Ep 2 (Step 049875): Train loss 1.276, Val loss 1.779\n",
      "Ep 2 (Step 049880): Train loss 1.319, Val loss 1.779\n",
      "Ep 2 (Step 049885): Train loss 0.886, Val loss 1.778\n",
      "Ep 2 (Step 049890): Train loss 1.099, Val loss 1.779\n",
      "Ep 2 (Step 049895): Train loss 0.998, Val loss 1.779\n",
      "Ep 2 (Step 049900): Train loss 1.249, Val loss 1.778\n",
      "Ep 2 (Step 049905): Train loss 0.985, Val loss 1.778\n",
      "Ep 2 (Step 049910): Train loss 1.117, Val loss 1.779\n",
      "Ep 2 (Step 049915): Train loss 1.150, Val loss 1.779\n",
      "Ep 2 (Step 049920): Train loss 1.319, Val loss 1.778\n",
      "Ep 2 (Step 049925): Train loss 0.972, Val loss 1.777\n",
      "Ep 2 (Step 049930): Train loss 0.832, Val loss 1.775\n",
      "Ep 2 (Step 049935): Train loss 1.085, Val loss 1.773\n",
      "Ep 2 (Step 049940): Train loss 1.218, Val loss 1.772\n",
      "Ep 2 (Step 049945): Train loss 1.181, Val loss 1.772\n",
      "Ep 2 (Step 049950): Train loss 0.991, Val loss 1.773\n",
      "Ep 2 (Step 049955): Train loss 0.953, Val loss 1.773\n",
      "Ep 2 (Step 049960): Train loss 0.893, Val loss 1.772\n",
      "Ep 2 (Step 049965): Train loss 1.375, Val loss 1.770\n",
      "Ep 2 (Step 049970): Train loss 1.261, Val loss 1.769\n",
      "Ep 2 (Step 049975): Train loss 1.039, Val loss 1.768\n",
      "Ep 2 (Step 049980): Train loss 0.847, Val loss 1.768\n",
      "Ep 2 (Step 049985): Train loss 1.237, Val loss 1.769\n",
      "Ep 2 (Step 049990): Train loss 0.979, Val loss 1.769\n",
      "Ep 2 (Step 049995): Train loss 1.024, Val loss 1.769\n",
      "Ep 2 (Step 050000): Train loss 1.224, Val loss 1.770\n",
      "Ep 2 (Step 050005): Train loss 1.246, Val loss 1.772\n",
      "Ep 2 (Step 050010): Train loss 1.180, Val loss 1.774\n",
      "Ep 2 (Step 050015): Train loss 0.982, Val loss 1.774\n",
      "Ep 2 (Step 050020): Train loss 0.906, Val loss 1.774\n",
      "Ep 2 (Step 050025): Train loss 1.461, Val loss 1.775\n",
      "Ep 2 (Step 050030): Train loss 0.762, Val loss 1.777\n",
      "Ep 2 (Step 050035): Train loss 0.946, Val loss 1.778\n",
      "Ep 2 (Step 050040): Train loss 1.031, Val loss 1.779\n",
      "Ep 2 (Step 050045): Train loss 1.093, Val loss 1.780\n",
      "Ep 2 (Step 050050): Train loss 1.040, Val loss 1.781\n",
      "Ep 2 (Step 050055): Train loss 1.140, Val loss 1.783\n",
      "Ep 2 (Step 050060): Train loss 1.321, Val loss 1.784\n",
      "Ep 2 (Step 050065): Train loss 1.009, Val loss 1.782\n",
      "Ep 2 (Step 050070): Train loss 1.097, Val loss 1.781\n",
      "Ep 2 (Step 050075): Train loss 1.239, Val loss 1.782\n",
      "Ep 2 (Step 050080): Train loss 1.351, Val loss 1.783\n",
      "Ep 2 (Step 050085): Train loss 0.929, Val loss 1.783\n",
      "Ep 2 (Step 050090): Train loss 1.118, Val loss 1.782\n",
      "Ep 2 (Step 050095): Train loss 1.075, Val loss 1.781\n",
      "Ep 2 (Step 050100): Train loss 1.186, Val loss 1.781\n",
      "Ep 2 (Step 050105): Train loss 1.017, Val loss 1.781\n",
      "Ep 2 (Step 050110): Train loss 0.965, Val loss 1.780\n",
      "Ep 2 (Step 050115): Train loss 0.883, Val loss 1.778\n",
      "Ep 2 (Step 050120): Train loss 1.187, Val loss 1.777\n",
      "Ep 2 (Step 050125): Train loss 1.093, Val loss 1.777\n",
      "Ep 2 (Step 050130): Train loss 1.242, Val loss 1.777\n",
      "Ep 2 (Step 050135): Train loss 1.402, Val loss 1.776\n",
      "Ep 2 (Step 050140): Train loss 0.991, Val loss 1.777\n",
      "Ep 2 (Step 050145): Train loss 1.428, Val loss 1.778\n",
      "Ep 2 (Step 050150): Train loss 1.066, Val loss 1.779\n",
      "Ep 2 (Step 050155): Train loss 1.167, Val loss 1.779\n",
      "Ep 2 (Step 050160): Train loss 0.833, Val loss 1.778\n",
      "Ep 2 (Step 050165): Train loss 1.101, Val loss 1.777\n",
      "Ep 2 (Step 050170): Train loss 0.911, Val loss 1.776\n",
      "Ep 2 (Step 050175): Train loss 0.979, Val loss 1.778\n",
      "Ep 2 (Step 050180): Train loss 1.235, Val loss 1.779\n",
      "Ep 2 (Step 050185): Train loss 1.121, Val loss 1.781\n",
      "Ep 2 (Step 050190): Train loss 0.893, Val loss 1.782\n",
      "Ep 2 (Step 050195): Train loss 1.010, Val loss 1.784\n",
      "Ep 2 (Step 050200): Train loss 0.902, Val loss 1.786\n",
      "Ep 2 (Step 050205): Train loss 0.975, Val loss 1.788\n",
      "Ep 2 (Step 050210): Train loss 1.492, Val loss 1.789\n",
      "Ep 2 (Step 050215): Train loss 1.216, Val loss 1.789\n",
      "Ep 2 (Step 050220): Train loss 1.289, Val loss 1.788\n",
      "Ep 2 (Step 050225): Train loss 1.027, Val loss 1.786\n",
      "Ep 2 (Step 050230): Train loss 1.317, Val loss 1.784\n",
      "Ep 2 (Step 050235): Train loss 0.975, Val loss 1.781\n",
      "Ep 2 (Step 050240): Train loss 0.805, Val loss 1.779\n",
      "Ep 2 (Step 050245): Train loss 0.923, Val loss 1.776\n",
      "Ep 2 (Step 050250): Train loss 1.079, Val loss 1.774\n",
      "Ep 2 (Step 050255): Train loss 0.890, Val loss 1.773\n",
      "Ep 2 (Step 050260): Train loss 1.036, Val loss 1.772\n",
      "Ep 2 (Step 050265): Train loss 0.778, Val loss 1.772\n",
      "Ep 2 (Step 050270): Train loss 0.923, Val loss 1.774\n",
      "Ep 2 (Step 050275): Train loss 1.037, Val loss 1.775\n",
      "Ep 2 (Step 050280): Train loss 1.032, Val loss 1.777\n",
      "Ep 2 (Step 050285): Train loss 1.134, Val loss 1.777\n",
      "Ep 2 (Step 050290): Train loss 1.199, Val loss 1.778\n",
      "Ep 2 (Step 050295): Train loss 0.909, Val loss 1.779\n",
      "Ep 2 (Step 050300): Train loss 1.063, Val loss 1.780\n",
      "Ep 2 (Step 050305): Train loss 1.146, Val loss 1.782\n",
      "Ep 2 (Step 050310): Train loss 1.007, Val loss 1.784\n",
      "Ep 2 (Step 050315): Train loss 1.148, Val loss 1.785\n",
      "Ep 2 (Step 050320): Train loss 1.305, Val loss 1.786\n",
      "Ep 2 (Step 050325): Train loss 0.886, Val loss 1.787\n",
      "Ep 2 (Step 050330): Train loss 0.886, Val loss 1.788\n",
      "Ep 2 (Step 050335): Train loss 1.246, Val loss 1.789\n",
      "Ep 2 (Step 050340): Train loss 0.979, Val loss 1.788\n",
      "Ep 2 (Step 050345): Train loss 1.174, Val loss 1.787\n",
      "Ep 2 (Step 050350): Train loss 1.252, Val loss 1.784\n",
      "Ep 2 (Step 050355): Train loss 1.050, Val loss 1.781\n",
      "Ep 2 (Step 050360): Train loss 1.037, Val loss 1.779\n",
      "Ep 2 (Step 050365): Train loss 1.166, Val loss 1.778\n",
      "Ep 2 (Step 050370): Train loss 1.088, Val loss 1.778\n",
      "Ep 2 (Step 050375): Train loss 1.208, Val loss 1.777\n",
      "Ep 2 (Step 050380): Train loss 1.151, Val loss 1.777\n",
      "Ep 2 (Step 050385): Train loss 1.241, Val loss 1.778\n",
      "Ep 2 (Step 050390): Train loss 1.115, Val loss 1.777\n",
      "Ep 2 (Step 050395): Train loss 1.133, Val loss 1.777\n",
      "Ep 2 (Step 050400): Train loss 0.955, Val loss 1.775\n",
      "Ep 2 (Step 050405): Train loss 0.969, Val loss 1.774\n",
      "Ep 2 (Step 050410): Train loss 0.787, Val loss 1.773\n",
      "Ep 2 (Step 050415): Train loss 1.111, Val loss 1.772\n",
      "Ep 2 (Step 050420): Train loss 1.068, Val loss 1.772\n",
      "Ep 2 (Step 050425): Train loss 0.929, Val loss 1.772\n",
      "Ep 2 (Step 050430): Train loss 1.161, Val loss 1.771\n",
      "Ep 2 (Step 050435): Train loss 1.170, Val loss 1.770\n",
      "Ep 2 (Step 050440): Train loss 0.867, Val loss 1.770\n",
      "Ep 2 (Step 050445): Train loss 1.250, Val loss 1.770\n",
      "Ep 2 (Step 050450): Train loss 1.121, Val loss 1.771\n",
      "Ep 2 (Step 050455): Train loss 1.023, Val loss 1.772\n",
      "Ep 2 (Step 050460): Train loss 1.348, Val loss 1.773\n",
      "Ep 2 (Step 050465): Train loss 1.483, Val loss 1.772\n",
      "Ep 2 (Step 050470): Train loss 0.990, Val loss 1.773\n",
      "Ep 2 (Step 050475): Train loss 1.004, Val loss 1.774\n",
      "Ep 2 (Step 050480): Train loss 1.339, Val loss 1.774\n",
      "Ep 2 (Step 050485): Train loss 1.369, Val loss 1.774\n",
      "Ep 2 (Step 050490): Train loss 1.374, Val loss 1.772\n",
      "Ep 2 (Step 050495): Train loss 1.109, Val loss 1.770\n",
      "Ep 2 (Step 050500): Train loss 1.099, Val loss 1.769\n",
      "Ep 2 (Step 050505): Train loss 1.081, Val loss 1.765\n",
      "Ep 2 (Step 050510): Train loss 0.907, Val loss 1.765\n",
      "Ep 2 (Step 050515): Train loss 1.432, Val loss 1.765\n",
      "Ep 2 (Step 050520): Train loss 0.821, Val loss 1.765\n",
      "Ep 2 (Step 050525): Train loss 0.895, Val loss 1.764\n",
      "Ep 2 (Step 050530): Train loss 0.800, Val loss 1.764\n",
      "Ep 2 (Step 050535): Train loss 1.325, Val loss 1.764\n",
      "Ep 2 (Step 050540): Train loss 0.986, Val loss 1.763\n",
      "Ep 2 (Step 050545): Train loss 1.018, Val loss 1.763\n",
      "Ep 2 (Step 050550): Train loss 1.022, Val loss 1.763\n",
      "Ep 2 (Step 050555): Train loss 1.154, Val loss 1.764\n",
      "Ep 2 (Step 050560): Train loss 0.888, Val loss 1.765\n",
      "Ep 2 (Step 050565): Train loss 0.851, Val loss 1.766\n",
      "Ep 2 (Step 050570): Train loss 0.879, Val loss 1.766\n",
      "Ep 2 (Step 050575): Train loss 0.917, Val loss 1.767\n",
      "Ep 2 (Step 050580): Train loss 0.983, Val loss 1.769\n",
      "Ep 2 (Step 050585): Train loss 1.111, Val loss 1.771\n",
      "Ep 2 (Step 050590): Train loss 0.983, Val loss 1.773\n",
      "Ep 2 (Step 050595): Train loss 1.196, Val loss 1.775\n",
      "Ep 2 (Step 050600): Train loss 1.101, Val loss 1.776\n",
      "Ep 2 (Step 050605): Train loss 0.963, Val loss 1.776\n",
      "Ep 2 (Step 050610): Train loss 1.234, Val loss 1.777\n",
      "Ep 2 (Step 050615): Train loss 1.154, Val loss 1.777\n",
      "Ep 2 (Step 050620): Train loss 1.310, Val loss 1.775\n",
      "Ep 2 (Step 050625): Train loss 1.183, Val loss 1.772\n",
      "Ep 2 (Step 050630): Train loss 1.089, Val loss 1.770\n",
      "Ep 2 (Step 050635): Train loss 1.219, Val loss 1.769\n",
      "Ep 2 (Step 050640): Train loss 0.893, Val loss 1.770\n",
      "Ep 2 (Step 050645): Train loss 0.942, Val loss 1.770\n",
      "Ep 2 (Step 050650): Train loss 1.079, Val loss 1.771\n",
      "Ep 2 (Step 050655): Train loss 0.913, Val loss 1.770\n",
      "Ep 2 (Step 050660): Train loss 1.193, Val loss 1.769\n",
      "Ep 2 (Step 050665): Train loss 1.040, Val loss 1.767\n",
      "Ep 2 (Step 050670): Train loss 0.978, Val loss 1.766\n",
      "Ep 2 (Step 050675): Train loss 1.132, Val loss 1.764\n",
      "Ep 2 (Step 050680): Train loss 0.873, Val loss 1.763\n",
      "Ep 2 (Step 050685): Train loss 0.995, Val loss 1.761\n",
      "Ep 2 (Step 050690): Train loss 0.953, Val loss 1.759\n",
      "Ep 2 (Step 050695): Train loss 1.042, Val loss 1.758\n",
      "Ep 2 (Step 050700): Train loss 1.106, Val loss 1.760\n",
      "Ep 2 (Step 050705): Train loss 1.360, Val loss 1.762\n",
      "Ep 2 (Step 050710): Train loss 1.058, Val loss 1.764\n",
      "Ep 2 (Step 050715): Train loss 1.036, Val loss 1.766\n",
      "Ep 2 (Step 050720): Train loss 0.825, Val loss 1.767\n",
      "Ep 2 (Step 050725): Train loss 1.016, Val loss 1.768\n",
      "Ep 2 (Step 050730): Train loss 1.022, Val loss 1.770\n",
      "Ep 2 (Step 050735): Train loss 0.864, Val loss 1.771\n",
      "Ep 2 (Step 050740): Train loss 1.017, Val loss 1.771\n",
      "Ep 2 (Step 050745): Train loss 1.066, Val loss 1.770\n",
      "Ep 2 (Step 050750): Train loss 0.952, Val loss 1.770\n",
      "Ep 2 (Step 050755): Train loss 1.046, Val loss 1.769\n",
      "Ep 2 (Step 050760): Train loss 0.969, Val loss 1.769\n",
      "Ep 2 (Step 050765): Train loss 1.289, Val loss 1.769\n",
      "Ep 2 (Step 050770): Train loss 0.875, Val loss 1.768\n",
      "Ep 2 (Step 050775): Train loss 0.981, Val loss 1.766\n",
      "Ep 2 (Step 050780): Train loss 0.912, Val loss 1.765\n",
      "Ep 2 (Step 050785): Train loss 0.894, Val loss 1.766\n",
      "Ep 2 (Step 050790): Train loss 0.938, Val loss 1.767\n",
      "Ep 2 (Step 050795): Train loss 0.900, Val loss 1.768\n",
      "Ep 2 (Step 050800): Train loss 0.963, Val loss 1.768\n",
      "Ep 2 (Step 050805): Train loss 1.095, Val loss 1.768\n",
      "Ep 2 (Step 050810): Train loss 0.940, Val loss 1.767\n",
      "Ep 2 (Step 050815): Train loss 1.119, Val loss 1.767\n",
      "Ep 2 (Step 050820): Train loss 1.372, Val loss 1.768\n",
      "Ep 2 (Step 050825): Train loss 1.209, Val loss 1.769\n",
      "Ep 2 (Step 050830): Train loss 1.070, Val loss 1.770\n",
      "Ep 2 (Step 050835): Train loss 1.098, Val loss 1.773\n",
      "Ep 2 (Step 050840): Train loss 1.054, Val loss 1.776\n",
      "Ep 2 (Step 050845): Train loss 1.288, Val loss 1.778\n",
      "Ep 2 (Step 050850): Train loss 1.438, Val loss 1.779\n",
      "Ep 2 (Step 050855): Train loss 1.138, Val loss 1.779\n",
      "Ep 2 (Step 050860): Train loss 1.080, Val loss 1.780\n",
      "Ep 2 (Step 050865): Train loss 0.918, Val loss 1.781\n",
      "Ep 2 (Step 050870): Train loss 0.919, Val loss 1.782\n",
      "Ep 2 (Step 050875): Train loss 1.118, Val loss 1.780\n",
      "Ep 2 (Step 050880): Train loss 1.085, Val loss 1.778\n",
      "Ep 2 (Step 050885): Train loss 1.204, Val loss 1.777\n",
      "Ep 2 (Step 050890): Train loss 1.356, Val loss 1.775\n",
      "Ep 2 (Step 050895): Train loss 0.931, Val loss 1.774\n",
      "Ep 2 (Step 050900): Train loss 0.870, Val loss 1.773\n",
      "Ep 2 (Step 050905): Train loss 1.038, Val loss 1.771\n",
      "Ep 2 (Step 050910): Train loss 1.176, Val loss 1.770\n",
      "Ep 2 (Step 050915): Train loss 1.352, Val loss 1.769\n",
      "Ep 2 (Step 050920): Train loss 0.988, Val loss 1.768\n",
      "Ep 2 (Step 050925): Train loss 1.413, Val loss 1.768\n",
      "Ep 2 (Step 050930): Train loss 0.937, Val loss 1.769\n",
      "Ep 2 (Step 050935): Train loss 1.202, Val loss 1.771\n",
      "Ep 2 (Step 050940): Train loss 1.144, Val loss 1.773\n",
      "Ep 2 (Step 050945): Train loss 0.815, Val loss 1.774\n",
      "Ep 2 (Step 050950): Train loss 0.790, Val loss 1.775\n",
      "Ep 2 (Step 050955): Train loss 1.084, Val loss 1.776\n",
      "Ep 2 (Step 050960): Train loss 0.990, Val loss 1.776\n",
      "Ep 2 (Step 050965): Train loss 0.809, Val loss 1.776\n",
      "Ep 2 (Step 050970): Train loss 1.096, Val loss 1.773\n",
      "Ep 2 (Step 050975): Train loss 0.955, Val loss 1.770\n",
      "Ep 2 (Step 050980): Train loss 1.097, Val loss 1.766\n",
      "Ep 2 (Step 050985): Train loss 1.102, Val loss 1.764\n",
      "Ep 2 (Step 050990): Train loss 1.331, Val loss 1.764\n",
      "Ep 2 (Step 050995): Train loss 1.032, Val loss 1.765\n",
      "Ep 2 (Step 051000): Train loss 0.986, Val loss 1.766\n",
      "Ep 2 (Step 051005): Train loss 1.369, Val loss 1.765\n",
      "Ep 2 (Step 051010): Train loss 1.326, Val loss 1.765\n",
      "Ep 2 (Step 051015): Train loss 1.072, Val loss 1.766\n",
      "Ep 2 (Step 051020): Train loss 0.815, Val loss 1.766\n",
      "Ep 2 (Step 051025): Train loss 1.233, Val loss 1.766\n",
      "Ep 2 (Step 051030): Train loss 1.122, Val loss 1.767\n",
      "Ep 2 (Step 051035): Train loss 0.924, Val loss 1.767\n",
      "Ep 2 (Step 051040): Train loss 1.118, Val loss 1.768\n",
      "Ep 2 (Step 051045): Train loss 0.998, Val loss 1.769\n",
      "Ep 2 (Step 051050): Train loss 1.012, Val loss 1.768\n",
      "Ep 2 (Step 051055): Train loss 1.496, Val loss 1.766\n",
      "Ep 2 (Step 051060): Train loss 0.920, Val loss 1.765\n",
      "Ep 2 (Step 051065): Train loss 1.232, Val loss 1.767\n",
      "Ep 2 (Step 051070): Train loss 1.004, Val loss 1.769\n",
      "Ep 2 (Step 051075): Train loss 0.937, Val loss 1.770\n",
      "Ep 2 (Step 051080): Train loss 1.085, Val loss 1.770\n",
      "Ep 2 (Step 051085): Train loss 1.012, Val loss 1.770\n",
      "Ep 2 (Step 051090): Train loss 1.165, Val loss 1.771\n",
      "Ep 2 (Step 051095): Train loss 1.100, Val loss 1.771\n",
      "Ep 2 (Step 051100): Train loss 1.098, Val loss 1.770\n",
      "Ep 2 (Step 051105): Train loss 1.159, Val loss 1.768\n",
      "Ep 2 (Step 051110): Train loss 0.809, Val loss 1.766\n",
      "Ep 2 (Step 051115): Train loss 1.385, Val loss 1.765\n",
      "Ep 2 (Step 051120): Train loss 1.148, Val loss 1.765\n",
      "Ep 2 (Step 051125): Train loss 1.118, Val loss 1.765\n",
      "Ep 2 (Step 051130): Train loss 0.935, Val loss 1.765\n",
      "Ep 2 (Step 051135): Train loss 0.919, Val loss 1.765\n",
      "Ep 2 (Step 051140): Train loss 1.287, Val loss 1.766\n",
      "Ep 2 (Step 051145): Train loss 0.852, Val loss 1.766\n",
      "Ep 2 (Step 051150): Train loss 1.142, Val loss 1.766\n",
      "Ep 2 (Step 051155): Train loss 1.075, Val loss 1.767\n",
      "Ep 2 (Step 051160): Train loss 0.951, Val loss 1.767\n",
      "Ep 2 (Step 051165): Train loss 1.286, Val loss 1.767\n",
      "Ep 2 (Step 051170): Train loss 1.343, Val loss 1.766\n",
      "Ep 2 (Step 051175): Train loss 1.172, Val loss 1.766\n",
      "Ep 2 (Step 051180): Train loss 1.134, Val loss 1.766\n",
      "Ep 2 (Step 051185): Train loss 0.966, Val loss 1.764\n",
      "Ep 2 (Step 051190): Train loss 1.205, Val loss 1.765\n",
      "Ep 2 (Step 051195): Train loss 1.219, Val loss 1.766\n",
      "Ep 2 (Step 051200): Train loss 0.975, Val loss 1.767\n",
      "Ep 2 (Step 051205): Train loss 1.116, Val loss 1.769\n",
      "Ep 2 (Step 051210): Train loss 0.943, Val loss 1.771\n",
      "Ep 2 (Step 051215): Train loss 1.094, Val loss 1.772\n",
      "Ep 2 (Step 051220): Train loss 1.139, Val loss 1.771\n",
      "Ep 2 (Step 051225): Train loss 1.055, Val loss 1.772\n",
      "Ep 2 (Step 051230): Train loss 1.140, Val loss 1.770\n",
      "Ep 2 (Step 051235): Train loss 1.084, Val loss 1.768\n",
      "Ep 2 (Step 051240): Train loss 0.999, Val loss 1.767\n",
      "Ep 2 (Step 051245): Train loss 0.916, Val loss 1.766\n",
      "Ep 2 (Step 051250): Train loss 1.192, Val loss 1.767\n",
      "Ep 2 (Step 051255): Train loss 0.998, Val loss 1.766\n",
      "Ep 2 (Step 051260): Train loss 1.120, Val loss 1.767\n",
      "Ep 2 (Step 051265): Train loss 1.031, Val loss 1.768\n",
      "Ep 2 (Step 051270): Train loss 0.821, Val loss 1.767\n",
      "Ep 2 (Step 051275): Train loss 0.956, Val loss 1.766\n",
      "Ep 2 (Step 051280): Train loss 1.147, Val loss 1.767\n",
      "Ep 2 (Step 051285): Train loss 0.938, Val loss 1.768\n",
      "Ep 2 (Step 051290): Train loss 1.098, Val loss 1.770\n",
      "Ep 2 (Step 051295): Train loss 1.091, Val loss 1.770\n",
      "Ep 2 (Step 051300): Train loss 1.022, Val loss 1.769\n",
      "Ep 2 (Step 051305): Train loss 0.936, Val loss 1.768\n",
      "Ep 2 (Step 051310): Train loss 1.123, Val loss 1.767\n",
      "Ep 2 (Step 051315): Train loss 1.371, Val loss 1.766\n",
      "Ep 2 (Step 051320): Train loss 1.298, Val loss 1.765\n",
      "Ep 2 (Step 051325): Train loss 0.889, Val loss 1.766\n",
      "Ep 2 (Step 051330): Train loss 1.088, Val loss 1.766\n",
      "Ep 2 (Step 051335): Train loss 1.141, Val loss 1.764\n",
      "Ep 2 (Step 051340): Train loss 0.984, Val loss 1.763\n",
      "Ep 2 (Step 051345): Train loss 1.247, Val loss 1.763\n",
      "Ep 2 (Step 051350): Train loss 0.845, Val loss 1.762\n",
      "Ep 2 (Step 051355): Train loss 1.080, Val loss 1.762\n",
      "Ep 2 (Step 051360): Train loss 1.004, Val loss 1.764\n",
      "Ep 2 (Step 051365): Train loss 1.062, Val loss 1.764\n",
      "Ep 2 (Step 051370): Train loss 0.825, Val loss 1.763\n",
      "Ep 2 (Step 051375): Train loss 0.950, Val loss 1.763\n",
      "Ep 2 (Step 051380): Train loss 1.057, Val loss 1.763\n",
      "Ep 2 (Step 051385): Train loss 1.163, Val loss 1.765\n",
      "Ep 2 (Step 051390): Train loss 1.361, Val loss 1.766\n",
      "Ep 2 (Step 051395): Train loss 1.128, Val loss 1.769\n",
      "Ep 2 (Step 051400): Train loss 1.179, Val loss 1.771\n",
      "Ep 2 (Step 051405): Train loss 1.090, Val loss 1.772\n",
      "Ep 2 (Step 051410): Train loss 1.124, Val loss 1.772\n",
      "Ep 2 (Step 051415): Train loss 1.130, Val loss 1.772\n",
      "Ep 2 (Step 051420): Train loss 1.240, Val loss 1.771\n",
      "Ep 2 (Step 051425): Train loss 1.067, Val loss 1.771\n",
      "Ep 2 (Step 051430): Train loss 0.991, Val loss 1.770\n",
      "Ep 2 (Step 051435): Train loss 0.737, Val loss 1.768\n",
      "Ep 2 (Step 051440): Train loss 1.052, Val loss 1.766\n",
      "Ep 2 (Step 051445): Train loss 0.980, Val loss 1.765\n",
      "Ep 2 (Step 051450): Train loss 0.986, Val loss 1.765\n",
      "Ep 2 (Step 051455): Train loss 0.814, Val loss 1.766\n",
      "Ep 2 (Step 051460): Train loss 1.207, Val loss 1.767\n",
      "Ep 2 (Step 051465): Train loss 1.102, Val loss 1.769\n",
      "Ep 2 (Step 051470): Train loss 1.598, Val loss 1.771\n",
      "Ep 2 (Step 051475): Train loss 0.964, Val loss 1.771\n",
      "Ep 2 (Step 051480): Train loss 1.144, Val loss 1.771\n",
      "Ep 2 (Step 051485): Train loss 1.107, Val loss 1.770\n",
      "Ep 2 (Step 051490): Train loss 1.198, Val loss 1.770\n",
      "Ep 2 (Step 051495): Train loss 1.140, Val loss 1.769\n",
      "Ep 2 (Step 051500): Train loss 1.017, Val loss 1.769\n",
      "Ep 2 (Step 051505): Train loss 1.168, Val loss 1.769\n",
      "Ep 2 (Step 051510): Train loss 1.199, Val loss 1.770\n",
      "Ep 2 (Step 051515): Train loss 1.353, Val loss 1.769\n",
      "Ep 2 (Step 051520): Train loss 1.195, Val loss 1.768\n",
      "Ep 2 (Step 051525): Train loss 1.151, Val loss 1.766\n",
      "Ep 2 (Step 051530): Train loss 1.160, Val loss 1.765\n",
      "Ep 2 (Step 051535): Train loss 0.952, Val loss 1.765\n",
      "Ep 2 (Step 051540): Train loss 1.202, Val loss 1.767\n",
      "Ep 2 (Step 051545): Train loss 1.401, Val loss 1.768\n",
      "Ep 2 (Step 051550): Train loss 1.088, Val loss 1.769\n",
      "Ep 2 (Step 051555): Train loss 0.886, Val loss 1.770\n",
      "Ep 2 (Step 051560): Train loss 1.066, Val loss 1.772\n",
      "Ep 2 (Step 051565): Train loss 1.059, Val loss 1.772\n",
      "Ep 2 (Step 051570): Train loss 1.435, Val loss 1.771\n",
      "Ep 2 (Step 051575): Train loss 1.003, Val loss 1.771\n",
      "Ep 2 (Step 051580): Train loss 1.086, Val loss 1.771\n",
      "Ep 2 (Step 051585): Train loss 0.973, Val loss 1.772\n",
      "Ep 2 (Step 051590): Train loss 1.171, Val loss 1.772\n",
      "Ep 2 (Step 051595): Train loss 1.100, Val loss 1.772\n",
      "Ep 2 (Step 051600): Train loss 1.075, Val loss 1.773\n",
      "Ep 2 (Step 051605): Train loss 1.202, Val loss 1.772\n",
      "Ep 2 (Step 051610): Train loss 1.257, Val loss 1.771\n",
      "Ep 2 (Step 051615): Train loss 1.144, Val loss 1.770\n",
      "Ep 2 (Step 051620): Train loss 0.880, Val loss 1.770\n",
      "Ep 2 (Step 051625): Train loss 1.066, Val loss 1.773\n",
      "Ep 2 (Step 051630): Train loss 1.154, Val loss 1.775\n",
      "Ep 2 (Step 051635): Train loss 0.917, Val loss 1.776\n",
      "Ep 2 (Step 051640): Train loss 0.954, Val loss 1.777\n",
      "Ep 2 (Step 051645): Train loss 0.955, Val loss 1.777\n",
      "Ep 2 (Step 051650): Train loss 0.844, Val loss 1.778\n",
      "Ep 2 (Step 051655): Train loss 1.136, Val loss 1.779\n",
      "Ep 2 (Step 051660): Train loss 1.187, Val loss 1.779\n",
      "Ep 2 (Step 051665): Train loss 1.105, Val loss 1.777\n",
      "Ep 2 (Step 051670): Train loss 1.110, Val loss 1.775\n",
      "Ep 2 (Step 051675): Train loss 1.258, Val loss 1.774\n",
      "Ep 2 (Step 051680): Train loss 1.052, Val loss 1.773\n",
      "Ep 2 (Step 051685): Train loss 0.898, Val loss 1.772\n",
      "Ep 2 (Step 051690): Train loss 1.258, Val loss 1.772\n",
      "Ep 2 (Step 051695): Train loss 1.218, Val loss 1.773\n",
      "Ep 2 (Step 051700): Train loss 1.147, Val loss 1.772\n",
      "Ep 2 (Step 051705): Train loss 1.017, Val loss 1.773\n",
      "Ep 2 (Step 051710): Train loss 1.116, Val loss 1.773\n",
      "Ep 2 (Step 051715): Train loss 0.846, Val loss 1.773\n",
      "Ep 2 (Step 051720): Train loss 0.923, Val loss 1.773\n",
      "Ep 2 (Step 051725): Train loss 1.069, Val loss 1.773\n",
      "Ep 2 (Step 051730): Train loss 0.980, Val loss 1.771\n",
      "Ep 2 (Step 051735): Train loss 0.952, Val loss 1.769\n",
      "Ep 2 (Step 051740): Train loss 1.202, Val loss 1.768\n",
      "Ep 2 (Step 051745): Train loss 1.049, Val loss 1.766\n",
      "Ep 2 (Step 051750): Train loss 1.320, Val loss 1.765\n",
      "Ep 2 (Step 051755): Train loss 0.879, Val loss 1.764\n",
      "Ep 2 (Step 051760): Train loss 1.129, Val loss 1.764\n",
      "Ep 2 (Step 051765): Train loss 1.052, Val loss 1.764\n",
      "Ep 2 (Step 051770): Train loss 0.975, Val loss 1.765\n",
      "Ep 2 (Step 051775): Train loss 0.963, Val loss 1.765\n",
      "Ep 2 (Step 051780): Train loss 0.897, Val loss 1.768\n",
      "Ep 2 (Step 051785): Train loss 0.838, Val loss 1.771\n",
      "Ep 2 (Step 051790): Train loss 1.258, Val loss 1.770\n",
      "Ep 2 (Step 051795): Train loss 0.851, Val loss 1.769\n",
      "Ep 2 (Step 051800): Train loss 0.970, Val loss 1.769\n",
      "Ep 2 (Step 051805): Train loss 1.319, Val loss 1.767\n",
      "Ep 2 (Step 051810): Train loss 1.194, Val loss 1.764\n",
      "Ep 2 (Step 051815): Train loss 0.868, Val loss 1.762\n",
      "Ep 2 (Step 051820): Train loss 0.737, Val loss 1.762\n",
      "Ep 2 (Step 051825): Train loss 0.917, Val loss 1.762\n",
      "Ep 2 (Step 051830): Train loss 1.037, Val loss 1.761\n",
      "Ep 2 (Step 051835): Train loss 1.251, Val loss 1.760\n",
      "Ep 2 (Step 051840): Train loss 0.931, Val loss 1.759\n",
      "Ep 2 (Step 051845): Train loss 1.374, Val loss 1.758\n",
      "Ep 2 (Step 051850): Train loss 1.084, Val loss 1.757\n",
      "Ep 2 (Step 051855): Train loss 1.108, Val loss 1.756\n",
      "Ep 2 (Step 051860): Train loss 0.958, Val loss 1.756\n",
      "Ep 2 (Step 051865): Train loss 1.309, Val loss 1.757\n",
      "Ep 2 (Step 051870): Train loss 1.047, Val loss 1.758\n",
      "Ep 2 (Step 051875): Train loss 1.097, Val loss 1.760\n",
      "Ep 2 (Step 051880): Train loss 0.814, Val loss 1.763\n",
      "Ep 2 (Step 051885): Train loss 1.031, Val loss 1.766\n",
      "Ep 2 (Step 051890): Train loss 1.034, Val loss 1.767\n",
      "Ep 2 (Step 051895): Train loss 1.067, Val loss 1.768\n",
      "Ep 2 (Step 051900): Train loss 1.116, Val loss 1.768\n",
      "Ep 2 (Step 051905): Train loss 1.049, Val loss 1.768\n",
      "Ep 2 (Step 051910): Train loss 1.353, Val loss 1.770\n",
      "Ep 2 (Step 051915): Train loss 1.071, Val loss 1.771\n",
      "Ep 2 (Step 051920): Train loss 1.423, Val loss 1.771\n",
      "Ep 2 (Step 051925): Train loss 1.164, Val loss 1.770\n",
      "Ep 2 (Step 051930): Train loss 0.827, Val loss 1.768\n",
      "Ep 2 (Step 051935): Train loss 1.210, Val loss 1.766\n",
      "Ep 2 (Step 051940): Train loss 1.299, Val loss 1.764\n",
      "Ep 2 (Step 051945): Train loss 1.212, Val loss 1.763\n",
      "Ep 2 (Step 051950): Train loss 1.092, Val loss 1.763\n",
      "Ep 2 (Step 051955): Train loss 0.850, Val loss 1.763\n",
      "Ep 2 (Step 051960): Train loss 0.796, Val loss 1.762\n",
      "Ep 2 (Step 051965): Train loss 0.972, Val loss 1.762\n",
      "Ep 2 (Step 051970): Train loss 1.166, Val loss 1.762\n",
      "Ep 2 (Step 051975): Train loss 1.031, Val loss 1.762\n",
      "Ep 2 (Step 051980): Train loss 1.361, Val loss 1.763\n",
      "Ep 2 (Step 051985): Train loss 0.758, Val loss 1.765\n",
      "Ep 2 (Step 051990): Train loss 0.831, Val loss 1.767\n",
      "Ep 2 (Step 051995): Train loss 0.879, Val loss 1.769\n",
      "Ep 2 (Step 052000): Train loss 1.411, Val loss 1.768\n",
      "Ep 2 (Step 052005): Train loss 0.878, Val loss 1.768\n",
      "Ep 2 (Step 052010): Train loss 0.995, Val loss 1.768\n",
      "Ep 2 (Step 052015): Train loss 0.832, Val loss 1.767\n",
      "Ep 2 (Step 052020): Train loss 1.198, Val loss 1.765\n",
      "Ep 2 (Step 052025): Train loss 1.021, Val loss 1.763\n",
      "Ep 2 (Step 052030): Train loss 1.034, Val loss 1.761\n",
      "Ep 2 (Step 052035): Train loss 1.373, Val loss 1.760\n",
      "Ep 2 (Step 052040): Train loss 1.095, Val loss 1.759\n",
      "Ep 2 (Step 052045): Train loss 1.352, Val loss 1.758\n",
      "Ep 2 (Step 052050): Train loss 0.955, Val loss 1.758\n",
      "Ep 2 (Step 052055): Train loss 1.353, Val loss 1.758\n",
      "Ep 2 (Step 052060): Train loss 0.738, Val loss 1.759\n",
      "Ep 2 (Step 052065): Train loss 0.925, Val loss 1.760\n",
      "Ep 2 (Step 052070): Train loss 1.099, Val loss 1.761\n",
      "Ep 2 (Step 052075): Train loss 1.199, Val loss 1.761\n",
      "Ep 2 (Step 052080): Train loss 1.257, Val loss 1.762\n",
      "Ep 2 (Step 052085): Train loss 1.152, Val loss 1.762\n",
      "Ep 2 (Step 052090): Train loss 1.077, Val loss 1.764\n",
      "Ep 2 (Step 052095): Train loss 1.266, Val loss 1.765\n",
      "Ep 2 (Step 052100): Train loss 0.851, Val loss 1.767\n",
      "Ep 2 (Step 052105): Train loss 1.133, Val loss 1.768\n",
      "Ep 2 (Step 052110): Train loss 1.297, Val loss 1.770\n",
      "Ep 2 (Step 052115): Train loss 1.250, Val loss 1.772\n",
      "Ep 2 (Step 052120): Train loss 0.951, Val loss 1.774\n",
      "Ep 2 (Step 052125): Train loss 1.184, Val loss 1.776\n",
      "Ep 2 (Step 052130): Train loss 1.090, Val loss 1.776\n",
      "Ep 2 (Step 052135): Train loss 1.042, Val loss 1.775\n",
      "Ep 2 (Step 052140): Train loss 1.000, Val loss 1.773\n",
      "Ep 2 (Step 052145): Train loss 0.921, Val loss 1.770\n",
      "Ep 2 (Step 052150): Train loss 1.082, Val loss 1.766\n",
      "Ep 2 (Step 052155): Train loss 1.240, Val loss 1.763\n",
      "Ep 2 (Step 052160): Train loss 1.088, Val loss 1.761\n",
      "Ep 2 (Step 052165): Train loss 1.343, Val loss 1.760\n",
      "Ep 2 (Step 052170): Train loss 0.919, Val loss 1.760\n",
      "Ep 2 (Step 052175): Train loss 0.861, Val loss 1.759\n",
      "Ep 2 (Step 052180): Train loss 1.235, Val loss 1.757\n",
      "Ep 2 (Step 052185): Train loss 1.513, Val loss 1.755\n",
      "Ep 2 (Step 052190): Train loss 1.328, Val loss 1.753\n",
      "Ep 2 (Step 052195): Train loss 1.226, Val loss 1.752\n",
      "Ep 2 (Step 052200): Train loss 1.280, Val loss 1.752\n",
      "Ep 2 (Step 052205): Train loss 1.205, Val loss 1.754\n",
      "Ep 2 (Step 052210): Train loss 1.123, Val loss 1.755\n",
      "Ep 2 (Step 052215): Train loss 1.094, Val loss 1.757\n",
      "Ep 2 (Step 052220): Train loss 1.003, Val loss 1.759\n",
      "Ep 2 (Step 052225): Train loss 1.284, Val loss 1.760\n",
      "Ep 2 (Step 052230): Train loss 0.706, Val loss 1.762\n",
      "Ep 2 (Step 052235): Train loss 1.064, Val loss 1.763\n",
      "Ep 2 (Step 052240): Train loss 1.270, Val loss 1.763\n",
      "Ep 2 (Step 052245): Train loss 1.142, Val loss 1.763\n",
      "Ep 2 (Step 052250): Train loss 1.426, Val loss 1.762\n",
      "Ep 2 (Step 052255): Train loss 0.996, Val loss 1.762\n",
      "Ep 2 (Step 052260): Train loss 1.078, Val loss 1.761\n",
      "Ep 2 (Step 052265): Train loss 0.994, Val loss 1.761\n",
      "Ep 2 (Step 052270): Train loss 0.850, Val loss 1.758\n",
      "Ep 2 (Step 052275): Train loss 0.881, Val loss 1.757\n",
      "Ep 2 (Step 052280): Train loss 0.921, Val loss 1.756\n",
      "Ep 2 (Step 052285): Train loss 1.284, Val loss 1.756\n",
      "Ep 2 (Step 052290): Train loss 0.977, Val loss 1.756\n",
      "Ep 2 (Step 052295): Train loss 0.915, Val loss 1.757\n",
      "Ep 2 (Step 052300): Train loss 0.912, Val loss 1.757\n",
      "Ep 2 (Step 052305): Train loss 1.255, Val loss 1.756\n",
      "Ep 2 (Step 052310): Train loss 0.878, Val loss 1.755\n",
      "Ep 2 (Step 052315): Train loss 1.187, Val loss 1.753\n",
      "Ep 2 (Step 052320): Train loss 1.160, Val loss 1.753\n",
      "Ep 2 (Step 052325): Train loss 1.283, Val loss 1.752\n",
      "Ep 2 (Step 052330): Train loss 1.129, Val loss 1.752\n",
      "Ep 2 (Step 052335): Train loss 1.192, Val loss 1.753\n",
      "Ep 2 (Step 052340): Train loss 0.993, Val loss 1.755\n",
      "Ep 2 (Step 052345): Train loss 1.159, Val loss 1.758\n",
      "Ep 2 (Step 052350): Train loss 0.931, Val loss 1.759\n",
      "Ep 2 (Step 052355): Train loss 0.879, Val loss 1.761\n",
      "Ep 2 (Step 052360): Train loss 1.134, Val loss 1.763\n",
      "Ep 2 (Step 052365): Train loss 1.114, Val loss 1.765\n",
      "Ep 2 (Step 052370): Train loss 0.944, Val loss 1.766\n",
      "Ep 2 (Step 052375): Train loss 1.256, Val loss 1.766\n",
      "Ep 2 (Step 052380): Train loss 1.035, Val loss 1.767\n",
      "Ep 2 (Step 052385): Train loss 0.929, Val loss 1.769\n",
      "Ep 2 (Step 052390): Train loss 1.106, Val loss 1.770\n",
      "Ep 2 (Step 052395): Train loss 1.217, Val loss 1.772\n",
      "Ep 2 (Step 052400): Train loss 0.965, Val loss 1.773\n",
      "Ep 2 (Step 052405): Train loss 1.005, Val loss 1.774\n",
      "Ep 2 (Step 052410): Train loss 1.264, Val loss 1.774\n",
      "Ep 2 (Step 052415): Train loss 0.842, Val loss 1.772\n",
      "Ep 2 (Step 052420): Train loss 1.234, Val loss 1.770\n",
      "Ep 2 (Step 052425): Train loss 0.861, Val loss 1.769\n",
      "Ep 2 (Step 052430): Train loss 1.319, Val loss 1.769\n",
      "Ep 2 (Step 052435): Train loss 1.283, Val loss 1.768\n",
      "Ep 2 (Step 052440): Train loss 0.949, Val loss 1.767\n",
      "Ep 2 (Step 052445): Train loss 0.992, Val loss 1.765\n",
      "Ep 2 (Step 052450): Train loss 1.041, Val loss 1.762\n",
      "Ep 2 (Step 052455): Train loss 0.904, Val loss 1.762\n",
      "Ep 2 (Step 052460): Train loss 1.247, Val loss 1.762\n",
      "Ep 2 (Step 052465): Train loss 1.195, Val loss 1.764\n",
      "Ep 2 (Step 052470): Train loss 1.211, Val loss 1.766\n",
      "Ep 2 (Step 052475): Train loss 1.105, Val loss 1.769\n",
      "Ep 2 (Step 052480): Train loss 0.909, Val loss 1.770\n",
      "Ep 2 (Step 052485): Train loss 1.005, Val loss 1.772\n",
      "Ep 2 (Step 052490): Train loss 1.113, Val loss 1.772\n",
      "Ep 2 (Step 052495): Train loss 1.080, Val loss 1.772\n",
      "Ep 2 (Step 052500): Train loss 0.924, Val loss 1.771\n",
      "Ep 2 (Step 052505): Train loss 1.022, Val loss 1.770\n",
      "Ep 2 (Step 052510): Train loss 1.303, Val loss 1.769\n",
      "Ep 2 (Step 052515): Train loss 1.342, Val loss 1.768\n",
      "Ep 2 (Step 052520): Train loss 0.866, Val loss 1.767\n",
      "Ep 2 (Step 052525): Train loss 1.059, Val loss 1.766\n",
      "Ep 2 (Step 052530): Train loss 1.062, Val loss 1.766\n",
      "Ep 2 (Step 052535): Train loss 1.174, Val loss 1.766\n",
      "Ep 2 (Step 052540): Train loss 1.113, Val loss 1.767\n",
      "Ep 2 (Step 052545): Train loss 1.040, Val loss 1.768\n",
      "Ep 2 (Step 052550): Train loss 1.144, Val loss 1.769\n",
      "Ep 2 (Step 052555): Train loss 1.265, Val loss 1.771\n",
      "Ep 2 (Step 052560): Train loss 1.325, Val loss 1.773\n",
      "Ep 2 (Step 052565): Train loss 1.256, Val loss 1.774\n",
      "Ep 2 (Step 052570): Train loss 1.099, Val loss 1.774\n",
      "Ep 2 (Step 052575): Train loss 1.381, Val loss 1.773\n",
      "Ep 2 (Step 052580): Train loss 0.982, Val loss 1.772\n",
      "Ep 2 (Step 052585): Train loss 1.359, Val loss 1.771\n",
      "Ep 2 (Step 052590): Train loss 1.092, Val loss 1.769\n",
      "Ep 2 (Step 052595): Train loss 1.184, Val loss 1.768\n",
      "Ep 2 (Step 052600): Train loss 0.996, Val loss 1.768\n",
      "Ep 2 (Step 052605): Train loss 0.991, Val loss 1.767\n",
      "Ep 2 (Step 052610): Train loss 1.097, Val loss 1.765\n",
      "Ep 2 (Step 052615): Train loss 1.173, Val loss 1.763\n",
      "Ep 2 (Step 052620): Train loss 0.882, Val loss 1.762\n",
      "Ep 2 (Step 052625): Train loss 1.318, Val loss 1.761\n",
      "Ep 2 (Step 052630): Train loss 1.201, Val loss 1.761\n",
      "Ep 2 (Step 052635): Train loss 1.347, Val loss 1.761\n",
      "Ep 2 (Step 052640): Train loss 1.015, Val loss 1.762\n",
      "Ep 2 (Step 052645): Train loss 1.160, Val loss 1.764\n",
      "Ep 2 (Step 052650): Train loss 0.886, Val loss 1.766\n",
      "Ep 2 (Step 052655): Train loss 1.268, Val loss 1.768\n",
      "Ep 2 (Step 052660): Train loss 0.947, Val loss 1.768\n",
      "Ep 2 (Step 052665): Train loss 0.684, Val loss 1.767\n",
      "Ep 2 (Step 052670): Train loss 1.059, Val loss 1.768\n",
      "Ep 2 (Step 052675): Train loss 1.040, Val loss 1.767\n",
      "Ep 2 (Step 052680): Train loss 1.038, Val loss 1.766\n",
      "Ep 2 (Step 052685): Train loss 1.061, Val loss 1.766\n",
      "Ep 2 (Step 052690): Train loss 1.296, Val loss 1.764\n",
      "Ep 2 (Step 052695): Train loss 1.164, Val loss 1.764\n",
      "Ep 2 (Step 052700): Train loss 0.787, Val loss 1.763\n",
      "Ep 2 (Step 052705): Train loss 1.148, Val loss 1.762\n",
      "Ep 2 (Step 052710): Train loss 0.956, Val loss 1.761\n",
      "Ep 2 (Step 052715): Train loss 0.918, Val loss 1.760\n",
      "Ep 2 (Step 052720): Train loss 0.920, Val loss 1.758\n",
      "Ep 2 (Step 052725): Train loss 0.890, Val loss 1.756\n",
      "Ep 2 (Step 052730): Train loss 1.121, Val loss 1.755\n",
      "Ep 2 (Step 052735): Train loss 1.078, Val loss 1.754\n",
      "Ep 2 (Step 052740): Train loss 1.099, Val loss 1.755\n",
      "Ep 2 (Step 052745): Train loss 1.135, Val loss 1.755\n",
      "Ep 2 (Step 052750): Train loss 1.162, Val loss 1.756\n",
      "Ep 2 (Step 052755): Train loss 1.194, Val loss 1.756\n",
      "Ep 2 (Step 052760): Train loss 1.149, Val loss 1.757\n",
      "Ep 2 (Step 052765): Train loss 1.020, Val loss 1.756\n",
      "Ep 2 (Step 052770): Train loss 1.084, Val loss 1.755\n",
      "Ep 2 (Step 052775): Train loss 1.226, Val loss 1.754\n",
      "Ep 2 (Step 052780): Train loss 1.355, Val loss 1.753\n",
      "Ep 2 (Step 052785): Train loss 0.771, Val loss 1.752\n",
      "Ep 2 (Step 052790): Train loss 1.087, Val loss 1.752\n",
      "Ep 2 (Step 052795): Train loss 1.115, Val loss 1.752\n",
      "Ep 2 (Step 052800): Train loss 0.758, Val loss 1.752\n",
      "Ep 2 (Step 052805): Train loss 0.896, Val loss 1.751\n",
      "Ep 2 (Step 052810): Train loss 1.165, Val loss 1.751\n",
      "Ep 2 (Step 052815): Train loss 1.215, Val loss 1.751\n",
      "Ep 2 (Step 052820): Train loss 0.755, Val loss 1.751\n",
      "Ep 2 (Step 052825): Train loss 1.210, Val loss 1.753\n",
      "Ep 2 (Step 052830): Train loss 1.272, Val loss 1.754\n",
      "Ep 2 (Step 052835): Train loss 1.133, Val loss 1.757\n",
      "Ep 2 (Step 052840): Train loss 1.180, Val loss 1.757\n",
      "Ep 2 (Step 052845): Train loss 1.204, Val loss 1.758\n",
      "Ep 2 (Step 052850): Train loss 1.079, Val loss 1.758\n",
      "Ep 2 (Step 052855): Train loss 1.167, Val loss 1.759\n",
      "Ep 2 (Step 052860): Train loss 1.086, Val loss 1.760\n",
      "Ep 2 (Step 052865): Train loss 1.033, Val loss 1.762\n",
      "Ep 2 (Step 052870): Train loss 1.074, Val loss 1.763\n",
      "Ep 2 (Step 052875): Train loss 1.134, Val loss 1.763\n",
      "Ep 2 (Step 052880): Train loss 0.867, Val loss 1.762\n",
      "Ep 2 (Step 052885): Train loss 0.954, Val loss 1.762\n",
      "Ep 2 (Step 052890): Train loss 1.192, Val loss 1.762\n",
      "Ep 2 (Step 052895): Train loss 0.781, Val loss 1.762\n",
      "Ep 2 (Step 052900): Train loss 0.928, Val loss 1.764\n",
      "Ep 2 (Step 052905): Train loss 0.807, Val loss 1.766\n",
      "Ep 2 (Step 052910): Train loss 0.832, Val loss 1.766\n",
      "Ep 2 (Step 052915): Train loss 1.186, Val loss 1.764\n",
      "Ep 2 (Step 052920): Train loss 1.186, Val loss 1.762\n",
      "Ep 2 (Step 052925): Train loss 1.053, Val loss 1.760\n",
      "Ep 2 (Step 052930): Train loss 1.099, Val loss 1.757\n",
      "Ep 2 (Step 052935): Train loss 1.249, Val loss 1.756\n",
      "Ep 2 (Step 052940): Train loss 0.815, Val loss 1.757\n",
      "Ep 2 (Step 052945): Train loss 1.021, Val loss 1.757\n",
      "Ep 2 (Step 052950): Train loss 1.257, Val loss 1.756\n",
      "Ep 2 (Step 052955): Train loss 1.080, Val loss 1.757\n",
      "Ep 2 (Step 052960): Train loss 0.825, Val loss 1.758\n",
      "Ep 2 (Step 052965): Train loss 1.024, Val loss 1.758\n",
      "Ep 2 (Step 052970): Train loss 0.919, Val loss 1.757\n",
      "Ep 2 (Step 052975): Train loss 1.269, Val loss 1.757\n",
      "Ep 2 (Step 052980): Train loss 0.843, Val loss 1.756\n",
      "Ep 2 (Step 052985): Train loss 0.881, Val loss 1.756\n",
      "Ep 2 (Step 052990): Train loss 1.187, Val loss 1.755\n",
      "Ep 2 (Step 052995): Train loss 0.910, Val loss 1.755\n",
      "Ep 2 (Step 053000): Train loss 1.053, Val loss 1.755\n",
      "Ep 2 (Step 053005): Train loss 1.209, Val loss 1.754\n",
      "Ep 2 (Step 053010): Train loss 1.061, Val loss 1.755\n",
      "Ep 2 (Step 053015): Train loss 1.104, Val loss 1.755\n",
      "Ep 2 (Step 053020): Train loss 1.230, Val loss 1.755\n",
      "Ep 2 (Step 053025): Train loss 1.334, Val loss 1.755\n",
      "Ep 2 (Step 053030): Train loss 1.102, Val loss 1.754\n",
      "Ep 2 (Step 053035): Train loss 1.224, Val loss 1.755\n",
      "Ep 2 (Step 053040): Train loss 1.055, Val loss 1.755\n",
      "Ep 2 (Step 053045): Train loss 1.170, Val loss 1.755\n",
      "Ep 2 (Step 053050): Train loss 0.837, Val loss 1.755\n",
      "Ep 2 (Step 053055): Train loss 1.032, Val loss 1.754\n",
      "Ep 2 (Step 053060): Train loss 0.961, Val loss 1.753\n",
      "Ep 2 (Step 053065): Train loss 1.133, Val loss 1.754\n",
      "Ep 2 (Step 053070): Train loss 1.110, Val loss 1.754\n",
      "Ep 2 (Step 053075): Train loss 1.016, Val loss 1.754\n",
      "Ep 2 (Step 053080): Train loss 0.881, Val loss 1.754\n",
      "Ep 2 (Step 053085): Train loss 0.818, Val loss 1.755\n",
      "Ep 2 (Step 053090): Train loss 1.036, Val loss 1.756\n",
      "Ep 2 (Step 053095): Train loss 0.985, Val loss 1.757\n",
      "Ep 2 (Step 053100): Train loss 1.210, Val loss 1.758\n",
      "Ep 2 (Step 053105): Train loss 1.190, Val loss 1.759\n",
      "Ep 2 (Step 053110): Train loss 0.996, Val loss 1.762\n",
      "Ep 2 (Step 053115): Train loss 1.096, Val loss 1.763\n",
      "Ep 2 (Step 053120): Train loss 0.973, Val loss 1.763\n",
      "Ep 2 (Step 053125): Train loss 0.994, Val loss 1.763\n",
      "Ep 2 (Step 053130): Train loss 0.821, Val loss 1.764\n",
      "Ep 2 (Step 053135): Train loss 1.504, Val loss 1.764\n",
      "Ep 2 (Step 053140): Train loss 1.148, Val loss 1.765\n",
      "Ep 2 (Step 053145): Train loss 0.895, Val loss 1.765\n",
      "Ep 2 (Step 053150): Train loss 1.120, Val loss 1.766\n",
      "Ep 2 (Step 053155): Train loss 1.017, Val loss 1.766\n",
      "Ep 2 (Step 053160): Train loss 1.064, Val loss 1.767\n",
      "Ep 2 (Step 053165): Train loss 0.973, Val loss 1.768\n",
      "Ep 2 (Step 053170): Train loss 1.506, Val loss 1.769\n",
      "Ep 2 (Step 053175): Train loss 1.071, Val loss 1.770\n",
      "Ep 2 (Step 053180): Train loss 1.068, Val loss 1.770\n",
      "Ep 2 (Step 053185): Train loss 1.166, Val loss 1.770\n",
      "Ep 2 (Step 053190): Train loss 0.971, Val loss 1.769\n",
      "Ep 2 (Step 053195): Train loss 1.127, Val loss 1.770\n",
      "Ep 2 (Step 053200): Train loss 1.351, Val loss 1.769\n",
      "Ep 2 (Step 053205): Train loss 1.166, Val loss 1.768\n",
      "Ep 2 (Step 053210): Train loss 1.248, Val loss 1.765\n",
      "Ep 2 (Step 053215): Train loss 1.269, Val loss 1.764\n",
      "Ep 2 (Step 053220): Train loss 0.997, Val loss 1.762\n",
      "Ep 2 (Step 053225): Train loss 1.163, Val loss 1.761\n",
      "Ep 2 (Step 053230): Train loss 1.273, Val loss 1.762\n",
      "Ep 2 (Step 053235): Train loss 0.939, Val loss 1.763\n",
      "Ep 2 (Step 053240): Train loss 1.238, Val loss 1.763\n",
      "Ep 2 (Step 053245): Train loss 1.082, Val loss 1.763\n",
      "Ep 2 (Step 053250): Train loss 0.980, Val loss 1.763\n",
      "Ep 2 (Step 053255): Train loss 0.833, Val loss 1.762\n",
      "Ep 2 (Step 053260): Train loss 1.033, Val loss 1.763\n",
      "Ep 2 (Step 053265): Train loss 1.036, Val loss 1.763\n",
      "Ep 2 (Step 053270): Train loss 1.141, Val loss 1.762\n",
      "Ep 2 (Step 053275): Train loss 1.150, Val loss 1.761\n",
      "Ep 2 (Step 053280): Train loss 1.122, Val loss 1.762\n",
      "Ep 2 (Step 053285): Train loss 1.255, Val loss 1.764\n",
      "Ep 2 (Step 053290): Train loss 0.865, Val loss 1.765\n",
      "Ep 2 (Step 053295): Train loss 0.964, Val loss 1.766\n",
      "Ep 2 (Step 053300): Train loss 1.006, Val loss 1.768\n",
      "Ep 2 (Step 053305): Train loss 1.154, Val loss 1.769\n",
      "Ep 2 (Step 053310): Train loss 1.041, Val loss 1.769\n",
      "Ep 2 (Step 053315): Train loss 0.948, Val loss 1.768\n",
      "Ep 2 (Step 053320): Train loss 1.161, Val loss 1.767\n",
      "Ep 2 (Step 053325): Train loss 1.099, Val loss 1.766\n",
      "Ep 2 (Step 053330): Train loss 0.944, Val loss 1.767\n",
      "Ep 2 (Step 053335): Train loss 0.920, Val loss 1.768\n",
      "Ep 2 (Step 053340): Train loss 0.888, Val loss 1.770\n",
      "Ep 2 (Step 053345): Train loss 1.186, Val loss 1.771\n",
      "Ep 2 (Step 053350): Train loss 1.049, Val loss 1.772\n",
      "Ep 2 (Step 053355): Train loss 1.312, Val loss 1.773\n",
      "Ep 2 (Step 053360): Train loss 1.219, Val loss 1.772\n",
      "Ep 2 (Step 053365): Train loss 0.954, Val loss 1.771\n",
      "Ep 2 (Step 053370): Train loss 1.028, Val loss 1.770\n",
      "Ep 2 (Step 053375): Train loss 0.870, Val loss 1.769\n",
      "Ep 2 (Step 053380): Train loss 0.902, Val loss 1.769\n",
      "Ep 2 (Step 053385): Train loss 1.042, Val loss 1.768\n",
      "Ep 2 (Step 053390): Train loss 0.990, Val loss 1.768\n",
      "Ep 2 (Step 053395): Train loss 1.053, Val loss 1.767\n",
      "Ep 2 (Step 053400): Train loss 1.242, Val loss 1.765\n",
      "Ep 2 (Step 053405): Train loss 0.966, Val loss 1.765\n",
      "Ep 2 (Step 053410): Train loss 1.093, Val loss 1.763\n",
      "Ep 2 (Step 053415): Train loss 0.999, Val loss 1.763\n",
      "Ep 2 (Step 053420): Train loss 1.075, Val loss 1.763\n",
      "Ep 2 (Step 053425): Train loss 1.315, Val loss 1.763\n",
      "Ep 2 (Step 053430): Train loss 0.926, Val loss 1.763\n",
      "Ep 2 (Step 053435): Train loss 1.172, Val loss 1.763\n",
      "Ep 2 (Step 053440): Train loss 1.092, Val loss 1.764\n",
      "Ep 2 (Step 053445): Train loss 1.225, Val loss 1.765\n",
      "Ep 2 (Step 053450): Train loss 1.095, Val loss 1.766\n",
      "Ep 2 (Step 053455): Train loss 1.232, Val loss 1.768\n",
      "Ep 2 (Step 053460): Train loss 1.046, Val loss 1.769\n",
      "Ep 2 (Step 053465): Train loss 1.098, Val loss 1.769\n",
      "Ep 2 (Step 053470): Train loss 1.115, Val loss 1.770\n",
      "Ep 2 (Step 053475): Train loss 1.429, Val loss 1.770\n",
      "Ep 2 (Step 053480): Train loss 1.123, Val loss 1.769\n",
      "Ep 2 (Step 053485): Train loss 1.126, Val loss 1.769\n",
      "Ep 2 (Step 053490): Train loss 1.071, Val loss 1.769\n",
      "Ep 2 (Step 053495): Train loss 1.267, Val loss 1.770\n",
      "Ep 2 (Step 053500): Train loss 1.104, Val loss 1.772\n",
      "Ep 2 (Step 053505): Train loss 0.940, Val loss 1.772\n",
      "Ep 2 (Step 053510): Train loss 1.099, Val loss 1.772\n",
      "Ep 2 (Step 053515): Train loss 0.801, Val loss 1.772\n",
      "Ep 2 (Step 053520): Train loss 1.060, Val loss 1.773\n",
      "Ep 2 (Step 053525): Train loss 1.219, Val loss 1.774\n",
      "Ep 2 (Step 053530): Train loss 0.825, Val loss 1.775\n",
      "Ep 2 (Step 053535): Train loss 1.094, Val loss 1.775\n",
      "Ep 2 (Step 053540): Train loss 1.216, Val loss 1.774\n",
      "Ep 2 (Step 053545): Train loss 0.759, Val loss 1.775\n",
      "Ep 2 (Step 053550): Train loss 1.461, Val loss 1.774\n",
      "Ep 2 (Step 053555): Train loss 1.183, Val loss 1.774\n",
      "Ep 2 (Step 053560): Train loss 1.233, Val loss 1.772\n",
      "Ep 2 (Step 053565): Train loss 1.061, Val loss 1.770\n",
      "Ep 2 (Step 053570): Train loss 1.033, Val loss 1.771\n",
      "Ep 2 (Step 053575): Train loss 1.172, Val loss 1.772\n",
      "Ep 2 (Step 053580): Train loss 1.189, Val loss 1.771\n",
      "Ep 2 (Step 053585): Train loss 0.950, Val loss 1.770\n",
      "Ep 2 (Step 053590): Train loss 0.987, Val loss 1.770\n",
      "Ep 2 (Step 053595): Train loss 0.898, Val loss 1.771\n",
      "Ep 2 (Step 053600): Train loss 1.278, Val loss 1.773\n",
      "Ep 2 (Step 053605): Train loss 0.941, Val loss 1.773\n",
      "Ep 2 (Step 053610): Train loss 1.093, Val loss 1.772\n",
      "Ep 2 (Step 053615): Train loss 1.150, Val loss 1.773\n",
      "Ep 2 (Step 053620): Train loss 1.122, Val loss 1.773\n",
      "Ep 2 (Step 053625): Train loss 0.930, Val loss 1.774\n",
      "Ep 2 (Step 053630): Train loss 1.257, Val loss 1.774\n",
      "Ep 2 (Step 053635): Train loss 1.139, Val loss 1.775\n",
      "Ep 2 (Step 053640): Train loss 1.150, Val loss 1.775\n",
      "Ep 2 (Step 053645): Train loss 1.183, Val loss 1.776\n",
      "Ep 2 (Step 053650): Train loss 1.372, Val loss 1.776\n",
      "Ep 2 (Step 053655): Train loss 0.922, Val loss 1.775\n",
      "Ep 2 (Step 053660): Train loss 1.028, Val loss 1.775\n",
      "Ep 2 (Step 053665): Train loss 1.021, Val loss 1.775\n",
      "Ep 2 (Step 053670): Train loss 0.947, Val loss 1.775\n",
      "Ep 2 (Step 053675): Train loss 0.805, Val loss 1.775\n",
      "Ep 2 (Step 053680): Train loss 0.987, Val loss 1.776\n",
      "Ep 2 (Step 053685): Train loss 1.012, Val loss 1.778\n",
      "Ep 2 (Step 053690): Train loss 1.043, Val loss 1.779\n",
      "Ep 2 (Step 053695): Train loss 1.220, Val loss 1.780\n",
      "Ep 2 (Step 053700): Train loss 0.743, Val loss 1.782\n",
      "Ep 2 (Step 053705): Train loss 1.101, Val loss 1.783\n",
      "Ep 2 (Step 053710): Train loss 1.091, Val loss 1.784\n",
      "Ep 2 (Step 053715): Train loss 1.042, Val loss 1.786\n",
      "Ep 2 (Step 053720): Train loss 1.495, Val loss 1.786\n",
      "Ep 2 (Step 053725): Train loss 1.085, Val loss 1.785\n",
      "Ep 2 (Step 053730): Train loss 1.108, Val loss 1.783\n",
      "Ep 2 (Step 053735): Train loss 1.203, Val loss 1.783\n",
      "Ep 2 (Step 053740): Train loss 1.072, Val loss 1.783\n",
      "Ep 2 (Step 053745): Train loss 0.949, Val loss 1.784\n",
      "Ep 2 (Step 053750): Train loss 1.164, Val loss 1.784\n",
      "Ep 2 (Step 053755): Train loss 1.071, Val loss 1.784\n",
      "Ep 2 (Step 053760): Train loss 1.533, Val loss 1.784\n",
      "Ep 2 (Step 053765): Train loss 1.108, Val loss 1.785\n",
      "Ep 2 (Step 053770): Train loss 1.168, Val loss 1.787\n",
      "Ep 2 (Step 053775): Train loss 1.210, Val loss 1.787\n",
      "Ep 2 (Step 053780): Train loss 1.155, Val loss 1.786\n",
      "Ep 2 (Step 053785): Train loss 1.060, Val loss 1.785\n",
      "Ep 2 (Step 053790): Train loss 1.229, Val loss 1.783\n",
      "Ep 2 (Step 053795): Train loss 1.060, Val loss 1.781\n",
      "Ep 2 (Step 053800): Train loss 1.182, Val loss 1.781\n",
      "Ep 2 (Step 053805): Train loss 0.945, Val loss 1.781\n",
      "Ep 2 (Step 053810): Train loss 1.030, Val loss 1.781\n",
      "Ep 2 (Step 053815): Train loss 1.284, Val loss 1.781\n",
      "Ep 2 (Step 053820): Train loss 0.890, Val loss 1.780\n",
      "Ep 2 (Step 053825): Train loss 1.194, Val loss 1.778\n",
      "Ep 2 (Step 053830): Train loss 1.084, Val loss 1.776\n",
      "Ep 2 (Step 053835): Train loss 1.236, Val loss 1.773\n",
      "Ep 2 (Step 053840): Train loss 1.255, Val loss 1.770\n",
      "Ep 2 (Step 053845): Train loss 0.962, Val loss 1.769\n",
      "Ep 2 (Step 053850): Train loss 1.089, Val loss 1.768\n",
      "Ep 2 (Step 053855): Train loss 0.956, Val loss 1.768\n",
      "Ep 2 (Step 053860): Train loss 1.062, Val loss 1.768\n",
      "Ep 2 (Step 053865): Train loss 0.969, Val loss 1.769\n",
      "Ep 2 (Step 053870): Train loss 1.189, Val loss 1.771\n",
      "Ep 2 (Step 053875): Train loss 1.315, Val loss 1.772\n",
      "Ep 2 (Step 053880): Train loss 1.436, Val loss 1.773\n",
      "Ep 2 (Step 053885): Train loss 0.879, Val loss 1.772\n",
      "Ep 2 (Step 053890): Train loss 1.043, Val loss 1.770\n",
      "Ep 2 (Step 053895): Train loss 1.130, Val loss 1.768\n",
      "Ep 2 (Step 053900): Train loss 1.020, Val loss 1.767\n",
      "Ep 2 (Step 053905): Train loss 1.272, Val loss 1.767\n",
      "Ep 2 (Step 053910): Train loss 1.024, Val loss 1.767\n",
      "Ep 2 (Step 053915): Train loss 1.110, Val loss 1.768\n",
      "Ep 2 (Step 053920): Train loss 1.012, Val loss 1.767\n",
      "Ep 2 (Step 053925): Train loss 1.277, Val loss 1.766\n",
      "Ep 2 (Step 053930): Train loss 1.046, Val loss 1.765\n",
      "Ep 2 (Step 053935): Train loss 0.929, Val loss 1.765\n",
      "Ep 2 (Step 053940): Train loss 0.861, Val loss 1.765\n",
      "Ep 2 (Step 053945): Train loss 1.157, Val loss 1.766\n",
      "Ep 2 (Step 053950): Train loss 1.026, Val loss 1.767\n",
      "Ep 2 (Step 053955): Train loss 0.990, Val loss 1.769\n",
      "Ep 2 (Step 053960): Train loss 1.423, Val loss 1.769\n",
      "Ep 2 (Step 053965): Train loss 1.143, Val loss 1.770\n",
      "Ep 2 (Step 053970): Train loss 1.249, Val loss 1.770\n",
      "Ep 2 (Step 053975): Train loss 0.961, Val loss 1.770\n",
      "Ep 2 (Step 053980): Train loss 1.148, Val loss 1.770\n",
      "Ep 2 (Step 053985): Train loss 1.057, Val loss 1.770\n",
      "Ep 2 (Step 053990): Train loss 1.132, Val loss 1.769\n",
      "Ep 2 (Step 053995): Train loss 1.275, Val loss 1.769\n",
      "Ep 2 (Step 054000): Train loss 0.969, Val loss 1.770\n",
      "Ep 2 (Step 054005): Train loss 0.782, Val loss 1.772\n",
      "Ep 2 (Step 054010): Train loss 0.949, Val loss 1.773\n",
      "Ep 2 (Step 054015): Train loss 1.300, Val loss 1.773\n",
      "Ep 2 (Step 054020): Train loss 1.105, Val loss 1.773\n",
      "Ep 2 (Step 054025): Train loss 0.811, Val loss 1.772\n",
      "Ep 2 (Step 054030): Train loss 1.128, Val loss 1.772\n",
      "Ep 2 (Step 054035): Train loss 0.838, Val loss 1.771\n",
      "Ep 2 (Step 054040): Train loss 1.221, Val loss 1.770\n",
      "Ep 2 (Step 054045): Train loss 0.868, Val loss 1.770\n",
      "Ep 2 (Step 054050): Train loss 1.102, Val loss 1.768\n",
      "Ep 2 (Step 054055): Train loss 0.986, Val loss 1.767\n",
      "Ep 2 (Step 054060): Train loss 1.289, Val loss 1.768\n",
      "Ep 2 (Step 054065): Train loss 1.121, Val loss 1.770\n",
      "Ep 2 (Step 054070): Train loss 0.873, Val loss 1.772\n",
      "Ep 2 (Step 054075): Train loss 0.941, Val loss 1.773\n",
      "Ep 2 (Step 054080): Train loss 0.878, Val loss 1.771\n",
      "Ep 2 (Step 054085): Train loss 1.309, Val loss 1.770\n",
      "Ep 2 (Step 054090): Train loss 0.894, Val loss 1.770\n",
      "Ep 2 (Step 054095): Train loss 1.119, Val loss 1.772\n",
      "Ep 2 (Step 054100): Train loss 0.840, Val loss 1.772\n",
      "Ep 2 (Step 054105): Train loss 1.146, Val loss 1.773\n",
      "Ep 2 (Step 054110): Train loss 1.246, Val loss 1.772\n",
      "Ep 2 (Step 054115): Train loss 0.789, Val loss 1.772\n",
      "Ep 2 (Step 054120): Train loss 1.236, Val loss 1.772\n",
      "Ep 2 (Step 054125): Train loss 1.124, Val loss 1.773\n",
      "Ep 2 (Step 054130): Train loss 1.174, Val loss 1.774\n",
      "Ep 2 (Step 054135): Train loss 1.036, Val loss 1.773\n",
      "Ep 2 (Step 054140): Train loss 1.300, Val loss 1.773\n",
      "Ep 2 (Step 054145): Train loss 1.232, Val loss 1.774\n",
      "Ep 2 (Step 054150): Train loss 1.198, Val loss 1.774\n",
      "Ep 2 (Step 054155): Train loss 0.965, Val loss 1.774\n",
      "Ep 2 (Step 054160): Train loss 1.237, Val loss 1.775\n",
      "Ep 2 (Step 054165): Train loss 0.937, Val loss 1.773\n",
      "Ep 2 (Step 054170): Train loss 1.334, Val loss 1.772\n",
      "Ep 2 (Step 054175): Train loss 1.258, Val loss 1.771\n",
      "Ep 2 (Step 054180): Train loss 1.035, Val loss 1.772\n",
      "Ep 2 (Step 054185): Train loss 0.943, Val loss 1.771\n",
      "Ep 2 (Step 054190): Train loss 0.802, Val loss 1.771\n",
      "Ep 2 (Step 054195): Train loss 0.869, Val loss 1.769\n",
      "Ep 2 (Step 054200): Train loss 0.977, Val loss 1.768\n",
      "Ep 2 (Step 054205): Train loss 1.090, Val loss 1.768\n",
      "Ep 2 (Step 054210): Train loss 0.925, Val loss 1.769\n",
      "Ep 2 (Step 054215): Train loss 0.929, Val loss 1.768\n",
      "Ep 2 (Step 054220): Train loss 1.258, Val loss 1.767\n",
      "Ep 2 (Step 054225): Train loss 1.036, Val loss 1.769\n",
      "Ep 2 (Step 054230): Train loss 0.859, Val loss 1.770\n",
      "Ep 2 (Step 054235): Train loss 1.125, Val loss 1.771\n",
      "Ep 2 (Step 054240): Train loss 1.140, Val loss 1.773\n",
      "Ep 2 (Step 054245): Train loss 0.938, Val loss 1.774\n",
      "Ep 2 (Step 054250): Train loss 0.738, Val loss 1.774\n",
      "Ep 2 (Step 054255): Train loss 1.167, Val loss 1.774\n",
      "Ep 2 (Step 054260): Train loss 1.303, Val loss 1.770\n",
      "Ep 2 (Step 054265): Train loss 1.273, Val loss 1.766\n",
      "Ep 2 (Step 054270): Train loss 1.137, Val loss 1.764\n",
      "Ep 2 (Step 054275): Train loss 0.979, Val loss 1.763\n",
      "Ep 2 (Step 054280): Train loss 1.088, Val loss 1.763\n",
      "Ep 2 (Step 054285): Train loss 1.141, Val loss 1.762\n",
      "Ep 2 (Step 054290): Train loss 1.026, Val loss 1.763\n",
      "Ep 2 (Step 054295): Train loss 1.170, Val loss 1.764\n",
      "Ep 2 (Step 054300): Train loss 1.182, Val loss 1.765\n",
      "Ep 2 (Step 054305): Train loss 1.040, Val loss 1.766\n",
      "Ep 2 (Step 054310): Train loss 1.240, Val loss 1.767\n",
      "Ep 2 (Step 054315): Train loss 1.160, Val loss 1.769\n",
      "Ep 2 (Step 054320): Train loss 1.370, Val loss 1.770\n",
      "Ep 2 (Step 054325): Train loss 1.164, Val loss 1.770\n",
      "Ep 2 (Step 054330): Train loss 1.005, Val loss 1.769\n",
      "Ep 2 (Step 054335): Train loss 1.092, Val loss 1.770\n",
      "Ep 2 (Step 054340): Train loss 0.898, Val loss 1.769\n",
      "Ep 2 (Step 054345): Train loss 1.061, Val loss 1.767\n",
      "Ep 2 (Step 054350): Train loss 1.027, Val loss 1.766\n",
      "Ep 2 (Step 054355): Train loss 1.161, Val loss 1.764\n",
      "Ep 2 (Step 054360): Train loss 0.901, Val loss 1.764\n",
      "Ep 2 (Step 054365): Train loss 1.079, Val loss 1.766\n",
      "Ep 2 (Step 054370): Train loss 1.130, Val loss 1.768\n",
      "Ep 2 (Step 054375): Train loss 1.023, Val loss 1.769\n",
      "Ep 2 (Step 054380): Train loss 1.206, Val loss 1.769\n",
      "Ep 2 (Step 054385): Train loss 1.252, Val loss 1.769\n",
      "Ep 2 (Step 054390): Train loss 1.231, Val loss 1.769\n",
      "Ep 2 (Step 054395): Train loss 0.936, Val loss 1.770\n",
      "Ep 2 (Step 054400): Train loss 0.796, Val loss 1.770\n",
      "Ep 2 (Step 054405): Train loss 1.299, Val loss 1.768\n",
      "Ep 2 (Step 054410): Train loss 0.779, Val loss 1.767\n",
      "Ep 2 (Step 054415): Train loss 1.301, Val loss 1.765\n",
      "Ep 2 (Step 054420): Train loss 1.076, Val loss 1.763\n",
      "Ep 2 (Step 054425): Train loss 0.839, Val loss 1.762\n",
      "Ep 2 (Step 054430): Train loss 1.153, Val loss 1.760\n",
      "Ep 2 (Step 054435): Train loss 1.279, Val loss 1.758\n",
      "Ep 2 (Step 054440): Train loss 0.902, Val loss 1.758\n",
      "Ep 2 (Step 054445): Train loss 1.263, Val loss 1.757\n",
      "Ep 2 (Step 054450): Train loss 0.890, Val loss 1.756\n",
      "Ep 2 (Step 054455): Train loss 1.201, Val loss 1.757\n",
      "Ep 2 (Step 054460): Train loss 0.906, Val loss 1.760\n",
      "Ep 2 (Step 054465): Train loss 1.033, Val loss 1.761\n",
      "Ep 2 (Step 054470): Train loss 0.835, Val loss 1.762\n",
      "Ep 2 (Step 054475): Train loss 0.724, Val loss 1.765\n",
      "Ep 2 (Step 054480): Train loss 1.175, Val loss 1.765\n",
      "Ep 2 (Step 054485): Train loss 0.936, Val loss 1.766\n",
      "Ep 2 (Step 054490): Train loss 1.139, Val loss 1.767\n",
      "Ep 2 (Step 054495): Train loss 1.011, Val loss 1.770\n",
      "Ep 2 (Step 054500): Train loss 1.212, Val loss 1.771\n",
      "Ep 2 (Step 054505): Train loss 1.162, Val loss 1.773\n",
      "Ep 2 (Step 054510): Train loss 1.018, Val loss 1.773\n",
      "Ep 2 (Step 054515): Train loss 1.307, Val loss 1.770\n",
      "Ep 2 (Step 054520): Train loss 0.983, Val loss 1.767\n",
      "Ep 2 (Step 054525): Train loss 1.214, Val loss 1.765\n",
      "Ep 2 (Step 054530): Train loss 1.105, Val loss 1.765\n",
      "Ep 2 (Step 054535): Train loss 1.162, Val loss 1.765\n",
      "Ep 2 (Step 054540): Train loss 1.230, Val loss 1.765\n",
      "Ep 2 (Step 054545): Train loss 0.982, Val loss 1.766\n",
      "Ep 2 (Step 054550): Train loss 1.296, Val loss 1.765\n",
      "Ep 2 (Step 054555): Train loss 1.050, Val loss 1.765\n",
      "Ep 2 (Step 054560): Train loss 0.835, Val loss 1.765\n",
      "Ep 2 (Step 054565): Train loss 1.056, Val loss 1.766\n",
      "Ep 2 (Step 054570): Train loss 1.084, Val loss 1.768\n",
      "Ep 2 (Step 054575): Train loss 0.746, Val loss 1.769\n",
      "Ep 2 (Step 054580): Train loss 1.131, Val loss 1.770\n",
      "Ep 2 (Step 054585): Train loss 0.952, Val loss 1.771\n",
      "Ep 2 (Step 054590): Train loss 0.858, Val loss 1.771\n",
      "Ep 2 (Step 054595): Train loss 1.218, Val loss 1.772\n",
      "Ep 2 (Step 054600): Train loss 0.946, Val loss 1.772\n",
      "Ep 2 (Step 054605): Train loss 1.318, Val loss 1.771\n",
      "Ep 2 (Step 054610): Train loss 1.018, Val loss 1.769\n",
      "Ep 2 (Step 054615): Train loss 0.696, Val loss 1.769\n",
      "Ep 2 (Step 054620): Train loss 1.103, Val loss 1.768\n",
      "Ep 2 (Step 054625): Train loss 1.247, Val loss 1.769\n",
      "Ep 2 (Step 054630): Train loss 1.068, Val loss 1.769\n",
      "Ep 2 (Step 054635): Train loss 1.022, Val loss 1.768\n",
      "Ep 2 (Step 054640): Train loss 1.375, Val loss 1.767\n",
      "Ep 2 (Step 054645): Train loss 1.135, Val loss 1.767\n",
      "Ep 2 (Step 054650): Train loss 1.311, Val loss 1.768\n",
      "Ep 2 (Step 054655): Train loss 1.429, Val loss 1.769\n",
      "Ep 2 (Step 054660): Train loss 0.940, Val loss 1.769\n",
      "Ep 2 (Step 054665): Train loss 1.183, Val loss 1.769\n",
      "Ep 2 (Step 054670): Train loss 1.243, Val loss 1.769\n",
      "Ep 2 (Step 054675): Train loss 1.199, Val loss 1.769\n",
      "Ep 2 (Step 054680): Train loss 1.232, Val loss 1.770\n",
      "Ep 2 (Step 054685): Train loss 0.975, Val loss 1.771\n",
      "Ep 2 (Step 054690): Train loss 1.189, Val loss 1.773\n",
      "Ep 2 (Step 054695): Train loss 0.898, Val loss 1.774\n",
      "Ep 2 (Step 054700): Train loss 1.140, Val loss 1.776\n",
      "Ep 2 (Step 054705): Train loss 1.045, Val loss 1.777\n",
      "Ep 2 (Step 054710): Train loss 0.979, Val loss 1.779\n",
      "Ep 2 (Step 054715): Train loss 1.187, Val loss 1.780\n",
      "Ep 2 (Step 054720): Train loss 0.944, Val loss 1.780\n",
      "Ep 2 (Step 054725): Train loss 1.065, Val loss 1.779\n",
      "Ep 2 (Step 054730): Train loss 1.093, Val loss 1.776\n",
      "Ep 2 (Step 054735): Train loss 1.167, Val loss 1.773\n",
      "Ep 2 (Step 054740): Train loss 1.147, Val loss 1.770\n",
      "Ep 2 (Step 054745): Train loss 1.211, Val loss 1.767\n",
      "Ep 2 (Step 054750): Train loss 1.215, Val loss 1.766\n",
      "Ep 2 (Step 054755): Train loss 0.935, Val loss 1.766\n",
      "Ep 2 (Step 054760): Train loss 0.969, Val loss 1.767\n",
      "Ep 2 (Step 054765): Train loss 1.311, Val loss 1.768\n",
      "Ep 2 (Step 054770): Train loss 0.898, Val loss 1.768\n",
      "Ep 2 (Step 054775): Train loss 0.866, Val loss 1.767\n",
      "Ep 2 (Step 054780): Train loss 1.217, Val loss 1.765\n",
      "Ep 2 (Step 054785): Train loss 1.259, Val loss 1.764\n",
      "Ep 2 (Step 054790): Train loss 0.774, Val loss 1.764\n",
      "Ep 2 (Step 054795): Train loss 1.079, Val loss 1.763\n",
      "Ep 2 (Step 054800): Train loss 1.075, Val loss 1.762\n",
      "Ep 2 (Step 054805): Train loss 1.106, Val loss 1.761\n",
      "Ep 2 (Step 054810): Train loss 1.032, Val loss 1.760\n",
      "Ep 2 (Step 054815): Train loss 1.127, Val loss 1.759\n",
      "Ep 2 (Step 054820): Train loss 1.215, Val loss 1.758\n",
      "Ep 2 (Step 054825): Train loss 0.841, Val loss 1.759\n",
      "Ep 2 (Step 054830): Train loss 0.836, Val loss 1.760\n",
      "Ep 2 (Step 054835): Train loss 1.136, Val loss 1.760\n",
      "Ep 2 (Step 054840): Train loss 1.204, Val loss 1.761\n",
      "Ep 2 (Step 054845): Train loss 1.209, Val loss 1.762\n",
      "Ep 2 (Step 054850): Train loss 1.189, Val loss 1.762\n",
      "Ep 2 (Step 054855): Train loss 1.137, Val loss 1.761\n",
      "Ep 2 (Step 054860): Train loss 0.990, Val loss 1.761\n",
      "Ep 2 (Step 054865): Train loss 1.034, Val loss 1.762\n",
      "Ep 2 (Step 054870): Train loss 1.143, Val loss 1.763\n",
      "Ep 2 (Step 054875): Train loss 0.907, Val loss 1.764\n",
      "Ep 2 (Step 054880): Train loss 0.882, Val loss 1.764\n",
      "Ep 2 (Step 054885): Train loss 1.029, Val loss 1.766\n",
      "Ep 2 (Step 054890): Train loss 0.773, Val loss 1.769\n",
      "Ep 2 (Step 054895): Train loss 1.220, Val loss 1.773\n",
      "Ep 2 (Step 054900): Train loss 0.778, Val loss 1.775\n",
      "Ep 2 (Step 054905): Train loss 1.141, Val loss 1.775\n",
      "Ep 2 (Step 054910): Train loss 1.039, Val loss 1.776\n",
      "Ep 2 (Step 054915): Train loss 1.494, Val loss 1.776\n",
      "Ep 2 (Step 054920): Train loss 0.934, Val loss 1.775\n",
      "Ep 2 (Step 054925): Train loss 0.909, Val loss 1.773\n",
      "Ep 2 (Step 054930): Train loss 1.222, Val loss 1.771\n",
      "Ep 2 (Step 054935): Train loss 1.177, Val loss 1.769\n",
      "Ep 2 (Step 054940): Train loss 1.242, Val loss 1.768\n",
      "Ep 2 (Step 054945): Train loss 0.916, Val loss 1.768\n",
      "Ep 2 (Step 054950): Train loss 1.016, Val loss 1.769\n",
      "Ep 2 (Step 054955): Train loss 1.192, Val loss 1.770\n",
      "Ep 2 (Step 054960): Train loss 1.059, Val loss 1.771\n",
      "Ep 2 (Step 054965): Train loss 1.098, Val loss 1.771\n",
      "Ep 2 (Step 054970): Train loss 1.188, Val loss 1.770\n",
      "Ep 2 (Step 054975): Train loss 0.978, Val loss 1.769\n",
      "Ep 2 (Step 054980): Train loss 1.064, Val loss 1.768\n",
      "Ep 2 (Step 054985): Train loss 1.236, Val loss 1.767\n",
      "Ep 2 (Step 054990): Train loss 1.165, Val loss 1.766\n",
      "Ep 2 (Step 054995): Train loss 1.093, Val loss 1.766\n",
      "Ep 2 (Step 055000): Train loss 1.153, Val loss 1.766\n",
      "Ep 2 (Step 055005): Train loss 1.295, Val loss 1.766\n",
      "Ep 2 (Step 055010): Train loss 1.171, Val loss 1.765\n",
      "Ep 2 (Step 055015): Train loss 0.849, Val loss 1.764\n",
      "Ep 2 (Step 055020): Train loss 1.153, Val loss 1.763\n",
      "Ep 2 (Step 055025): Train loss 0.907, Val loss 1.762\n",
      "Ep 2 (Step 055030): Train loss 0.882, Val loss 1.762\n",
      "Ep 2 (Step 055035): Train loss 1.177, Val loss 1.762\n",
      "Ep 2 (Step 055040): Train loss 0.951, Val loss 1.764\n",
      "Ep 2 (Step 055045): Train loss 0.996, Val loss 1.764\n",
      "Ep 2 (Step 055050): Train loss 0.926, Val loss 1.764\n",
      "Ep 2 (Step 055055): Train loss 1.120, Val loss 1.764\n",
      "Ep 2 (Step 055060): Train loss 0.966, Val loss 1.764\n",
      "Ep 2 (Step 055065): Train loss 0.838, Val loss 1.765\n",
      "Ep 2 (Step 055070): Train loss 1.351, Val loss 1.765\n",
      "Ep 2 (Step 055075): Train loss 1.096, Val loss 1.765\n",
      "Ep 2 (Step 055080): Train loss 0.845, Val loss 1.767\n",
      "Ep 2 (Step 055085): Train loss 1.301, Val loss 1.768\n",
      "Ep 2 (Step 055090): Train loss 1.220, Val loss 1.771\n",
      "Ep 2 (Step 055095): Train loss 1.194, Val loss 1.774\n",
      "Ep 2 (Step 055100): Train loss 0.757, Val loss 1.775\n",
      "Ep 2 (Step 055105): Train loss 0.825, Val loss 1.777\n",
      "Ep 2 (Step 055110): Train loss 1.070, Val loss 1.779\n",
      "Ep 2 (Step 055115): Train loss 1.094, Val loss 1.781\n",
      "Ep 2 (Step 055120): Train loss 1.327, Val loss 1.782\n",
      "Ep 2 (Step 055125): Train loss 1.265, Val loss 1.783\n",
      "Ep 2 (Step 055130): Train loss 1.073, Val loss 1.783\n",
      "Ep 2 (Step 055135): Train loss 0.977, Val loss 1.781\n",
      "Ep 2 (Step 055140): Train loss 1.063, Val loss 1.780\n",
      "Ep 2 (Step 055145): Train loss 1.097, Val loss 1.779\n",
      "Ep 2 (Step 055150): Train loss 1.220, Val loss 1.778\n",
      "Ep 2 (Step 055155): Train loss 1.291, Val loss 1.777\n",
      "Ep 2 (Step 055160): Train loss 1.003, Val loss 1.776\n",
      "Ep 2 (Step 055165): Train loss 0.988, Val loss 1.776\n",
      "Ep 2 (Step 055170): Train loss 1.253, Val loss 1.775\n",
      "Ep 2 (Step 055175): Train loss 0.973, Val loss 1.773\n",
      "Ep 2 (Step 055180): Train loss 1.220, Val loss 1.772\n",
      "Ep 2 (Step 055185): Train loss 1.151, Val loss 1.772\n",
      "Ep 2 (Step 055190): Train loss 1.307, Val loss 1.773\n",
      "Ep 2 (Step 055195): Train loss 0.943, Val loss 1.774\n",
      "Ep 2 (Step 055200): Train loss 0.916, Val loss 1.773\n",
      "Ep 2 (Step 055205): Train loss 0.945, Val loss 1.772\n",
      "Ep 2 (Step 055210): Train loss 1.159, Val loss 1.772\n",
      "Ep 2 (Step 055215): Train loss 0.965, Val loss 1.772\n",
      "Ep 2 (Step 055220): Train loss 1.213, Val loss 1.772\n",
      "Ep 2 (Step 055225): Train loss 1.173, Val loss 1.772\n",
      "Ep 2 (Step 055230): Train loss 1.090, Val loss 1.771\n",
      "Ep 2 (Step 055235): Train loss 0.914, Val loss 1.771\n",
      "Ep 2 (Step 055240): Train loss 1.090, Val loss 1.772\n",
      "Ep 2 (Step 055245): Train loss 0.996, Val loss 1.774\n",
      "Ep 2 (Step 055250): Train loss 1.099, Val loss 1.775\n",
      "Ep 2 (Step 055255): Train loss 1.195, Val loss 1.776\n",
      "Ep 2 (Step 055260): Train loss 1.091, Val loss 1.777\n",
      "Ep 2 (Step 055265): Train loss 0.854, Val loss 1.777\n",
      "Ep 2 (Step 055270): Train loss 1.150, Val loss 1.776\n",
      "Ep 2 (Step 055275): Train loss 0.990, Val loss 1.775\n",
      "Ep 2 (Step 055280): Train loss 0.943, Val loss 1.775\n",
      "Ep 2 (Step 055285): Train loss 0.951, Val loss 1.773\n",
      "Ep 2 (Step 055290): Train loss 1.163, Val loss 1.772\n",
      "Ep 2 (Step 055295): Train loss 1.049, Val loss 1.772\n",
      "Ep 2 (Step 055300): Train loss 1.133, Val loss 1.771\n",
      "Ep 2 (Step 055305): Train loss 0.957, Val loss 1.771\n",
      "Ep 2 (Step 055310): Train loss 0.923, Val loss 1.771\n",
      "Ep 2 (Step 055315): Train loss 0.885, Val loss 1.770\n",
      "Ep 2 (Step 055320): Train loss 1.238, Val loss 1.768\n",
      "Ep 2 (Step 055325): Train loss 1.225, Val loss 1.767\n",
      "Ep 2 (Step 055330): Train loss 1.148, Val loss 1.765\n",
      "Ep 2 (Step 055335): Train loss 0.968, Val loss 1.763\n",
      "Ep 2 (Step 055340): Train loss 1.244, Val loss 1.763\n",
      "Ep 2 (Step 055345): Train loss 1.210, Val loss 1.762\n",
      "Ep 2 (Step 055350): Train loss 0.876, Val loss 1.763\n",
      "Ep 2 (Step 055355): Train loss 0.954, Val loss 1.765\n",
      "Ep 2 (Step 055360): Train loss 1.024, Val loss 1.766\n",
      "Ep 2 (Step 055365): Train loss 0.973, Val loss 1.767\n",
      "Ep 2 (Step 055370): Train loss 1.419, Val loss 1.768\n",
      "Ep 2 (Step 055375): Train loss 0.889, Val loss 1.770\n",
      "Ep 2 (Step 055380): Train loss 1.091, Val loss 1.771\n",
      "Ep 2 (Step 055385): Train loss 1.272, Val loss 1.771\n",
      "Ep 2 (Step 055390): Train loss 0.896, Val loss 1.770\n",
      "Ep 2 (Step 055395): Train loss 0.862, Val loss 1.770\n",
      "Ep 2 (Step 055400): Train loss 1.278, Val loss 1.770\n",
      "Ep 2 (Step 055405): Train loss 1.067, Val loss 1.770\n",
      "Ep 2 (Step 055410): Train loss 0.939, Val loss 1.770\n",
      "Ep 2 (Step 055415): Train loss 1.151, Val loss 1.770\n",
      "Ep 2 (Step 055420): Train loss 0.886, Val loss 1.769\n",
      "Ep 2 (Step 055425): Train loss 1.251, Val loss 1.768\n",
      "Ep 2 (Step 055430): Train loss 1.008, Val loss 1.767\n",
      "Ep 2 (Step 055435): Train loss 1.224, Val loss 1.766\n",
      "Ep 2 (Step 055440): Train loss 0.937, Val loss 1.765\n",
      "Ep 2 (Step 055445): Train loss 1.051, Val loss 1.764\n",
      "Ep 2 (Step 055450): Train loss 0.932, Val loss 1.762\n",
      "Ep 2 (Step 055455): Train loss 0.934, Val loss 1.760\n",
      "Ep 2 (Step 055460): Train loss 1.272, Val loss 1.759\n",
      "Ep 2 (Step 055465): Train loss 1.125, Val loss 1.759\n",
      "Ep 2 (Step 055470): Train loss 0.989, Val loss 1.758\n",
      "Ep 2 (Step 055475): Train loss 1.069, Val loss 1.758\n",
      "Ep 2 (Step 055480): Train loss 1.372, Val loss 1.758\n",
      "Ep 2 (Step 055485): Train loss 0.923, Val loss 1.759\n",
      "Ep 2 (Step 055490): Train loss 0.933, Val loss 1.760\n",
      "Ep 2 (Step 055495): Train loss 0.997, Val loss 1.760\n",
      "Ep 2 (Step 055500): Train loss 1.092, Val loss 1.760\n",
      "Ep 2 (Step 055505): Train loss 0.917, Val loss 1.761\n",
      "Ep 2 (Step 055510): Train loss 0.861, Val loss 1.763\n",
      "Ep 2 (Step 055515): Train loss 1.030, Val loss 1.763\n",
      "Ep 2 (Step 055520): Train loss 1.091, Val loss 1.764\n",
      "Ep 2 (Step 055525): Train loss 0.890, Val loss 1.764\n",
      "Ep 2 (Step 055530): Train loss 1.295, Val loss 1.764\n",
      "Ep 2 (Step 055535): Train loss 1.144, Val loss 1.764\n",
      "Ep 2 (Step 055540): Train loss 0.952, Val loss 1.766\n",
      "Ep 2 (Step 055545): Train loss 1.115, Val loss 1.767\n",
      "Ep 2 (Step 055550): Train loss 1.340, Val loss 1.768\n",
      "Ep 2 (Step 055555): Train loss 1.093, Val loss 1.769\n",
      "Ep 2 (Step 055560): Train loss 0.758, Val loss 1.771\n",
      "Ep 2 (Step 055565): Train loss 0.974, Val loss 1.773\n",
      "Ep 2 (Step 055570): Train loss 1.230, Val loss 1.774\n",
      "Ep 2 (Step 055575): Train loss 0.823, Val loss 1.774\n",
      "Ep 2 (Step 055580): Train loss 1.022, Val loss 1.775\n",
      "Ep 2 (Step 055585): Train loss 1.005, Val loss 1.775\n",
      "Ep 2 (Step 055590): Train loss 1.027, Val loss 1.776\n",
      "Ep 2 (Step 055595): Train loss 1.143, Val loss 1.777\n",
      "Ep 2 (Step 055600): Train loss 1.226, Val loss 1.777\n",
      "Ep 2 (Step 055605): Train loss 1.028, Val loss 1.775\n",
      "Ep 2 (Step 055610): Train loss 1.425, Val loss 1.773\n",
      "Ep 2 (Step 055615): Train loss 0.906, Val loss 1.770\n",
      "Ep 2 (Step 055620): Train loss 1.091, Val loss 1.768\n",
      "Ep 2 (Step 055625): Train loss 1.168, Val loss 1.767\n",
      "Ep 2 (Step 055630): Train loss 1.347, Val loss 1.768\n",
      "Ep 2 (Step 055635): Train loss 1.165, Val loss 1.768\n",
      "Ep 2 (Step 055640): Train loss 1.258, Val loss 1.768\n",
      "Ep 2 (Step 055645): Train loss 1.041, Val loss 1.768\n",
      "Ep 2 (Step 055650): Train loss 1.360, Val loss 1.769\n",
      "Ep 2 (Step 055655): Train loss 1.000, Val loss 1.772\n",
      "Ep 2 (Step 055660): Train loss 0.998, Val loss 1.775\n",
      "Ep 2 (Step 055665): Train loss 1.088, Val loss 1.779\n",
      "Ep 2 (Step 055670): Train loss 1.028, Val loss 1.779\n",
      "Ep 2 (Step 055675): Train loss 1.074, Val loss 1.782\n",
      "Ep 2 (Step 055680): Train loss 1.134, Val loss 1.783\n",
      "Ep 2 (Step 055685): Train loss 0.744, Val loss 1.784\n",
      "Ep 2 (Step 055690): Train loss 1.165, Val loss 1.786\n",
      "Ep 2 (Step 055695): Train loss 1.347, Val loss 1.787\n",
      "Ep 2 (Step 055700): Train loss 0.968, Val loss 1.788\n",
      "Ep 2 (Step 055705): Train loss 1.073, Val loss 1.787\n",
      "Ep 2 (Step 055710): Train loss 0.782, Val loss 1.785\n",
      "Ep 2 (Step 055715): Train loss 1.087, Val loss 1.784\n",
      "Ep 2 (Step 055720): Train loss 0.783, Val loss 1.783\n",
      "Ep 2 (Step 055725): Train loss 0.992, Val loss 1.783\n",
      "Ep 2 (Step 055730): Train loss 0.754, Val loss 1.782\n",
      "Ep 2 (Step 055735): Train loss 0.909, Val loss 1.782\n",
      "Ep 2 (Step 055740): Train loss 1.036, Val loss 1.781\n",
      "Ep 2 (Step 055745): Train loss 1.225, Val loss 1.778\n",
      "Ep 2 (Step 055750): Train loss 0.987, Val loss 1.776\n",
      "Ep 2 (Step 055755): Train loss 1.196, Val loss 1.774\n",
      "Ep 2 (Step 055760): Train loss 1.228, Val loss 1.773\n",
      "Ep 2 (Step 055765): Train loss 0.920, Val loss 1.773\n",
      "Ep 2 (Step 055770): Train loss 1.347, Val loss 1.773\n",
      "Ep 2 (Step 055775): Train loss 1.059, Val loss 1.773\n",
      "Ep 2 (Step 055780): Train loss 0.915, Val loss 1.773\n",
      "Ep 2 (Step 055785): Train loss 0.827, Val loss 1.773\n",
      "Ep 2 (Step 055790): Train loss 1.094, Val loss 1.772\n",
      "Ep 2 (Step 055795): Train loss 1.077, Val loss 1.772\n",
      "Ep 2 (Step 055800): Train loss 0.813, Val loss 1.772\n",
      "Ep 2 (Step 055805): Train loss 0.991, Val loss 1.770\n",
      "Ep 2 (Step 055810): Train loss 0.905, Val loss 1.769\n",
      "Ep 2 (Step 055815): Train loss 1.084, Val loss 1.769\n",
      "Ep 2 (Step 055820): Train loss 0.997, Val loss 1.769\n",
      "Ep 2 (Step 055825): Train loss 0.923, Val loss 1.768\n",
      "Ep 2 (Step 055830): Train loss 1.178, Val loss 1.767\n",
      "Ep 2 (Step 055835): Train loss 1.210, Val loss 1.767\n",
      "Ep 2 (Step 055840): Train loss 1.118, Val loss 1.767\n",
      "Ep 2 (Step 055845): Train loss 1.017, Val loss 1.770\n",
      "Ep 2 (Step 055850): Train loss 0.998, Val loss 1.771\n",
      "Ep 2 (Step 055855): Train loss 1.157, Val loss 1.772\n",
      "Ep 2 (Step 055860): Train loss 1.030, Val loss 1.772\n",
      "Ep 2 (Step 055865): Train loss 1.379, Val loss 1.773\n",
      "Ep 2 (Step 055870): Train loss 1.036, Val loss 1.774\n",
      "Ep 2 (Step 055875): Train loss 1.044, Val loss 1.775\n",
      "Ep 2 (Step 055880): Train loss 1.072, Val loss 1.774\n",
      "Ep 2 (Step 055885): Train loss 1.002, Val loss 1.772\n",
      "Ep 2 (Step 055890): Train loss 0.859, Val loss 1.770\n",
      "Ep 2 (Step 055895): Train loss 0.726, Val loss 1.768\n",
      "Ep 2 (Step 055900): Train loss 1.152, Val loss 1.767\n",
      "Ep 2 (Step 055905): Train loss 1.342, Val loss 1.766\n",
      "Ep 2 (Step 055910): Train loss 1.083, Val loss 1.767\n",
      "Ep 2 (Step 055915): Train loss 0.920, Val loss 1.769\n",
      "Ep 2 (Step 055920): Train loss 0.886, Val loss 1.769\n",
      "Ep 2 (Step 055925): Train loss 1.049, Val loss 1.769\n",
      "Ep 2 (Step 055930): Train loss 1.023, Val loss 1.769\n",
      "Ep 2 (Step 055935): Train loss 1.019, Val loss 1.769\n",
      "Ep 2 (Step 055940): Train loss 1.254, Val loss 1.770\n",
      "Ep 2 (Step 055945): Train loss 1.068, Val loss 1.770\n",
      "Ep 2 (Step 055950): Train loss 1.170, Val loss 1.770\n",
      "Ep 2 (Step 055955): Train loss 1.132, Val loss 1.769\n",
      "Ep 2 (Step 055960): Train loss 0.888, Val loss 1.768\n",
      "Ep 2 (Step 055965): Train loss 1.015, Val loss 1.767\n",
      "Ep 2 (Step 055970): Train loss 1.398, Val loss 1.766\n",
      "Ep 2 (Step 055975): Train loss 1.166, Val loss 1.767\n",
      "Ep 2 (Step 055980): Train loss 1.035, Val loss 1.767\n",
      "Ep 2 (Step 055985): Train loss 0.828, Val loss 1.765\n",
      "Ep 2 (Step 055990): Train loss 0.965, Val loss 1.764\n",
      "Ep 2 (Step 055995): Train loss 1.308, Val loss 1.763\n",
      "Ep 2 (Step 056000): Train loss 1.158, Val loss 1.762\n",
      "Ep 2 (Step 056005): Train loss 1.071, Val loss 1.762\n",
      "Ep 2 (Step 056010): Train loss 0.743, Val loss 1.762\n",
      "Ep 2 (Step 056015): Train loss 1.221, Val loss 1.761\n",
      "Ep 2 (Step 056020): Train loss 0.953, Val loss 1.762\n",
      "Ep 2 (Step 056025): Train loss 1.268, Val loss 1.762\n",
      "Ep 2 (Step 056030): Train loss 1.151, Val loss 1.764\n",
      "Ep 2 (Step 056035): Train loss 0.990, Val loss 1.766\n",
      "Ep 2 (Step 056040): Train loss 1.124, Val loss 1.766\n",
      "Ep 2 (Step 056045): Train loss 1.011, Val loss 1.768\n",
      "Ep 2 (Step 056050): Train loss 1.022, Val loss 1.768\n",
      "Ep 2 (Step 056055): Train loss 0.878, Val loss 1.768\n",
      "Ep 2 (Step 056060): Train loss 1.125, Val loss 1.768\n",
      "Ep 2 (Step 056065): Train loss 0.937, Val loss 1.769\n",
      "Ep 2 (Step 056070): Train loss 0.978, Val loss 1.769\n",
      "Ep 2 (Step 056075): Train loss 1.131, Val loss 1.770\n",
      "Ep 2 (Step 056080): Train loss 1.264, Val loss 1.770\n",
      "Ep 2 (Step 056085): Train loss 0.827, Val loss 1.769\n",
      "Ep 2 (Step 056090): Train loss 1.021, Val loss 1.769\n",
      "Ep 2 (Step 056095): Train loss 0.993, Val loss 1.768\n",
      "Ep 2 (Step 056100): Train loss 1.056, Val loss 1.766\n",
      "Ep 2 (Step 056105): Train loss 1.149, Val loss 1.765\n",
      "Ep 2 (Step 056110): Train loss 1.219, Val loss 1.765\n",
      "Ep 2 (Step 056115): Train loss 1.425, Val loss 1.767\n",
      "Ep 2 (Step 056120): Train loss 1.186, Val loss 1.769\n",
      "Ep 2 (Step 056125): Train loss 0.881, Val loss 1.772\n",
      "Ep 2 (Step 056130): Train loss 0.931, Val loss 1.775\n",
      "Ep 2 (Step 056135): Train loss 1.109, Val loss 1.778\n",
      "Ep 2 (Step 056140): Train loss 1.071, Val loss 1.779\n",
      "Ep 2 (Step 056145): Train loss 0.902, Val loss 1.777\n",
      "Ep 2 (Step 056150): Train loss 0.958, Val loss 1.776\n",
      "Ep 2 (Step 056155): Train loss 1.113, Val loss 1.775\n",
      "Ep 2 (Step 056160): Train loss 1.236, Val loss 1.774\n",
      "Ep 2 (Step 056165): Train loss 1.065, Val loss 1.774\n",
      "Ep 2 (Step 056170): Train loss 1.046, Val loss 1.773\n",
      "Ep 2 (Step 056175): Train loss 1.072, Val loss 1.771\n",
      "Ep 2 (Step 056180): Train loss 1.069, Val loss 1.770\n",
      "Ep 2 (Step 056185): Train loss 0.830, Val loss 1.767\n",
      "Ep 2 (Step 056190): Train loss 0.779, Val loss 1.767\n",
      "Ep 2 (Step 056195): Train loss 1.242, Val loss 1.768\n",
      "Ep 2 (Step 056200): Train loss 1.283, Val loss 1.769\n",
      "Ep 2 (Step 056205): Train loss 1.137, Val loss 1.772\n",
      "Ep 2 (Step 056210): Train loss 1.130, Val loss 1.773\n",
      "Ep 2 (Step 056215): Train loss 1.164, Val loss 1.775\n",
      "Ep 2 (Step 056220): Train loss 1.126, Val loss 1.777\n",
      "Ep 2 (Step 056225): Train loss 0.864, Val loss 1.777\n",
      "Ep 2 (Step 056230): Train loss 1.393, Val loss 1.777\n",
      "Ep 2 (Step 056235): Train loss 0.950, Val loss 1.778\n",
      "Ep 2 (Step 056240): Train loss 0.948, Val loss 1.780\n",
      "Ep 2 (Step 056245): Train loss 0.939, Val loss 1.780\n",
      "Ep 2 (Step 056250): Train loss 1.177, Val loss 1.780\n",
      "Ep 2 (Step 056255): Train loss 1.131, Val loss 1.780\n",
      "Ep 2 (Step 056260): Train loss 1.218, Val loss 1.779\n",
      "Ep 2 (Step 056265): Train loss 1.472, Val loss 1.780\n",
      "Ep 2 (Step 056270): Train loss 1.326, Val loss 1.780\n",
      "Ep 2 (Step 056275): Train loss 0.987, Val loss 1.780\n",
      "Ep 2 (Step 056280): Train loss 1.429, Val loss 1.781\n",
      "Ep 2 (Step 056285): Train loss 1.139, Val loss 1.781\n",
      "Ep 2 (Step 056290): Train loss 0.813, Val loss 1.781\n",
      "Ep 2 (Step 056295): Train loss 1.008, Val loss 1.781\n",
      "Ep 2 (Step 056300): Train loss 1.300, Val loss 1.782\n",
      "Ep 2 (Step 056305): Train loss 1.129, Val loss 1.783\n",
      "Ep 2 (Step 056310): Train loss 1.185, Val loss 1.784\n",
      "Ep 2 (Step 056315): Train loss 0.851, Val loss 1.785\n",
      "Ep 2 (Step 056320): Train loss 1.028, Val loss 1.786\n",
      "Ep 2 (Step 056325): Train loss 1.171, Val loss 1.787\n",
      "Ep 2 (Step 056330): Train loss 1.193, Val loss 1.787\n",
      "Ep 2 (Step 056335): Train loss 1.054, Val loss 1.788\n",
      "Ep 2 (Step 056340): Train loss 1.054, Val loss 1.790\n",
      "Ep 2 (Step 056345): Train loss 1.269, Val loss 1.788\n",
      "Ep 2 (Step 056350): Train loss 1.021, Val loss 1.785\n",
      "Ep 2 (Step 056355): Train loss 0.881, Val loss 1.782\n",
      "Ep 2 (Step 056360): Train loss 0.979, Val loss 1.780\n",
      "Ep 2 (Step 056365): Train loss 0.854, Val loss 1.779\n",
      "Ep 2 (Step 056370): Train loss 0.955, Val loss 1.779\n",
      "Ep 2 (Step 056375): Train loss 1.198, Val loss 1.780\n",
      "Ep 2 (Step 056380): Train loss 1.007, Val loss 1.780\n",
      "Ep 2 (Step 056385): Train loss 0.956, Val loss 1.780\n",
      "Ep 2 (Step 056390): Train loss 1.034, Val loss 1.779\n",
      "Ep 2 (Step 056395): Train loss 1.080, Val loss 1.779\n",
      "Ep 2 (Step 056400): Train loss 0.819, Val loss 1.779\n",
      "Ep 2 (Step 056405): Train loss 1.148, Val loss 1.778\n",
      "Ep 2 (Step 056410): Train loss 1.036, Val loss 1.776\n",
      "Ep 2 (Step 056415): Train loss 1.199, Val loss 1.776\n",
      "Ep 2 (Step 056420): Train loss 1.231, Val loss 1.777\n",
      "Ep 2 (Step 056425): Train loss 0.816, Val loss 1.779\n",
      "Ep 2 (Step 056430): Train loss 0.924, Val loss 1.778\n",
      "Ep 2 (Step 056435): Train loss 0.807, Val loss 1.777\n",
      "Ep 2 (Step 056440): Train loss 1.208, Val loss 1.776\n",
      "Ep 2 (Step 056445): Train loss 1.318, Val loss 1.776\n",
      "Ep 2 (Step 056450): Train loss 1.072, Val loss 1.777\n",
      "Ep 2 (Step 056455): Train loss 0.936, Val loss 1.778\n",
      "Ep 2 (Step 056460): Train loss 1.132, Val loss 1.779\n",
      "Ep 2 (Step 056465): Train loss 1.237, Val loss 1.780\n",
      "Ep 2 (Step 056470): Train loss 0.933, Val loss 1.780\n",
      "Ep 2 (Step 056475): Train loss 1.115, Val loss 1.780\n",
      "Ep 2 (Step 056480): Train loss 1.003, Val loss 1.779\n",
      "Ep 2 (Step 056485): Train loss 1.156, Val loss 1.777\n",
      "Ep 2 (Step 056490): Train loss 1.033, Val loss 1.777\n",
      "Ep 2 (Step 056495): Train loss 1.147, Val loss 1.776\n",
      "Ep 2 (Step 056500): Train loss 1.100, Val loss 1.773\n",
      "Ep 2 (Step 056505): Train loss 1.039, Val loss 1.770\n",
      "Ep 2 (Step 056510): Train loss 1.130, Val loss 1.769\n",
      "Ep 2 (Step 056515): Train loss 0.973, Val loss 1.769\n",
      "Ep 2 (Step 056520): Train loss 0.971, Val loss 1.769\n",
      "Ep 2 (Step 056525): Train loss 0.885, Val loss 1.770\n",
      "Ep 2 (Step 056530): Train loss 0.808, Val loss 1.771\n",
      "Ep 2 (Step 056535): Train loss 1.076, Val loss 1.771\n",
      "Ep 2 (Step 056540): Train loss 0.840, Val loss 1.772\n",
      "Ep 2 (Step 056545): Train loss 1.225, Val loss 1.772\n",
      "Ep 2 (Step 056550): Train loss 0.941, Val loss 1.772\n",
      "Ep 2 (Step 056555): Train loss 0.942, Val loss 1.773\n",
      "Ep 2 (Step 056560): Train loss 0.954, Val loss 1.774\n",
      "Ep 2 (Step 056565): Train loss 1.083, Val loss 1.774\n",
      "Ep 2 (Step 056570): Train loss 0.970, Val loss 1.774\n",
      "Ep 2 (Step 056575): Train loss 0.997, Val loss 1.776\n",
      "Ep 2 (Step 056580): Train loss 1.070, Val loss 1.776\n",
      "Ep 2 (Step 056585): Train loss 0.797, Val loss 1.776\n",
      "Ep 2 (Step 056590): Train loss 1.137, Val loss 1.776\n",
      "Ep 2 (Step 056595): Train loss 0.969, Val loss 1.777\n",
      "Ep 2 (Step 056600): Train loss 1.042, Val loss 1.778\n",
      "Ep 2 (Step 056605): Train loss 0.834, Val loss 1.780\n",
      "Ep 2 (Step 056610): Train loss 1.079, Val loss 1.781\n",
      "Ep 2 (Step 056615): Train loss 1.236, Val loss 1.782\n",
      "Ep 2 (Step 056620): Train loss 1.337, Val loss 1.785\n",
      "Ep 2 (Step 056625): Train loss 0.873, Val loss 1.786\n",
      "Ep 2 (Step 056630): Train loss 1.145, Val loss 1.787\n",
      "Ep 2 (Step 056635): Train loss 1.226, Val loss 1.788\n",
      "Ep 2 (Step 056640): Train loss 1.005, Val loss 1.787\n",
      "Ep 2 (Step 056645): Train loss 1.179, Val loss 1.787\n",
      "Ep 2 (Step 056650): Train loss 1.234, Val loss 1.786\n",
      "Ep 2 (Step 056655): Train loss 1.167, Val loss 1.785\n",
      "Ep 2 (Step 056660): Train loss 1.384, Val loss 1.785\n",
      "Ep 2 (Step 056665): Train loss 1.013, Val loss 1.783\n",
      "Ep 2 (Step 056670): Train loss 1.008, Val loss 1.779\n",
      "Ep 2 (Step 056675): Train loss 0.976, Val loss 1.777\n",
      "Ep 2 (Step 056680): Train loss 1.004, Val loss 1.776\n",
      "Ep 2 (Step 056685): Train loss 0.953, Val loss 1.777\n",
      "Ep 2 (Step 056690): Train loss 1.090, Val loss 1.780\n",
      "Ep 2 (Step 056695): Train loss 1.304, Val loss 1.782\n",
      "Ep 2 (Step 056700): Train loss 0.761, Val loss 1.783\n",
      "Ep 2 (Step 056705): Train loss 0.983, Val loss 1.784\n",
      "Ep 2 (Step 056710): Train loss 1.066, Val loss 1.784\n",
      "Ep 2 (Step 056715): Train loss 1.052, Val loss 1.782\n",
      "Ep 2 (Step 056720): Train loss 0.988, Val loss 1.781\n",
      "Ep 2 (Step 056725): Train loss 0.984, Val loss 1.780\n",
      "Ep 2 (Step 056730): Train loss 0.878, Val loss 1.780\n",
      "Ep 2 (Step 056735): Train loss 1.190, Val loss 1.781\n",
      "Ep 2 (Step 056740): Train loss 1.121, Val loss 1.782\n",
      "Ep 2 (Step 056745): Train loss 1.194, Val loss 1.782\n",
      "Ep 2 (Step 056750): Train loss 1.170, Val loss 1.782\n",
      "Ep 2 (Step 056755): Train loss 1.627, Val loss 1.782\n",
      "Ep 2 (Step 056760): Train loss 1.304, Val loss 1.782\n",
      "Ep 2 (Step 056765): Train loss 1.079, Val loss 1.784\n",
      "Ep 2 (Step 056770): Train loss 1.051, Val loss 1.784\n",
      "Ep 2 (Step 056775): Train loss 0.932, Val loss 1.786\n",
      "Ep 2 (Step 056780): Train loss 1.126, Val loss 1.787\n",
      "Ep 2 (Step 056785): Train loss 1.100, Val loss 1.790\n",
      "Ep 2 (Step 056790): Train loss 1.211, Val loss 1.791\n",
      "Ep 2 (Step 056795): Train loss 1.199, Val loss 1.791\n",
      "Ep 2 (Step 056800): Train loss 1.228, Val loss 1.793\n",
      "Ep 2 (Step 056805): Train loss 1.094, Val loss 1.794\n",
      "Ep 2 (Step 056810): Train loss 0.881, Val loss 1.794\n",
      "Ep 2 (Step 056815): Train loss 1.101, Val loss 1.794\n",
      "Ep 2 (Step 056820): Train loss 1.275, Val loss 1.793\n",
      "Ep 2 (Step 056825): Train loss 1.059, Val loss 1.792\n",
      "Ep 2 (Step 056830): Train loss 0.956, Val loss 1.791\n",
      "Ep 2 (Step 056835): Train loss 1.067, Val loss 1.790\n",
      "Ep 2 (Step 056840): Train loss 1.124, Val loss 1.790\n",
      "Ep 2 (Step 056845): Train loss 1.052, Val loss 1.791\n",
      "Ep 2 (Step 056850): Train loss 1.117, Val loss 1.790\n",
      "Ep 2 (Step 056855): Train loss 1.012, Val loss 1.792\n",
      "Ep 2 (Step 056860): Train loss 1.277, Val loss 1.794\n",
      "Ep 2 (Step 056865): Train loss 0.873, Val loss 1.795\n",
      "Ep 2 (Step 056870): Train loss 1.047, Val loss 1.796\n",
      "Ep 2 (Step 056875): Train loss 1.186, Val loss 1.797\n",
      "Ep 2 (Step 056880): Train loss 1.158, Val loss 1.796\n",
      "Ep 2 (Step 056885): Train loss 1.178, Val loss 1.796\n",
      "Ep 2 (Step 056890): Train loss 1.276, Val loss 1.795\n",
      "Ep 2 (Step 056895): Train loss 1.214, Val loss 1.793\n",
      "Ep 2 (Step 056900): Train loss 1.080, Val loss 1.790\n",
      "Ep 2 (Step 056905): Train loss 1.106, Val loss 1.787\n",
      "Ep 2 (Step 056910): Train loss 1.431, Val loss 1.783\n",
      "Ep 2 (Step 056915): Train loss 1.125, Val loss 1.780\n",
      "Ep 2 (Step 056920): Train loss 1.020, Val loss 1.777\n",
      "Ep 2 (Step 056925): Train loss 1.055, Val loss 1.775\n",
      "Ep 2 (Step 056930): Train loss 1.365, Val loss 1.772\n",
      "Ep 2 (Step 056935): Train loss 1.298, Val loss 1.770\n",
      "Ep 2 (Step 056940): Train loss 0.882, Val loss 1.769\n",
      "Ep 2 (Step 056945): Train loss 1.290, Val loss 1.770\n",
      "Ep 2 (Step 056950): Train loss 1.077, Val loss 1.772\n",
      "Ep 2 (Step 056955): Train loss 0.889, Val loss 1.774\n",
      "Ep 2 (Step 056960): Train loss 0.860, Val loss 1.776\n",
      "Ep 2 (Step 056965): Train loss 1.099, Val loss 1.777\n",
      "Ep 2 (Step 056970): Train loss 0.972, Val loss 1.777\n",
      "Ep 2 (Step 056975): Train loss 0.921, Val loss 1.777\n",
      "Ep 2 (Step 056980): Train loss 1.319, Val loss 1.775\n",
      "Ep 2 (Step 056985): Train loss 1.244, Val loss 1.775\n",
      "Ep 2 (Step 056990): Train loss 1.144, Val loss 1.775\n",
      "Ep 2 (Step 056995): Train loss 1.095, Val loss 1.775\n",
      "Ep 2 (Step 057000): Train loss 1.381, Val loss 1.773\n",
      "Ep 2 (Step 057005): Train loss 1.125, Val loss 1.772\n",
      "Ep 2 (Step 057010): Train loss 1.227, Val loss 1.771\n",
      "Ep 2 (Step 057015): Train loss 1.421, Val loss 1.769\n",
      "Ep 2 (Step 057020): Train loss 0.938, Val loss 1.768\n",
      "Ep 2 (Step 057025): Train loss 1.186, Val loss 1.768\n",
      "Ep 2 (Step 057030): Train loss 0.970, Val loss 1.767\n",
      "Ep 2 (Step 057035): Train loss 1.112, Val loss 1.768\n",
      "Ep 2 (Step 057040): Train loss 1.053, Val loss 1.770\n",
      "Ep 2 (Step 057045): Train loss 0.962, Val loss 1.773\n",
      "Ep 2 (Step 057050): Train loss 1.198, Val loss 1.774\n",
      "Ep 2 (Step 057055): Train loss 1.009, Val loss 1.776\n",
      "Ep 2 (Step 057060): Train loss 1.378, Val loss 1.778\n",
      "Ep 2 (Step 057065): Train loss 1.190, Val loss 1.778\n",
      "Ep 2 (Step 057070): Train loss 1.071, Val loss 1.776\n",
      "Ep 2 (Step 057075): Train loss 1.237, Val loss 1.776\n",
      "Ep 2 (Step 057080): Train loss 1.096, Val loss 1.774\n",
      "Ep 2 (Step 057085): Train loss 1.161, Val loss 1.773\n",
      "Ep 2 (Step 057090): Train loss 1.237, Val loss 1.773\n",
      "Ep 2 (Step 057095): Train loss 1.188, Val loss 1.773\n",
      "Ep 2 (Step 057100): Train loss 1.453, Val loss 1.773\n",
      "Ep 2 (Step 057105): Train loss 1.287, Val loss 1.774\n",
      "Ep 2 (Step 057110): Train loss 1.104, Val loss 1.774\n",
      "Ep 2 (Step 057115): Train loss 1.084, Val loss 1.774\n",
      "Ep 2 (Step 057120): Train loss 1.146, Val loss 1.774\n",
      "Ep 2 (Step 057125): Train loss 1.255, Val loss 1.774\n",
      "Ep 2 (Step 057130): Train loss 0.811, Val loss 1.773\n",
      "Ep 2 (Step 057135): Train loss 0.921, Val loss 1.771\n",
      "Ep 2 (Step 057140): Train loss 0.948, Val loss 1.771\n",
      "Ep 2 (Step 057145): Train loss 1.045, Val loss 1.772\n",
      "Ep 2 (Step 057150): Train loss 1.547, Val loss 1.772\n",
      "Ep 2 (Step 057155): Train loss 1.186, Val loss 1.772\n",
      "Ep 2 (Step 057160): Train loss 1.130, Val loss 1.771\n",
      "Ep 2 (Step 057165): Train loss 0.913, Val loss 1.771\n",
      "Ep 2 (Step 057170): Train loss 0.875, Val loss 1.770\n",
      "Ep 2 (Step 057175): Train loss 1.141, Val loss 1.768\n",
      "Ep 2 (Step 057180): Train loss 1.140, Val loss 1.768\n",
      "Ep 2 (Step 057185): Train loss 0.832, Val loss 1.768\n",
      "Ep 2 (Step 057190): Train loss 0.857, Val loss 1.767\n",
      "Ep 2 (Step 057195): Train loss 0.977, Val loss 1.767\n",
      "Ep 2 (Step 057200): Train loss 1.120, Val loss 1.767\n",
      "Ep 2 (Step 057205): Train loss 1.034, Val loss 1.768\n",
      "Ep 2 (Step 057210): Train loss 0.614, Val loss 1.770\n",
      "Ep 2 (Step 057215): Train loss 1.110, Val loss 1.772\n",
      "Ep 2 (Step 057220): Train loss 1.038, Val loss 1.772\n",
      "Ep 2 (Step 057225): Train loss 1.005, Val loss 1.773\n",
      "Ep 2 (Step 057230): Train loss 1.134, Val loss 1.773\n",
      "Ep 2 (Step 057235): Train loss 1.112, Val loss 1.773\n",
      "Ep 2 (Step 057240): Train loss 0.982, Val loss 1.772\n",
      "Ep 2 (Step 057245): Train loss 1.214, Val loss 1.772\n",
      "Ep 2 (Step 057250): Train loss 1.070, Val loss 1.772\n",
      "Ep 2 (Step 057255): Train loss 1.070, Val loss 1.772\n",
      "Ep 2 (Step 057260): Train loss 0.765, Val loss 1.772\n",
      "Ep 2 (Step 057265): Train loss 0.958, Val loss 1.772\n",
      "Ep 2 (Step 057270): Train loss 1.147, Val loss 1.773\n",
      "Ep 2 (Step 057275): Train loss 1.051, Val loss 1.773\n",
      "Ep 2 (Step 057280): Train loss 1.114, Val loss 1.774\n",
      "Ep 2 (Step 057285): Train loss 0.842, Val loss 1.773\n",
      "Ep 2 (Step 057290): Train loss 1.249, Val loss 1.774\n",
      "Ep 2 (Step 057295): Train loss 1.183, Val loss 1.774\n",
      "Ep 2 (Step 057300): Train loss 1.155, Val loss 1.772\n",
      "Ep 2 (Step 057305): Train loss 1.034, Val loss 1.771\n",
      "Ep 2 (Step 057310): Train loss 0.887, Val loss 1.770\n",
      "Ep 2 (Step 057315): Train loss 0.645, Val loss 1.770\n",
      "Ep 2 (Step 057320): Train loss 0.995, Val loss 1.770\n",
      "Ep 2 (Step 057325): Train loss 0.957, Val loss 1.771\n",
      "Ep 2 (Step 057330): Train loss 0.863, Val loss 1.771\n",
      "Ep 2 (Step 057335): Train loss 0.799, Val loss 1.771\n",
      "Ep 2 (Step 057340): Train loss 0.965, Val loss 1.771\n",
      "Ep 2 (Step 057345): Train loss 1.000, Val loss 1.769\n",
      "Ep 2 (Step 057350): Train loss 1.417, Val loss 1.768\n",
      "Ep 2 (Step 057355): Train loss 0.975, Val loss 1.768\n",
      "Ep 2 (Step 057360): Train loss 1.172, Val loss 1.768\n",
      "Ep 2 (Step 057365): Train loss 0.969, Val loss 1.769\n",
      "Ep 2 (Step 057370): Train loss 0.899, Val loss 1.770\n",
      "Ep 2 (Step 057375): Train loss 1.352, Val loss 1.771\n",
      "Ep 2 (Step 057380): Train loss 0.963, Val loss 1.770\n",
      "Ep 2 (Step 057385): Train loss 1.091, Val loss 1.770\n",
      "Ep 2 (Step 057390): Train loss 1.016, Val loss 1.770\n",
      "Ep 2 (Step 057395): Train loss 0.955, Val loss 1.771\n",
      "Ep 2 (Step 057400): Train loss 1.251, Val loss 1.771\n",
      "Ep 2 (Step 057405): Train loss 1.026, Val loss 1.772\n",
      "Ep 2 (Step 057410): Train loss 1.098, Val loss 1.774\n",
      "Ep 2 (Step 057415): Train loss 0.803, Val loss 1.775\n",
      "Ep 2 (Step 057420): Train loss 0.696, Val loss 1.777\n",
      "Ep 2 (Step 057425): Train loss 1.339, Val loss 1.777\n",
      "Ep 2 (Step 057430): Train loss 0.903, Val loss 1.775\n",
      "Ep 2 (Step 057435): Train loss 1.060, Val loss 1.774\n",
      "Ep 2 (Step 057440): Train loss 0.968, Val loss 1.773\n",
      "Ep 2 (Step 057445): Train loss 1.204, Val loss 1.772\n",
      "Ep 2 (Step 057450): Train loss 1.273, Val loss 1.769\n",
      "Ep 2 (Step 057455): Train loss 1.193, Val loss 1.768\n",
      "Ep 2 (Step 057460): Train loss 1.129, Val loss 1.769\n",
      "Ep 2 (Step 057465): Train loss 0.950, Val loss 1.770\n",
      "Ep 2 (Step 057470): Train loss 1.006, Val loss 1.770\n",
      "Ep 2 (Step 057475): Train loss 1.199, Val loss 1.771\n",
      "Ep 2 (Step 057480): Train loss 1.143, Val loss 1.773\n",
      "Ep 2 (Step 057485): Train loss 1.006, Val loss 1.774\n",
      "Ep 2 (Step 057490): Train loss 1.148, Val loss 1.775\n",
      "Ep 2 (Step 057495): Train loss 1.098, Val loss 1.775\n",
      "Ep 2 (Step 057500): Train loss 1.076, Val loss 1.777\n",
      "Ep 2 (Step 057505): Train loss 1.087, Val loss 1.779\n",
      "Ep 2 (Step 057510): Train loss 1.175, Val loss 1.779\n",
      "Ep 2 (Step 057515): Train loss 1.051, Val loss 1.779\n",
      "Ep 2 (Step 057520): Train loss 1.169, Val loss 1.779\n",
      "Ep 2 (Step 057525): Train loss 0.882, Val loss 1.779\n",
      "Ep 2 (Step 057530): Train loss 1.379, Val loss 1.779\n",
      "Ep 2 (Step 057535): Train loss 1.444, Val loss 1.776\n",
      "Ep 2 (Step 057540): Train loss 0.995, Val loss 1.773\n",
      "Ep 2 (Step 057545): Train loss 1.170, Val loss 1.771\n",
      "Ep 2 (Step 057550): Train loss 1.098, Val loss 1.770\n",
      "Ep 2 (Step 057555): Train loss 1.119, Val loss 1.768\n",
      "Ep 2 (Step 057560): Train loss 0.836, Val loss 1.767\n",
      "Ep 2 (Step 057565): Train loss 1.094, Val loss 1.766\n",
      "Ep 2 (Step 057570): Train loss 1.219, Val loss 1.767\n",
      "Ep 2 (Step 057575): Train loss 1.215, Val loss 1.767\n",
      "Ep 2 (Step 057580): Train loss 0.896, Val loss 1.768\n",
      "Ep 2 (Step 057585): Train loss 0.792, Val loss 1.770\n",
      "Ep 2 (Step 057590): Train loss 0.940, Val loss 1.774\n",
      "Ep 2 (Step 057595): Train loss 1.020, Val loss 1.777\n",
      "Ep 2 (Step 057600): Train loss 1.155, Val loss 1.779\n",
      "Ep 2 (Step 057605): Train loss 0.977, Val loss 1.781\n",
      "Ep 2 (Step 057610): Train loss 0.966, Val loss 1.782\n",
      "Ep 2 (Step 057615): Train loss 1.261, Val loss 1.782\n",
      "Ep 2 (Step 057620): Train loss 1.081, Val loss 1.781\n",
      "Ep 2 (Step 057625): Train loss 1.229, Val loss 1.781\n",
      "Ep 2 (Step 057630): Train loss 1.261, Val loss 1.781\n",
      "Ep 2 (Step 057635): Train loss 1.208, Val loss 1.780\n",
      "Ep 2 (Step 057640): Train loss 1.075, Val loss 1.779\n",
      "Ep 2 (Step 057645): Train loss 1.041, Val loss 1.777\n",
      "Ep 2 (Step 057650): Train loss 1.061, Val loss 1.775\n",
      "Ep 2 (Step 057655): Train loss 1.149, Val loss 1.774\n",
      "Ep 2 (Step 057660): Train loss 0.997, Val loss 1.772\n",
      "Ep 2 (Step 057665): Train loss 0.953, Val loss 1.771\n",
      "Ep 2 (Step 057670): Train loss 1.004, Val loss 1.769\n",
      "Ep 2 (Step 057675): Train loss 0.953, Val loss 1.769\n",
      "Ep 2 (Step 057680): Train loss 1.339, Val loss 1.768\n",
      "Ep 2 (Step 057685): Train loss 1.186, Val loss 1.767\n",
      "Ep 2 (Step 057690): Train loss 0.923, Val loss 1.764\n",
      "Ep 2 (Step 057695): Train loss 1.191, Val loss 1.763\n",
      "Ep 2 (Step 057700): Train loss 1.294, Val loss 1.762\n",
      "Ep 2 (Step 057705): Train loss 1.133, Val loss 1.762\n",
      "Ep 2 (Step 057710): Train loss 1.203, Val loss 1.761\n",
      "Ep 2 (Step 057715): Train loss 0.653, Val loss 1.761\n",
      "Ep 2 (Step 057720): Train loss 0.733, Val loss 1.762\n",
      "Ep 2 (Step 057725): Train loss 1.326, Val loss 1.763\n",
      "Ep 2 (Step 057730): Train loss 1.047, Val loss 1.766\n",
      "Ep 2 (Step 057735): Train loss 0.955, Val loss 1.769\n",
      "Ep 2 (Step 057740): Train loss 1.268, Val loss 1.771\n",
      "Ep 2 (Step 057745): Train loss 1.004, Val loss 1.771\n",
      "Ep 2 (Step 057750): Train loss 1.151, Val loss 1.771\n",
      "Ep 2 (Step 057755): Train loss 1.022, Val loss 1.770\n",
      "Ep 2 (Step 057760): Train loss 1.223, Val loss 1.769\n",
      "Ep 2 (Step 057765): Train loss 0.900, Val loss 1.771\n",
      "Ep 2 (Step 057770): Train loss 0.894, Val loss 1.773\n",
      "Ep 2 (Step 057775): Train loss 1.045, Val loss 1.775\n",
      "Ep 2 (Step 057780): Train loss 1.178, Val loss 1.778\n",
      "Ep 2 (Step 057785): Train loss 0.998, Val loss 1.780\n",
      "Ep 2 (Step 057790): Train loss 1.153, Val loss 1.780\n",
      "Ep 2 (Step 057795): Train loss 0.968, Val loss 1.780\n",
      "Ep 2 (Step 057800): Train loss 0.925, Val loss 1.780\n",
      "Ep 2 (Step 057805): Train loss 1.161, Val loss 1.780\n",
      "Ep 2 (Step 057810): Train loss 1.027, Val loss 1.780\n",
      "Ep 2 (Step 057815): Train loss 0.986, Val loss 1.780\n",
      "Ep 2 (Step 057820): Train loss 1.132, Val loss 1.781\n",
      "Ep 2 (Step 057825): Train loss 1.283, Val loss 1.782\n",
      "Ep 2 (Step 057830): Train loss 1.040, Val loss 1.784\n",
      "Ep 2 (Step 057835): Train loss 0.933, Val loss 1.785\n",
      "Ep 2 (Step 057840): Train loss 0.960, Val loss 1.786\n",
      "Ep 2 (Step 057845): Train loss 0.850, Val loss 1.787\n",
      "Ep 2 (Step 057850): Train loss 0.863, Val loss 1.786\n",
      "Ep 2 (Step 057855): Train loss 1.323, Val loss 1.787\n",
      "Ep 2 (Step 057860): Train loss 0.863, Val loss 1.787\n",
      "Ep 2 (Step 057865): Train loss 1.125, Val loss 1.785\n",
      "Ep 2 (Step 057870): Train loss 1.087, Val loss 1.783\n",
      "Ep 2 (Step 057875): Train loss 0.984, Val loss 1.781\n",
      "Ep 2 (Step 057880): Train loss 0.839, Val loss 1.779\n",
      "Ep 2 (Step 057885): Train loss 1.215, Val loss 1.776\n",
      "Ep 2 (Step 057890): Train loss 0.918, Val loss 1.775\n",
      "Ep 2 (Step 057895): Train loss 0.965, Val loss 1.775\n",
      "Ep 2 (Step 057900): Train loss 1.064, Val loss 1.776\n",
      "Ep 2 (Step 057905): Train loss 1.120, Val loss 1.777\n",
      "Ep 2 (Step 057910): Train loss 1.205, Val loss 1.777\n",
      "Ep 2 (Step 057915): Train loss 1.063, Val loss 1.777\n",
      "Ep 2 (Step 057920): Train loss 1.129, Val loss 1.778\n",
      "Ep 2 (Step 057925): Train loss 1.076, Val loss 1.779\n",
      "Ep 2 (Step 057930): Train loss 0.925, Val loss 1.779\n",
      "Ep 2 (Step 057935): Train loss 1.111, Val loss 1.777\n",
      "Ep 2 (Step 057940): Train loss 1.144, Val loss 1.776\n",
      "Ep 2 (Step 057945): Train loss 0.961, Val loss 1.776\n",
      "Ep 2 (Step 057950): Train loss 1.153, Val loss 1.776\n",
      "Ep 2 (Step 057955): Train loss 0.936, Val loss 1.776\n",
      "Ep 2 (Step 057960): Train loss 0.974, Val loss 1.776\n",
      "Ep 2 (Step 057965): Train loss 1.040, Val loss 1.777\n",
      "Ep 2 (Step 057970): Train loss 1.053, Val loss 1.779\n",
      "Ep 2 (Step 057975): Train loss 1.352, Val loss 1.780\n",
      "Ep 2 (Step 057980): Train loss 1.265, Val loss 1.780\n",
      "Ep 2 (Step 057985): Train loss 0.922, Val loss 1.780\n",
      "Ep 2 (Step 057990): Train loss 0.977, Val loss 1.781\n",
      "Ep 2 (Step 057995): Train loss 1.074, Val loss 1.782\n",
      "Ep 2 (Step 058000): Train loss 1.056, Val loss 1.782\n",
      "Ep 2 (Step 058005): Train loss 0.881, Val loss 1.783\n",
      "Ep 2 (Step 058010): Train loss 0.943, Val loss 1.783\n",
      "Ep 2 (Step 058015): Train loss 1.100, Val loss 1.783\n",
      "Ep 2 (Step 058020): Train loss 0.969, Val loss 1.783\n",
      "Ep 2 (Step 058025): Train loss 1.061, Val loss 1.783\n",
      "Ep 2 (Step 058030): Train loss 1.034, Val loss 1.782\n",
      "Ep 2 (Step 058035): Train loss 0.993, Val loss 1.782\n",
      "Ep 2 (Step 058040): Train loss 1.167, Val loss 1.782\n",
      "Ep 2 (Step 058045): Train loss 1.371, Val loss 1.782\n",
      "Ep 2 (Step 058050): Train loss 1.133, Val loss 1.782\n",
      "Ep 2 (Step 058055): Train loss 0.739, Val loss 1.782\n",
      "Ep 2 (Step 058060): Train loss 0.946, Val loss 1.782\n",
      "Ep 2 (Step 058065): Train loss 0.993, Val loss 1.781\n",
      "Ep 2 (Step 058070): Train loss 1.074, Val loss 1.781\n",
      "Ep 2 (Step 058075): Train loss 1.192, Val loss 1.780\n",
      "Ep 2 (Step 058080): Train loss 1.106, Val loss 1.780\n",
      "Ep 2 (Step 058085): Train loss 1.221, Val loss 1.781\n",
      "Ep 2 (Step 058090): Train loss 1.194, Val loss 1.782\n",
      "Ep 2 (Step 058095): Train loss 1.100, Val loss 1.782\n",
      "Ep 2 (Step 058100): Train loss 1.087, Val loss 1.781\n",
      "Ep 2 (Step 058105): Train loss 1.053, Val loss 1.780\n",
      "Ep 2 (Step 058110): Train loss 1.255, Val loss 1.779\n",
      "Ep 2 (Step 058115): Train loss 1.171, Val loss 1.778\n",
      "Ep 2 (Step 058120): Train loss 0.951, Val loss 1.775\n",
      "Ep 2 (Step 058125): Train loss 1.079, Val loss 1.774\n",
      "Ep 2 (Step 058130): Train loss 1.279, Val loss 1.772\n",
      "Ep 2 (Step 058135): Train loss 1.308, Val loss 1.771\n",
      "Ep 2 (Step 058140): Train loss 0.889, Val loss 1.770\n",
      "Ep 2 (Step 058145): Train loss 1.269, Val loss 1.770\n",
      "Ep 2 (Step 058150): Train loss 0.891, Val loss 1.771\n",
      "Ep 2 (Step 058155): Train loss 1.280, Val loss 1.770\n",
      "Ep 2 (Step 058160): Train loss 1.099, Val loss 1.770\n",
      "Ep 2 (Step 058165): Train loss 1.022, Val loss 1.770\n",
      "Ep 2 (Step 058170): Train loss 1.189, Val loss 1.769\n",
      "Ep 2 (Step 058175): Train loss 1.060, Val loss 1.767\n",
      "Ep 2 (Step 058180): Train loss 1.212, Val loss 1.766\n",
      "Ep 2 (Step 058185): Train loss 1.183, Val loss 1.767\n",
      "Ep 2 (Step 058190): Train loss 1.254, Val loss 1.768\n",
      "Ep 2 (Step 058195): Train loss 1.106, Val loss 1.768\n",
      "Ep 2 (Step 058200): Train loss 1.173, Val loss 1.768\n",
      "Ep 2 (Step 058205): Train loss 1.044, Val loss 1.769\n",
      "Ep 2 (Step 058210): Train loss 1.048, Val loss 1.772\n",
      "Ep 2 (Step 058215): Train loss 1.599, Val loss 1.774\n",
      "Ep 2 (Step 058220): Train loss 1.181, Val loss 1.775\n",
      "Ep 2 (Step 058225): Train loss 1.235, Val loss 1.776\n",
      "Ep 2 (Step 058230): Train loss 1.178, Val loss 1.777\n",
      "Ep 2 (Step 058235): Train loss 0.940, Val loss 1.777\n",
      "Ep 2 (Step 058240): Train loss 1.113, Val loss 1.777\n",
      "Ep 2 (Step 058245): Train loss 1.386, Val loss 1.777\n",
      "Ep 2 (Step 058250): Train loss 0.844, Val loss 1.776\n",
      "Ep 2 (Step 058255): Train loss 0.955, Val loss 1.776\n",
      "Ep 2 (Step 058260): Train loss 1.150, Val loss 1.777\n",
      "Ep 2 (Step 058265): Train loss 0.944, Val loss 1.778\n",
      "Ep 2 (Step 058270): Train loss 1.218, Val loss 1.780\n",
      "Ep 2 (Step 058275): Train loss 1.300, Val loss 1.781\n",
      "Ep 2 (Step 058280): Train loss 0.965, Val loss 1.781\n",
      "Ep 2 (Step 058285): Train loss 1.057, Val loss 1.781\n",
      "Ep 2 (Step 058290): Train loss 1.005, Val loss 1.782\n",
      "Ep 2 (Step 058295): Train loss 1.076, Val loss 1.782\n",
      "Ep 2 (Step 058300): Train loss 1.416, Val loss 1.782\n",
      "Ep 2 (Step 058305): Train loss 1.284, Val loss 1.780\n",
      "Ep 2 (Step 058310): Train loss 1.119, Val loss 1.779\n",
      "Ep 2 (Step 058315): Train loss 0.939, Val loss 1.778\n",
      "Ep 2 (Step 058320): Train loss 1.088, Val loss 1.777\n",
      "Ep 2 (Step 058325): Train loss 1.206, Val loss 1.776\n",
      "Ep 2 (Step 058330): Train loss 1.189, Val loss 1.775\n",
      "Ep 2 (Step 058335): Train loss 1.239, Val loss 1.775\n",
      "Ep 2 (Step 058340): Train loss 1.007, Val loss 1.776\n",
      "Ep 2 (Step 058345): Train loss 1.041, Val loss 1.778\n",
      "Ep 2 (Step 058350): Train loss 0.786, Val loss 1.780\n",
      "Ep 2 (Step 058355): Train loss 1.038, Val loss 1.781\n",
      "Ep 2 (Step 058360): Train loss 0.945, Val loss 1.781\n",
      "Ep 2 (Step 058365): Train loss 1.078, Val loss 1.782\n",
      "Ep 2 (Step 058370): Train loss 1.071, Val loss 1.782\n",
      "Ep 2 (Step 058375): Train loss 1.385, Val loss 1.783\n",
      "Ep 2 (Step 058380): Train loss 0.980, Val loss 1.782\n",
      "Ep 2 (Step 058385): Train loss 1.039, Val loss 1.782\n",
      "Ep 2 (Step 058390): Train loss 1.108, Val loss 1.782\n",
      "Ep 2 (Step 058395): Train loss 0.947, Val loss 1.783\n",
      "Ep 2 (Step 058400): Train loss 0.872, Val loss 1.784\n",
      "Ep 2 (Step 058405): Train loss 0.990, Val loss 1.784\n",
      "Ep 2 (Step 058410): Train loss 1.210, Val loss 1.785\n",
      "Ep 2 (Step 058415): Train loss 0.940, Val loss 1.785\n",
      "Ep 2 (Step 058420): Train loss 0.960, Val loss 1.785\n",
      "Ep 2 (Step 058425): Train loss 1.025, Val loss 1.786\n",
      "Ep 2 (Step 058430): Train loss 1.032, Val loss 1.787\n",
      "Ep 2 (Step 058435): Train loss 1.216, Val loss 1.788\n",
      "Ep 2 (Step 058440): Train loss 1.083, Val loss 1.790\n",
      "Ep 2 (Step 058445): Train loss 1.067, Val loss 1.790\n",
      "Ep 2 (Step 058450): Train loss 0.969, Val loss 1.791\n",
      "Ep 2 (Step 058455): Train loss 1.059, Val loss 1.793\n",
      "Ep 2 (Step 058460): Train loss 1.193, Val loss 1.795\n",
      "Ep 2 (Step 058465): Train loss 1.130, Val loss 1.796\n",
      "Ep 2 (Step 058470): Train loss 0.879, Val loss 1.795\n",
      "Ep 2 (Step 058475): Train loss 0.962, Val loss 1.793\n",
      "Ep 2 (Step 058480): Train loss 0.988, Val loss 1.792\n",
      "Ep 2 (Step 058485): Train loss 0.946, Val loss 1.792\n",
      "Ep 2 (Step 058490): Train loss 1.007, Val loss 1.791\n",
      "Ep 2 (Step 058495): Train loss 1.391, Val loss 1.792\n",
      "Ep 2 (Step 058500): Train loss 1.249, Val loss 1.792\n",
      "Ep 2 (Step 058505): Train loss 1.149, Val loss 1.792\n",
      "Ep 2 (Step 058510): Train loss 1.043, Val loss 1.793\n",
      "Ep 2 (Step 058515): Train loss 0.972, Val loss 1.793\n",
      "Ep 2 (Step 058520): Train loss 1.034, Val loss 1.793\n",
      "Ep 2 (Step 058525): Train loss 0.817, Val loss 1.793\n",
      "Ep 2 (Step 058530): Train loss 0.972, Val loss 1.794\n",
      "Ep 2 (Step 058535): Train loss 0.915, Val loss 1.794\n",
      "Ep 2 (Step 058540): Train loss 0.928, Val loss 1.793\n",
      "Ep 2 (Step 058545): Train loss 1.276, Val loss 1.794\n",
      "Ep 2 (Step 058550): Train loss 1.450, Val loss 1.794\n",
      "Ep 2 (Step 058555): Train loss 1.253, Val loss 1.794\n",
      "Ep 2 (Step 058560): Train loss 1.010, Val loss 1.794\n",
      "Ep 2 (Step 058565): Train loss 0.968, Val loss 1.794\n",
      "Ep 2 (Step 058570): Train loss 0.928, Val loss 1.795\n",
      "Ep 2 (Step 058575): Train loss 0.903, Val loss 1.795\n",
      "Ep 2 (Step 058580): Train loss 1.065, Val loss 1.795\n",
      "Ep 2 (Step 058585): Train loss 1.229, Val loss 1.796\n",
      "Ep 2 (Step 058590): Train loss 1.028, Val loss 1.796\n",
      "Ep 2 (Step 058595): Train loss 1.273, Val loss 1.793\n",
      "Ep 2 (Step 058600): Train loss 0.744, Val loss 1.792\n",
      "Ep 2 (Step 058605): Train loss 0.926, Val loss 1.791\n",
      "Ep 2 (Step 058610): Train loss 1.034, Val loss 1.789\n",
      "Ep 2 (Step 058615): Train loss 1.018, Val loss 1.788\n",
      "Ep 2 (Step 058620): Train loss 1.203, Val loss 1.787\n",
      "Ep 2 (Step 058625): Train loss 1.119, Val loss 1.786\n",
      "Ep 2 (Step 058630): Train loss 1.021, Val loss 1.787\n",
      "Ep 2 (Step 058635): Train loss 1.062, Val loss 1.787\n",
      "Ep 2 (Step 058640): Train loss 0.971, Val loss 1.786\n",
      "Ep 2 (Step 058645): Train loss 1.015, Val loss 1.785\n",
      "Ep 2 (Step 058650): Train loss 0.812, Val loss 1.784\n",
      "Ep 2 (Step 058655): Train loss 1.088, Val loss 1.784\n",
      "Ep 2 (Step 058660): Train loss 0.988, Val loss 1.786\n",
      "Ep 2 (Step 058665): Train loss 1.029, Val loss 1.788\n",
      "Ep 2 (Step 058670): Train loss 1.044, Val loss 1.791\n",
      "Ep 2 (Step 058675): Train loss 1.225, Val loss 1.793\n",
      "Ep 2 (Step 058680): Train loss 1.313, Val loss 1.792\n",
      "Ep 2 (Step 058685): Train loss 1.337, Val loss 1.790\n",
      "Ep 2 (Step 058690): Train loss 0.970, Val loss 1.788\n",
      "Ep 2 (Step 058695): Train loss 0.918, Val loss 1.786\n",
      "Ep 2 (Step 058700): Train loss 0.849, Val loss 1.785\n",
      "Ep 2 (Step 058705): Train loss 0.921, Val loss 1.784\n",
      "Ep 2 (Step 058710): Train loss 1.257, Val loss 1.784\n",
      "Ep 2 (Step 058715): Train loss 0.909, Val loss 1.785\n",
      "Ep 2 (Step 058720): Train loss 0.875, Val loss 1.787\n",
      "Ep 2 (Step 058725): Train loss 1.192, Val loss 1.788\n",
      "Ep 2 (Step 058730): Train loss 0.940, Val loss 1.789\n",
      "Ep 2 (Step 058735): Train loss 1.321, Val loss 1.789\n",
      "Ep 2 (Step 058740): Train loss 0.926, Val loss 1.789\n",
      "Ep 2 (Step 058745): Train loss 1.290, Val loss 1.789\n",
      "Ep 2 (Step 058750): Train loss 1.005, Val loss 1.787\n",
      "Ep 2 (Step 058755): Train loss 1.109, Val loss 1.785\n",
      "Ep 2 (Step 058760): Train loss 0.948, Val loss 1.783\n",
      "Ep 2 (Step 058765): Train loss 1.142, Val loss 1.780\n",
      "Ep 2 (Step 058770): Train loss 0.760, Val loss 1.778\n",
      "Ep 2 (Step 058775): Train loss 1.071, Val loss 1.778\n",
      "Ep 2 (Step 058780): Train loss 1.062, Val loss 1.779\n",
      "Ep 2 (Step 058785): Train loss 1.155, Val loss 1.779\n",
      "Ep 2 (Step 058790): Train loss 1.216, Val loss 1.779\n",
      "Ep 2 (Step 058795): Train loss 1.169, Val loss 1.780\n",
      "Ep 2 (Step 058800): Train loss 1.202, Val loss 1.781\n",
      "Ep 2 (Step 058805): Train loss 0.862, Val loss 1.781\n",
      "Ep 2 (Step 058810): Train loss 1.019, Val loss 1.781\n",
      "Ep 2 (Step 058815): Train loss 0.820, Val loss 1.782\n",
      "Ep 2 (Step 058820): Train loss 0.825, Val loss 1.783\n",
      "Ep 2 (Step 058825): Train loss 0.956, Val loss 1.783\n",
      "Ep 2 (Step 058830): Train loss 0.951, Val loss 1.783\n",
      "Ep 2 (Step 058835): Train loss 0.760, Val loss 1.784\n",
      "Ep 2 (Step 058840): Train loss 1.170, Val loss 1.784\n",
      "Ep 2 (Step 058845): Train loss 1.085, Val loss 1.785\n",
      "Ep 2 (Step 058850): Train loss 1.256, Val loss 1.784\n",
      "Ep 2 (Step 058855): Train loss 1.012, Val loss 1.784\n",
      "Ep 2 (Step 058860): Train loss 1.130, Val loss 1.782\n",
      "Ep 2 (Step 058865): Train loss 1.169, Val loss 1.782\n",
      "Ep 2 (Step 058870): Train loss 1.119, Val loss 1.782\n",
      "Ep 2 (Step 058875): Train loss 1.016, Val loss 1.784\n",
      "Ep 2 (Step 058880): Train loss 1.138, Val loss 1.785\n",
      "Ep 2 (Step 058885): Train loss 0.919, Val loss 1.785\n",
      "Ep 2 (Step 058890): Train loss 1.099, Val loss 1.786\n",
      "Ep 2 (Step 058895): Train loss 1.223, Val loss 1.787\n",
      "Ep 2 (Step 058900): Train loss 1.077, Val loss 1.788\n",
      "Ep 2 (Step 058905): Train loss 0.780, Val loss 1.787\n",
      "Ep 2 (Step 058910): Train loss 1.395, Val loss 1.786\n",
      "Ep 2 (Step 058915): Train loss 0.989, Val loss 1.784\n",
      "Ep 2 (Step 058920): Train loss 0.648, Val loss 1.783\n",
      "Ep 2 (Step 058925): Train loss 1.026, Val loss 1.782\n",
      "Ep 2 (Step 058930): Train loss 0.997, Val loss 1.781\n",
      "Ep 2 (Step 058935): Train loss 1.276, Val loss 1.782\n",
      "Ep 2 (Step 058940): Train loss 1.209, Val loss 1.783\n",
      "Ep 2 (Step 058945): Train loss 0.889, Val loss 1.782\n",
      "Ep 2 (Step 058950): Train loss 0.947, Val loss 1.781\n",
      "Ep 2 (Step 058955): Train loss 0.967, Val loss 1.781\n",
      "Ep 2 (Step 058960): Train loss 0.878, Val loss 1.781\n",
      "Ep 2 (Step 058965): Train loss 1.059, Val loss 1.782\n",
      "Ep 2 (Step 058970): Train loss 0.920, Val loss 1.781\n",
      "Ep 2 (Step 058975): Train loss 1.115, Val loss 1.780\n",
      "Ep 2 (Step 058980): Train loss 0.875, Val loss 1.781\n",
      "Ep 2 (Step 058985): Train loss 1.190, Val loss 1.782\n",
      "Ep 2 (Step 058990): Train loss 1.223, Val loss 1.783\n",
      "Ep 2 (Step 058995): Train loss 0.998, Val loss 1.784\n",
      "Ep 2 (Step 059000): Train loss 1.308, Val loss 1.786\n",
      "Ep 2 (Step 059005): Train loss 1.266, Val loss 1.788\n",
      "Ep 2 (Step 059010): Train loss 0.985, Val loss 1.787\n",
      "Ep 2 (Step 059015): Train loss 1.190, Val loss 1.787\n",
      "Ep 2 (Step 059020): Train loss 0.695, Val loss 1.786\n",
      "Ep 2 (Step 059025): Train loss 1.242, Val loss 1.785\n",
      "Ep 2 (Step 059030): Train loss 1.196, Val loss 1.784\n",
      "Ep 2 (Step 059035): Train loss 1.105, Val loss 1.783\n",
      "Ep 2 (Step 059040): Train loss 1.017, Val loss 1.783\n",
      "Ep 2 (Step 059045): Train loss 1.013, Val loss 1.784\n",
      "Ep 2 (Step 059050): Train loss 0.783, Val loss 1.784\n",
      "Ep 2 (Step 059055): Train loss 0.950, Val loss 1.786\n",
      "Ep 2 (Step 059060): Train loss 1.080, Val loss 1.787\n",
      "Ep 2 (Step 059065): Train loss 1.055, Val loss 1.789\n",
      "Ep 2 (Step 059070): Train loss 1.003, Val loss 1.789\n",
      "Ep 2 (Step 059075): Train loss 1.320, Val loss 1.791\n",
      "Ep 2 (Step 059080): Train loss 1.291, Val loss 1.792\n",
      "Ep 2 (Step 059085): Train loss 1.035, Val loss 1.792\n",
      "Ep 2 (Step 059090): Train loss 1.045, Val loss 1.793\n",
      "Ep 2 (Step 059095): Train loss 1.180, Val loss 1.794\n",
      "Ep 2 (Step 059100): Train loss 1.175, Val loss 1.794\n",
      "Ep 2 (Step 059105): Train loss 0.967, Val loss 1.794\n",
      "Ep 2 (Step 059110): Train loss 1.142, Val loss 1.794\n",
      "Ep 2 (Step 059115): Train loss 1.278, Val loss 1.792\n",
      "Ep 2 (Step 059120): Train loss 1.180, Val loss 1.792\n",
      "Ep 2 (Step 059125): Train loss 0.980, Val loss 1.792\n",
      "Ep 2 (Step 059130): Train loss 0.723, Val loss 1.792\n",
      "Ep 2 (Step 059135): Train loss 1.085, Val loss 1.793\n",
      "Ep 2 (Step 059140): Train loss 1.046, Val loss 1.794\n",
      "Ep 2 (Step 059145): Train loss 0.830, Val loss 1.795\n",
      "Ep 2 (Step 059150): Train loss 1.047, Val loss 1.795\n",
      "Ep 2 (Step 059155): Train loss 1.513, Val loss 1.795\n",
      "Ep 2 (Step 059160): Train loss 1.165, Val loss 1.795\n",
      "Ep 2 (Step 059165): Train loss 0.958, Val loss 1.794\n",
      "Ep 2 (Step 059170): Train loss 1.013, Val loss 1.792\n",
      "Ep 2 (Step 059175): Train loss 0.890, Val loss 1.791\n",
      "Ep 2 (Step 059180): Train loss 1.179, Val loss 1.789\n",
      "Ep 2 (Step 059185): Train loss 1.255, Val loss 1.787\n",
      "Ep 2 (Step 059190): Train loss 0.904, Val loss 1.787\n",
      "Ep 2 (Step 059195): Train loss 0.942, Val loss 1.785\n",
      "Ep 2 (Step 059200): Train loss 1.273, Val loss 1.784\n",
      "Ep 2 (Step 059205): Train loss 0.980, Val loss 1.783\n",
      "Ep 2 (Step 059210): Train loss 1.303, Val loss 1.784\n",
      "Ep 2 (Step 059215): Train loss 1.050, Val loss 1.785\n",
      "Ep 2 (Step 059220): Train loss 1.271, Val loss 1.785\n",
      "Ep 2 (Step 059225): Train loss 1.224, Val loss 1.787\n",
      "Ep 2 (Step 059230): Train loss 1.187, Val loss 1.788\n",
      "Ep 2 (Step 059235): Train loss 0.884, Val loss 1.788\n",
      "Ep 2 (Step 059240): Train loss 1.009, Val loss 1.789\n",
      "Ep 2 (Step 059245): Train loss 1.167, Val loss 1.788\n",
      "Ep 2 (Step 059250): Train loss 1.148, Val loss 1.788\n",
      "Ep 2 (Step 059255): Train loss 1.054, Val loss 1.787\n",
      "Ep 2 (Step 059260): Train loss 0.932, Val loss 1.786\n",
      "Ep 2 (Step 059265): Train loss 1.298, Val loss 1.786\n",
      "Ep 2 (Step 059270): Train loss 1.256, Val loss 1.787\n",
      "Ep 2 (Step 059275): Train loss 0.895, Val loss 1.789\n",
      "Ep 2 (Step 059280): Train loss 1.161, Val loss 1.791\n",
      "Ep 2 (Step 059285): Train loss 0.909, Val loss 1.792\n",
      "Ep 2 (Step 059290): Train loss 0.969, Val loss 1.793\n",
      "Ep 2 (Step 059295): Train loss 1.041, Val loss 1.795\n",
      "Ep 2 (Step 059300): Train loss 1.106, Val loss 1.797\n",
      "Ep 2 (Step 059305): Train loss 0.821, Val loss 1.799\n",
      "Ep 2 (Step 059310): Train loss 1.092, Val loss 1.797\n",
      "Ep 2 (Step 059315): Train loss 0.860, Val loss 1.795\n",
      "Ep 2 (Step 059320): Train loss 0.998, Val loss 1.793\n",
      "Ep 2 (Step 059325): Train loss 1.174, Val loss 1.792\n",
      "Ep 2 (Step 059330): Train loss 1.024, Val loss 1.791\n",
      "Ep 2 (Step 059335): Train loss 1.039, Val loss 1.790\n",
      "Ep 2 (Step 059340): Train loss 1.212, Val loss 1.791\n",
      "Ep 2 (Step 059345): Train loss 1.135, Val loss 1.792\n",
      "Ep 2 (Step 059350): Train loss 1.052, Val loss 1.791\n",
      "Ep 2 (Step 059355): Train loss 1.163, Val loss 1.789\n",
      "Ep 2 (Step 059360): Train loss 1.078, Val loss 1.789\n",
      "Ep 2 (Step 059365): Train loss 0.915, Val loss 1.790\n",
      "Ep 2 (Step 059370): Train loss 0.932, Val loss 1.791\n",
      "Ep 2 (Step 059375): Train loss 0.911, Val loss 1.792\n",
      "Ep 2 (Step 059380): Train loss 0.842, Val loss 1.794\n",
      "Ep 2 (Step 059385): Train loss 0.814, Val loss 1.795\n",
      "Ep 2 (Step 059390): Train loss 0.893, Val loss 1.794\n",
      "Ep 2 (Step 059395): Train loss 1.163, Val loss 1.793\n",
      "Ep 2 (Step 059400): Train loss 1.118, Val loss 1.794\n",
      "Ep 2 (Step 059405): Train loss 0.852, Val loss 1.794\n",
      "Ep 2 (Step 059410): Train loss 1.088, Val loss 1.792\n",
      "Ep 2 (Step 059415): Train loss 1.281, Val loss 1.790\n",
      "Ep 2 (Step 059420): Train loss 1.022, Val loss 1.790\n",
      "Ep 2 (Step 059425): Train loss 0.997, Val loss 1.790\n",
      "Ep 2 (Step 059430): Train loss 1.145, Val loss 1.789\n",
      "Ep 2 (Step 059435): Train loss 1.093, Val loss 1.787\n",
      "Ep 2 (Step 059440): Train loss 1.124, Val loss 1.785\n",
      "Ep 2 (Step 059445): Train loss 1.244, Val loss 1.784\n",
      "Ep 2 (Step 059450): Train loss 1.026, Val loss 1.783\n",
      "Ep 2 (Step 059455): Train loss 0.974, Val loss 1.782\n",
      "Ep 2 (Step 059460): Train loss 0.835, Val loss 1.783\n",
      "Ep 2 (Step 059465): Train loss 1.145, Val loss 1.783\n",
      "Ep 2 (Step 059470): Train loss 0.948, Val loss 1.782\n",
      "Ep 2 (Step 059475): Train loss 1.011, Val loss 1.781\n",
      "Ep 2 (Step 059480): Train loss 0.968, Val loss 1.779\n",
      "Ep 2 (Step 059485): Train loss 0.998, Val loss 1.778\n",
      "Ep 2 (Step 059490): Train loss 1.315, Val loss 1.778\n",
      "Ep 2 (Step 059495): Train loss 0.988, Val loss 1.779\n",
      "Ep 2 (Step 059500): Train loss 1.202, Val loss 1.779\n",
      "Ep 2 (Step 059505): Train loss 1.028, Val loss 1.778\n",
      "Ep 2 (Step 059510): Train loss 1.532, Val loss 1.778\n",
      "Ep 2 (Step 059515): Train loss 1.187, Val loss 1.778\n",
      "Ep 2 (Step 059520): Train loss 0.871, Val loss 1.779\n",
      "Ep 2 (Step 059525): Train loss 0.832, Val loss 1.780\n",
      "Ep 2 (Step 059530): Train loss 1.050, Val loss 1.779\n",
      "Ep 2 (Step 059535): Train loss 1.144, Val loss 1.780\n",
      "Ep 2 (Step 059540): Train loss 1.077, Val loss 1.780\n",
      "Ep 2 (Step 059545): Train loss 1.043, Val loss 1.779\n",
      "Ep 2 (Step 059550): Train loss 1.133, Val loss 1.777\n",
      "Ep 2 (Step 059555): Train loss 0.885, Val loss 1.778\n",
      "Ep 2 (Step 059560): Train loss 1.000, Val loss 1.779\n",
      "Ep 2 (Step 059565): Train loss 0.933, Val loss 1.780\n",
      "Ep 2 (Step 059570): Train loss 0.868, Val loss 1.780\n",
      "Ep 2 (Step 059575): Train loss 0.888, Val loss 1.780\n",
      "Ep 2 (Step 059580): Train loss 0.845, Val loss 1.778\n",
      "Ep 2 (Step 059585): Train loss 0.879, Val loss 1.777\n",
      "Ep 2 (Step 059590): Train loss 0.974, Val loss 1.775\n",
      "Ep 2 (Step 059595): Train loss 1.056, Val loss 1.773\n",
      "Ep 2 (Step 059600): Train loss 0.819, Val loss 1.773\n",
      "Ep 2 (Step 059605): Train loss 1.281, Val loss 1.774\n",
      "Ep 2 (Step 059610): Train loss 0.796, Val loss 1.777\n",
      "Ep 2 (Step 059615): Train loss 1.160, Val loss 1.777\n",
      "Ep 2 (Step 059620): Train loss 1.300, Val loss 1.777\n",
      "Ep 2 (Step 059625): Train loss 1.161, Val loss 1.779\n",
      "Ep 2 (Step 059630): Train loss 0.996, Val loss 1.781\n",
      "Ep 2 (Step 059635): Train loss 1.059, Val loss 1.783\n",
      "Ep 2 (Step 059640): Train loss 1.045, Val loss 1.784\n",
      "Ep 2 (Step 059645): Train loss 0.921, Val loss 1.784\n",
      "Ep 2 (Step 059650): Train loss 1.020, Val loss 1.784\n",
      "Ep 2 (Step 059655): Train loss 0.824, Val loss 1.784\n",
      "Ep 2 (Step 059660): Train loss 0.882, Val loss 1.784\n",
      "Ep 2 (Step 059665): Train loss 0.781, Val loss 1.783\n",
      "Ep 2 (Step 059670): Train loss 1.019, Val loss 1.781\n",
      "Ep 2 (Step 059675): Train loss 0.973, Val loss 1.782\n",
      "Ep 2 (Step 059680): Train loss 1.169, Val loss 1.783\n",
      "Ep 2 (Step 059685): Train loss 1.117, Val loss 1.783\n",
      "Ep 2 (Step 059690): Train loss 1.113, Val loss 1.782\n",
      "Ep 2 (Step 059695): Train loss 0.763, Val loss 1.781\n",
      "Ep 2 (Step 059700): Train loss 0.978, Val loss 1.780\n",
      "Ep 2 (Step 059705): Train loss 0.942, Val loss 1.780\n",
      "Ep 2 (Step 059710): Train loss 0.835, Val loss 1.781\n",
      "Ep 2 (Step 059715): Train loss 1.050, Val loss 1.782\n",
      "Ep 2 (Step 059720): Train loss 1.005, Val loss 1.783\n",
      "Ep 2 (Step 059725): Train loss 1.171, Val loss 1.784\n",
      "Ep 2 (Step 059730): Train loss 1.107, Val loss 1.784\n",
      "Ep 2 (Step 059735): Train loss 1.061, Val loss 1.786\n",
      "Ep 2 (Step 059740): Train loss 0.965, Val loss 1.786\n",
      "Ep 2 (Step 059745): Train loss 0.785, Val loss 1.787\n",
      "Ep 2 (Step 059750): Train loss 0.808, Val loss 1.786\n",
      "Ep 2 (Step 059755): Train loss 0.816, Val loss 1.787\n",
      "Ep 2 (Step 059760): Train loss 1.056, Val loss 1.787\n",
      "Ep 2 (Step 059765): Train loss 0.981, Val loss 1.787\n",
      "Ep 2 (Step 059770): Train loss 1.120, Val loss 1.787\n",
      "Ep 2 (Step 059775): Train loss 1.059, Val loss 1.788\n",
      "Ep 2 (Step 059780): Train loss 1.315, Val loss 1.790\n",
      "Ep 2 (Step 059785): Train loss 0.984, Val loss 1.789\n",
      "Ep 2 (Step 059790): Train loss 1.163, Val loss 1.789\n",
      "Ep 2 (Step 059795): Train loss 0.990, Val loss 1.786\n",
      "Ep 2 (Step 059800): Train loss 0.978, Val loss 1.785\n",
      "Ep 2 (Step 059805): Train loss 1.077, Val loss 1.784\n",
      "Ep 2 (Step 059810): Train loss 1.035, Val loss 1.784\n",
      "Ep 2 (Step 059815): Train loss 1.163, Val loss 1.783\n",
      "Ep 2 (Step 059820): Train loss 1.109, Val loss 1.783\n",
      "Ep 2 (Step 059825): Train loss 1.080, Val loss 1.783\n",
      "Ep 2 (Step 059830): Train loss 0.913, Val loss 1.784\n",
      "Ep 2 (Step 059835): Train loss 1.035, Val loss 1.783\n",
      "Ep 2 (Step 059840): Train loss 0.891, Val loss 1.782\n",
      "Ep 2 (Step 059845): Train loss 0.684, Val loss 1.782\n",
      "Ep 2 (Step 059850): Train loss 1.186, Val loss 1.783\n",
      "Ep 2 (Step 059855): Train loss 0.734, Val loss 1.784\n",
      "Ep 2 (Step 059860): Train loss 1.310, Val loss 1.784\n",
      "Ep 2 (Step 059865): Train loss 1.161, Val loss 1.784\n",
      "Ep 2 (Step 059870): Train loss 0.974, Val loss 1.784\n",
      "Ep 2 (Step 059875): Train loss 1.052, Val loss 1.786\n",
      "Ep 2 (Step 059880): Train loss 0.814, Val loss 1.786\n",
      "Ep 2 (Step 059885): Train loss 1.052, Val loss 1.785\n",
      "Ep 2 (Step 059890): Train loss 0.935, Val loss 1.785\n",
      "Ep 2 (Step 059895): Train loss 1.061, Val loss 1.786\n",
      "Ep 2 (Step 059900): Train loss 1.121, Val loss 1.784\n",
      "Ep 2 (Step 059905): Train loss 1.187, Val loss 1.783\n",
      "Ep 2 (Step 059910): Train loss 1.043, Val loss 1.782\n",
      "Ep 2 (Step 059915): Train loss 1.004, Val loss 1.782\n",
      "Ep 2 (Step 059920): Train loss 0.882, Val loss 1.782\n",
      "Ep 2 (Step 059925): Train loss 1.112, Val loss 1.782\n",
      "Ep 2 (Step 059930): Train loss 0.742, Val loss 1.782\n",
      "Ep 2 (Step 059935): Train loss 0.908, Val loss 1.782\n",
      "Ep 2 (Step 059940): Train loss 1.275, Val loss 1.783\n",
      "Ep 2 (Step 059945): Train loss 0.822, Val loss 1.783\n",
      "Ep 2 (Step 059950): Train loss 1.204, Val loss 1.784\n",
      "Ep 2 (Step 059955): Train loss 1.286, Val loss 1.784\n",
      "Ep 2 (Step 059960): Train loss 1.313, Val loss 1.783\n",
      "Ep 2 (Step 059965): Train loss 0.891, Val loss 1.782\n",
      "Ep 2 (Step 059970): Train loss 0.653, Val loss 1.781\n",
      "Ep 2 (Step 059975): Train loss 1.018, Val loss 1.781\n",
      "Ep 2 (Step 059980): Train loss 0.976, Val loss 1.781\n",
      "Ep 2 (Step 059985): Train loss 1.118, Val loss 1.782\n",
      "Ep 2 (Step 059990): Train loss 1.012, Val loss 1.784\n",
      "Ep 2 (Step 059995): Train loss 0.860, Val loss 1.783\n",
      "Ep 2 (Step 060000): Train loss 0.776, Val loss 1.782\n",
      "Ep 2 (Step 060005): Train loss 0.901, Val loss 1.783\n",
      "Ep 2 (Step 060010): Train loss 1.100, Val loss 1.782\n",
      "Ep 2 (Step 060015): Train loss 0.937, Val loss 1.782\n",
      "Ep 2 (Step 060020): Train loss 0.844, Val loss 1.783\n",
      "Ep 2 (Step 060025): Train loss 1.103, Val loss 1.784\n",
      "Ep 2 (Step 060030): Train loss 1.140, Val loss 1.783\n",
      "Ep 2 (Step 060035): Train loss 0.637, Val loss 1.780\n",
      "Ep 2 (Step 060040): Train loss 0.839, Val loss 1.778\n",
      "Ep 2 (Step 060045): Train loss 1.279, Val loss 1.776\n",
      "Ep 2 (Step 060050): Train loss 1.175, Val loss 1.775\n",
      "Ep 2 (Step 060055): Train loss 1.197, Val loss 1.774\n",
      "Ep 2 (Step 060060): Train loss 1.214, Val loss 1.772\n",
      "Ep 2 (Step 060065): Train loss 1.144, Val loss 1.771\n",
      "Ep 2 (Step 060070): Train loss 1.063, Val loss 1.769\n",
      "Ep 2 (Step 060075): Train loss 0.815, Val loss 1.768\n",
      "Ep 2 (Step 060080): Train loss 1.037, Val loss 1.768\n",
      "Ep 2 (Step 060085): Train loss 1.259, Val loss 1.769\n",
      "Ep 2 (Step 060090): Train loss 0.808, Val loss 1.769\n",
      "Ep 2 (Step 060095): Train loss 1.116, Val loss 1.770\n",
      "Ep 2 (Step 060100): Train loss 1.146, Val loss 1.771\n",
      "Ep 2 (Step 060105): Train loss 0.924, Val loss 1.772\n",
      "Ep 2 (Step 060110): Train loss 0.925, Val loss 1.773\n",
      "Ep 2 (Step 060115): Train loss 0.960, Val loss 1.774\n",
      "Ep 2 (Step 060120): Train loss 1.155, Val loss 1.775\n",
      "Ep 2 (Step 060125): Train loss 1.088, Val loss 1.775\n",
      "Ep 2 (Step 060130): Train loss 1.012, Val loss 1.777\n",
      "Ep 2 (Step 060135): Train loss 0.979, Val loss 1.780\n",
      "Ep 2 (Step 060140): Train loss 1.090, Val loss 1.780\n",
      "Ep 2 (Step 060145): Train loss 0.861, Val loss 1.779\n",
      "Ep 2 (Step 060150): Train loss 0.786, Val loss 1.780\n",
      "Ep 2 (Step 060155): Train loss 0.876, Val loss 1.782\n",
      "Ep 2 (Step 060160): Train loss 1.032, Val loss 1.784\n",
      "Ep 2 (Step 060165): Train loss 1.028, Val loss 1.785\n",
      "Ep 2 (Step 060170): Train loss 1.092, Val loss 1.785\n",
      "Ep 2 (Step 060175): Train loss 0.903, Val loss 1.785\n",
      "Ep 2 (Step 060180): Train loss 1.112, Val loss 1.784\n",
      "Ep 2 (Step 060185): Train loss 1.177, Val loss 1.782\n",
      "Ep 2 (Step 060190): Train loss 1.140, Val loss 1.780\n",
      "Ep 2 (Step 060195): Train loss 0.857, Val loss 1.779\n",
      "Ep 2 (Step 060200): Train loss 1.194, Val loss 1.779\n",
      "Ep 2 (Step 060205): Train loss 0.905, Val loss 1.780\n",
      "Ep 2 (Step 060210): Train loss 1.119, Val loss 1.781\n",
      "Ep 2 (Step 060215): Train loss 0.837, Val loss 1.781\n",
      "Ep 2 (Step 060220): Train loss 1.227, Val loss 1.780\n",
      "Ep 2 (Step 060225): Train loss 1.098, Val loss 1.780\n",
      "Ep 2 (Step 060230): Train loss 1.087, Val loss 1.779\n",
      "Ep 2 (Step 060235): Train loss 1.293, Val loss 1.779\n",
      "Ep 2 (Step 060240): Train loss 0.926, Val loss 1.780\n",
      "Ep 2 (Step 060245): Train loss 1.227, Val loss 1.780\n",
      "Ep 2 (Step 060250): Train loss 1.169, Val loss 1.778\n",
      "Ep 2 (Step 060255): Train loss 0.910, Val loss 1.776\n",
      "Ep 2 (Step 060260): Train loss 0.958, Val loss 1.774\n",
      "Ep 2 (Step 060265): Train loss 0.867, Val loss 1.773\n",
      "Ep 2 (Step 060270): Train loss 1.009, Val loss 1.774\n",
      "Ep 2 (Step 060275): Train loss 1.155, Val loss 1.774\n",
      "Ep 2 (Step 060280): Train loss 0.876, Val loss 1.774\n",
      "Ep 2 (Step 060285): Train loss 0.973, Val loss 1.775\n",
      "Ep 2 (Step 060290): Train loss 0.991, Val loss 1.774\n",
      "Ep 2 (Step 060295): Train loss 0.903, Val loss 1.774\n",
      "Ep 2 (Step 060300): Train loss 1.242, Val loss 1.773\n",
      "Ep 2 (Step 060305): Train loss 0.802, Val loss 1.774\n",
      "Ep 2 (Step 060310): Train loss 1.013, Val loss 1.776\n",
      "Ep 2 (Step 060315): Train loss 1.091, Val loss 1.777\n",
      "Ep 2 (Step 060320): Train loss 1.013, Val loss 1.779\n",
      "Ep 2 (Step 060325): Train loss 1.085, Val loss 1.780\n",
      "Ep 2 (Step 060330): Train loss 0.905, Val loss 1.780\n",
      "Ep 2 (Step 060335): Train loss 1.344, Val loss 1.777\n",
      "Ep 2 (Step 060340): Train loss 1.163, Val loss 1.777\n",
      "Ep 2 (Step 060345): Train loss 1.048, Val loss 1.777\n",
      "Ep 2 (Step 060350): Train loss 1.256, Val loss 1.778\n",
      "Ep 2 (Step 060355): Train loss 1.016, Val loss 1.781\n",
      "Ep 2 (Step 060360): Train loss 0.909, Val loss 1.782\n",
      "Ep 2 (Step 060365): Train loss 1.223, Val loss 1.783\n",
      "Ep 2 (Step 060370): Train loss 1.127, Val loss 1.782\n",
      "Ep 2 (Step 060375): Train loss 1.155, Val loss 1.782\n",
      "Ep 2 (Step 060380): Train loss 0.975, Val loss 1.783\n",
      "Ep 2 (Step 060385): Train loss 1.080, Val loss 1.785\n",
      "Ep 2 (Step 060390): Train loss 0.908, Val loss 1.784\n",
      "Ep 2 (Step 060395): Train loss 1.081, Val loss 1.781\n",
      "Ep 2 (Step 060400): Train loss 1.074, Val loss 1.778\n",
      "Ep 2 (Step 060405): Train loss 1.163, Val loss 1.777\n",
      "Ep 2 (Step 060410): Train loss 1.209, Val loss 1.776\n",
      "Ep 2 (Step 060415): Train loss 0.996, Val loss 1.775\n",
      "Ep 2 (Step 060420): Train loss 1.293, Val loss 1.774\n",
      "Ep 2 (Step 060425): Train loss 1.016, Val loss 1.774\n",
      "Ep 2 (Step 060430): Train loss 1.005, Val loss 1.773\n",
      "Ep 2 (Step 060435): Train loss 0.971, Val loss 1.772\n",
      "Ep 2 (Step 060440): Train loss 1.085, Val loss 1.773\n",
      "Ep 2 (Step 060445): Train loss 1.070, Val loss 1.772\n",
      "Ep 2 (Step 060450): Train loss 1.036, Val loss 1.774\n",
      "Ep 2 (Step 060455): Train loss 1.101, Val loss 1.775\n",
      "Ep 2 (Step 060460): Train loss 0.950, Val loss 1.776\n",
      "Ep 2 (Step 060465): Train loss 1.157, Val loss 1.776\n",
      "Ep 2 (Step 060470): Train loss 0.763, Val loss 1.776\n",
      "Ep 2 (Step 060475): Train loss 0.869, Val loss 1.775\n",
      "Ep 2 (Step 060480): Train loss 0.924, Val loss 1.774\n",
      "Ep 2 (Step 060485): Train loss 0.801, Val loss 1.773\n",
      "Ep 2 (Step 060490): Train loss 1.020, Val loss 1.771\n",
      "Ep 2 (Step 060495): Train loss 1.160, Val loss 1.770\n",
      "Ep 2 (Step 060500): Train loss 1.173, Val loss 1.769\n",
      "Ep 2 (Step 060505): Train loss 1.135, Val loss 1.769\n",
      "Ep 2 (Step 060510): Train loss 0.990, Val loss 1.768\n",
      "Ep 2 (Step 060515): Train loss 1.222, Val loss 1.768\n",
      "Ep 2 (Step 060520): Train loss 0.931, Val loss 1.769\n",
      "Ep 2 (Step 060525): Train loss 1.139, Val loss 1.773\n",
      "Ep 2 (Step 060530): Train loss 1.260, Val loss 1.774\n",
      "Ep 2 (Step 060535): Train loss 0.912, Val loss 1.775\n",
      "Ep 2 (Step 060540): Train loss 1.003, Val loss 1.773\n",
      "Ep 2 (Step 060545): Train loss 1.057, Val loss 1.771\n",
      "Ep 2 (Step 060550): Train loss 0.991, Val loss 1.769\n",
      "Ep 2 (Step 060555): Train loss 1.080, Val loss 1.768\n",
      "Ep 2 (Step 060560): Train loss 1.028, Val loss 1.768\n",
      "Ep 2 (Step 060565): Train loss 0.958, Val loss 1.767\n",
      "Ep 2 (Step 060570): Train loss 1.173, Val loss 1.768\n",
      "Ep 2 (Step 060575): Train loss 0.937, Val loss 1.770\n",
      "Ep 2 (Step 060580): Train loss 1.292, Val loss 1.771\n",
      "Ep 2 (Step 060585): Train loss 1.134, Val loss 1.770\n",
      "Ep 2 (Step 060590): Train loss 0.759, Val loss 1.770\n",
      "Ep 2 (Step 060595): Train loss 1.000, Val loss 1.768\n",
      "Ep 2 (Step 060600): Train loss 0.777, Val loss 1.768\n",
      "Ep 2 (Step 060605): Train loss 0.939, Val loss 1.768\n",
      "Ep 2 (Step 060610): Train loss 0.940, Val loss 1.769\n",
      "Ep 2 (Step 060615): Train loss 1.134, Val loss 1.769\n",
      "Ep 2 (Step 060620): Train loss 0.855, Val loss 1.768\n",
      "Ep 2 (Step 060625): Train loss 1.395, Val loss 1.767\n",
      "Ep 2 (Step 060630): Train loss 0.753, Val loss 1.765\n",
      "Ep 2 (Step 060635): Train loss 1.008, Val loss 1.764\n",
      "Ep 2 (Step 060640): Train loss 1.011, Val loss 1.765\n",
      "Ep 2 (Step 060645): Train loss 1.227, Val loss 1.767\n",
      "Ep 2 (Step 060650): Train loss 0.899, Val loss 1.768\n",
      "Ep 2 (Step 060655): Train loss 1.314, Val loss 1.769\n",
      "Ep 2 (Step 060660): Train loss 0.768, Val loss 1.771\n",
      "Ep 2 (Step 060665): Train loss 1.038, Val loss 1.771\n",
      "Ep 2 (Step 060670): Train loss 1.091, Val loss 1.772\n",
      "Ep 2 (Step 060675): Train loss 1.102, Val loss 1.773\n",
      "Ep 2 (Step 060680): Train loss 1.049, Val loss 1.774\n",
      "Ep 2 (Step 060685): Train loss 1.171, Val loss 1.776\n",
      "Ep 2 (Step 060690): Train loss 1.128, Val loss 1.777\n",
      "Ep 2 (Step 060695): Train loss 1.350, Val loss 1.778\n",
      "Ep 2 (Step 060700): Train loss 1.156, Val loss 1.780\n",
      "Ep 2 (Step 060705): Train loss 0.992, Val loss 1.781\n",
      "Ep 2 (Step 060710): Train loss 1.197, Val loss 1.783\n",
      "Ep 2 (Step 060715): Train loss 0.936, Val loss 1.783\n",
      "Ep 2 (Step 060720): Train loss 0.819, Val loss 1.783\n",
      "Ep 2 (Step 060725): Train loss 1.057, Val loss 1.782\n",
      "Ep 2 (Step 060730): Train loss 1.127, Val loss 1.781\n",
      "Ep 2 (Step 060735): Train loss 0.986, Val loss 1.782\n",
      "Ep 2 (Step 060740): Train loss 0.957, Val loss 1.781\n",
      "Ep 2 (Step 060745): Train loss 1.232, Val loss 1.781\n",
      "Ep 2 (Step 060750): Train loss 0.960, Val loss 1.781\n",
      "Ep 2 (Step 060755): Train loss 1.169, Val loss 1.783\n",
      "Ep 2 (Step 060760): Train loss 1.073, Val loss 1.785\n",
      "Ep 2 (Step 060765): Train loss 0.970, Val loss 1.788\n",
      "Ep 2 (Step 060770): Train loss 1.034, Val loss 1.790\n",
      "Ep 2 (Step 060775): Train loss 0.931, Val loss 1.789\n",
      "Ep 2 (Step 060780): Train loss 1.061, Val loss 1.787\n",
      "Ep 2 (Step 060785): Train loss 1.321, Val loss 1.784\n",
      "Ep 2 (Step 060790): Train loss 0.887, Val loss 1.783\n",
      "Ep 2 (Step 060795): Train loss 1.164, Val loss 1.783\n",
      "Ep 2 (Step 060800): Train loss 1.272, Val loss 1.783\n",
      "Ep 2 (Step 060805): Train loss 1.095, Val loss 1.783\n",
      "Ep 2 (Step 060810): Train loss 1.188, Val loss 1.783\n",
      "Ep 2 (Step 060815): Train loss 1.127, Val loss 1.783\n",
      "Ep 2 (Step 060820): Train loss 1.038, Val loss 1.783\n",
      "Ep 2 (Step 060825): Train loss 1.139, Val loss 1.784\n",
      "Ep 2 (Step 060830): Train loss 1.084, Val loss 1.785\n",
      "Ep 2 (Step 060835): Train loss 0.833, Val loss 1.785\n",
      "Ep 2 (Step 060840): Train loss 1.074, Val loss 1.785\n",
      "Ep 2 (Step 060845): Train loss 1.032, Val loss 1.785\n",
      "Ep 2 (Step 060850): Train loss 1.081, Val loss 1.785\n",
      "Ep 2 (Step 060855): Train loss 1.084, Val loss 1.784\n",
      "Ep 2 (Step 060860): Train loss 0.938, Val loss 1.785\n",
      "Ep 2 (Step 060865): Train loss 1.192, Val loss 1.784\n",
      "Ep 2 (Step 060870): Train loss 1.125, Val loss 1.785\n",
      "Ep 2 (Step 060875): Train loss 1.137, Val loss 1.783\n",
      "Ep 2 (Step 060880): Train loss 0.941, Val loss 1.782\n",
      "Ep 2 (Step 060885): Train loss 1.428, Val loss 1.782\n",
      "Ep 2 (Step 060890): Train loss 0.884, Val loss 1.781\n",
      "Ep 2 (Step 060895): Train loss 1.055, Val loss 1.781\n",
      "Ep 2 (Step 060900): Train loss 1.380, Val loss 1.782\n",
      "Ep 2 (Step 060905): Train loss 0.921, Val loss 1.783\n",
      "Ep 2 (Step 060910): Train loss 0.926, Val loss 1.784\n",
      "Ep 2 (Step 060915): Train loss 0.907, Val loss 1.783\n",
      "Ep 2 (Step 060920): Train loss 1.203, Val loss 1.781\n",
      "Ep 2 (Step 060925): Train loss 1.147, Val loss 1.779\n",
      "Ep 2 (Step 060930): Train loss 0.985, Val loss 1.777\n",
      "Ep 2 (Step 060935): Train loss 0.943, Val loss 1.775\n",
      "Ep 2 (Step 060940): Train loss 0.654, Val loss 1.775\n",
      "Ep 2 (Step 060945): Train loss 1.010, Val loss 1.774\n",
      "Ep 2 (Step 060950): Train loss 1.096, Val loss 1.772\n",
      "Ep 2 (Step 060955): Train loss 1.098, Val loss 1.772\n",
      "Ep 2 (Step 060960): Train loss 0.968, Val loss 1.772\n",
      "Ep 2 (Step 060965): Train loss 1.077, Val loss 1.773\n",
      "Ep 2 (Step 060970): Train loss 1.217, Val loss 1.775\n",
      "Ep 2 (Step 060975): Train loss 1.091, Val loss 1.775\n",
      "Ep 2 (Step 060980): Train loss 0.954, Val loss 1.775\n",
      "Ep 2 (Step 060985): Train loss 0.941, Val loss 1.778\n",
      "Ep 2 (Step 060990): Train loss 1.085, Val loss 1.778\n",
      "Ep 2 (Step 060995): Train loss 1.084, Val loss 1.779\n",
      "Ep 2 (Step 061000): Train loss 1.080, Val loss 1.779\n",
      "Ep 2 (Step 061005): Train loss 1.047, Val loss 1.779\n",
      "Ep 2 (Step 061010): Train loss 0.828, Val loss 1.778\n",
      "Ep 2 (Step 061015): Train loss 0.713, Val loss 1.778\n",
      "Ep 2 (Step 061020): Train loss 0.951, Val loss 1.779\n",
      "Ep 2 (Step 061025): Train loss 1.128, Val loss 1.781\n",
      "Ep 2 (Step 061030): Train loss 0.815, Val loss 1.781\n",
      "Ep 2 (Step 061035): Train loss 1.098, Val loss 1.780\n",
      "Ep 2 (Step 061040): Train loss 0.863, Val loss 1.781\n",
      "Ep 2 (Step 061045): Train loss 1.049, Val loss 1.781\n",
      "Ep 2 (Step 061050): Train loss 1.116, Val loss 1.782\n",
      "Ep 2 (Step 061055): Train loss 0.913, Val loss 1.782\n",
      "Ep 2 (Step 061060): Train loss 1.176, Val loss 1.781\n",
      "Ep 2 (Step 061065): Train loss 0.965, Val loss 1.779\n",
      "Ep 2 (Step 061070): Train loss 0.992, Val loss 1.779\n",
      "Ep 2 (Step 061075): Train loss 0.922, Val loss 1.778\n",
      "Ep 2 (Step 061080): Train loss 0.923, Val loss 1.777\n",
      "Ep 2 (Step 061085): Train loss 0.871, Val loss 1.778\n",
      "Ep 2 (Step 061090): Train loss 1.018, Val loss 1.778\n",
      "Ep 2 (Step 061095): Train loss 1.130, Val loss 1.778\n",
      "Ep 2 (Step 061100): Train loss 0.884, Val loss 1.779\n",
      "Ep 2 (Step 061105): Train loss 1.017, Val loss 1.780\n",
      "Ep 2 (Step 061110): Train loss 1.009, Val loss 1.780\n",
      "Ep 2 (Step 061115): Train loss 1.094, Val loss 1.781\n",
      "Ep 2 (Step 061120): Train loss 0.992, Val loss 1.781\n",
      "Ep 2 (Step 061125): Train loss 0.765, Val loss 1.780\n",
      "Ep 2 (Step 061130): Train loss 1.367, Val loss 1.779\n",
      "Ep 2 (Step 061135): Train loss 0.937, Val loss 1.778\n",
      "Ep 2 (Step 061140): Train loss 1.158, Val loss 1.779\n",
      "Ep 2 (Step 061145): Train loss 1.221, Val loss 1.780\n",
      "Ep 2 (Step 061150): Train loss 1.052, Val loss 1.783\n",
      "Ep 2 (Step 061155): Train loss 1.016, Val loss 1.785\n",
      "Ep 2 (Step 061160): Train loss 1.154, Val loss 1.786\n",
      "Ep 2 (Step 061165): Train loss 1.004, Val loss 1.789\n",
      "Ep 2 (Step 061170): Train loss 1.167, Val loss 1.790\n",
      "Ep 2 (Step 061175): Train loss 1.301, Val loss 1.791\n",
      "Ep 2 (Step 061180): Train loss 1.032, Val loss 1.792\n",
      "Ep 2 (Step 061185): Train loss 0.802, Val loss 1.793\n",
      "Ep 2 (Step 061190): Train loss 1.060, Val loss 1.793\n",
      "Ep 2 (Step 061195): Train loss 0.993, Val loss 1.794\n",
      "Ep 2 (Step 061200): Train loss 1.099, Val loss 1.795\n",
      "Ep 2 (Step 061205): Train loss 0.921, Val loss 1.794\n",
      "Ep 2 (Step 061210): Train loss 1.037, Val loss 1.792\n",
      "Ep 2 (Step 061215): Train loss 0.948, Val loss 1.789\n",
      "Ep 2 (Step 061220): Train loss 1.201, Val loss 1.788\n",
      "Ep 2 (Step 061225): Train loss 1.019, Val loss 1.788\n",
      "Ep 2 (Step 061230): Train loss 0.960, Val loss 1.787\n",
      "Ep 2 (Step 061235): Train loss 0.955, Val loss 1.787\n",
      "Ep 2 (Step 061240): Train loss 0.982, Val loss 1.786\n",
      "Ep 2 (Step 061245): Train loss 1.009, Val loss 1.786\n",
      "Ep 2 (Step 061250): Train loss 0.990, Val loss 1.787\n",
      "Ep 2 (Step 061255): Train loss 0.930, Val loss 1.786\n",
      "Ep 2 (Step 061260): Train loss 1.004, Val loss 1.786\n",
      "Ep 2 (Step 061265): Train loss 0.911, Val loss 1.786\n",
      "Ep 2 (Step 061270): Train loss 0.696, Val loss 1.787\n",
      "Ep 2 (Step 061275): Train loss 1.108, Val loss 1.788\n",
      "Ep 2 (Step 061280): Train loss 0.956, Val loss 1.790\n",
      "Ep 2 (Step 061285): Train loss 1.200, Val loss 1.792\n",
      "Ep 2 (Step 061290): Train loss 1.185, Val loss 1.793\n",
      "Ep 2 (Step 061295): Train loss 1.080, Val loss 1.793\n",
      "Ep 2 (Step 061300): Train loss 1.012, Val loss 1.791\n",
      "Ep 2 (Step 061305): Train loss 0.863, Val loss 1.788\n",
      "Ep 2 (Step 061310): Train loss 0.983, Val loss 1.786\n",
      "Ep 2 (Step 061315): Train loss 1.102, Val loss 1.784\n",
      "Ep 2 (Step 061320): Train loss 1.408, Val loss 1.784\n",
      "Ep 2 (Step 061325): Train loss 1.271, Val loss 1.783\n",
      "Ep 2 (Step 061330): Train loss 1.132, Val loss 1.782\n",
      "Ep 2 (Step 061335): Train loss 1.091, Val loss 1.781\n",
      "Ep 2 (Step 061340): Train loss 1.121, Val loss 1.780\n",
      "Ep 2 (Step 061345): Train loss 1.157, Val loss 1.779\n",
      "Ep 2 (Step 061350): Train loss 1.311, Val loss 1.780\n",
      "Ep 2 (Step 061355): Train loss 1.085, Val loss 1.782\n",
      "Ep 2 (Step 061360): Train loss 1.179, Val loss 1.784\n",
      "Ep 2 (Step 061365): Train loss 1.019, Val loss 1.785\n",
      "Ep 2 (Step 061370): Train loss 0.939, Val loss 1.784\n",
      "Ep 2 (Step 061375): Train loss 1.324, Val loss 1.785\n",
      "Ep 2 (Step 061380): Train loss 1.119, Val loss 1.784\n",
      "Ep 2 (Step 061385): Train loss 1.106, Val loss 1.785\n",
      "Ep 2 (Step 061390): Train loss 1.067, Val loss 1.785\n",
      "Ep 2 (Step 061395): Train loss 0.960, Val loss 1.784\n",
      "Ep 2 (Step 061400): Train loss 0.826, Val loss 1.784\n",
      "Ep 2 (Step 061405): Train loss 1.109, Val loss 1.785\n",
      "Ep 2 (Step 061410): Train loss 1.055, Val loss 1.785\n",
      "Ep 2 (Step 061415): Train loss 0.995, Val loss 1.785\n",
      "Ep 2 (Step 061420): Train loss 1.146, Val loss 1.785\n",
      "Ep 2 (Step 061425): Train loss 1.057, Val loss 1.785\n",
      "Ep 2 (Step 061430): Train loss 0.815, Val loss 1.784\n",
      "Ep 2 (Step 061435): Train loss 1.114, Val loss 1.783\n",
      "Ep 2 (Step 061440): Train loss 1.240, Val loss 1.782\n",
      "Ep 2 (Step 061445): Train loss 1.092, Val loss 1.781\n",
      "Ep 2 (Step 061450): Train loss 0.977, Val loss 1.779\n",
      "Ep 2 (Step 061455): Train loss 0.861, Val loss 1.778\n",
      "Ep 2 (Step 061460): Train loss 1.029, Val loss 1.776\n",
      "Ep 2 (Step 061465): Train loss 0.850, Val loss 1.774\n",
      "Ep 2 (Step 061470): Train loss 1.104, Val loss 1.773\n",
      "Ep 2 (Step 061475): Train loss 0.967, Val loss 1.771\n",
      "Ep 2 (Step 061480): Train loss 1.150, Val loss 1.770\n",
      "Ep 2 (Step 061485): Train loss 1.052, Val loss 1.771\n",
      "Ep 2 (Step 061490): Train loss 0.720, Val loss 1.770\n",
      "Ep 2 (Step 061495): Train loss 1.136, Val loss 1.768\n",
      "Ep 2 (Step 061500): Train loss 1.062, Val loss 1.768\n",
      "Ep 2 (Step 061505): Train loss 1.326, Val loss 1.768\n",
      "Ep 2 (Step 061510): Train loss 0.630, Val loss 1.768\n",
      "Ep 2 (Step 061515): Train loss 1.037, Val loss 1.768\n",
      "Ep 2 (Step 061520): Train loss 1.323, Val loss 1.768\n",
      "Ep 2 (Step 061525): Train loss 1.264, Val loss 1.768\n",
      "Ep 2 (Step 061530): Train loss 1.080, Val loss 1.770\n",
      "Ep 2 (Step 061535): Train loss 0.826, Val loss 1.770\n",
      "Ep 2 (Step 061540): Train loss 1.012, Val loss 1.770\n",
      "Ep 2 (Step 061545): Train loss 1.040, Val loss 1.769\n",
      "Ep 2 (Step 061550): Train loss 1.080, Val loss 1.768\n",
      "Ep 2 (Step 061555): Train loss 1.168, Val loss 1.766\n",
      "Ep 2 (Step 061560): Train loss 1.108, Val loss 1.765\n",
      "Ep 2 (Step 061565): Train loss 1.127, Val loss 1.764\n",
      "Ep 2 (Step 061570): Train loss 0.987, Val loss 1.764\n",
      "Ep 2 (Step 061575): Train loss 0.936, Val loss 1.764\n",
      "Ep 2 (Step 061580): Train loss 1.125, Val loss 1.766\n",
      "Ep 2 (Step 061585): Train loss 0.864, Val loss 1.767\n",
      "Ep 2 (Step 061590): Train loss 1.120, Val loss 1.768\n",
      "Ep 2 (Step 061595): Train loss 0.997, Val loss 1.770\n",
      "Ep 2 (Step 061600): Train loss 1.279, Val loss 1.771\n",
      "Ep 2 (Step 061605): Train loss 1.016, Val loss 1.772\n",
      "Ep 2 (Step 061610): Train loss 1.217, Val loss 1.771\n",
      "Ep 2 (Step 061615): Train loss 0.941, Val loss 1.771\n",
      "Ep 2 (Step 061620): Train loss 1.082, Val loss 1.771\n",
      "Ep 2 (Step 061625): Train loss 1.035, Val loss 1.771\n",
      "Ep 2 (Step 061630): Train loss 0.885, Val loss 1.771\n",
      "Ep 2 (Step 061635): Train loss 0.853, Val loss 1.770\n",
      "Ep 2 (Step 061640): Train loss 1.080, Val loss 1.770\n",
      "Ep 2 (Step 061645): Train loss 1.192, Val loss 1.770\n",
      "Ep 2 (Step 061650): Train loss 0.977, Val loss 1.770\n",
      "Ep 2 (Step 061655): Train loss 1.215, Val loss 1.770\n",
      "Ep 2 (Step 061660): Train loss 1.203, Val loss 1.771\n",
      "Ep 2 (Step 061665): Train loss 1.049, Val loss 1.772\n",
      "Ep 2 (Step 061670): Train loss 0.718, Val loss 1.772\n",
      "Ep 2 (Step 061675): Train loss 1.112, Val loss 1.772\n",
      "Ep 2 (Step 061680): Train loss 1.002, Val loss 1.771\n",
      "Ep 2 (Step 061685): Train loss 1.047, Val loss 1.770\n",
      "Ep 2 (Step 061690): Train loss 1.242, Val loss 1.768\n",
      "Ep 2 (Step 061695): Train loss 0.986, Val loss 1.766\n",
      "Ep 2 (Step 061700): Train loss 1.174, Val loss 1.766\n",
      "Ep 2 (Step 061705): Train loss 1.040, Val loss 1.768\n",
      "Ep 2 (Step 061710): Train loss 1.068, Val loss 1.767\n",
      "Ep 2 (Step 061715): Train loss 1.179, Val loss 1.766\n",
      "Ep 2 (Step 061720): Train loss 1.090, Val loss 1.766\n",
      "Ep 2 (Step 061725): Train loss 0.979, Val loss 1.766\n",
      "Ep 2 (Step 061730): Train loss 1.032, Val loss 1.766\n",
      "Ep 2 (Step 061735): Train loss 1.256, Val loss 1.766\n",
      "Ep 2 (Step 061740): Train loss 0.951, Val loss 1.767\n",
      "Ep 2 (Step 061745): Train loss 1.248, Val loss 1.768\n",
      "Ep 2 (Step 061750): Train loss 0.684, Val loss 1.769\n",
      "Ep 2 (Step 061755): Train loss 1.091, Val loss 1.769\n",
      "Ep 2 (Step 061760): Train loss 1.282, Val loss 1.769\n",
      "Ep 2 (Step 061765): Train loss 1.159, Val loss 1.768\n",
      "Ep 2 (Step 061770): Train loss 1.062, Val loss 1.768\n",
      "Ep 2 (Step 061775): Train loss 1.084, Val loss 1.769\n",
      "Ep 2 (Step 061780): Train loss 1.062, Val loss 1.770\n",
      "Ep 2 (Step 061785): Train loss 1.105, Val loss 1.770\n",
      "Ep 2 (Step 061790): Train loss 0.925, Val loss 1.770\n",
      "Ep 2 (Step 061795): Train loss 1.056, Val loss 1.770\n",
      "Ep 2 (Step 061800): Train loss 1.151, Val loss 1.769\n",
      "Ep 2 (Step 061805): Train loss 1.017, Val loss 1.769\n",
      "Ep 2 (Step 061810): Train loss 1.118, Val loss 1.768\n",
      "Ep 2 (Step 061815): Train loss 0.863, Val loss 1.768\n",
      "Ep 2 (Step 061820): Train loss 1.035, Val loss 1.767\n",
      "Ep 2 (Step 061825): Train loss 1.322, Val loss 1.768\n",
      "Ep 2 (Step 061830): Train loss 1.020, Val loss 1.769\n",
      "Ep 2 (Step 061835): Train loss 0.972, Val loss 1.770\n",
      "Ep 2 (Step 061840): Train loss 1.030, Val loss 1.772\n",
      "Ep 2 (Step 061845): Train loss 1.009, Val loss 1.771\n",
      "Ep 2 (Step 061850): Train loss 1.090, Val loss 1.770\n",
      "Ep 2 (Step 061855): Train loss 1.168, Val loss 1.769\n",
      "Ep 2 (Step 061860): Train loss 0.986, Val loss 1.770\n",
      "Ep 2 (Step 061865): Train loss 1.185, Val loss 1.771\n",
      "Ep 2 (Step 061870): Train loss 1.313, Val loss 1.771\n",
      "Ep 2 (Step 061875): Train loss 1.257, Val loss 1.773\n",
      "Ep 2 (Step 061880): Train loss 0.942, Val loss 1.774\n",
      "Ep 2 (Step 061885): Train loss 0.987, Val loss 1.774\n",
      "Ep 2 (Step 061890): Train loss 1.033, Val loss 1.772\n",
      "Ep 2 (Step 061895): Train loss 1.119, Val loss 1.769\n",
      "Ep 2 (Step 061900): Train loss 1.061, Val loss 1.767\n",
      "Ep 2 (Step 061905): Train loss 0.975, Val loss 1.766\n",
      "Ep 2 (Step 061910): Train loss 0.956, Val loss 1.764\n",
      "Ep 2 (Step 061915): Train loss 0.918, Val loss 1.764\n",
      "Ep 2 (Step 061920): Train loss 1.172, Val loss 1.764\n",
      "Ep 2 (Step 061925): Train loss 1.027, Val loss 1.764\n",
      "Ep 2 (Step 061930): Train loss 1.029, Val loss 1.764\n",
      "Ep 2 (Step 061935): Train loss 1.209, Val loss 1.764\n",
      "Ep 2 (Step 061940): Train loss 0.883, Val loss 1.764\n",
      "Ep 2 (Step 061945): Train loss 0.944, Val loss 1.765\n",
      "Ep 2 (Step 061950): Train loss 1.374, Val loss 1.765\n",
      "Ep 2 (Step 061955): Train loss 1.021, Val loss 1.766\n",
      "Ep 2 (Step 061960): Train loss 1.211, Val loss 1.766\n",
      "Ep 2 (Step 061965): Train loss 0.953, Val loss 1.766\n",
      "Ep 2 (Step 061970): Train loss 1.325, Val loss 1.766\n",
      "Ep 2 (Step 061975): Train loss 1.110, Val loss 1.765\n",
      "Ep 2 (Step 061980): Train loss 1.010, Val loss 1.766\n",
      "Ep 2 (Step 061985): Train loss 1.188, Val loss 1.765\n",
      "Ep 2 (Step 061990): Train loss 1.127, Val loss 1.765\n",
      "Ep 2 (Step 061995): Train loss 0.969, Val loss 1.765\n",
      "Ep 2 (Step 062000): Train loss 1.104, Val loss 1.767\n",
      "Ep 2 (Step 062005): Train loss 0.812, Val loss 1.768\n",
      "Ep 2 (Step 062010): Train loss 1.083, Val loss 1.770\n",
      "Ep 2 (Step 062015): Train loss 1.214, Val loss 1.772\n",
      "Ep 2 (Step 062020): Train loss 0.990, Val loss 1.773\n",
      "Ep 2 (Step 062025): Train loss 1.067, Val loss 1.773\n",
      "Ep 2 (Step 062030): Train loss 0.982, Val loss 1.772\n",
      "Ep 2 (Step 062035): Train loss 1.230, Val loss 1.771\n",
      "Ep 2 (Step 062040): Train loss 0.894, Val loss 1.772\n",
      "Ep 2 (Step 062045): Train loss 1.233, Val loss 1.772\n",
      "Ep 2 (Step 062050): Train loss 1.310, Val loss 1.773\n",
      "Ep 2 (Step 062055): Train loss 1.343, Val loss 1.773\n",
      "Ep 2 (Step 062060): Train loss 1.089, Val loss 1.773\n",
      "Ep 2 (Step 062065): Train loss 0.937, Val loss 1.775\n",
      "Ep 2 (Step 062070): Train loss 1.327, Val loss 1.777\n",
      "Ep 2 (Step 062075): Train loss 0.840, Val loss 1.777\n",
      "Ep 2 (Step 062080): Train loss 0.828, Val loss 1.777\n",
      "Ep 2 (Step 062085): Train loss 1.093, Val loss 1.776\n",
      "Ep 2 (Step 062090): Train loss 1.196, Val loss 1.775\n",
      "Ep 2 (Step 062095): Train loss 1.141, Val loss 1.774\n",
      "Ep 2 (Step 062100): Train loss 1.220, Val loss 1.772\n",
      "Ep 2 (Step 062105): Train loss 1.085, Val loss 1.770\n",
      "Ep 2 (Step 062110): Train loss 1.442, Val loss 1.770\n",
      "Ep 2 (Step 062115): Train loss 1.020, Val loss 1.770\n",
      "Ep 2 (Step 062120): Train loss 0.862, Val loss 1.770\n",
      "Ep 2 (Step 062125): Train loss 0.896, Val loss 1.771\n",
      "Ep 2 (Step 062130): Train loss 0.889, Val loss 1.772\n",
      "Ep 2 (Step 062135): Train loss 0.810, Val loss 1.772\n",
      "Ep 2 (Step 062140): Train loss 1.314, Val loss 1.774\n",
      "Ep 2 (Step 062145): Train loss 0.907, Val loss 1.775\n",
      "Ep 2 (Step 062150): Train loss 0.873, Val loss 1.775\n",
      "Ep 2 (Step 062155): Train loss 1.101, Val loss 1.776\n",
      "Ep 2 (Step 062160): Train loss 1.134, Val loss 1.777\n",
      "Ep 2 (Step 062165): Train loss 1.017, Val loss 1.776\n",
      "Ep 2 (Step 062170): Train loss 1.587, Val loss 1.775\n",
      "Ep 2 (Step 062175): Train loss 0.888, Val loss 1.776\n",
      "Ep 2 (Step 062180): Train loss 1.079, Val loss 1.777\n",
      "Ep 2 (Step 062185): Train loss 1.072, Val loss 1.777\n",
      "Ep 2 (Step 062190): Train loss 0.873, Val loss 1.777\n",
      "Ep 2 (Step 062195): Train loss 1.434, Val loss 1.776\n",
      "Ep 2 (Step 062200): Train loss 1.071, Val loss 1.776\n",
      "Ep 2 (Step 062205): Train loss 0.799, Val loss 1.777\n",
      "Ep 2 (Step 062210): Train loss 0.750, Val loss 1.779\n",
      "Ep 2 (Step 062215): Train loss 0.829, Val loss 1.781\n",
      "Ep 2 (Step 062220): Train loss 0.836, Val loss 1.784\n",
      "Ep 2 (Step 062225): Train loss 1.059, Val loss 1.788\n",
      "Ep 2 (Step 062230): Train loss 1.111, Val loss 1.790\n",
      "Ep 2 (Step 062235): Train loss 1.204, Val loss 1.791\n",
      "Ep 2 (Step 062240): Train loss 0.937, Val loss 1.793\n",
      "Ep 2 (Step 062245): Train loss 1.044, Val loss 1.793\n",
      "Ep 2 (Step 062250): Train loss 1.027, Val loss 1.792\n",
      "Ep 2 (Step 062255): Train loss 0.983, Val loss 1.789\n",
      "Ep 2 (Step 062260): Train loss 1.093, Val loss 1.787\n",
      "Ep 2 (Step 062265): Train loss 1.211, Val loss 1.787\n",
      "Ep 2 (Step 062270): Train loss 0.922, Val loss 1.787\n",
      "Ep 2 (Step 062275): Train loss 1.285, Val loss 1.787\n",
      "Ep 2 (Step 062280): Train loss 0.928, Val loss 1.787\n",
      "Ep 2 (Step 062285): Train loss 1.320, Val loss 1.787\n",
      "Ep 2 (Step 062290): Train loss 0.950, Val loss 1.785\n",
      "Ep 2 (Step 062295): Train loss 1.037, Val loss 1.783\n",
      "Ep 2 (Step 062300): Train loss 1.007, Val loss 1.783\n",
      "Ep 2 (Step 062305): Train loss 0.777, Val loss 1.781\n",
      "Ep 2 (Step 062310): Train loss 0.858, Val loss 1.778\n",
      "Ep 2 (Step 062315): Train loss 1.107, Val loss 1.777\n",
      "Ep 2 (Step 062320): Train loss 1.013, Val loss 1.778\n",
      "Ep 2 (Step 062325): Train loss 1.110, Val loss 1.777\n",
      "Ep 2 (Step 062330): Train loss 1.156, Val loss 1.780\n",
      "Ep 2 (Step 062335): Train loss 1.037, Val loss 1.782\n",
      "Ep 2 (Step 062340): Train loss 1.090, Val loss 1.783\n",
      "Ep 2 (Step 062345): Train loss 1.031, Val loss 1.783\n",
      "Ep 2 (Step 062350): Train loss 1.016, Val loss 1.784\n",
      "Ep 2 (Step 062355): Train loss 0.793, Val loss 1.784\n",
      "Ep 2 (Step 062360): Train loss 0.915, Val loss 1.784\n",
      "Ep 2 (Step 062365): Train loss 1.039, Val loss 1.785\n",
      "Ep 2 (Step 062370): Train loss 1.247, Val loss 1.785\n",
      "Ep 2 (Step 062375): Train loss 0.945, Val loss 1.784\n",
      "Ep 2 (Step 062380): Train loss 1.017, Val loss 1.785\n",
      "Ep 2 (Step 062385): Train loss 0.871, Val loss 1.786\n",
      "Ep 2 (Step 062390): Train loss 1.043, Val loss 1.787\n",
      "Ep 2 (Step 062395): Train loss 1.012, Val loss 1.787\n",
      "Ep 2 (Step 062400): Train loss 0.937, Val loss 1.788\n",
      "Ep 2 (Step 062405): Train loss 1.115, Val loss 1.787\n",
      "Ep 2 (Step 062410): Train loss 1.322, Val loss 1.787\n",
      "Ep 2 (Step 062415): Train loss 1.033, Val loss 1.786\n",
      "Ep 2 (Step 062420): Train loss 1.222, Val loss 1.784\n",
      "Ep 2 (Step 062425): Train loss 0.952, Val loss 1.782\n",
      "Ep 2 (Step 062430): Train loss 1.101, Val loss 1.781\n",
      "Ep 2 (Step 062435): Train loss 0.898, Val loss 1.780\n",
      "Ep 2 (Step 062440): Train loss 0.845, Val loss 1.779\n",
      "Ep 2 (Step 062445): Train loss 1.324, Val loss 1.778\n",
      "Ep 2 (Step 062450): Train loss 0.967, Val loss 1.776\n",
      "Ep 2 (Step 062455): Train loss 0.925, Val loss 1.774\n",
      "Ep 2 (Step 062460): Train loss 1.160, Val loss 1.774\n",
      "Ep 2 (Step 062465): Train loss 0.962, Val loss 1.773\n",
      "Ep 2 (Step 062470): Train loss 1.099, Val loss 1.773\n",
      "Ep 2 (Step 062475): Train loss 0.951, Val loss 1.773\n",
      "Ep 2 (Step 062480): Train loss 0.916, Val loss 1.774\n",
      "Ep 2 (Step 062485): Train loss 1.224, Val loss 1.776\n",
      "Ep 2 (Step 062490): Train loss 0.914, Val loss 1.778\n",
      "Ep 2 (Step 062495): Train loss 1.191, Val loss 1.779\n",
      "Ep 2 (Step 062500): Train loss 1.357, Val loss 1.781\n",
      "Ep 2 (Step 062505): Train loss 1.278, Val loss 1.783\n",
      "Ep 2 (Step 062510): Train loss 1.136, Val loss 1.783\n",
      "Ep 2 (Step 062515): Train loss 0.994, Val loss 1.783\n",
      "Ep 2 (Step 062520): Train loss 1.202, Val loss 1.783\n",
      "Ep 2 (Step 062525): Train loss 0.916, Val loss 1.782\n",
      "Ep 2 (Step 062530): Train loss 0.973, Val loss 1.782\n",
      "Ep 2 (Step 062535): Train loss 1.149, Val loss 1.782\n",
      "Ep 2 (Step 062540): Train loss 1.114, Val loss 1.781\n",
      "Ep 2 (Step 062545): Train loss 0.744, Val loss 1.782\n",
      "Ep 2 (Step 062550): Train loss 1.116, Val loss 1.783\n",
      "Ep 2 (Step 062555): Train loss 1.107, Val loss 1.784\n",
      "Ep 2 (Step 062560): Train loss 1.337, Val loss 1.785\n",
      "Ep 2 (Step 062565): Train loss 1.033, Val loss 1.785\n",
      "Ep 2 (Step 062570): Train loss 1.033, Val loss 1.785\n",
      "Ep 2 (Step 062575): Train loss 1.123, Val loss 1.783\n",
      "Ep 2 (Step 062580): Train loss 1.133, Val loss 1.783\n",
      "Ep 2 (Step 062585): Train loss 0.962, Val loss 1.785\n",
      "Ep 2 (Step 062590): Train loss 1.045, Val loss 1.786\n",
      "Ep 2 (Step 062595): Train loss 1.061, Val loss 1.785\n",
      "Ep 2 (Step 062600): Train loss 0.815, Val loss 1.782\n",
      "Ep 2 (Step 062605): Train loss 1.129, Val loss 1.782\n",
      "Ep 2 (Step 062610): Train loss 1.297, Val loss 1.783\n",
      "Ep 2 (Step 062615): Train loss 1.075, Val loss 1.784\n",
      "Ep 2 (Step 062620): Train loss 0.746, Val loss 1.783\n",
      "Ep 2 (Step 062625): Train loss 1.033, Val loss 1.781\n",
      "Ep 2 (Step 062630): Train loss 1.013, Val loss 1.778\n",
      "Ep 2 (Step 062635): Train loss 0.956, Val loss 1.776\n",
      "Ep 2 (Step 062640): Train loss 1.051, Val loss 1.775\n",
      "Ep 2 (Step 062645): Train loss 1.025, Val loss 1.774\n",
      "Ep 2 (Step 062650): Train loss 1.030, Val loss 1.773\n",
      "Ep 2 (Step 062655): Train loss 1.211, Val loss 1.772\n",
      "Ep 2 (Step 062660): Train loss 1.035, Val loss 1.771\n",
      "Ep 2 (Step 062665): Train loss 1.035, Val loss 1.770\n",
      "Ep 2 (Step 062670): Train loss 0.999, Val loss 1.770\n",
      "Ep 2 (Step 062675): Train loss 0.783, Val loss 1.769\n",
      "Ep 2 (Step 062680): Train loss 1.055, Val loss 1.771\n",
      "Ep 2 (Step 062685): Train loss 0.872, Val loss 1.773\n",
      "Ep 2 (Step 062690): Train loss 0.691, Val loss 1.774\n",
      "Ep 2 (Step 062695): Train loss 0.716, Val loss 1.776\n",
      "Ep 2 (Step 062700): Train loss 0.712, Val loss 1.777\n",
      "Ep 2 (Step 062705): Train loss 0.746, Val loss 1.779\n",
      "Ep 2 (Step 062710): Train loss 0.885, Val loss 1.780\n",
      "Ep 2 (Step 062715): Train loss 1.135, Val loss 1.780\n",
      "Ep 2 (Step 062720): Train loss 1.036, Val loss 1.780\n",
      "Ep 2 (Step 062725): Train loss 0.795, Val loss 1.778\n",
      "Ep 2 (Step 062730): Train loss 0.850, Val loss 1.777\n",
      "Ep 2 (Step 062735): Train loss 0.947, Val loss 1.776\n",
      "Ep 2 (Step 062740): Train loss 1.347, Val loss 1.777\n",
      "Ep 2 (Step 062745): Train loss 0.969, Val loss 1.778\n",
      "Ep 2 (Step 062750): Train loss 1.260, Val loss 1.778\n",
      "Ep 2 (Step 062755): Train loss 1.081, Val loss 1.776\n",
      "Ep 2 (Step 062760): Train loss 0.898, Val loss 1.775\n",
      "Ep 2 (Step 062765): Train loss 0.959, Val loss 1.774\n",
      "Ep 2 (Step 062770): Train loss 1.183, Val loss 1.773\n",
      "Ep 2 (Step 062775): Train loss 0.969, Val loss 1.772\n",
      "Ep 2 (Step 062780): Train loss 1.035, Val loss 1.771\n",
      "Ep 2 (Step 062785): Train loss 1.229, Val loss 1.769\n",
      "Ep 2 (Step 062790): Train loss 1.088, Val loss 1.768\n",
      "Ep 2 (Step 062795): Train loss 1.313, Val loss 1.770\n",
      "Ep 2 (Step 062800): Train loss 1.112, Val loss 1.770\n",
      "Ep 2 (Step 062805): Train loss 1.041, Val loss 1.770\n",
      "Ep 2 (Step 062810): Train loss 1.060, Val loss 1.767\n",
      "Ep 2 (Step 062815): Train loss 1.104, Val loss 1.765\n",
      "Ep 2 (Step 062820): Train loss 1.074, Val loss 1.764\n",
      "Ep 2 (Step 062825): Train loss 1.117, Val loss 1.765\n",
      "Ep 2 (Step 062830): Train loss 1.088, Val loss 1.765\n",
      "Ep 2 (Step 062835): Train loss 0.973, Val loss 1.765\n",
      "Ep 2 (Step 062840): Train loss 0.809, Val loss 1.765\n",
      "Ep 2 (Step 062845): Train loss 0.975, Val loss 1.765\n",
      "Ep 2 (Step 062850): Train loss 0.903, Val loss 1.764\n",
      "Ep 2 (Step 062855): Train loss 1.249, Val loss 1.764\n",
      "Ep 2 (Step 062860): Train loss 1.285, Val loss 1.764\n",
      "Ep 2 (Step 062865): Train loss 0.992, Val loss 1.764\n",
      "Ep 2 (Step 062870): Train loss 0.895, Val loss 1.763\n",
      "Ep 2 (Step 062875): Train loss 1.120, Val loss 1.764\n",
      "Ep 2 (Step 062880): Train loss 0.968, Val loss 1.763\n",
      "Ep 2 (Step 062885): Train loss 1.169, Val loss 1.762\n",
      "Ep 2 (Step 062890): Train loss 1.225, Val loss 1.760\n",
      "Ep 2 (Step 062895): Train loss 1.220, Val loss 1.759\n",
      "Ep 2 (Step 062900): Train loss 0.904, Val loss 1.759\n",
      "Ep 2 (Step 062905): Train loss 1.149, Val loss 1.759\n",
      "Ep 2 (Step 062910): Train loss 1.286, Val loss 1.759\n",
      "Ep 2 (Step 062915): Train loss 0.983, Val loss 1.759\n",
      "Ep 2 (Step 062920): Train loss 1.047, Val loss 1.760\n",
      "Ep 2 (Step 062925): Train loss 1.139, Val loss 1.761\n",
      "Ep 2 (Step 062930): Train loss 1.107, Val loss 1.764\n",
      "Ep 2 (Step 062935): Train loss 1.283, Val loss 1.767\n",
      "Ep 2 (Step 062940): Train loss 1.277, Val loss 1.768\n",
      "Ep 2 (Step 062945): Train loss 0.740, Val loss 1.770\n",
      "Ep 2 (Step 062950): Train loss 1.087, Val loss 1.771\n",
      "Ep 2 (Step 062955): Train loss 0.962, Val loss 1.771\n",
      "Ep 2 (Step 062960): Train loss 1.269, Val loss 1.771\n",
      "Ep 2 (Step 062965): Train loss 0.987, Val loss 1.771\n",
      "Ep 2 (Step 062970): Train loss 0.856, Val loss 1.772\n",
      "Ep 2 (Step 062975): Train loss 1.253, Val loss 1.772\n",
      "Ep 2 (Step 062980): Train loss 0.929, Val loss 1.773\n",
      "Ep 2 (Step 062985): Train loss 1.192, Val loss 1.775\n",
      "Ep 2 (Step 062990): Train loss 1.210, Val loss 1.778\n",
      "Ep 2 (Step 062995): Train loss 0.988, Val loss 1.779\n",
      "Ep 2 (Step 063000): Train loss 1.167, Val loss 1.781\n",
      "Ep 2 (Step 063005): Train loss 0.928, Val loss 1.780\n",
      "Ep 2 (Step 063010): Train loss 0.975, Val loss 1.779\n",
      "Ep 2 (Step 063015): Train loss 1.142, Val loss 1.778\n",
      "Ep 2 (Step 063020): Train loss 1.203, Val loss 1.775\n",
      "Ep 2 (Step 063025): Train loss 0.824, Val loss 1.775\n",
      "Ep 2 (Step 063030): Train loss 1.152, Val loss 1.774\n",
      "Ep 2 (Step 063035): Train loss 1.114, Val loss 1.773\n",
      "Ep 2 (Step 063040): Train loss 0.947, Val loss 1.771\n",
      "Ep 2 (Step 063045): Train loss 1.088, Val loss 1.769\n",
      "Ep 2 (Step 063050): Train loss 1.187, Val loss 1.768\n",
      "Ep 2 (Step 063055): Train loss 1.125, Val loss 1.766\n",
      "Ep 2 (Step 063060): Train loss 0.995, Val loss 1.765\n",
      "Ep 2 (Step 063065): Train loss 1.155, Val loss 1.765\n",
      "Ep 2 (Step 063070): Train loss 0.961, Val loss 1.766\n",
      "Ep 2 (Step 063075): Train loss 0.985, Val loss 1.767\n",
      "Ep 2 (Step 063080): Train loss 1.156, Val loss 1.768\n",
      "Ep 2 (Step 063085): Train loss 1.096, Val loss 1.769\n",
      "Ep 2 (Step 063090): Train loss 0.599, Val loss 1.769\n",
      "Ep 2 (Step 063095): Train loss 0.950, Val loss 1.768\n",
      "Ep 2 (Step 063100): Train loss 1.082, Val loss 1.768\n",
      "Ep 2 (Step 063105): Train loss 1.211, Val loss 1.768\n",
      "Ep 2 (Step 063110): Train loss 0.935, Val loss 1.767\n",
      "Ep 2 (Step 063115): Train loss 1.005, Val loss 1.766\n",
      "Ep 2 (Step 063120): Train loss 1.000, Val loss 1.766\n",
      "Ep 2 (Step 063125): Train loss 1.022, Val loss 1.765\n",
      "Ep 2 (Step 063130): Train loss 1.140, Val loss 1.764\n",
      "Ep 2 (Step 063135): Train loss 1.120, Val loss 1.764\n",
      "Ep 2 (Step 063140): Train loss 0.684, Val loss 1.764\n",
      "Ep 2 (Step 063145): Train loss 1.081, Val loss 1.764\n",
      "Ep 2 (Step 063150): Train loss 1.121, Val loss 1.763\n",
      "Ep 2 (Step 063155): Train loss 0.857, Val loss 1.764\n",
      "Ep 2 (Step 063160): Train loss 1.108, Val loss 1.765\n",
      "Ep 2 (Step 063165): Train loss 0.974, Val loss 1.765\n",
      "Ep 2 (Step 063170): Train loss 0.928, Val loss 1.766\n",
      "Ep 2 (Step 063175): Train loss 1.164, Val loss 1.767\n",
      "Ep 2 (Step 063180): Train loss 0.958, Val loss 1.767\n",
      "Ep 2 (Step 063185): Train loss 1.083, Val loss 1.768\n",
      "Ep 2 (Step 063190): Train loss 1.003, Val loss 1.769\n",
      "Ep 2 (Step 063195): Train loss 1.081, Val loss 1.769\n",
      "Ep 2 (Step 063200): Train loss 1.200, Val loss 1.770\n",
      "Ep 2 (Step 063205): Train loss 1.144, Val loss 1.772\n",
      "Ep 2 (Step 063210): Train loss 1.152, Val loss 1.773\n",
      "Ep 2 (Step 063215): Train loss 1.078, Val loss 1.774\n",
      "Ep 2 (Step 063220): Train loss 1.079, Val loss 1.776\n",
      "Ep 2 (Step 063225): Train loss 0.890, Val loss 1.776\n",
      "Ep 2 (Step 063230): Train loss 0.903, Val loss 1.776\n",
      "Ep 2 (Step 063235): Train loss 0.948, Val loss 1.774\n",
      "Ep 2 (Step 063240): Train loss 0.932, Val loss 1.774\n",
      "Ep 2 (Step 063245): Train loss 1.022, Val loss 1.773\n",
      "Ep 2 (Step 063250): Train loss 0.811, Val loss 1.773\n",
      "Ep 2 (Step 063255): Train loss 1.183, Val loss 1.774\n",
      "Ep 2 (Step 063260): Train loss 1.190, Val loss 1.775\n",
      "Ep 2 (Step 063265): Train loss 1.024, Val loss 1.777\n",
      "Ep 2 (Step 063270): Train loss 0.979, Val loss 1.777\n",
      "Ep 2 (Step 063275): Train loss 1.084, Val loss 1.778\n",
      "Ep 2 (Step 063280): Train loss 0.701, Val loss 1.777\n",
      "Ep 2 (Step 063285): Train loss 0.829, Val loss 1.775\n",
      "Ep 2 (Step 063290): Train loss 0.982, Val loss 1.774\n",
      "Ep 2 (Step 063295): Train loss 0.830, Val loss 1.775\n",
      "Ep 2 (Step 063300): Train loss 1.217, Val loss 1.775\n",
      "Ep 2 (Step 063305): Train loss 0.966, Val loss 1.774\n",
      "Ep 2 (Step 063310): Train loss 1.195, Val loss 1.773\n",
      "Ep 2 (Step 063315): Train loss 1.038, Val loss 1.772\n",
      "Ep 2 (Step 063320): Train loss 1.142, Val loss 1.771\n",
      "Ep 2 (Step 063325): Train loss 1.182, Val loss 1.771\n",
      "Ep 2 (Step 063330): Train loss 1.080, Val loss 1.770\n",
      "Ep 2 (Step 063335): Train loss 0.846, Val loss 1.770\n",
      "Ep 2 (Step 063340): Train loss 1.189, Val loss 1.770\n",
      "Ep 2 (Step 063345): Train loss 1.088, Val loss 1.770\n",
      "Ep 2 (Step 063350): Train loss 1.118, Val loss 1.772\n",
      "Ep 2 (Step 063355): Train loss 1.116, Val loss 1.773\n",
      "Ep 2 (Step 063360): Train loss 0.874, Val loss 1.772\n",
      "Ep 2 (Step 063365): Train loss 1.104, Val loss 1.771\n",
      "Ep 2 (Step 063370): Train loss 1.161, Val loss 1.771\n",
      "Ep 2 (Step 063375): Train loss 0.860, Val loss 1.771\n",
      "Ep 2 (Step 063380): Train loss 0.769, Val loss 1.772\n",
      "Ep 2 (Step 063385): Train loss 0.844, Val loss 1.771\n",
      "Ep 2 (Step 063390): Train loss 1.071, Val loss 1.772\n",
      "Ep 2 (Step 063395): Train loss 0.953, Val loss 1.773\n",
      "Ep 2 (Step 063400): Train loss 0.987, Val loss 1.774\n",
      "Ep 2 (Step 063405): Train loss 1.007, Val loss 1.774\n",
      "Ep 2 (Step 063410): Train loss 1.107, Val loss 1.774\n",
      "Ep 2 (Step 063415): Train loss 0.845, Val loss 1.774\n",
      "Ep 2 (Step 063420): Train loss 1.010, Val loss 1.773\n",
      "Ep 2 (Step 063425): Train loss 1.168, Val loss 1.772\n",
      "Ep 2 (Step 063430): Train loss 1.255, Val loss 1.771\n",
      "Ep 2 (Step 063435): Train loss 1.111, Val loss 1.771\n",
      "Ep 2 (Step 063440): Train loss 0.966, Val loss 1.771\n",
      "Ep 2 (Step 063445): Train loss 0.960, Val loss 1.769\n",
      "Ep 2 (Step 063450): Train loss 1.097, Val loss 1.769\n",
      "Ep 2 (Step 063455): Train loss 1.124, Val loss 1.768\n",
      "Ep 2 (Step 063460): Train loss 1.024, Val loss 1.767\n",
      "Ep 2 (Step 063465): Train loss 1.253, Val loss 1.769\n",
      "Ep 2 (Step 063470): Train loss 1.177, Val loss 1.770\n",
      "Ep 2 (Step 063475): Train loss 1.101, Val loss 1.770\n",
      "Ep 2 (Step 063480): Train loss 1.134, Val loss 1.771\n",
      "Ep 2 (Step 063485): Train loss 1.192, Val loss 1.772\n",
      "Ep 2 (Step 063490): Train loss 1.073, Val loss 1.773\n",
      "Ep 2 (Step 063495): Train loss 0.925, Val loss 1.773\n",
      "Ep 2 (Step 063500): Train loss 0.853, Val loss 1.773\n",
      "Ep 2 (Step 063505): Train loss 0.952, Val loss 1.773\n",
      "Ep 2 (Step 063510): Train loss 1.022, Val loss 1.774\n",
      "Ep 2 (Step 063515): Train loss 0.982, Val loss 1.777\n",
      "Ep 2 (Step 063520): Train loss 1.074, Val loss 1.778\n",
      "Ep 2 (Step 063525): Train loss 1.094, Val loss 1.777\n",
      "Ep 2 (Step 063530): Train loss 0.969, Val loss 1.776\n",
      "Ep 2 (Step 063535): Train loss 0.899, Val loss 1.775\n",
      "Ep 2 (Step 063540): Train loss 0.949, Val loss 1.772\n",
      "Ep 2 (Step 063545): Train loss 1.192, Val loss 1.772\n",
      "Ep 2 (Step 063550): Train loss 0.948, Val loss 1.772\n",
      "Ep 2 (Step 063555): Train loss 1.135, Val loss 1.773\n",
      "Ep 2 (Step 063560): Train loss 0.980, Val loss 1.774\n",
      "Ep 2 (Step 063565): Train loss 1.263, Val loss 1.774\n",
      "Ep 2 (Step 063570): Train loss 0.683, Val loss 1.774\n",
      "Ep 2 (Step 063575): Train loss 1.098, Val loss 1.774\n",
      "Ep 2 (Step 063580): Train loss 1.070, Val loss 1.775\n",
      "Ep 2 (Step 063585): Train loss 1.250, Val loss 1.775\n",
      "Ep 2 (Step 063590): Train loss 1.243, Val loss 1.772\n",
      "Ep 2 (Step 063595): Train loss 1.429, Val loss 1.769\n",
      "Ep 2 (Step 063600): Train loss 1.177, Val loss 1.767\n",
      "Ep 2 (Step 063605): Train loss 0.857, Val loss 1.764\n",
      "Ep 2 (Step 063610): Train loss 0.890, Val loss 1.763\n",
      "Ep 2 (Step 063615): Train loss 0.924, Val loss 1.762\n",
      "Ep 2 (Step 063620): Train loss 1.049, Val loss 1.762\n",
      "Ep 2 (Step 063625): Train loss 1.129, Val loss 1.762\n",
      "Ep 2 (Step 063630): Train loss 0.910, Val loss 1.762\n",
      "Ep 2 (Step 063635): Train loss 1.158, Val loss 1.763\n",
      "Ep 2 (Step 063640): Train loss 0.900, Val loss 1.764\n",
      "Ep 2 (Step 063645): Train loss 1.214, Val loss 1.763\n",
      "Ep 2 (Step 063650): Train loss 1.068, Val loss 1.765\n",
      "Ep 2 (Step 063655): Train loss 1.199, Val loss 1.767\n",
      "Ep 2 (Step 063660): Train loss 0.994, Val loss 1.769\n",
      "Ep 2 (Step 063665): Train loss 1.104, Val loss 1.770\n",
      "Ep 2 (Step 063670): Train loss 1.050, Val loss 1.769\n",
      "Ep 2 (Step 063675): Train loss 0.910, Val loss 1.768\n",
      "Ep 2 (Step 063680): Train loss 1.150, Val loss 1.767\n",
      "Ep 2 (Step 063685): Train loss 0.986, Val loss 1.766\n",
      "Ep 2 (Step 063690): Train loss 1.236, Val loss 1.767\n",
      "Ep 2 (Step 063695): Train loss 0.822, Val loss 1.766\n",
      "Ep 2 (Step 063700): Train loss 0.899, Val loss 1.765\n",
      "Ep 2 (Step 063705): Train loss 1.088, Val loss 1.764\n",
      "Ep 2 (Step 063710): Train loss 0.911, Val loss 1.763\n",
      "Ep 2 (Step 063715): Train loss 1.169, Val loss 1.762\n",
      "Ep 2 (Step 063720): Train loss 1.079, Val loss 1.762\n",
      "Ep 2 (Step 063725): Train loss 1.103, Val loss 1.761\n",
      "Ep 2 (Step 063730): Train loss 1.019, Val loss 1.758\n",
      "Ep 2 (Step 063735): Train loss 1.063, Val loss 1.757\n",
      "Ep 2 (Step 063740): Train loss 1.184, Val loss 1.757\n",
      "Ep 2 (Step 063745): Train loss 1.200, Val loss 1.756\n",
      "Ep 2 (Step 063750): Train loss 1.179, Val loss 1.755\n",
      "Ep 2 (Step 063755): Train loss 0.989, Val loss 1.754\n",
      "Ep 2 (Step 063760): Train loss 1.319, Val loss 1.754\n",
      "Ep 2 (Step 063765): Train loss 1.221, Val loss 1.754\n",
      "Ep 2 (Step 063770): Train loss 1.072, Val loss 1.755\n",
      "Ep 2 (Step 063775): Train loss 0.886, Val loss 1.755\n",
      "Ep 2 (Step 063780): Train loss 1.023, Val loss 1.755\n",
      "Ep 2 (Step 063785): Train loss 1.030, Val loss 1.755\n",
      "Ep 2 (Step 063790): Train loss 1.030, Val loss 1.754\n",
      "Ep 2 (Step 063795): Train loss 1.027, Val loss 1.756\n",
      "Ep 2 (Step 063800): Train loss 0.622, Val loss 1.756\n",
      "Ep 2 (Step 063805): Train loss 0.962, Val loss 1.756\n",
      "Ep 2 (Step 063810): Train loss 1.007, Val loss 1.758\n",
      "Ep 2 (Step 063815): Train loss 1.181, Val loss 1.758\n",
      "Ep 2 (Step 063820): Train loss 1.021, Val loss 1.758\n",
      "Ep 2 (Step 063825): Train loss 1.091, Val loss 1.759\n",
      "Ep 2 (Step 063830): Train loss 0.918, Val loss 1.759\n",
      "Ep 2 (Step 063835): Train loss 1.361, Val loss 1.758\n",
      "Ep 2 (Step 063840): Train loss 1.021, Val loss 1.758\n",
      "Ep 2 (Step 063845): Train loss 0.968, Val loss 1.757\n",
      "Ep 2 (Step 063850): Train loss 1.060, Val loss 1.757\n",
      "Ep 2 (Step 063855): Train loss 1.290, Val loss 1.759\n",
      "Ep 2 (Step 063860): Train loss 1.068, Val loss 1.760\n",
      "Ep 2 (Step 063865): Train loss 0.933, Val loss 1.761\n",
      "Ep 2 (Step 063870): Train loss 0.922, Val loss 1.762\n",
      "Ep 2 (Step 063875): Train loss 1.009, Val loss 1.761\n",
      "Ep 2 (Step 063880): Train loss 0.883, Val loss 1.761\n",
      "Ep 2 (Step 063885): Train loss 0.821, Val loss 1.759\n",
      "Ep 2 (Step 063890): Train loss 0.888, Val loss 1.759\n",
      "Ep 2 (Step 063895): Train loss 0.853, Val loss 1.759\n",
      "Ep 2 (Step 063900): Train loss 0.835, Val loss 1.759\n",
      "Ep 2 (Step 063905): Train loss 0.826, Val loss 1.760\n",
      "Ep 2 (Step 063910): Train loss 1.026, Val loss 1.761\n",
      "Ep 2 (Step 063915): Train loss 0.691, Val loss 1.761\n",
      "Ep 2 (Step 063920): Train loss 0.957, Val loss 1.761\n",
      "Ep 2 (Step 063925): Train loss 1.110, Val loss 1.761\n",
      "Ep 2 (Step 063930): Train loss 0.982, Val loss 1.762\n",
      "Ep 2 (Step 063935): Train loss 0.909, Val loss 1.764\n",
      "Ep 2 (Step 063940): Train loss 0.871, Val loss 1.765\n",
      "Ep 2 (Step 063945): Train loss 1.041, Val loss 1.767\n",
      "Ep 2 (Step 063950): Train loss 1.278, Val loss 1.768\n",
      "Ep 2 (Step 063955): Train loss 1.202, Val loss 1.770\n",
      "Ep 2 (Step 063960): Train loss 1.002, Val loss 1.770\n",
      "Ep 2 (Step 063965): Train loss 0.934, Val loss 1.769\n",
      "Ep 2 (Step 063970): Train loss 1.074, Val loss 1.769\n",
      "Ep 2 (Step 063975): Train loss 1.092, Val loss 1.768\n",
      "Ep 2 (Step 063980): Train loss 0.920, Val loss 1.768\n",
      "Ep 2 (Step 063985): Train loss 1.078, Val loss 1.769\n",
      "Ep 2 (Step 063990): Train loss 0.889, Val loss 1.770\n",
      "Ep 2 (Step 063995): Train loss 1.052, Val loss 1.768\n",
      "Ep 2 (Step 064000): Train loss 1.055, Val loss 1.768\n",
      "Ep 2 (Step 064005): Train loss 1.150, Val loss 1.768\n",
      "Ep 2 (Step 064010): Train loss 0.893, Val loss 1.767\n",
      "Ep 2 (Step 064015): Train loss 0.808, Val loss 1.767\n",
      "Ep 2 (Step 064020): Train loss 1.181, Val loss 1.765\n",
      "Ep 2 (Step 064025): Train loss 1.053, Val loss 1.764\n",
      "Ep 2 (Step 064030): Train loss 0.829, Val loss 1.763\n",
      "Ep 2 (Step 064035): Train loss 1.258, Val loss 1.761\n",
      "Ep 2 (Step 064040): Train loss 1.283, Val loss 1.760\n",
      "Ep 2 (Step 064045): Train loss 1.030, Val loss 1.759\n",
      "Ep 2 (Step 064050): Train loss 1.068, Val loss 1.757\n",
      "Ep 2 (Step 064055): Train loss 0.931, Val loss 1.759\n",
      "Ep 2 (Step 064060): Train loss 0.896, Val loss 1.761\n",
      "Ep 2 (Step 064065): Train loss 1.178, Val loss 1.760\n",
      "Ep 2 (Step 064070): Train loss 1.261, Val loss 1.759\n",
      "Ep 2 (Step 064075): Train loss 0.783, Val loss 1.759\n",
      "Ep 2 (Step 064080): Train loss 1.175, Val loss 1.761\n",
      "Ep 2 (Step 064085): Train loss 1.020, Val loss 1.761\n",
      "Ep 2 (Step 064090): Train loss 1.234, Val loss 1.763\n",
      "Ep 2 (Step 064095): Train loss 1.104, Val loss 1.765\n",
      "Ep 2 (Step 064100): Train loss 0.731, Val loss 1.766\n",
      "Ep 2 (Step 064105): Train loss 0.922, Val loss 1.766\n",
      "Ep 2 (Step 064110): Train loss 0.819, Val loss 1.767\n",
      "Ep 2 (Step 064115): Train loss 1.011, Val loss 1.769\n",
      "Ep 2 (Step 064120): Train loss 1.190, Val loss 1.768\n",
      "Ep 2 (Step 064125): Train loss 1.194, Val loss 1.766\n",
      "Ep 2 (Step 064130): Train loss 0.897, Val loss 1.764\n",
      "Ep 2 (Step 064135): Train loss 1.316, Val loss 1.762\n",
      "Ep 2 (Step 064140): Train loss 1.229, Val loss 1.761\n",
      "Ep 2 (Step 064145): Train loss 0.862, Val loss 1.761\n",
      "Ep 2 (Step 064150): Train loss 0.897, Val loss 1.761\n",
      "Ep 2 (Step 064155): Train loss 0.990, Val loss 1.761\n",
      "Ep 2 (Step 064160): Train loss 1.335, Val loss 1.762\n",
      "Ep 2 (Step 064165): Train loss 1.307, Val loss 1.761\n",
      "Ep 2 (Step 064170): Train loss 1.256, Val loss 1.763\n",
      "Ep 2 (Step 064175): Train loss 1.017, Val loss 1.764\n",
      "Ep 2 (Step 064180): Train loss 0.999, Val loss 1.764\n",
      "Ep 2 (Step 064185): Train loss 1.022, Val loss 1.764\n",
      "Ep 2 (Step 064190): Train loss 0.946, Val loss 1.764\n",
      "Ep 2 (Step 064195): Train loss 1.015, Val loss 1.763\n",
      "Ep 2 (Step 064200): Train loss 0.903, Val loss 1.762\n",
      "Ep 2 (Step 064205): Train loss 1.181, Val loss 1.760\n",
      "Ep 2 (Step 064210): Train loss 1.100, Val loss 1.758\n",
      "Ep 2 (Step 064215): Train loss 0.879, Val loss 1.758\n",
      "Ep 2 (Step 064220): Train loss 1.010, Val loss 1.758\n",
      "Ep 2 (Step 064225): Train loss 1.196, Val loss 1.758\n",
      "Ep 2 (Step 064230): Train loss 1.088, Val loss 1.759\n",
      "Ep 2 (Step 064235): Train loss 0.967, Val loss 1.760\n",
      "Ep 2 (Step 064240): Train loss 1.082, Val loss 1.760\n",
      "Ep 2 (Step 064245): Train loss 0.896, Val loss 1.759\n",
      "Ep 2 (Step 064250): Train loss 0.942, Val loss 1.759\n",
      "Ep 2 (Step 064255): Train loss 0.938, Val loss 1.758\n",
      "Ep 2 (Step 064260): Train loss 1.018, Val loss 1.758\n",
      "Ep 2 (Step 064265): Train loss 1.023, Val loss 1.757\n",
      "Ep 2 (Step 064270): Train loss 0.963, Val loss 1.755\n",
      "Ep 2 (Step 064275): Train loss 1.201, Val loss 1.753\n",
      "Ep 2 (Step 064280): Train loss 0.865, Val loss 1.753\n",
      "Ep 2 (Step 064285): Train loss 1.099, Val loss 1.754\n",
      "Ep 2 (Step 064290): Train loss 1.183, Val loss 1.754\n",
      "Ep 2 (Step 064295): Train loss 1.261, Val loss 1.754\n",
      "Ep 2 (Step 064300): Train loss 1.025, Val loss 1.753\n",
      "Ep 2 (Step 064305): Train loss 0.921, Val loss 1.753\n",
      "Ep 2 (Step 064310): Train loss 1.073, Val loss 1.751\n",
      "Ep 2 (Step 064315): Train loss 1.182, Val loss 1.751\n",
      "Ep 2 (Step 064320): Train loss 0.834, Val loss 1.751\n",
      "Ep 2 (Step 064325): Train loss 1.066, Val loss 1.751\n",
      "Ep 2 (Step 064330): Train loss 1.045, Val loss 1.751\n",
      "Ep 2 (Step 064335): Train loss 1.290, Val loss 1.749\n",
      "Ep 2 (Step 064340): Train loss 0.951, Val loss 1.748\n",
      "Ep 2 (Step 064345): Train loss 1.051, Val loss 1.746\n",
      "Ep 2 (Step 064350): Train loss 1.143, Val loss 1.746\n",
      "Ep 2 (Step 064355): Train loss 1.167, Val loss 1.746\n",
      "Ep 2 (Step 064360): Train loss 1.025, Val loss 1.747\n",
      "Ep 2 (Step 064365): Train loss 1.148, Val loss 1.748\n",
      "Ep 2 (Step 064370): Train loss 1.268, Val loss 1.749\n",
      "Ep 2 (Step 064375): Train loss 1.244, Val loss 1.750\n",
      "Ep 2 (Step 064380): Train loss 1.263, Val loss 1.751\n",
      "Ep 2 (Step 064385): Train loss 1.303, Val loss 1.752\n",
      "Ep 2 (Step 064390): Train loss 0.734, Val loss 1.752\n",
      "Ep 2 (Step 064395): Train loss 1.122, Val loss 1.750\n",
      "Ep 2 (Step 064400): Train loss 0.935, Val loss 1.750\n",
      "Ep 2 (Step 064405): Train loss 0.958, Val loss 1.750\n",
      "Ep 2 (Step 064410): Train loss 1.156, Val loss 1.750\n",
      "Ep 2 (Step 064415): Train loss 0.831, Val loss 1.751\n",
      "Ep 2 (Step 064420): Train loss 0.869, Val loss 1.753\n",
      "Ep 2 (Step 064425): Train loss 1.134, Val loss 1.754\n",
      "Ep 2 (Step 064430): Train loss 1.027, Val loss 1.755\n",
      "Ep 2 (Step 064435): Train loss 1.026, Val loss 1.756\n",
      "Ep 2 (Step 064440): Train loss 0.767, Val loss 1.755\n",
      "Ep 2 (Step 064445): Train loss 1.048, Val loss 1.754\n",
      "Ep 2 (Step 064450): Train loss 1.054, Val loss 1.754\n",
      "Ep 2 (Step 064455): Train loss 0.776, Val loss 1.754\n",
      "Ep 2 (Step 064460): Train loss 1.016, Val loss 1.754\n",
      "Ep 2 (Step 064465): Train loss 0.831, Val loss 1.754\n",
      "Ep 2 (Step 064470): Train loss 1.077, Val loss 1.756\n",
      "Ep 2 (Step 064475): Train loss 1.195, Val loss 1.756\n",
      "Ep 2 (Step 064480): Train loss 1.121, Val loss 1.755\n",
      "Ep 2 (Step 064485): Train loss 0.926, Val loss 1.755\n",
      "Ep 2 (Step 064490): Train loss 0.989, Val loss 1.755\n",
      "Ep 2 (Step 064495): Train loss 0.874, Val loss 1.756\n",
      "Ep 2 (Step 064500): Train loss 1.113, Val loss 1.756\n",
      "Ep 2 (Step 064505): Train loss 0.892, Val loss 1.757\n",
      "Ep 2 (Step 064510): Train loss 1.148, Val loss 1.755\n",
      "Ep 2 (Step 064515): Train loss 1.023, Val loss 1.754\n",
      "Ep 2 (Step 064520): Train loss 1.032, Val loss 1.753\n",
      "Ep 2 (Step 064525): Train loss 0.999, Val loss 1.754\n",
      "Ep 2 (Step 064530): Train loss 1.174, Val loss 1.755\n",
      "Ep 2 (Step 064535): Train loss 0.761, Val loss 1.754\n",
      "Ep 2 (Step 064540): Train loss 1.464, Val loss 1.753\n",
      "Ep 2 (Step 064545): Train loss 0.966, Val loss 1.752\n",
      "Ep 2 (Step 064550): Train loss 0.831, Val loss 1.753\n",
      "Ep 2 (Step 064555): Train loss 1.114, Val loss 1.756\n",
      "Ep 2 (Step 064560): Train loss 1.062, Val loss 1.756\n",
      "Ep 2 (Step 064565): Train loss 1.111, Val loss 1.755\n",
      "Ep 2 (Step 064570): Train loss 1.357, Val loss 1.755\n",
      "Ep 2 (Step 064575): Train loss 1.027, Val loss 1.756\n",
      "Ep 2 (Step 064580): Train loss 1.216, Val loss 1.755\n",
      "Ep 2 (Step 064585): Train loss 0.921, Val loss 1.753\n",
      "Ep 2 (Step 064590): Train loss 1.204, Val loss 1.755\n",
      "Ep 2 (Step 064595): Train loss 1.054, Val loss 1.756\n",
      "Ep 2 (Step 064600): Train loss 1.225, Val loss 1.758\n",
      "Ep 2 (Step 064605): Train loss 1.135, Val loss 1.761\n",
      "Ep 2 (Step 064610): Train loss 1.151, Val loss 1.765\n",
      "Ep 2 (Step 064615): Train loss 1.009, Val loss 1.765\n",
      "Ep 2 (Step 064620): Train loss 0.964, Val loss 1.766\n",
      "Ep 2 (Step 064625): Train loss 1.150, Val loss 1.765\n",
      "Ep 2 (Step 064630): Train loss 1.264, Val loss 1.764\n",
      "Ep 2 (Step 064635): Train loss 1.234, Val loss 1.763\n",
      "Ep 2 (Step 064640): Train loss 1.268, Val loss 1.763\n",
      "Ep 2 (Step 064645): Train loss 1.183, Val loss 1.764\n",
      "Ep 2 (Step 064650): Train loss 0.877, Val loss 1.764\n",
      "Ep 2 (Step 064655): Train loss 0.866, Val loss 1.764\n",
      "Ep 2 (Step 064660): Train loss 0.855, Val loss 1.764\n",
      "Ep 2 (Step 064665): Train loss 1.130, Val loss 1.765\n",
      "Ep 2 (Step 064670): Train loss 0.975, Val loss 1.766\n",
      "Ep 2 (Step 064675): Train loss 1.080, Val loss 1.767\n",
      "Ep 2 (Step 064680): Train loss 1.053, Val loss 1.768\n",
      "Ep 2 (Step 064685): Train loss 1.070, Val loss 1.769\n",
      "Ep 2 (Step 064690): Train loss 0.909, Val loss 1.769\n",
      "Ep 2 (Step 064695): Train loss 0.991, Val loss 1.767\n",
      "Ep 2 (Step 064700): Train loss 1.062, Val loss 1.766\n",
      "Ep 2 (Step 064705): Train loss 0.973, Val loss 1.764\n",
      "Ep 2 (Step 064710): Train loss 1.031, Val loss 1.764\n",
      "Ep 2 (Step 064715): Train loss 0.928, Val loss 1.765\n",
      "Ep 2 (Step 064720): Train loss 1.063, Val loss 1.764\n",
      "Ep 2 (Step 064725): Train loss 1.075, Val loss 1.764\n",
      "Ep 2 (Step 064730): Train loss 0.835, Val loss 1.763\n",
      "Ep 2 (Step 064735): Train loss 1.224, Val loss 1.764\n",
      "Ep 2 (Step 064740): Train loss 1.347, Val loss 1.765\n",
      "Ep 2 (Step 064745): Train loss 0.804, Val loss 1.767\n",
      "Ep 2 (Step 064750): Train loss 1.058, Val loss 1.769\n",
      "Ep 2 (Step 064755): Train loss 1.254, Val loss 1.771\n",
      "Ep 2 (Step 064760): Train loss 0.891, Val loss 1.773\n",
      "Ep 2 (Step 064765): Train loss 0.895, Val loss 1.774\n",
      "Ep 2 (Step 064770): Train loss 1.182, Val loss 1.774\n",
      "Ep 2 (Step 064775): Train loss 1.058, Val loss 1.772\n",
      "Ep 2 (Step 064780): Train loss 0.993, Val loss 1.772\n",
      "Ep 2 (Step 064785): Train loss 1.050, Val loss 1.774\n",
      "Ep 2 (Step 064790): Train loss 0.850, Val loss 1.773\n",
      "Ep 2 (Step 064795): Train loss 1.384, Val loss 1.772\n",
      "Ep 2 (Step 064800): Train loss 1.167, Val loss 1.772\n",
      "Ep 2 (Step 064805): Train loss 1.044, Val loss 1.772\n",
      "Ep 2 (Step 064810): Train loss 1.238, Val loss 1.770\n",
      "Ep 2 (Step 064815): Train loss 0.923, Val loss 1.769\n",
      "Ep 2 (Step 064820): Train loss 1.193, Val loss 1.768\n",
      "Ep 2 (Step 064825): Train loss 1.290, Val loss 1.767\n",
      "Ep 2 (Step 064830): Train loss 0.850, Val loss 1.766\n",
      "Ep 2 (Step 064835): Train loss 0.884, Val loss 1.766\n",
      "Ep 2 (Step 064840): Train loss 0.988, Val loss 1.766\n",
      "Ep 2 (Step 064845): Train loss 0.945, Val loss 1.768\n",
      "Ep 2 (Step 064850): Train loss 0.965, Val loss 1.769\n",
      "Ep 2 (Step 064855): Train loss 0.881, Val loss 1.770\n",
      "Ep 2 (Step 064860): Train loss 0.991, Val loss 1.770\n",
      "Ep 2 (Step 064865): Train loss 0.970, Val loss 1.771\n",
      "Ep 2 (Step 064870): Train loss 0.963, Val loss 1.771\n",
      "Ep 2 (Step 064875): Train loss 0.929, Val loss 1.771\n",
      "Ep 2 (Step 064880): Train loss 0.854, Val loss 1.771\n",
      "Ep 2 (Step 064885): Train loss 1.002, Val loss 1.771\n",
      "Ep 2 (Step 064890): Train loss 1.121, Val loss 1.771\n",
      "Ep 2 (Step 064895): Train loss 1.085, Val loss 1.769\n",
      "Ep 2 (Step 064900): Train loss 1.294, Val loss 1.769\n",
      "Ep 2 (Step 064905): Train loss 1.033, Val loss 1.769\n",
      "Ep 2 (Step 064910): Train loss 1.112, Val loss 1.768\n",
      "Ep 2 (Step 064915): Train loss 1.141, Val loss 1.768\n",
      "Ep 2 (Step 064920): Train loss 0.819, Val loss 1.767\n",
      "Ep 2 (Step 064925): Train loss 1.017, Val loss 1.768\n",
      "Ep 2 (Step 064930): Train loss 1.023, Val loss 1.769\n",
      "Ep 2 (Step 064935): Train loss 1.131, Val loss 1.769\n",
      "Ep 2 (Step 064940): Train loss 0.968, Val loss 1.769\n",
      "Ep 2 (Step 064945): Train loss 1.243, Val loss 1.770\n",
      "Ep 2 (Step 064950): Train loss 1.133, Val loss 1.770\n",
      "Ep 2 (Step 064955): Train loss 0.934, Val loss 1.771\n",
      "Ep 2 (Step 064960): Train loss 0.812, Val loss 1.772\n",
      "Ep 2 (Step 064965): Train loss 0.729, Val loss 1.773\n",
      "Ep 2 (Step 064970): Train loss 1.233, Val loss 1.774\n",
      "Ep 2 (Step 064975): Train loss 1.030, Val loss 1.774\n",
      "Ep 2 (Step 064980): Train loss 0.925, Val loss 1.775\n",
      "Ep 2 (Step 064985): Train loss 1.095, Val loss 1.776\n",
      "Ep 2 (Step 064990): Train loss 1.258, Val loss 1.778\n",
      "Ep 2 (Step 064995): Train loss 0.987, Val loss 1.780\n",
      "Ep 2 (Step 065000): Train loss 0.723, Val loss 1.780\n",
      "Ep 2 (Step 065005): Train loss 0.870, Val loss 1.781\n",
      "Ep 2 (Step 065010): Train loss 0.951, Val loss 1.780\n",
      "Ep 2 (Step 065015): Train loss 0.912, Val loss 1.780\n",
      "Ep 2 (Step 065020): Train loss 1.154, Val loss 1.779\n",
      "Ep 2 (Step 065025): Train loss 0.983, Val loss 1.778\n",
      "Ep 2 (Step 065030): Train loss 0.844, Val loss 1.777\n",
      "Ep 2 (Step 065035): Train loss 1.112, Val loss 1.776\n",
      "Ep 2 (Step 065040): Train loss 1.174, Val loss 1.774\n",
      "Ep 2 (Step 065045): Train loss 0.965, Val loss 1.775\n",
      "Ep 2 (Step 065050): Train loss 1.302, Val loss 1.777\n",
      "Ep 2 (Step 065055): Train loss 1.009, Val loss 1.778\n",
      "Ep 2 (Step 065060): Train loss 0.980, Val loss 1.778\n",
      "Ep 2 (Step 065065): Train loss 1.109, Val loss 1.776\n",
      "Ep 2 (Step 065070): Train loss 1.157, Val loss 1.774\n",
      "Ep 2 (Step 065075): Train loss 0.993, Val loss 1.774\n",
      "Ep 2 (Step 065080): Train loss 1.028, Val loss 1.775\n",
      "Ep 2 (Step 065085): Train loss 1.059, Val loss 1.776\n",
      "Ep 2 (Step 065090): Train loss 0.980, Val loss 1.778\n",
      "Ep 2 (Step 065095): Train loss 1.209, Val loss 1.778\n",
      "Ep 2 (Step 065100): Train loss 1.299, Val loss 1.777\n",
      "Ep 2 (Step 065105): Train loss 1.149, Val loss 1.777\n",
      "Ep 2 (Step 065110): Train loss 1.247, Val loss 1.779\n",
      "Ep 2 (Step 065115): Train loss 1.222, Val loss 1.781\n",
      "Ep 2 (Step 065120): Train loss 1.054, Val loss 1.783\n",
      "Ep 2 (Step 065125): Train loss 0.936, Val loss 1.785\n",
      "Ep 2 (Step 065130): Train loss 1.061, Val loss 1.786\n",
      "Ep 2 (Step 065135): Train loss 1.160, Val loss 1.787\n",
      "Ep 2 (Step 065140): Train loss 1.022, Val loss 1.787\n",
      "Ep 2 (Step 065145): Train loss 0.993, Val loss 1.787\n",
      "Ep 2 (Step 065150): Train loss 0.836, Val loss 1.786\n",
      "Ep 2 (Step 065155): Train loss 1.139, Val loss 1.785\n",
      "Ep 2 (Step 065160): Train loss 1.213, Val loss 1.784\n",
      "Ep 2 (Step 065165): Train loss 0.983, Val loss 1.783\n",
      "Ep 2 (Step 065170): Train loss 1.153, Val loss 1.783\n",
      "Ep 2 (Step 065175): Train loss 0.618, Val loss 1.783\n",
      "Ep 2 (Step 065180): Train loss 0.877, Val loss 1.785\n",
      "Ep 2 (Step 065185): Train loss 1.197, Val loss 1.784\n",
      "Ep 2 (Step 065190): Train loss 1.085, Val loss 1.783\n",
      "Ep 2 (Step 065195): Train loss 1.141, Val loss 1.782\n",
      "Ep 2 (Step 065200): Train loss 0.927, Val loss 1.783\n",
      "Ep 2 (Step 065205): Train loss 1.089, Val loss 1.784\n",
      "Ep 2 (Step 065210): Train loss 1.146, Val loss 1.785\n",
      "Ep 2 (Step 065215): Train loss 0.927, Val loss 1.785\n",
      "Ep 2 (Step 065220): Train loss 1.195, Val loss 1.784\n",
      "Ep 2 (Step 065225): Train loss 1.090, Val loss 1.784\n",
      "Ep 2 (Step 065230): Train loss 1.143, Val loss 1.784\n",
      "Ep 2 (Step 065235): Train loss 1.357, Val loss 1.785\n",
      "Ep 2 (Step 065240): Train loss 0.796, Val loss 1.787\n",
      "Ep 2 (Step 065245): Train loss 1.128, Val loss 1.789\n",
      "Ep 2 (Step 065250): Train loss 1.139, Val loss 1.789\n",
      "Ep 2 (Step 065255): Train loss 0.962, Val loss 1.788\n",
      "Ep 2 (Step 065260): Train loss 1.084, Val loss 1.787\n",
      "Ep 2 (Step 065265): Train loss 0.939, Val loss 1.786\n",
      "Ep 2 (Step 065270): Train loss 1.025, Val loss 1.785\n",
      "Ep 2 (Step 065275): Train loss 1.042, Val loss 1.785\n",
      "Ep 2 (Step 065280): Train loss 1.173, Val loss 1.784\n",
      "Ep 2 (Step 065285): Train loss 1.088, Val loss 1.782\n",
      "Ep 2 (Step 065290): Train loss 1.026, Val loss 1.781\n",
      "Ep 2 (Step 065295): Train loss 1.335, Val loss 1.781\n",
      "Ep 2 (Step 065300): Train loss 1.078, Val loss 1.781\n",
      "Ep 2 (Step 065305): Train loss 1.020, Val loss 1.781\n",
      "Ep 2 (Step 065310): Train loss 1.093, Val loss 1.780\n",
      "Ep 2 (Step 065315): Train loss 0.968, Val loss 1.779\n",
      "Ep 2 (Step 065320): Train loss 1.060, Val loss 1.779\n",
      "Ep 2 (Step 065325): Train loss 1.122, Val loss 1.778\n",
      "Ep 2 (Step 065330): Train loss 1.027, Val loss 1.777\n",
      "Ep 2 (Step 065335): Train loss 0.989, Val loss 1.777\n",
      "Ep 2 (Step 065340): Train loss 0.814, Val loss 1.776\n",
      "Ep 2 (Step 065345): Train loss 1.026, Val loss 1.777\n",
      "Ep 2 (Step 065350): Train loss 1.142, Val loss 1.777\n",
      "Ep 2 (Step 065355): Train loss 1.013, Val loss 1.777\n",
      "Ep 2 (Step 065360): Train loss 1.013, Val loss 1.779\n",
      "Ep 2 (Step 065365): Train loss 1.286, Val loss 1.781\n",
      "Ep 2 (Step 065370): Train loss 0.913, Val loss 1.782\n",
      "Ep 2 (Step 065375): Train loss 0.939, Val loss 1.782\n",
      "Ep 2 (Step 065380): Train loss 1.150, Val loss 1.782\n",
      "Ep 2 (Step 065385): Train loss 1.040, Val loss 1.781\n",
      "Ep 2 (Step 065390): Train loss 0.867, Val loss 1.780\n",
      "Ep 2 (Step 065395): Train loss 0.907, Val loss 1.781\n",
      "Ep 2 (Step 065400): Train loss 1.224, Val loss 1.780\n",
      "Ep 2 (Step 065405): Train loss 1.125, Val loss 1.779\n",
      "Ep 2 (Step 065410): Train loss 1.033, Val loss 1.780\n",
      "Ep 2 (Step 065415): Train loss 1.076, Val loss 1.783\n",
      "Ep 2 (Step 065420): Train loss 1.002, Val loss 1.785\n",
      "Ep 2 (Step 065425): Train loss 1.023, Val loss 1.786\n",
      "Ep 2 (Step 065430): Train loss 0.853, Val loss 1.785\n",
      "Ep 2 (Step 065435): Train loss 1.121, Val loss 1.785\n",
      "Ep 2 (Step 065440): Train loss 0.937, Val loss 1.785\n",
      "Ep 2 (Step 065445): Train loss 1.065, Val loss 1.785\n",
      "Ep 2 (Step 065450): Train loss 0.950, Val loss 1.783\n",
      "Ep 2 (Step 065455): Train loss 1.145, Val loss 1.783\n",
      "Ep 2 (Step 065460): Train loss 0.870, Val loss 1.782\n",
      "Ep 2 (Step 065465): Train loss 1.128, Val loss 1.782\n",
      "Ep 2 (Step 065470): Train loss 0.982, Val loss 1.782\n",
      "Ep 2 (Step 065475): Train loss 0.813, Val loss 1.782\n",
      "Ep 2 (Step 065480): Train loss 0.943, Val loss 1.784\n",
      "Ep 2 (Step 065485): Train loss 0.954, Val loss 1.783\n",
      "Ep 2 (Step 065490): Train loss 0.956, Val loss 1.782\n",
      "Ep 2 (Step 065495): Train loss 0.979, Val loss 1.781\n",
      "Ep 2 (Step 065500): Train loss 1.076, Val loss 1.780\n",
      "Ep 2 (Step 065505): Train loss 0.873, Val loss 1.779\n",
      "Ep 2 (Step 065510): Train loss 1.066, Val loss 1.777\n",
      "Ep 2 (Step 065515): Train loss 1.131, Val loss 1.776\n",
      "Ep 2 (Step 065520): Train loss 1.046, Val loss 1.775\n",
      "Ep 2 (Step 065525): Train loss 0.973, Val loss 1.776\n",
      "Ep 2 (Step 065530): Train loss 1.135, Val loss 1.777\n",
      "Ep 2 (Step 065535): Train loss 1.041, Val loss 1.778\n",
      "Ep 2 (Step 065540): Train loss 0.959, Val loss 1.780\n",
      "Ep 2 (Step 065545): Train loss 1.047, Val loss 1.781\n",
      "Ep 2 (Step 065550): Train loss 0.797, Val loss 1.782\n",
      "Ep 2 (Step 065555): Train loss 0.785, Val loss 1.782\n",
      "Ep 2 (Step 065560): Train loss 0.850, Val loss 1.781\n",
      "Ep 2 (Step 065565): Train loss 1.045, Val loss 1.780\n",
      "Ep 2 (Step 065570): Train loss 1.209, Val loss 1.781\n",
      "Ep 2 (Step 065575): Train loss 1.050, Val loss 1.779\n",
      "Ep 2 (Step 065580): Train loss 0.871, Val loss 1.778\n",
      "Ep 2 (Step 065585): Train loss 1.280, Val loss 1.777\n",
      "Ep 2 (Step 065590): Train loss 0.995, Val loss 1.776\n",
      "Ep 2 (Step 065595): Train loss 1.384, Val loss 1.775\n",
      "Ep 2 (Step 065600): Train loss 1.138, Val loss 1.774\n",
      "Ep 2 (Step 065605): Train loss 0.881, Val loss 1.772\n",
      "Ep 2 (Step 065610): Train loss 0.791, Val loss 1.772\n",
      "Ep 2 (Step 065615): Train loss 0.799, Val loss 1.774\n",
      "Ep 2 (Step 065620): Train loss 1.269, Val loss 1.777\n",
      "Ep 2 (Step 065625): Train loss 0.907, Val loss 1.780\n",
      "Ep 2 (Step 065630): Train loss 0.799, Val loss 1.781\n",
      "Ep 2 (Step 065635): Train loss 1.091, Val loss 1.782\n",
      "Ep 2 (Step 065640): Train loss 1.327, Val loss 1.783\n",
      "Ep 2 (Step 065645): Train loss 1.052, Val loss 1.782\n",
      "Ep 2 (Step 065650): Train loss 0.992, Val loss 1.781\n",
      "Ep 2 (Step 065655): Train loss 0.921, Val loss 1.779\n",
      "Ep 2 (Step 065660): Train loss 0.995, Val loss 1.778\n",
      "Ep 2 (Step 065665): Train loss 1.088, Val loss 1.778\n",
      "Ep 2 (Step 065670): Train loss 1.041, Val loss 1.778\n",
      "Ep 2 (Step 065675): Train loss 0.977, Val loss 1.778\n",
      "Ep 2 (Step 065680): Train loss 1.199, Val loss 1.778\n",
      "Ep 2 (Step 065685): Train loss 1.158, Val loss 1.778\n",
      "Ep 2 (Step 065690): Train loss 0.861, Val loss 1.778\n",
      "Ep 2 (Step 065695): Train loss 1.162, Val loss 1.778\n",
      "Ep 2 (Step 065700): Train loss 1.013, Val loss 1.778\n",
      "Ep 2 (Step 065705): Train loss 0.973, Val loss 1.779\n",
      "Ep 2 (Step 065710): Train loss 0.778, Val loss 1.781\n",
      "Ep 2 (Step 065715): Train loss 1.263, Val loss 1.783\n",
      "Ep 2 (Step 065720): Train loss 0.981, Val loss 1.783\n",
      "Ep 2 (Step 065725): Train loss 1.008, Val loss 1.784\n",
      "Ep 2 (Step 065730): Train loss 0.970, Val loss 1.786\n",
      "Ep 2 (Step 065735): Train loss 1.134, Val loss 1.786\n",
      "Ep 2 (Step 065740): Train loss 0.884, Val loss 1.786\n",
      "Ep 2 (Step 065745): Train loss 1.177, Val loss 1.787\n",
      "Ep 2 (Step 065750): Train loss 1.053, Val loss 1.787\n",
      "Ep 2 (Step 065755): Train loss 1.165, Val loss 1.787\n",
      "Ep 2 (Step 065760): Train loss 0.989, Val loss 1.786\n",
      "Ep 2 (Step 065765): Train loss 0.965, Val loss 1.783\n",
      "Ep 2 (Step 065770): Train loss 0.895, Val loss 1.782\n",
      "Ep 2 (Step 065775): Train loss 1.225, Val loss 1.782\n",
      "Ep 2 (Step 065780): Train loss 1.119, Val loss 1.783\n",
      "Ep 2 (Step 065785): Train loss 0.874, Val loss 1.784\n",
      "Ep 2 (Step 065790): Train loss 0.968, Val loss 1.785\n",
      "Ep 2 (Step 065795): Train loss 0.717, Val loss 1.786\n",
      "Ep 2 (Step 065800): Train loss 1.085, Val loss 1.787\n",
      "Ep 2 (Step 065805): Train loss 1.137, Val loss 1.787\n",
      "Ep 2 (Step 065810): Train loss 0.932, Val loss 1.785\n",
      "Ep 2 (Step 065815): Train loss 1.052, Val loss 1.783\n",
      "Ep 2 (Step 065820): Train loss 1.171, Val loss 1.780\n",
      "Ep 2 (Step 065825): Train loss 0.966, Val loss 1.779\n",
      "Ep 2 (Step 065830): Train loss 1.095, Val loss 1.779\n",
      "Ep 2 (Step 065835): Train loss 1.036, Val loss 1.778\n",
      "Ep 2 (Step 065840): Train loss 1.179, Val loss 1.778\n",
      "Ep 2 (Step 065845): Train loss 1.022, Val loss 1.777\n",
      "Ep 2 (Step 065850): Train loss 0.876, Val loss 1.777\n",
      "Ep 2 (Step 065855): Train loss 0.842, Val loss 1.777\n",
      "Ep 2 (Step 065860): Train loss 1.046, Val loss 1.777\n",
      "Ep 2 (Step 065865): Train loss 1.136, Val loss 1.778\n",
      "Ep 2 (Step 065870): Train loss 1.081, Val loss 1.777\n",
      "Ep 2 (Step 065875): Train loss 1.070, Val loss 1.778\n",
      "Ep 2 (Step 065880): Train loss 0.985, Val loss 1.779\n",
      "Ep 2 (Step 065885): Train loss 1.034, Val loss 1.779\n",
      "Ep 2 (Step 065890): Train loss 0.952, Val loss 1.779\n",
      "Ep 2 (Step 065895): Train loss 1.039, Val loss 1.779\n",
      "Ep 2 (Step 065900): Train loss 1.129, Val loss 1.779\n",
      "Ep 2 (Step 065905): Train loss 1.189, Val loss 1.778\n",
      "Ep 2 (Step 065910): Train loss 0.899, Val loss 1.777\n",
      "Ep 2 (Step 065915): Train loss 0.828, Val loss 1.777\n",
      "Ep 2 (Step 065920): Train loss 1.090, Val loss 1.777\n",
      "Ep 2 (Step 065925): Train loss 0.705, Val loss 1.779\n",
      "Ep 2 (Step 065930): Train loss 1.287, Val loss 1.779\n",
      "Ep 2 (Step 065935): Train loss 0.831, Val loss 1.780\n",
      "Ep 2 (Step 065940): Train loss 0.894, Val loss 1.780\n",
      "Ep 2 (Step 065945): Train loss 1.091, Val loss 1.781\n",
      "Ep 2 (Step 065950): Train loss 1.090, Val loss 1.782\n",
      "Ep 2 (Step 065955): Train loss 0.847, Val loss 1.784\n",
      "Ep 2 (Step 065960): Train loss 1.073, Val loss 1.786\n",
      "Ep 2 (Step 065965): Train loss 1.220, Val loss 1.788\n",
      "Ep 2 (Step 065970): Train loss 1.197, Val loss 1.789\n",
      "Ep 2 (Step 065975): Train loss 1.126, Val loss 1.790\n",
      "Ep 2 (Step 065980): Train loss 0.876, Val loss 1.790\n",
      "Ep 2 (Step 065985): Train loss 1.071, Val loss 1.790\n",
      "Ep 2 (Step 065990): Train loss 1.354, Val loss 1.790\n",
      "Ep 2 (Step 065995): Train loss 1.117, Val loss 1.790\n",
      "Ep 2 (Step 066000): Train loss 0.916, Val loss 1.790\n",
      "Ep 2 (Step 066005): Train loss 1.075, Val loss 1.790\n",
      "Ep 2 (Step 066010): Train loss 1.363, Val loss 1.789\n",
      "Ep 2 (Step 066015): Train loss 1.113, Val loss 1.787\n",
      "Ep 2 (Step 066020): Train loss 1.077, Val loss 1.787\n",
      "Ep 2 (Step 066025): Train loss 1.101, Val loss 1.786\n",
      "Ep 2 (Step 066030): Train loss 0.973, Val loss 1.785\n",
      "Ep 2 (Step 066035): Train loss 1.017, Val loss 1.784\n",
      "Ep 2 (Step 066040): Train loss 0.756, Val loss 1.783\n",
      "Ep 2 (Step 066045): Train loss 1.061, Val loss 1.783\n",
      "Ep 2 (Step 066050): Train loss 1.076, Val loss 1.782\n",
      "Ep 2 (Step 066055): Train loss 1.119, Val loss 1.783\n",
      "Ep 2 (Step 066060): Train loss 0.963, Val loss 1.784\n",
      "Ep 2 (Step 066065): Train loss 0.947, Val loss 1.784\n",
      "Ep 2 (Step 066070): Train loss 1.181, Val loss 1.783\n",
      "Ep 2 (Step 066075): Train loss 1.187, Val loss 1.782\n",
      "Ep 2 (Step 066080): Train loss 0.955, Val loss 1.781\n",
      "Ep 2 (Step 066085): Train loss 0.954, Val loss 1.781\n",
      "Ep 2 (Step 066090): Train loss 1.164, Val loss 1.782\n",
      "Ep 2 (Step 066095): Train loss 0.868, Val loss 1.784\n",
      "Ep 2 (Step 066100): Train loss 1.057, Val loss 1.785\n",
      "Ep 2 (Step 066105): Train loss 0.967, Val loss 1.786\n",
      "Ep 2 (Step 066110): Train loss 0.880, Val loss 1.786\n",
      "Ep 2 (Step 066115): Train loss 1.053, Val loss 1.785\n",
      "Ep 2 (Step 066120): Train loss 0.956, Val loss 1.784\n",
      "Ep 2 (Step 066125): Train loss 1.039, Val loss 1.785\n",
      "Ep 2 (Step 066130): Train loss 0.949, Val loss 1.785\n",
      "Ep 2 (Step 066135): Train loss 0.796, Val loss 1.786\n",
      "Ep 2 (Step 066140): Train loss 0.916, Val loss 1.786\n",
      "Ep 2 (Step 066145): Train loss 0.767, Val loss 1.788\n",
      "Ep 2 (Step 066150): Train loss 0.918, Val loss 1.788\n",
      "Ep 2 (Step 066155): Train loss 0.933, Val loss 1.788\n",
      "Ep 2 (Step 066160): Train loss 0.756, Val loss 1.788\n",
      "Ep 2 (Step 066165): Train loss 0.997, Val loss 1.788\n",
      "Ep 2 (Step 066170): Train loss 1.091, Val loss 1.789\n",
      "Ep 2 (Step 066175): Train loss 1.309, Val loss 1.789\n",
      "Ep 2 (Step 066180): Train loss 1.158, Val loss 1.789\n",
      "Ep 2 (Step 066185): Train loss 1.039, Val loss 1.789\n",
      "Ep 2 (Step 066190): Train loss 0.950, Val loss 1.789\n",
      "Ep 2 (Step 066195): Train loss 0.948, Val loss 1.788\n",
      "Ep 2 (Step 066200): Train loss 1.274, Val loss 1.787\n",
      "Ep 2 (Step 066205): Train loss 1.161, Val loss 1.787\n",
      "Ep 2 (Step 066210): Train loss 1.051, Val loss 1.789\n",
      "Ep 2 (Step 066215): Train loss 1.204, Val loss 1.790\n",
      "Ep 2 (Step 066220): Train loss 0.901, Val loss 1.790\n",
      "Ep 2 (Step 066225): Train loss 0.856, Val loss 1.789\n",
      "Ep 2 (Step 066230): Train loss 1.068, Val loss 1.788\n",
      "Ep 2 (Step 066235): Train loss 0.897, Val loss 1.787\n",
      "Ep 2 (Step 066240): Train loss 0.956, Val loss 1.786\n",
      "Ep 2 (Step 066245): Train loss 1.033, Val loss 1.785\n",
      "Ep 2 (Step 066250): Train loss 1.158, Val loss 1.785\n",
      "Ep 2 (Step 066255): Train loss 1.070, Val loss 1.786\n",
      "Ep 2 (Step 066260): Train loss 0.814, Val loss 1.787\n",
      "Ep 2 (Step 066265): Train loss 1.049, Val loss 1.787\n",
      "Ep 2 (Step 066270): Train loss 1.029, Val loss 1.786\n",
      "Ep 2 (Step 066275): Train loss 1.154, Val loss 1.784\n",
      "Ep 2 (Step 066280): Train loss 1.031, Val loss 1.784\n",
      "Ep 2 (Step 066285): Train loss 0.769, Val loss 1.784\n",
      "Ep 2 (Step 066290): Train loss 1.358, Val loss 1.786\n",
      "Ep 2 (Step 066295): Train loss 1.093, Val loss 1.787\n",
      "Ep 2 (Step 066300): Train loss 1.415, Val loss 1.789\n",
      "Ep 2 (Step 066305): Train loss 0.921, Val loss 1.791\n",
      "Ep 2 (Step 066310): Train loss 1.061, Val loss 1.791\n",
      "Ep 2 (Step 066315): Train loss 0.738, Val loss 1.792\n",
      "Ep 2 (Step 066320): Train loss 0.983, Val loss 1.790\n",
      "Ep 2 (Step 066325): Train loss 0.729, Val loss 1.789\n",
      "Ep 2 (Step 066330): Train loss 1.185, Val loss 1.788\n",
      "Ep 2 (Step 066335): Train loss 1.133, Val loss 1.788\n",
      "Ep 2 (Step 066340): Train loss 0.860, Val loss 1.786\n",
      "Ep 2 (Step 066345): Train loss 1.008, Val loss 1.784\n",
      "Ep 2 (Step 066350): Train loss 0.964, Val loss 1.783\n",
      "Ep 2 (Step 066355): Train loss 0.994, Val loss 1.781\n",
      "Ep 2 (Step 066360): Train loss 0.972, Val loss 1.779\n",
      "Ep 2 (Step 066365): Train loss 0.871, Val loss 1.777\n",
      "Ep 2 (Step 066370): Train loss 1.086, Val loss 1.775\n",
      "Ep 2 (Step 066375): Train loss 1.201, Val loss 1.773\n",
      "Ep 2 (Step 066380): Train loss 0.984, Val loss 1.772\n",
      "Ep 2 (Step 066385): Train loss 0.921, Val loss 1.771\n",
      "Ep 2 (Step 066390): Train loss 0.911, Val loss 1.771\n",
      "Ep 2 (Step 066395): Train loss 1.024, Val loss 1.772\n",
      "Ep 2 (Step 066400): Train loss 0.909, Val loss 1.773\n",
      "Ep 2 (Step 066405): Train loss 0.903, Val loss 1.773\n",
      "Ep 2 (Step 066410): Train loss 1.066, Val loss 1.773\n",
      "Ep 2 (Step 066415): Train loss 1.065, Val loss 1.773\n",
      "Ep 2 (Step 066420): Train loss 1.044, Val loss 1.773\n",
      "Ep 2 (Step 066425): Train loss 1.206, Val loss 1.773\n",
      "Ep 2 (Step 066430): Train loss 0.949, Val loss 1.773\n",
      "Ep 2 (Step 066435): Train loss 1.109, Val loss 1.773\n",
      "Ep 2 (Step 066440): Train loss 0.960, Val loss 1.773\n",
      "Ep 2 (Step 066445): Train loss 0.937, Val loss 1.774\n",
      "Ep 2 (Step 066450): Train loss 0.991, Val loss 1.775\n",
      "Ep 2 (Step 066455): Train loss 1.203, Val loss 1.774\n",
      "Ep 2 (Step 066460): Train loss 0.883, Val loss 1.774\n",
      "Ep 2 (Step 066465): Train loss 1.242, Val loss 1.774\n",
      "Ep 2 (Step 066470): Train loss 1.044, Val loss 1.775\n",
      "Ep 2 (Step 066475): Train loss 1.063, Val loss 1.776\n",
      "Ep 2 (Step 066480): Train loss 1.165, Val loss 1.777\n",
      "Ep 2 (Step 066485): Train loss 0.829, Val loss 1.774\n",
      "Ep 2 (Step 066490): Train loss 0.998, Val loss 1.773\n",
      "Ep 2 (Step 066495): Train loss 1.016, Val loss 1.772\n",
      "Ep 2 (Step 066500): Train loss 0.798, Val loss 1.772\n",
      "Ep 2 (Step 066505): Train loss 1.008, Val loss 1.771\n",
      "Ep 2 (Step 066510): Train loss 1.138, Val loss 1.772\n",
      "Ep 2 (Step 066515): Train loss 0.888, Val loss 1.772\n",
      "Ep 2 (Step 066520): Train loss 0.867, Val loss 1.772\n",
      "Ep 2 (Step 066525): Train loss 1.045, Val loss 1.771\n",
      "Ep 2 (Step 066530): Train loss 1.045, Val loss 1.769\n",
      "Ep 2 (Step 066535): Train loss 0.913, Val loss 1.768\n",
      "Ep 2 (Step 066540): Train loss 0.669, Val loss 1.768\n",
      "Ep 2 (Step 066545): Train loss 1.048, Val loss 1.768\n",
      "Ep 2 (Step 066550): Train loss 1.205, Val loss 1.768\n",
      "Ep 2 (Step 066555): Train loss 1.070, Val loss 1.766\n",
      "Ep 2 (Step 066560): Train loss 1.006, Val loss 1.765\n",
      "Ep 2 (Step 066565): Train loss 0.810, Val loss 1.765\n",
      "Ep 2 (Step 066570): Train loss 1.080, Val loss 1.767\n",
      "Ep 2 (Step 066575): Train loss 1.014, Val loss 1.768\n",
      "Ep 2 (Step 066580): Train loss 1.055, Val loss 1.769\n",
      "Ep 2 (Step 066585): Train loss 1.132, Val loss 1.768\n",
      "Ep 2 (Step 066590): Train loss 0.951, Val loss 1.767\n",
      "Ep 2 (Step 066595): Train loss 1.070, Val loss 1.767\n",
      "Ep 2 (Step 066600): Train loss 1.219, Val loss 1.767\n",
      "Ep 2 (Step 066605): Train loss 1.383, Val loss 1.767\n",
      "Ep 2 (Step 066610): Train loss 0.863, Val loss 1.767\n",
      "Ep 2 (Step 066615): Train loss 1.133, Val loss 1.767\n",
      "Ep 2 (Step 066620): Train loss 1.240, Val loss 1.767\n",
      "Ep 2 (Step 066625): Train loss 1.083, Val loss 1.768\n",
      "Ep 2 (Step 066630): Train loss 0.988, Val loss 1.767\n",
      "Ep 2 (Step 066635): Train loss 0.958, Val loss 1.767\n",
      "Ep 2 (Step 066640): Train loss 1.053, Val loss 1.768\n",
      "Ep 2 (Step 066645): Train loss 1.139, Val loss 1.769\n",
      "Ep 2 (Step 066650): Train loss 1.183, Val loss 1.770\n",
      "Ep 2 (Step 066655): Train loss 0.998, Val loss 1.770\n",
      "Ep 2 (Step 066660): Train loss 0.973, Val loss 1.771\n",
      "Ep 2 (Step 066665): Train loss 1.129, Val loss 1.770\n",
      "Ep 2 (Step 066670): Train loss 1.427, Val loss 1.768\n",
      "Ep 2 (Step 066675): Train loss 1.065, Val loss 1.767\n",
      "Ep 2 (Step 066680): Train loss 1.244, Val loss 1.766\n",
      "Ep 2 (Step 066685): Train loss 0.983, Val loss 1.766\n",
      "Ep 2 (Step 066690): Train loss 1.062, Val loss 1.768\n",
      "Ep 2 (Step 066695): Train loss 1.004, Val loss 1.770\n",
      "Ep 2 (Step 066700): Train loss 1.202, Val loss 1.771\n",
      "Ep 2 (Step 066705): Train loss 1.038, Val loss 1.772\n",
      "Ep 2 (Step 066710): Train loss 1.206, Val loss 1.774\n",
      "Ep 2 (Step 066715): Train loss 0.793, Val loss 1.775\n",
      "Ep 2 (Step 066720): Train loss 1.006, Val loss 1.775\n",
      "Ep 2 (Step 066725): Train loss 0.871, Val loss 1.776\n",
      "Ep 2 (Step 066730): Train loss 1.051, Val loss 1.775\n",
      "Ep 2 (Step 066735): Train loss 1.154, Val loss 1.775\n",
      "Ep 2 (Step 066740): Train loss 1.070, Val loss 1.775\n",
      "Ep 2 (Step 066745): Train loss 0.952, Val loss 1.774\n",
      "Ep 2 (Step 066750): Train loss 0.999, Val loss 1.773\n",
      "Ep 2 (Step 066755): Train loss 0.984, Val loss 1.773\n",
      "Ep 2 (Step 066760): Train loss 1.305, Val loss 1.772\n",
      "Ep 2 (Step 066765): Train loss 1.165, Val loss 1.772\n",
      "Ep 2 (Step 066770): Train loss 1.003, Val loss 1.773\n",
      "Ep 2 (Step 066775): Train loss 1.211, Val loss 1.772\n",
      "Ep 2 (Step 066780): Train loss 1.030, Val loss 1.772\n",
      "Ep 2 (Step 066785): Train loss 0.984, Val loss 1.772\n",
      "Ep 2 (Step 066790): Train loss 1.062, Val loss 1.774\n",
      "Ep 2 (Step 066795): Train loss 1.002, Val loss 1.774\n",
      "Ep 2 (Step 066800): Train loss 1.370, Val loss 1.773\n",
      "Ep 2 (Step 066805): Train loss 0.962, Val loss 1.772\n",
      "Ep 2 (Step 066810): Train loss 1.066, Val loss 1.771\n",
      "Ep 2 (Step 066815): Train loss 1.061, Val loss 1.771\n",
      "Ep 2 (Step 066820): Train loss 1.087, Val loss 1.771\n",
      "Ep 2 (Step 066825): Train loss 1.021, Val loss 1.772\n",
      "Ep 2 (Step 066830): Train loss 1.094, Val loss 1.773\n",
      "Ep 2 (Step 066835): Train loss 0.973, Val loss 1.775\n",
      "Ep 2 (Step 066840): Train loss 0.972, Val loss 1.777\n",
      "Ep 2 (Step 066845): Train loss 1.095, Val loss 1.777\n",
      "Ep 2 (Step 066850): Train loss 0.966, Val loss 1.775\n",
      "Ep 2 (Step 066855): Train loss 1.047, Val loss 1.775\n",
      "Ep 2 (Step 066860): Train loss 0.955, Val loss 1.775\n",
      "Ep 2 (Step 066865): Train loss 1.047, Val loss 1.775\n",
      "Ep 2 (Step 066870): Train loss 1.240, Val loss 1.774\n",
      "Ep 2 (Step 066875): Train loss 0.863, Val loss 1.773\n",
      "Ep 2 (Step 066880): Train loss 0.891, Val loss 1.772\n",
      "Ep 2 (Step 066885): Train loss 1.014, Val loss 1.772\n",
      "Ep 2 (Step 066890): Train loss 0.942, Val loss 1.772\n",
      "Ep 2 (Step 066895): Train loss 1.018, Val loss 1.773\n",
      "Ep 2 (Step 066900): Train loss 1.057, Val loss 1.774\n",
      "Ep 2 (Step 066905): Train loss 1.113, Val loss 1.777\n",
      "Ep 2 (Step 066910): Train loss 0.955, Val loss 1.779\n",
      "Ep 2 (Step 066915): Train loss 0.960, Val loss 1.781\n",
      "Ep 2 (Step 066920): Train loss 1.220, Val loss 1.781\n",
      "Ep 2 (Step 066925): Train loss 1.201, Val loss 1.781\n",
      "Ep 2 (Step 066930): Train loss 0.898, Val loss 1.780\n",
      "Ep 2 (Step 066935): Train loss 0.890, Val loss 1.782\n",
      "Ep 2 (Step 066940): Train loss 1.111, Val loss 1.784\n",
      "Ep 2 (Step 066945): Train loss 1.110, Val loss 1.784\n",
      "Ep 2 (Step 066950): Train loss 0.993, Val loss 1.784\n",
      "Ep 2 (Step 066955): Train loss 0.827, Val loss 1.786\n",
      "Ep 2 (Step 066960): Train loss 0.960, Val loss 1.789\n",
      "Ep 2 (Step 066965): Train loss 1.034, Val loss 1.792\n",
      "Ep 2 (Step 066970): Train loss 0.886, Val loss 1.794\n",
      "Ep 2 (Step 066975): Train loss 0.946, Val loss 1.794\n",
      "Ep 2 (Step 066980): Train loss 1.105, Val loss 1.795\n",
      "Ep 2 (Step 066985): Train loss 1.081, Val loss 1.795\n",
      "Ep 2 (Step 066990): Train loss 1.130, Val loss 1.795\n",
      "Ep 2 (Step 066995): Train loss 0.946, Val loss 1.794\n",
      "Ep 2 (Step 067000): Train loss 0.888, Val loss 1.793\n",
      "Ep 2 (Step 067005): Train loss 1.167, Val loss 1.793\n",
      "Ep 2 (Step 067010): Train loss 1.285, Val loss 1.794\n",
      "Ep 2 (Step 067015): Train loss 1.053, Val loss 1.793\n",
      "Ep 2 (Step 067020): Train loss 1.056, Val loss 1.792\n",
      "Ep 2 (Step 067025): Train loss 0.903, Val loss 1.790\n",
      "Ep 2 (Step 067030): Train loss 0.744, Val loss 1.789\n",
      "Ep 2 (Step 067035): Train loss 1.232, Val loss 1.787\n",
      "Ep 2 (Step 067040): Train loss 0.853, Val loss 1.785\n",
      "Ep 2 (Step 067045): Train loss 0.904, Val loss 1.784\n",
      "Ep 2 (Step 067050): Train loss 0.939, Val loss 1.783\n",
      "Ep 2 (Step 067055): Train loss 0.938, Val loss 1.782\n",
      "Ep 2 (Step 067060): Train loss 0.913, Val loss 1.781\n",
      "Ep 2 (Step 067065): Train loss 1.026, Val loss 1.781\n",
      "Ep 2 (Step 067070): Train loss 0.714, Val loss 1.781\n",
      "Ep 2 (Step 067075): Train loss 0.827, Val loss 1.781\n",
      "Ep 2 (Step 067080): Train loss 1.105, Val loss 1.780\n",
      "Ep 2 (Step 067085): Train loss 0.923, Val loss 1.779\n",
      "Ep 2 (Step 067090): Train loss 1.041, Val loss 1.779\n",
      "Ep 2 (Step 067095): Train loss 1.017, Val loss 1.778\n",
      "Ep 2 (Step 067100): Train loss 1.005, Val loss 1.778\n",
      "Ep 2 (Step 067105): Train loss 1.237, Val loss 1.778\n",
      "Ep 2 (Step 067110): Train loss 1.104, Val loss 1.779\n",
      "Ep 2 (Step 067115): Train loss 1.203, Val loss 1.779\n",
      "Ep 2 (Step 067120): Train loss 0.938, Val loss 1.779\n",
      "Ep 2 (Step 067125): Train loss 0.916, Val loss 1.779\n",
      "Ep 2 (Step 067130): Train loss 1.039, Val loss 1.779\n",
      "Ep 2 (Step 067135): Train loss 1.147, Val loss 1.781\n",
      "Ep 2 (Step 067140): Train loss 0.734, Val loss 1.781\n",
      "Ep 2 (Step 067145): Train loss 1.150, Val loss 1.781\n",
      "Ep 2 (Step 067150): Train loss 1.223, Val loss 1.781\n",
      "Ep 2 (Step 067155): Train loss 1.190, Val loss 1.782\n",
      "Ep 2 (Step 067160): Train loss 1.178, Val loss 1.784\n",
      "Ep 2 (Step 067165): Train loss 0.874, Val loss 1.784\n",
      "Ep 2 (Step 067170): Train loss 0.978, Val loss 1.784\n",
      "Ep 2 (Step 067175): Train loss 1.165, Val loss 1.785\n",
      "Ep 2 (Step 067180): Train loss 0.848, Val loss 1.785\n",
      "Ep 2 (Step 067185): Train loss 1.143, Val loss 1.787\n",
      "Ep 2 (Step 067190): Train loss 0.903, Val loss 1.788\n",
      "Ep 2 (Step 067195): Train loss 1.161, Val loss 1.789\n",
      "Ep 2 (Step 067200): Train loss 1.170, Val loss 1.787\n",
      "Ep 2 (Step 067205): Train loss 1.043, Val loss 1.786\n",
      "Ep 2 (Step 067210): Train loss 0.985, Val loss 1.785\n",
      "Ep 2 (Step 067215): Train loss 1.283, Val loss 1.784\n",
      "Ep 2 (Step 067220): Train loss 1.217, Val loss 1.784\n",
      "Ep 2 (Step 067225): Train loss 1.208, Val loss 1.783\n",
      "Ep 2 (Step 067230): Train loss 0.909, Val loss 1.783\n",
      "Ep 2 (Step 067235): Train loss 1.379, Val loss 1.783\n",
      "Ep 2 (Step 067240): Train loss 0.961, Val loss 1.781\n",
      "Ep 2 (Step 067245): Train loss 0.990, Val loss 1.779\n",
      "Ep 2 (Step 067250): Train loss 0.973, Val loss 1.776\n",
      "Ep 2 (Step 067255): Train loss 0.956, Val loss 1.776\n",
      "Ep 2 (Step 067260): Train loss 0.902, Val loss 1.777\n",
      "Ep 2 (Step 067265): Train loss 1.000, Val loss 1.778\n",
      "Ep 2 (Step 067270): Train loss 1.157, Val loss 1.780\n",
      "Ep 2 (Step 067275): Train loss 1.063, Val loss 1.780\n",
      "Ep 2 (Step 067280): Train loss 0.987, Val loss 1.780\n",
      "Ep 2 (Step 067285): Train loss 1.017, Val loss 1.779\n",
      "Ep 2 (Step 067290): Train loss 1.124, Val loss 1.777\n",
      "Ep 2 (Step 067295): Train loss 1.046, Val loss 1.776\n",
      "Ep 2 (Step 067300): Train loss 1.068, Val loss 1.775\n",
      "Ep 2 (Step 067305): Train loss 0.895, Val loss 1.773\n",
      "Ep 2 (Step 067310): Train loss 1.004, Val loss 1.771\n",
      "Ep 2 (Step 067315): Train loss 0.943, Val loss 1.771\n",
      "Ep 2 (Step 067320): Train loss 1.200, Val loss 1.770\n",
      "Ep 2 (Step 067325): Train loss 0.993, Val loss 1.768\n",
      "Ep 2 (Step 067330): Train loss 1.076, Val loss 1.766\n",
      "Ep 2 (Step 067335): Train loss 1.190, Val loss 1.767\n",
      "Ep 2 (Step 067340): Train loss 1.245, Val loss 1.769\n",
      "Ep 2 (Step 067345): Train loss 1.077, Val loss 1.771\n",
      "Ep 2 (Step 067350): Train loss 0.976, Val loss 1.773\n",
      "Ep 2 (Step 067355): Train loss 1.190, Val loss 1.774\n",
      "Ep 2 (Step 067360): Train loss 1.332, Val loss 1.774\n",
      "Ep 2 (Step 067365): Train loss 0.983, Val loss 1.773\n",
      "Ep 2 (Step 067370): Train loss 0.933, Val loss 1.772\n",
      "Ep 2 (Step 067375): Train loss 0.900, Val loss 1.771\n",
      "Ep 2 (Step 067380): Train loss 1.131, Val loss 1.769\n",
      "Ep 2 (Step 067385): Train loss 1.088, Val loss 1.769\n",
      "Ep 2 (Step 067390): Train loss 1.270, Val loss 1.769\n",
      "Ep 2 (Step 067395): Train loss 0.999, Val loss 1.769\n",
      "Ep 2 (Step 067400): Train loss 0.799, Val loss 1.770\n",
      "Ep 2 (Step 067405): Train loss 0.964, Val loss 1.771\n",
      "Ep 2 (Step 067410): Train loss 0.873, Val loss 1.771\n",
      "Ep 2 (Step 067415): Train loss 1.258, Val loss 1.771\n",
      "Ep 2 (Step 067420): Train loss 0.942, Val loss 1.771\n",
      "Ep 2 (Step 067425): Train loss 1.306, Val loss 1.771\n",
      "Ep 2 (Step 067430): Train loss 1.000, Val loss 1.772\n",
      "Ep 2 (Step 067435): Train loss 0.954, Val loss 1.772\n",
      "Ep 2 (Step 067440): Train loss 0.941, Val loss 1.771\n",
      "Ep 2 (Step 067445): Train loss 1.044, Val loss 1.771\n",
      "Ep 2 (Step 067450): Train loss 1.163, Val loss 1.772\n",
      "Ep 2 (Step 067455): Train loss 1.226, Val loss 1.772\n",
      "Ep 2 (Step 067460): Train loss 0.943, Val loss 1.773\n",
      "Ep 2 (Step 067465): Train loss 0.898, Val loss 1.772\n",
      "Ep 2 (Step 067470): Train loss 0.911, Val loss 1.771\n",
      "Ep 2 (Step 067475): Train loss 0.964, Val loss 1.769\n",
      "Ep 2 (Step 067480): Train loss 0.995, Val loss 1.768\n",
      "Ep 2 (Step 067485): Train loss 0.691, Val loss 1.768\n",
      "Ep 2 (Step 067490): Train loss 0.900, Val loss 1.767\n",
      "Ep 2 (Step 067495): Train loss 1.282, Val loss 1.768\n",
      "Ep 2 (Step 067500): Train loss 0.814, Val loss 1.770\n",
      "Ep 2 (Step 067505): Train loss 0.878, Val loss 1.770\n",
      "Ep 2 (Step 067510): Train loss 0.985, Val loss 1.770\n",
      "Ep 2 (Step 067515): Train loss 0.964, Val loss 1.773\n",
      "Ep 2 (Step 067520): Train loss 0.859, Val loss 1.774\n",
      "Ep 2 (Step 067525): Train loss 0.980, Val loss 1.775\n",
      "Ep 2 (Step 067530): Train loss 1.172, Val loss 1.775\n",
      "Ep 2 (Step 067535): Train loss 1.291, Val loss 1.774\n",
      "Ep 2 (Step 067540): Train loss 0.982, Val loss 1.774\n",
      "Ep 2 (Step 067545): Train loss 1.475, Val loss 1.775\n",
      "Ep 2 (Step 067550): Train loss 0.971, Val loss 1.776\n",
      "Ep 2 (Step 067555): Train loss 0.989, Val loss 1.775\n",
      "Ep 2 (Step 067560): Train loss 0.977, Val loss 1.775\n",
      "Ep 2 (Step 067565): Train loss 0.962, Val loss 1.775\n",
      "Ep 2 (Step 067570): Train loss 1.212, Val loss 1.774\n",
      "Ep 2 (Step 067575): Train loss 0.796, Val loss 1.772\n",
      "Ep 2 (Step 067580): Train loss 1.098, Val loss 1.771\n",
      "Ep 2 (Step 067585): Train loss 0.786, Val loss 1.771\n",
      "Ep 2 (Step 067590): Train loss 0.844, Val loss 1.771\n",
      "Ep 2 (Step 067595): Train loss 1.184, Val loss 1.772\n",
      "Ep 2 (Step 067600): Train loss 1.090, Val loss 1.772\n",
      "Ep 2 (Step 067605): Train loss 1.517, Val loss 1.773\n",
      "Ep 2 (Step 067610): Train loss 1.114, Val loss 1.775\n",
      "Ep 2 (Step 067615): Train loss 1.098, Val loss 1.776\n",
      "Ep 2 (Step 067620): Train loss 0.877, Val loss 1.776\n",
      "Ep 2 (Step 067625): Train loss 1.121, Val loss 1.776\n",
      "Ep 2 (Step 067630): Train loss 0.927, Val loss 1.775\n",
      "Ep 2 (Step 067635): Train loss 1.103, Val loss 1.775\n",
      "Ep 2 (Step 067640): Train loss 0.930, Val loss 1.775\n",
      "Ep 2 (Step 067645): Train loss 1.126, Val loss 1.774\n",
      "Ep 2 (Step 067650): Train loss 0.871, Val loss 1.773\n",
      "Ep 2 (Step 067655): Train loss 1.012, Val loss 1.771\n",
      "Ep 2 (Step 067660): Train loss 1.042, Val loss 1.771\n",
      "Ep 2 (Step 067665): Train loss 0.917, Val loss 1.770\n",
      "Ep 2 (Step 067670): Train loss 0.785, Val loss 1.770\n",
      "Ep 2 (Step 067675): Train loss 1.326, Val loss 1.770\n",
      "Ep 2 (Step 067680): Train loss 0.862, Val loss 1.771\n",
      "Ep 2 (Step 067685): Train loss 1.220, Val loss 1.771\n",
      "Ep 2 (Step 067690): Train loss 1.107, Val loss 1.771\n",
      "Ep 2 (Step 067695): Train loss 0.997, Val loss 1.773\n",
      "Ep 2 (Step 067700): Train loss 1.123, Val loss 1.774\n",
      "Ep 2 (Step 067705): Train loss 1.210, Val loss 1.775\n",
      "Ep 2 (Step 067710): Train loss 0.945, Val loss 1.775\n",
      "Ep 2 (Step 067715): Train loss 1.304, Val loss 1.774\n",
      "Ep 2 (Step 067720): Train loss 0.911, Val loss 1.774\n",
      "Ep 2 (Step 067725): Train loss 0.887, Val loss 1.773\n",
      "Ep 2 (Step 067730): Train loss 0.932, Val loss 1.774\n",
      "Ep 2 (Step 067735): Train loss 1.072, Val loss 1.774\n",
      "Ep 2 (Step 067740): Train loss 0.882, Val loss 1.772\n",
      "Ep 2 (Step 067745): Train loss 0.992, Val loss 1.772\n",
      "Ep 2 (Step 067750): Train loss 1.005, Val loss 1.770\n",
      "Ep 2 (Step 067755): Train loss 0.962, Val loss 1.769\n",
      "Ep 2 (Step 067760): Train loss 1.044, Val loss 1.770\n",
      "Ep 2 (Step 067765): Train loss 1.357, Val loss 1.772\n",
      "Ep 2 (Step 067770): Train loss 1.257, Val loss 1.773\n",
      "Ep 2 (Step 067775): Train loss 1.083, Val loss 1.775\n",
      "Ep 2 (Step 067780): Train loss 0.804, Val loss 1.776\n",
      "Ep 2 (Step 067785): Train loss 0.946, Val loss 1.776\n",
      "Ep 2 (Step 067790): Train loss 0.846, Val loss 1.774\n",
      "Ep 2 (Step 067795): Train loss 1.186, Val loss 1.773\n",
      "Ep 2 (Step 067800): Train loss 0.939, Val loss 1.773\n",
      "Ep 2 (Step 067805): Train loss 1.114, Val loss 1.773\n",
      "Ep 2 (Step 067810): Train loss 0.938, Val loss 1.774\n",
      "Ep 2 (Step 067815): Train loss 0.863, Val loss 1.775\n",
      "Ep 2 (Step 067820): Train loss 1.166, Val loss 1.777\n",
      "Ep 2 (Step 067825): Train loss 0.967, Val loss 1.779\n",
      "Ep 2 (Step 067830): Train loss 1.053, Val loss 1.780\n",
      "Ep 2 (Step 067835): Train loss 0.921, Val loss 1.779\n",
      "Ep 2 (Step 067840): Train loss 1.002, Val loss 1.776\n",
      "Ep 2 (Step 067845): Train loss 1.135, Val loss 1.775\n",
      "Ep 2 (Step 067850): Train loss 0.835, Val loss 1.775\n",
      "Ep 2 (Step 067855): Train loss 0.689, Val loss 1.775\n",
      "Ep 2 (Step 067860): Train loss 0.998, Val loss 1.774\n",
      "Ep 2 (Step 067865): Train loss 0.770, Val loss 1.773\n",
      "Ep 2 (Step 067870): Train loss 0.987, Val loss 1.774\n",
      "Ep 2 (Step 067875): Train loss 1.258, Val loss 1.776\n",
      "Ep 2 (Step 067880): Train loss 1.002, Val loss 1.776\n",
      "Ep 2 (Step 067885): Train loss 0.898, Val loss 1.777\n",
      "Ep 2 (Step 067890): Train loss 0.820, Val loss 1.777\n",
      "Ep 2 (Step 067895): Train loss 0.880, Val loss 1.777\n",
      "Ep 2 (Step 067900): Train loss 1.164, Val loss 1.779\n",
      "Ep 2 (Step 067905): Train loss 0.978, Val loss 1.780\n",
      "Ep 2 (Step 067910): Train loss 1.150, Val loss 1.781\n",
      "Ep 2 (Step 067915): Train loss 0.883, Val loss 1.783\n",
      "Ep 2 (Step 067920): Train loss 1.188, Val loss 1.784\n",
      "Ep 2 (Step 067925): Train loss 1.157, Val loss 1.784\n",
      "Ep 2 (Step 067930): Train loss 1.114, Val loss 1.783\n",
      "Ep 2 (Step 067935): Train loss 0.980, Val loss 1.782\n",
      "Ep 2 (Step 067940): Train loss 1.021, Val loss 1.781\n",
      "Ep 2 (Step 067945): Train loss 0.751, Val loss 1.780\n",
      "Ep 2 (Step 067950): Train loss 0.902, Val loss 1.778\n",
      "Ep 2 (Step 067955): Train loss 0.944, Val loss 1.777\n",
      "Ep 2 (Step 067960): Train loss 0.785, Val loss 1.777\n",
      "Ep 2 (Step 067965): Train loss 0.962, Val loss 1.777\n",
      "Ep 2 (Step 067970): Train loss 0.790, Val loss 1.778\n",
      "Ep 2 (Step 067975): Train loss 1.168, Val loss 1.778\n",
      "Ep 2 (Step 067980): Train loss 1.250, Val loss 1.780\n",
      "Ep 2 (Step 067985): Train loss 1.188, Val loss 1.781\n",
      "Ep 2 (Step 067990): Train loss 1.119, Val loss 1.781\n",
      "Ep 2 (Step 067995): Train loss 0.987, Val loss 1.780\n",
      "Ep 2 (Step 068000): Train loss 0.838, Val loss 1.780\n",
      "Ep 2 (Step 068005): Train loss 1.135, Val loss 1.779\n",
      "Ep 2 (Step 068010): Train loss 0.796, Val loss 1.778\n",
      "Ep 2 (Step 068015): Train loss 0.863, Val loss 1.776\n",
      "Ep 2 (Step 068020): Train loss 0.696, Val loss 1.776\n",
      "Ep 2 (Step 068025): Train loss 0.899, Val loss 1.778\n",
      "Ep 2 (Step 068030): Train loss 0.756, Val loss 1.779\n",
      "Ep 2 (Step 068035): Train loss 0.930, Val loss 1.781\n",
      "Ep 2 (Step 068040): Train loss 1.162, Val loss 1.783\n",
      "Ep 2 (Step 068045): Train loss 1.011, Val loss 1.784\n",
      "Ep 2 (Step 068050): Train loss 1.168, Val loss 1.784\n",
      "Ep 2 (Step 068055): Train loss 1.220, Val loss 1.784\n",
      "Ep 2 (Step 068060): Train loss 1.002, Val loss 1.784\n",
      "Ep 2 (Step 068065): Train loss 0.973, Val loss 1.784\n",
      "Ep 2 (Step 068070): Train loss 1.089, Val loss 1.786\n",
      "Ep 2 (Step 068075): Train loss 1.101, Val loss 1.786\n",
      "Ep 2 (Step 068080): Train loss 1.293, Val loss 1.786\n",
      "Ep 2 (Step 068085): Train loss 0.968, Val loss 1.786\n",
      "Ep 2 (Step 068090): Train loss 1.126, Val loss 1.785\n",
      "Ep 2 (Step 068095): Train loss 1.384, Val loss 1.785\n",
      "Ep 2 (Step 068100): Train loss 1.117, Val loss 1.784\n",
      "Ep 2 (Step 068105): Train loss 1.044, Val loss 1.781\n",
      "Ep 2 (Step 068110): Train loss 1.100, Val loss 1.779\n",
      "Ep 2 (Step 068115): Train loss 1.106, Val loss 1.779\n",
      "Ep 2 (Step 068120): Train loss 0.934, Val loss 1.778\n",
      "Ep 2 (Step 068125): Train loss 1.133, Val loss 1.778\n",
      "Ep 2 (Step 068130): Train loss 1.262, Val loss 1.778\n",
      "Ep 2 (Step 068135): Train loss 1.098, Val loss 1.778\n",
      "Ep 2 (Step 068140): Train loss 1.177, Val loss 1.778\n",
      "Ep 2 (Step 068145): Train loss 0.824, Val loss 1.778\n",
      "Ep 2 (Step 068150): Train loss 1.156, Val loss 1.778\n",
      "Ep 2 (Step 068155): Train loss 0.881, Val loss 1.778\n",
      "Ep 2 (Step 068160): Train loss 1.269, Val loss 1.778\n",
      "Ep 2 (Step 068165): Train loss 0.818, Val loss 1.778\n",
      "Ep 2 (Step 068170): Train loss 1.023, Val loss 1.777\n",
      "Ep 2 (Step 068175): Train loss 1.106, Val loss 1.777\n",
      "Ep 2 (Step 068180): Train loss 1.255, Val loss 1.775\n",
      "Ep 2 (Step 068185): Train loss 1.217, Val loss 1.775\n",
      "Ep 2 (Step 068190): Train loss 1.091, Val loss 1.775\n",
      "Ep 2 (Step 068195): Train loss 1.052, Val loss 1.774\n",
      "Ep 2 (Step 068200): Train loss 1.024, Val loss 1.772\n",
      "Ep 2 (Step 068205): Train loss 1.169, Val loss 1.770\n",
      "Ep 2 (Step 068210): Train loss 1.101, Val loss 1.768\n",
      "Ep 2 (Step 068215): Train loss 1.076, Val loss 1.766\n",
      "Ep 2 (Step 068220): Train loss 0.837, Val loss 1.765\n",
      "Ep 2 (Step 068225): Train loss 0.955, Val loss 1.765\n",
      "Ep 2 (Step 068230): Train loss 1.029, Val loss 1.767\n",
      "Ep 2 (Step 068235): Train loss 1.114, Val loss 1.769\n",
      "Ep 2 (Step 068240): Train loss 0.993, Val loss 1.771\n",
      "Ep 2 (Step 068245): Train loss 1.156, Val loss 1.771\n",
      "Ep 2 (Step 068250): Train loss 0.914, Val loss 1.773\n",
      "Ep 2 (Step 068255): Train loss 1.009, Val loss 1.773\n",
      "Ep 2 (Step 068260): Train loss 1.018, Val loss 1.773\n",
      "Ep 2 (Step 068265): Train loss 1.092, Val loss 1.773\n",
      "Ep 2 (Step 068270): Train loss 0.655, Val loss 1.774\n",
      "Ep 2 (Step 068275): Train loss 0.940, Val loss 1.774\n",
      "Ep 2 (Step 068280): Train loss 1.010, Val loss 1.774\n",
      "Ep 2 (Step 068285): Train loss 0.917, Val loss 1.774\n",
      "Ep 2 (Step 068290): Train loss 1.254, Val loss 1.775\n",
      "Ep 2 (Step 068295): Train loss 1.250, Val loss 1.777\n",
      "Ep 2 (Step 068300): Train loss 1.369, Val loss 1.779\n",
      "Ep 2 (Step 068305): Train loss 0.918, Val loss 1.778\n",
      "Ep 2 (Step 068310): Train loss 1.127, Val loss 1.777\n",
      "Ep 2 (Step 068315): Train loss 1.047, Val loss 1.776\n",
      "Ep 2 (Step 068320): Train loss 1.213, Val loss 1.775\n",
      "Ep 2 (Step 068325): Train loss 1.150, Val loss 1.775\n",
      "Ep 2 (Step 068330): Train loss 0.923, Val loss 1.776\n",
      "Ep 2 (Step 068335): Train loss 1.040, Val loss 1.776\n",
      "Ep 2 (Step 068340): Train loss 1.394, Val loss 1.776\n",
      "Ep 2 (Step 068345): Train loss 1.192, Val loss 1.776\n",
      "Ep 2 (Step 068350): Train loss 0.889, Val loss 1.776\n",
      "Ep 2 (Step 068355): Train loss 0.924, Val loss 1.775\n",
      "Ep 2 (Step 068360): Train loss 0.979, Val loss 1.775\n",
      "Ep 2 (Step 068365): Train loss 1.077, Val loss 1.776\n",
      "Ep 2 (Step 068370): Train loss 1.105, Val loss 1.775\n",
      "Ep 2 (Step 068375): Train loss 1.061, Val loss 1.775\n",
      "Ep 2 (Step 068380): Train loss 1.240, Val loss 1.773\n",
      "Ep 2 (Step 068385): Train loss 1.079, Val loss 1.773\n",
      "Ep 2 (Step 068390): Train loss 1.076, Val loss 1.773\n",
      "Ep 2 (Step 068395): Train loss 1.051, Val loss 1.773\n",
      "Ep 2 (Step 068400): Train loss 1.086, Val loss 1.773\n",
      "Ep 2 (Step 068405): Train loss 1.337, Val loss 1.774\n",
      "Ep 2 (Step 068410): Train loss 1.224, Val loss 1.775\n",
      "Ep 2 (Step 068415): Train loss 1.136, Val loss 1.775\n",
      "Ep 2 (Step 068420): Train loss 1.199, Val loss 1.776\n",
      "Ep 2 (Step 068425): Train loss 0.854, Val loss 1.775\n",
      "Ep 2 (Step 068430): Train loss 1.209, Val loss 1.775\n",
      "Ep 2 (Step 068435): Train loss 1.122, Val loss 1.777\n",
      "Ep 2 (Step 068440): Train loss 1.007, Val loss 1.780\n",
      "Ep 2 (Step 068445): Train loss 1.202, Val loss 1.781\n",
      "Ep 2 (Step 068450): Train loss 1.041, Val loss 1.782\n",
      "Ep 2 (Step 068455): Train loss 1.169, Val loss 1.784\n",
      "Ep 2 (Step 068460): Train loss 0.939, Val loss 1.785\n",
      "Ep 2 (Step 068465): Train loss 0.944, Val loss 1.786\n",
      "Ep 2 (Step 068470): Train loss 1.060, Val loss 1.785\n",
      "Ep 2 (Step 068475): Train loss 0.709, Val loss 1.784\n",
      "Ep 2 (Step 068480): Train loss 0.937, Val loss 1.784\n",
      "Ep 2 (Step 068485): Train loss 1.222, Val loss 1.783\n",
      "Ep 2 (Step 068490): Train loss 1.056, Val loss 1.781\n",
      "Ep 2 (Step 068495): Train loss 0.952, Val loss 1.780\n",
      "Ep 2 (Step 068500): Train loss 1.069, Val loss 1.779\n",
      "Ep 2 (Step 068505): Train loss 1.070, Val loss 1.777\n",
      "Ep 2 (Step 068510): Train loss 1.212, Val loss 1.776\n",
      "Ep 2 (Step 068515): Train loss 0.881, Val loss 1.776\n",
      "Ep 2 (Step 068520): Train loss 1.019, Val loss 1.776\n",
      "Ep 2 (Step 068525): Train loss 0.956, Val loss 1.777\n",
      "Ep 2 (Step 068530): Train loss 1.096, Val loss 1.777\n",
      "Ep 2 (Step 068535): Train loss 0.973, Val loss 1.777\n",
      "Ep 2 (Step 068540): Train loss 1.063, Val loss 1.777\n",
      "Ep 2 (Step 068545): Train loss 0.886, Val loss 1.779\n",
      "Ep 2 (Step 068550): Train loss 1.239, Val loss 1.780\n",
      "Ep 2 (Step 068555): Train loss 1.083, Val loss 1.781\n",
      "Ep 2 (Step 068560): Train loss 0.763, Val loss 1.780\n",
      "Ep 2 (Step 068565): Train loss 1.088, Val loss 1.780\n",
      "Ep 2 (Step 068570): Train loss 1.073, Val loss 1.778\n",
      "Ep 2 (Step 068575): Train loss 1.021, Val loss 1.777\n",
      "Ep 2 (Step 068580): Train loss 0.863, Val loss 1.774\n",
      "Ep 2 (Step 068585): Train loss 1.067, Val loss 1.773\n",
      "Ep 2 (Step 068590): Train loss 0.930, Val loss 1.771\n",
      "Ep 2 (Step 068595): Train loss 1.075, Val loss 1.769\n",
      "Ep 2 (Step 068600): Train loss 1.320, Val loss 1.768\n",
      "Ep 2 (Step 068605): Train loss 0.975, Val loss 1.769\n",
      "Ep 2 (Step 068610): Train loss 0.920, Val loss 1.769\n",
      "Ep 2 (Step 068615): Train loss 0.883, Val loss 1.769\n",
      "Ep 2 (Step 068620): Train loss 1.267, Val loss 1.771\n",
      "Ep 2 (Step 068625): Train loss 1.035, Val loss 1.772\n",
      "Ep 2 (Step 068630): Train loss 1.033, Val loss 1.772\n",
      "Ep 2 (Step 068635): Train loss 1.170, Val loss 1.773\n",
      "Ep 2 (Step 068640): Train loss 0.935, Val loss 1.773\n",
      "Ep 2 (Step 068645): Train loss 0.974, Val loss 1.774\n",
      "Ep 2 (Step 068650): Train loss 1.024, Val loss 1.775\n",
      "Ep 2 (Step 068655): Train loss 0.911, Val loss 1.775\n",
      "Ep 2 (Step 068660): Train loss 1.134, Val loss 1.774\n",
      "Ep 2 (Step 068665): Train loss 0.997, Val loss 1.773\n",
      "Ep 2 (Step 068670): Train loss 1.228, Val loss 1.771\n",
      "Ep 2 (Step 068675): Train loss 0.960, Val loss 1.770\n",
      "Ep 2 (Step 068680): Train loss 0.932, Val loss 1.769\n",
      "Ep 2 (Step 068685): Train loss 0.794, Val loss 1.768\n",
      "Ep 2 (Step 068690): Train loss 1.375, Val loss 1.767\n",
      "Ep 2 (Step 068695): Train loss 0.745, Val loss 1.766\n",
      "Ep 2 (Step 068700): Train loss 0.977, Val loss 1.765\n",
      "Ep 2 (Step 068705): Train loss 1.022, Val loss 1.766\n",
      "Ep 2 (Step 068710): Train loss 1.229, Val loss 1.768\n",
      "Ep 2 (Step 068715): Train loss 0.840, Val loss 1.770\n",
      "Ep 2 (Step 068720): Train loss 0.955, Val loss 1.770\n",
      "Ep 2 (Step 068725): Train loss 1.099, Val loss 1.769\n",
      "Ep 2 (Step 068730): Train loss 0.911, Val loss 1.768\n",
      "Ep 2 (Step 068735): Train loss 1.010, Val loss 1.769\n",
      "Ep 2 (Step 068740): Train loss 0.901, Val loss 1.770\n",
      "Ep 2 (Step 068745): Train loss 1.264, Val loss 1.769\n",
      "Ep 2 (Step 068750): Train loss 0.934, Val loss 1.769\n",
      "Ep 2 (Step 068755): Train loss 0.905, Val loss 1.768\n",
      "Ep 2 (Step 068760): Train loss 1.053, Val loss 1.768\n",
      "Ep 2 (Step 068765): Train loss 1.109, Val loss 1.768\n",
      "Ep 2 (Step 068770): Train loss 0.858, Val loss 1.769\n",
      "Ep 2 (Step 068775): Train loss 1.421, Val loss 1.769\n",
      "Ep 2 (Step 068780): Train loss 1.333, Val loss 1.769\n",
      "Ep 2 (Step 068785): Train loss 1.117, Val loss 1.769\n",
      "Ep 2 (Step 068790): Train loss 1.120, Val loss 1.769\n",
      "Ep 2 (Step 068795): Train loss 0.980, Val loss 1.769\n",
      "Ep 2 (Step 068800): Train loss 1.141, Val loss 1.769\n",
      "Ep 2 (Step 068805): Train loss 0.812, Val loss 1.769\n",
      "Ep 2 (Step 068810): Train loss 1.099, Val loss 1.769\n",
      "Ep 2 (Step 068815): Train loss 1.001, Val loss 1.769\n",
      "Ep 2 (Step 068820): Train loss 1.160, Val loss 1.769\n",
      "Ep 2 (Step 068825): Train loss 0.846, Val loss 1.769\n",
      "Ep 2 (Step 068830): Train loss 1.145, Val loss 1.769\n",
      "Ep 2 (Step 068835): Train loss 1.006, Val loss 1.770\n",
      "Ep 2 (Step 068840): Train loss 0.809, Val loss 1.770\n",
      "Ep 2 (Step 068845): Train loss 1.135, Val loss 1.770\n",
      "Ep 2 (Step 068850): Train loss 0.967, Val loss 1.769\n",
      "Ep 2 (Step 068855): Train loss 1.076, Val loss 1.768\n",
      "Ep 2 (Step 068860): Train loss 0.843, Val loss 1.767\n",
      "Ep 2 (Step 068865): Train loss 1.053, Val loss 1.767\n",
      "Ep 2 (Step 068870): Train loss 1.060, Val loss 1.766\n",
      "Ep 2 (Step 068875): Train loss 0.749, Val loss 1.766\n",
      "Ep 2 (Step 068880): Train loss 0.998, Val loss 1.766\n",
      "Ep 2 (Step 068885): Train loss 1.083, Val loss 1.766\n",
      "Ep 2 (Step 068890): Train loss 0.867, Val loss 1.766\n",
      "Ep 2 (Step 068895): Train loss 1.284, Val loss 1.766\n",
      "Ep 2 (Step 068900): Train loss 1.005, Val loss 1.765\n",
      "Ep 2 (Step 068905): Train loss 0.924, Val loss 1.765\n",
      "Ep 2 (Step 068910): Train loss 0.943, Val loss 1.764\n",
      "Ep 2 (Step 068915): Train loss 1.072, Val loss 1.763\n",
      "Ep 2 (Step 068920): Train loss 1.115, Val loss 1.762\n",
      "Ep 2 (Step 068925): Train loss 1.088, Val loss 1.763\n",
      "Ep 2 (Step 068930): Train loss 0.785, Val loss 1.763\n",
      "Ep 2 (Step 068935): Train loss 0.826, Val loss 1.764\n",
      "Ep 2 (Step 068940): Train loss 1.151, Val loss 1.765\n",
      "Ep 2 (Step 068945): Train loss 1.035, Val loss 1.767\n",
      "Ep 2 (Step 068950): Train loss 1.108, Val loss 1.767\n",
      "Ep 2 (Step 068955): Train loss 0.975, Val loss 1.768\n",
      "Ep 2 (Step 068960): Train loss 0.878, Val loss 1.767\n",
      "Ep 2 (Step 068965): Train loss 1.121, Val loss 1.768\n",
      "Ep 2 (Step 068970): Train loss 1.117, Val loss 1.768\n",
      "Ep 2 (Step 068975): Train loss 1.004, Val loss 1.771\n",
      "Ep 2 (Step 068980): Train loss 0.860, Val loss 1.772\n",
      "Ep 2 (Step 068985): Train loss 1.056, Val loss 1.774\n",
      "Ep 2 (Step 068990): Train loss 1.205, Val loss 1.775\n",
      "Ep 2 (Step 068995): Train loss 0.986, Val loss 1.777\n",
      "Ep 2 (Step 069000): Train loss 0.995, Val loss 1.778\n",
      "Ep 2 (Step 069005): Train loss 0.871, Val loss 1.778\n",
      "Ep 2 (Step 069010): Train loss 0.727, Val loss 1.778\n",
      "Ep 2 (Step 069015): Train loss 1.118, Val loss 1.778\n",
      "Ep 2 (Step 069020): Train loss 1.122, Val loss 1.779\n",
      "Ep 2 (Step 069025): Train loss 1.022, Val loss 1.779\n",
      "Ep 2 (Step 069030): Train loss 1.140, Val loss 1.778\n",
      "Ep 2 (Step 069035): Train loss 1.161, Val loss 1.776\n",
      "Ep 2 (Step 069040): Train loss 1.019, Val loss 1.774\n",
      "Ep 2 (Step 069045): Train loss 0.884, Val loss 1.774\n",
      "Ep 2 (Step 069050): Train loss 0.974, Val loss 1.775\n",
      "Ep 2 (Step 069055): Train loss 0.857, Val loss 1.775\n",
      "Ep 2 (Step 069060): Train loss 1.074, Val loss 1.776\n",
      "Ep 2 (Step 069065): Train loss 1.050, Val loss 1.776\n",
      "Ep 2 (Step 069070): Train loss 1.406, Val loss 1.777\n",
      "Ep 2 (Step 069075): Train loss 0.868, Val loss 1.778\n",
      "Ep 2 (Step 069080): Train loss 0.912, Val loss 1.780\n",
      "Ep 2 (Step 069085): Train loss 0.991, Val loss 1.781\n",
      "Ep 2 (Step 069090): Train loss 0.853, Val loss 1.782\n",
      "Ep 2 (Step 069095): Train loss 1.009, Val loss 1.783\n",
      "Ep 2 (Step 069100): Train loss 1.019, Val loss 1.783\n",
      "Ep 2 (Step 069105): Train loss 1.161, Val loss 1.783\n",
      "Ep 2 (Step 069110): Train loss 0.930, Val loss 1.783\n",
      "Ep 2 (Step 069115): Train loss 0.790, Val loss 1.782\n",
      "Ep 2 (Step 069120): Train loss 0.997, Val loss 1.783\n",
      "Ep 2 (Step 069125): Train loss 0.935, Val loss 1.785\n",
      "Ep 2 (Step 069130): Train loss 1.013, Val loss 1.786\n",
      "Ep 2 (Step 069135): Train loss 1.203, Val loss 1.787\n",
      "Ep 2 (Step 069140): Train loss 1.118, Val loss 1.788\n",
      "Ep 2 (Step 069145): Train loss 1.106, Val loss 1.789\n",
      "Ep 2 (Step 069150): Train loss 0.850, Val loss 1.789\n",
      "Ep 2 (Step 069155): Train loss 1.039, Val loss 1.788\n",
      "Ep 2 (Step 069160): Train loss 1.066, Val loss 1.786\n",
      "Ep 2 (Step 069165): Train loss 1.027, Val loss 1.784\n",
      "Ep 2 (Step 069170): Train loss 0.867, Val loss 1.783\n",
      "Ep 2 (Step 069175): Train loss 1.107, Val loss 1.784\n",
      "Ep 2 (Step 069180): Train loss 0.861, Val loss 1.785\n",
      "Ep 2 (Step 069185): Train loss 1.053, Val loss 1.783\n",
      "Ep 2 (Step 069190): Train loss 0.901, Val loss 1.783\n",
      "Ep 2 (Step 069195): Train loss 0.822, Val loss 1.782\n",
      "Ep 2 (Step 069200): Train loss 1.261, Val loss 1.783\n",
      "Ep 2 (Step 069205): Train loss 0.912, Val loss 1.783\n",
      "Ep 2 (Step 069210): Train loss 1.280, Val loss 1.782\n",
      "Ep 2 (Step 069215): Train loss 0.940, Val loss 1.783\n",
      "Ep 2 (Step 069220): Train loss 0.993, Val loss 1.784\n",
      "Ep 2 (Step 069225): Train loss 1.414, Val loss 1.783\n",
      "Ep 2 (Step 069230): Train loss 1.013, Val loss 1.783\n",
      "Ep 2 (Step 069235): Train loss 1.038, Val loss 1.783\n",
      "Ep 2 (Step 069240): Train loss 1.149, Val loss 1.782\n",
      "Ep 2 (Step 069245): Train loss 1.168, Val loss 1.783\n",
      "Ep 2 (Step 069250): Train loss 0.930, Val loss 1.782\n",
      "Ep 2 (Step 069255): Train loss 1.221, Val loss 1.782\n",
      "Ep 2 (Step 069260): Train loss 1.423, Val loss 1.780\n",
      "Ep 2 (Step 069265): Train loss 0.951, Val loss 1.778\n",
      "Ep 2 (Step 069270): Train loss 0.950, Val loss 1.775\n",
      "Ep 2 (Step 069275): Train loss 0.922, Val loss 1.773\n",
      "Ep 2 (Step 069280): Train loss 0.830, Val loss 1.771\n",
      "Ep 2 (Step 069285): Train loss 1.135, Val loss 1.770\n",
      "Ep 2 (Step 069290): Train loss 0.873, Val loss 1.770\n",
      "Ep 2 (Step 069295): Train loss 1.060, Val loss 1.771\n",
      "Ep 2 (Step 069300): Train loss 1.105, Val loss 1.771\n",
      "Ep 2 (Step 069305): Train loss 0.884, Val loss 1.772\n",
      "Ep 2 (Step 069310): Train loss 1.031, Val loss 1.773\n",
      "Ep 2 (Step 069315): Train loss 1.203, Val loss 1.773\n",
      "Ep 2 (Step 069320): Train loss 1.293, Val loss 1.773\n",
      "Ep 2 (Step 069325): Train loss 1.138, Val loss 1.774\n",
      "Ep 2 (Step 069330): Train loss 0.905, Val loss 1.775\n",
      "Ep 2 (Step 069335): Train loss 1.020, Val loss 1.779\n",
      "Ep 2 (Step 069340): Train loss 0.993, Val loss 1.782\n",
      "Ep 2 (Step 069345): Train loss 1.058, Val loss 1.785\n",
      "Ep 2 (Step 069350): Train loss 1.040, Val loss 1.785\n",
      "Ep 2 (Step 069355): Train loss 0.859, Val loss 1.784\n",
      "Ep 2 (Step 069360): Train loss 0.908, Val loss 1.783\n",
      "Ep 2 (Step 069365): Train loss 1.073, Val loss 1.782\n",
      "Ep 2 (Step 069370): Train loss 0.739, Val loss 1.778\n",
      "Ep 2 (Step 069375): Train loss 1.138, Val loss 1.776\n",
      "Ep 2 (Step 069380): Train loss 1.255, Val loss 1.775\n",
      "Ep 2 (Step 069385): Train loss 1.335, Val loss 1.774\n",
      "Ep 2 (Step 069390): Train loss 1.199, Val loss 1.773\n",
      "Ep 2 (Step 069395): Train loss 1.206, Val loss 1.773\n",
      "Ep 2 (Step 069400): Train loss 0.873, Val loss 1.773\n",
      "Ep 2 (Step 069405): Train loss 1.078, Val loss 1.773\n",
      "Ep 2 (Step 069410): Train loss 0.981, Val loss 1.774\n",
      "Ep 2 (Step 069415): Train loss 1.041, Val loss 1.774\n",
      "Ep 2 (Step 069420): Train loss 0.809, Val loss 1.775\n",
      "Ep 2 (Step 069425): Train loss 0.785, Val loss 1.776\n",
      "Ep 2 (Step 069430): Train loss 0.932, Val loss 1.776\n",
      "Ep 2 (Step 069435): Train loss 1.189, Val loss 1.776\n",
      "Ep 2 (Step 069440): Train loss 0.986, Val loss 1.778\n",
      "Ep 2 (Step 069445): Train loss 1.272, Val loss 1.777\n",
      "Ep 2 (Step 069450): Train loss 1.131, Val loss 1.776\n",
      "Ep 2 (Step 069455): Train loss 0.848, Val loss 1.775\n",
      "Ep 2 (Step 069460): Train loss 0.909, Val loss 1.774\n",
      "Ep 2 (Step 069465): Train loss 0.901, Val loss 1.774\n",
      "Ep 2 (Step 069470): Train loss 0.734, Val loss 1.775\n",
      "Ep 2 (Step 069475): Train loss 1.327, Val loss 1.776\n",
      "Ep 2 (Step 069480): Train loss 1.096, Val loss 1.776\n",
      "Ep 2 (Step 069485): Train loss 1.039, Val loss 1.776\n",
      "Ep 2 (Step 069490): Train loss 1.078, Val loss 1.774\n",
      "Ep 2 (Step 069495): Train loss 1.052, Val loss 1.772\n",
      "Ep 2 (Step 069500): Train loss 0.823, Val loss 1.770\n",
      "Ep 2 (Step 069505): Train loss 0.827, Val loss 1.770\n",
      "Ep 2 (Step 069510): Train loss 0.770, Val loss 1.770\n",
      "Ep 2 (Step 069515): Train loss 1.129, Val loss 1.770\n",
      "Ep 2 (Step 069520): Train loss 0.972, Val loss 1.771\n",
      "Ep 2 (Step 069525): Train loss 0.982, Val loss 1.772\n",
      "Ep 2 (Step 069530): Train loss 1.099, Val loss 1.770\n",
      "Ep 2 (Step 069535): Train loss 1.156, Val loss 1.769\n",
      "Ep 2 (Step 069540): Train loss 0.843, Val loss 1.769\n",
      "Ep 2 (Step 069545): Train loss 1.094, Val loss 1.769\n",
      "Ep 2 (Step 069550): Train loss 0.812, Val loss 1.768\n",
      "Ep 2 (Step 069555): Train loss 1.293, Val loss 1.768\n",
      "Ep 2 (Step 069560): Train loss 0.883, Val loss 1.768\n",
      "Ep 2 (Step 069565): Train loss 0.747, Val loss 1.768\n",
      "Ep 2 (Step 069570): Train loss 1.387, Val loss 1.767\n",
      "Ep 2 (Step 069575): Train loss 1.126, Val loss 1.767\n",
      "Ep 2 (Step 069580): Train loss 1.236, Val loss 1.767\n",
      "Ep 2 (Step 069585): Train loss 1.030, Val loss 1.767\n",
      "Ep 2 (Step 069590): Train loss 1.016, Val loss 1.767\n",
      "Ep 2 (Step 069595): Train loss 0.845, Val loss 1.768\n",
      "Ep 2 (Step 069600): Train loss 1.103, Val loss 1.768\n",
      "Ep 2 (Step 069605): Train loss 0.966, Val loss 1.769\n",
      "Ep 2 (Step 069610): Train loss 1.056, Val loss 1.769\n",
      "Ep 2 (Step 069615): Train loss 1.068, Val loss 1.769\n",
      "Ep 2 (Step 069620): Train loss 1.107, Val loss 1.768\n",
      "Ep 2 (Step 069625): Train loss 0.968, Val loss 1.768\n",
      "Ep 2 (Step 069630): Train loss 1.157, Val loss 1.768\n",
      "Ep 2 (Step 069635): Train loss 0.890, Val loss 1.769\n",
      "Ep 2 (Step 069640): Train loss 1.027, Val loss 1.769\n",
      "Ep 2 (Step 069645): Train loss 1.053, Val loss 1.768\n",
      "Ep 2 (Step 069650): Train loss 0.855, Val loss 1.768\n",
      "Ep 2 (Step 069655): Train loss 1.206, Val loss 1.769\n",
      "Ep 2 (Step 069660): Train loss 0.923, Val loss 1.770\n",
      "Ep 2 (Step 069665): Train loss 1.072, Val loss 1.770\n",
      "Ep 2 (Step 069670): Train loss 0.958, Val loss 1.771\n",
      "Ep 2 (Step 069675): Train loss 0.842, Val loss 1.772\n",
      "Ep 2 (Step 069680): Train loss 1.015, Val loss 1.773\n",
      "Ep 2 (Step 069685): Train loss 1.042, Val loss 1.773\n",
      "Ep 2 (Step 069690): Train loss 0.812, Val loss 1.772\n",
      "Ep 2 (Step 069695): Train loss 1.153, Val loss 1.772\n",
      "Ep 2 (Step 069700): Train loss 0.952, Val loss 1.771\n",
      "Ep 2 (Step 069705): Train loss 1.226, Val loss 1.770\n",
      "Ep 2 (Step 069710): Train loss 0.859, Val loss 1.769\n",
      "Ep 2 (Step 069715): Train loss 0.766, Val loss 1.767\n",
      "Ep 2 (Step 069720): Train loss 0.852, Val loss 1.766\n",
      "Ep 2 (Step 069725): Train loss 1.249, Val loss 1.765\n",
      "Ep 2 (Step 069730): Train loss 0.915, Val loss 1.764\n",
      "Ep 2 (Step 069735): Train loss 0.956, Val loss 1.764\n",
      "Ep 2 (Step 069740): Train loss 1.339, Val loss 1.764\n",
      "Ep 2 (Step 069745): Train loss 0.935, Val loss 1.764\n",
      "Ep 2 (Step 069750): Train loss 1.120, Val loss 1.765\n",
      "Ep 2 (Step 069755): Train loss 0.925, Val loss 1.767\n",
      "Ep 2 (Step 069760): Train loss 0.908, Val loss 1.768\n",
      "Ep 2 (Step 069765): Train loss 1.040, Val loss 1.769\n",
      "Ep 2 (Step 069770): Train loss 0.838, Val loss 1.770\n",
      "Ep 2 (Step 069775): Train loss 1.174, Val loss 1.771\n",
      "Ep 2 (Step 069780): Train loss 1.016, Val loss 1.772\n",
      "Ep 2 (Step 069785): Train loss 1.232, Val loss 1.772\n",
      "Ep 2 (Step 069790): Train loss 0.904, Val loss 1.773\n",
      "Ep 2 (Step 069795): Train loss 1.099, Val loss 1.774\n",
      "Ep 2 (Step 069800): Train loss 1.153, Val loss 1.776\n",
      "Ep 2 (Step 069805): Train loss 1.085, Val loss 1.777\n",
      "Ep 2 (Step 069810): Train loss 0.926, Val loss 1.777\n",
      "Ep 2 (Step 069815): Train loss 0.862, Val loss 1.778\n",
      "Ep 2 (Step 069820): Train loss 0.847, Val loss 1.778\n",
      "Ep 2 (Step 069825): Train loss 1.090, Val loss 1.779\n",
      "Ep 2 (Step 069830): Train loss 0.846, Val loss 1.781\n",
      "Ep 2 (Step 069835): Train loss 0.861, Val loss 1.783\n",
      "Ep 2 (Step 069840): Train loss 1.242, Val loss 1.783\n",
      "Ep 2 (Step 069845): Train loss 1.177, Val loss 1.783\n",
      "Ep 2 (Step 069850): Train loss 1.092, Val loss 1.784\n",
      "Ep 2 (Step 069855): Train loss 0.817, Val loss 1.785\n",
      "Ep 2 (Step 069860): Train loss 1.069, Val loss 1.784\n",
      "Ep 2 (Step 069865): Train loss 0.708, Val loss 1.784\n",
      "Ep 2 (Step 069870): Train loss 0.998, Val loss 1.783\n",
      "Ep 2 (Step 069875): Train loss 1.170, Val loss 1.783\n",
      "Ep 2 (Step 069880): Train loss 1.036, Val loss 1.785\n",
      "Ep 2 (Step 069885): Train loss 1.229, Val loss 1.786\n",
      "Ep 2 (Step 069890): Train loss 0.900, Val loss 1.787\n",
      "Ep 2 (Step 069895): Train loss 1.193, Val loss 1.787\n",
      "Ep 2 (Step 069900): Train loss 0.723, Val loss 1.786\n",
      "Ep 2 (Step 069905): Train loss 0.906, Val loss 1.785\n",
      "Ep 2 (Step 069910): Train loss 0.767, Val loss 1.783\n",
      "Ep 2 (Step 069915): Train loss 1.023, Val loss 1.782\n",
      "Ep 2 (Step 069920): Train loss 0.874, Val loss 1.780\n",
      "Ep 2 (Step 069925): Train loss 1.196, Val loss 1.776\n",
      "Ep 2 (Step 069930): Train loss 1.314, Val loss 1.775\n",
      "Ep 2 (Step 069935): Train loss 1.060, Val loss 1.775\n",
      "Ep 2 (Step 069940): Train loss 1.308, Val loss 1.775\n",
      "Ep 2 (Step 069945): Train loss 1.020, Val loss 1.775\n",
      "Ep 2 (Step 069950): Train loss 1.112, Val loss 1.775\n",
      "Ep 2 (Step 069955): Train loss 1.472, Val loss 1.775\n",
      "Ep 2 (Step 069960): Train loss 1.219, Val loss 1.775\n",
      "Ep 2 (Step 069965): Train loss 0.994, Val loss 1.775\n",
      "Ep 2 (Step 069970): Train loss 0.832, Val loss 1.776\n",
      "Ep 2 (Step 069975): Train loss 0.999, Val loss 1.776\n",
      "Ep 2 (Step 069980): Train loss 0.856, Val loss 1.776\n",
      "Ep 2 (Step 069985): Train loss 1.216, Val loss 1.777\n",
      "Ep 2 (Step 069990): Train loss 1.015, Val loss 1.777\n",
      "Ep 2 (Step 069995): Train loss 1.014, Val loss 1.777\n",
      "Ep 2 (Step 070000): Train loss 1.078, Val loss 1.775\n",
      "Ep 2 (Step 070005): Train loss 0.770, Val loss 1.774\n",
      "Ep 2 (Step 070010): Train loss 1.232, Val loss 1.775\n",
      "Ep 2 (Step 070015): Train loss 1.017, Val loss 1.775\n",
      "Ep 2 (Step 070020): Train loss 1.039, Val loss 1.774\n",
      "Ep 2 (Step 070025): Train loss 0.888, Val loss 1.775\n",
      "Ep 2 (Step 070030): Train loss 0.897, Val loss 1.773\n",
      "Ep 2 (Step 070035): Train loss 0.810, Val loss 1.772\n",
      "Ep 2 (Step 070040): Train loss 0.884, Val loss 1.772\n",
      "Ep 2 (Step 070045): Train loss 1.046, Val loss 1.774\n",
      "Ep 2 (Step 070050): Train loss 1.029, Val loss 1.775\n",
      "Ep 2 (Step 070055): Train loss 1.286, Val loss 1.776\n",
      "Ep 2 (Step 070060): Train loss 0.887, Val loss 1.776\n",
      "Ep 2 (Step 070065): Train loss 1.065, Val loss 1.778\n",
      "Ep 2 (Step 070070): Train loss 0.950, Val loss 1.779\n",
      "Ep 2 (Step 070075): Train loss 1.086, Val loss 1.780\n",
      "Ep 2 (Step 070080): Train loss 1.002, Val loss 1.780\n",
      "Ep 2 (Step 070085): Train loss 1.007, Val loss 1.779\n",
      "Ep 2 (Step 070090): Train loss 0.777, Val loss 1.779\n",
      "Ep 2 (Step 070095): Train loss 1.024, Val loss 1.779\n",
      "Ep 2 (Step 070100): Train loss 0.970, Val loss 1.779\n",
      "Ep 2 (Step 070105): Train loss 1.317, Val loss 1.779\n",
      "Ep 2 (Step 070110): Train loss 1.025, Val loss 1.780\n",
      "Ep 2 (Step 070115): Train loss 1.254, Val loss 1.778\n",
      "Ep 2 (Step 070120): Train loss 1.013, Val loss 1.777\n",
      "Ep 2 (Step 070125): Train loss 0.882, Val loss 1.775\n",
      "Ep 2 (Step 070130): Train loss 0.931, Val loss 1.774\n",
      "Ep 2 (Step 070135): Train loss 1.114, Val loss 1.773\n",
      "Ep 2 (Step 070140): Train loss 1.115, Val loss 1.772\n",
      "Ep 2 (Step 070145): Train loss 0.976, Val loss 1.772\n",
      "Ep 2 (Step 070150): Train loss 0.906, Val loss 1.772\n",
      "Ep 2 (Step 070155): Train loss 1.000, Val loss 1.772\n",
      "Ep 2 (Step 070160): Train loss 1.025, Val loss 1.773\n",
      "Ep 2 (Step 070165): Train loss 0.915, Val loss 1.774\n",
      "Ep 2 (Step 070170): Train loss 0.962, Val loss 1.774\n",
      "Ep 2 (Step 070175): Train loss 1.132, Val loss 1.775\n",
      "Ep 2 (Step 070180): Train loss 1.017, Val loss 1.776\n",
      "Ep 2 (Step 070185): Train loss 0.902, Val loss 1.778\n",
      "Ep 2 (Step 070190): Train loss 0.896, Val loss 1.779\n",
      "Ep 2 (Step 070195): Train loss 0.984, Val loss 1.780\n",
      "Ep 2 (Step 070200): Train loss 1.167, Val loss 1.781\n",
      "Ep 2 (Step 070205): Train loss 1.024, Val loss 1.783\n",
      "Ep 2 (Step 070210): Train loss 1.122, Val loss 1.785\n",
      "Ep 2 (Step 070215): Train loss 1.092, Val loss 1.789\n",
      "Ep 2 (Step 070220): Train loss 1.442, Val loss 1.792\n",
      "Ep 2 (Step 070225): Train loss 1.082, Val loss 1.794\n",
      "Ep 2 (Step 070230): Train loss 1.253, Val loss 1.795\n",
      "Ep 2 (Step 070235): Train loss 1.143, Val loss 1.796\n",
      "Ep 2 (Step 070240): Train loss 0.759, Val loss 1.798\n",
      "Ep 2 (Step 070245): Train loss 0.957, Val loss 1.798\n",
      "Ep 2 (Step 070250): Train loss 0.902, Val loss 1.797\n",
      "Ep 2 (Step 070255): Train loss 1.038, Val loss 1.797\n",
      "Ep 2 (Step 070260): Train loss 0.744, Val loss 1.797\n",
      "Ep 2 (Step 070265): Train loss 0.962, Val loss 1.796\n",
      "Ep 2 (Step 070270): Train loss 0.962, Val loss 1.797\n",
      "Ep 2 (Step 070275): Train loss 0.909, Val loss 1.799\n",
      "Ep 2 (Step 070280): Train loss 1.121, Val loss 1.799\n",
      "Ep 2 (Step 070285): Train loss 1.264, Val loss 1.798\n",
      "Ep 2 (Step 070290): Train loss 0.979, Val loss 1.798\n",
      "Ep 2 (Step 070295): Train loss 1.132, Val loss 1.798\n",
      "Ep 2 (Step 070300): Train loss 1.047, Val loss 1.798\n",
      "Ep 2 (Step 070305): Train loss 0.901, Val loss 1.796\n",
      "Ep 2 (Step 070310): Train loss 0.986, Val loss 1.795\n",
      "Ep 2 (Step 070315): Train loss 0.976, Val loss 1.794\n",
      "Ep 2 (Step 070320): Train loss 1.077, Val loss 1.795\n",
      "Ep 2 (Step 070325): Train loss 1.021, Val loss 1.796\n",
      "Ep 2 (Step 070330): Train loss 0.891, Val loss 1.797\n",
      "Ep 2 (Step 070335): Train loss 0.999, Val loss 1.797\n",
      "Ep 2 (Step 070340): Train loss 1.072, Val loss 1.797\n",
      "Ep 2 (Step 070345): Train loss 0.860, Val loss 1.795\n",
      "Ep 2 (Step 070350): Train loss 0.928, Val loss 1.792\n",
      "Ep 2 (Step 070355): Train loss 1.031, Val loss 1.790\n",
      "Ep 2 (Step 070360): Train loss 0.870, Val loss 1.791\n",
      "Ep 2 (Step 070365): Train loss 1.074, Val loss 1.791\n",
      "Ep 2 (Step 070370): Train loss 1.209, Val loss 1.792\n",
      "Ep 2 (Step 070375): Train loss 0.852, Val loss 1.793\n",
      "Ep 2 (Step 070380): Train loss 1.077, Val loss 1.794\n",
      "Ep 2 (Step 070385): Train loss 1.152, Val loss 1.792\n",
      "Ep 2 (Step 070390): Train loss 0.892, Val loss 1.791\n",
      "Ep 2 (Step 070395): Train loss 0.804, Val loss 1.790\n",
      "Ep 2 (Step 070400): Train loss 0.926, Val loss 1.790\n",
      "Ep 2 (Step 070405): Train loss 1.067, Val loss 1.790\n",
      "Ep 2 (Step 070410): Train loss 1.281, Val loss 1.792\n",
      "Ep 2 (Step 070415): Train loss 0.946, Val loss 1.792\n",
      "Ep 2 (Step 070420): Train loss 1.057, Val loss 1.793\n",
      "Ep 2 (Step 070425): Train loss 1.069, Val loss 1.793\n",
      "Ep 2 (Step 070430): Train loss 1.004, Val loss 1.793\n",
      "Ep 2 (Step 070435): Train loss 1.265, Val loss 1.792\n",
      "Ep 2 (Step 070440): Train loss 0.913, Val loss 1.790\n",
      "Ep 2 (Step 070445): Train loss 1.236, Val loss 1.789\n",
      "Ep 2 (Step 070450): Train loss 0.921, Val loss 1.790\n",
      "Ep 2 (Step 070455): Train loss 1.401, Val loss 1.792\n",
      "Ep 2 (Step 070460): Train loss 0.980, Val loss 1.794\n",
      "Ep 2 (Step 070465): Train loss 1.099, Val loss 1.795\n",
      "Ep 2 (Step 070470): Train loss 0.939, Val loss 1.794\n",
      "Ep 2 (Step 070475): Train loss 0.844, Val loss 1.795\n",
      "Ep 2 (Step 070480): Train loss 1.214, Val loss 1.796\n",
      "Ep 2 (Step 070485): Train loss 1.209, Val loss 1.798\n",
      "Ep 2 (Step 070490): Train loss 0.984, Val loss 1.798\n",
      "Ep 2 (Step 070495): Train loss 1.033, Val loss 1.797\n",
      "Ep 2 (Step 070500): Train loss 0.879, Val loss 1.796\n",
      "Ep 2 (Step 070505): Train loss 1.072, Val loss 1.795\n",
      "Ep 2 (Step 070510): Train loss 1.046, Val loss 1.795\n",
      "Ep 2 (Step 070515): Train loss 0.899, Val loss 1.796\n",
      "Ep 2 (Step 070520): Train loss 1.234, Val loss 1.796\n",
      "Ep 2 (Step 070525): Train loss 0.975, Val loss 1.795\n",
      "Ep 2 (Step 070530): Train loss 1.246, Val loss 1.795\n",
      "Ep 2 (Step 070535): Train loss 1.231, Val loss 1.796\n",
      "Ep 2 (Step 070540): Train loss 1.526, Val loss 1.797\n",
      "Ep 2 (Step 070545): Train loss 1.239, Val loss 1.798\n",
      "Ep 2 (Step 070550): Train loss 1.086, Val loss 1.797\n",
      "Ep 2 (Step 070555): Train loss 1.133, Val loss 1.796\n",
      "Ep 2 (Step 070560): Train loss 1.027, Val loss 1.795\n",
      "Ep 2 (Step 070565): Train loss 1.157, Val loss 1.795\n",
      "Ep 2 (Step 070570): Train loss 0.803, Val loss 1.795\n",
      "Ep 2 (Step 070575): Train loss 0.922, Val loss 1.795\n",
      "Ep 2 (Step 070580): Train loss 1.162, Val loss 1.795\n",
      "Ep 2 (Step 070585): Train loss 0.779, Val loss 1.794\n",
      "Ep 2 (Step 070590): Train loss 0.922, Val loss 1.793\n",
      "Ep 2 (Step 070595): Train loss 1.175, Val loss 1.793\n",
      "Ep 2 (Step 070600): Train loss 0.666, Val loss 1.791\n",
      "Ep 2 (Step 070605): Train loss 1.072, Val loss 1.790\n",
      "Ep 2 (Step 070610): Train loss 1.381, Val loss 1.790\n",
      "Ep 2 (Step 070615): Train loss 1.164, Val loss 1.791\n",
      "Ep 2 (Step 070620): Train loss 0.903, Val loss 1.792\n",
      "Ep 2 (Step 070625): Train loss 1.242, Val loss 1.792\n",
      "Ep 2 (Step 070630): Train loss 0.885, Val loss 1.793\n",
      "Ep 2 (Step 070635): Train loss 0.921, Val loss 1.793\n",
      "Ep 2 (Step 070640): Train loss 0.947, Val loss 1.792\n",
      "Ep 2 (Step 070645): Train loss 0.809, Val loss 1.792\n",
      "Ep 2 (Step 070650): Train loss 0.902, Val loss 1.793\n",
      "Ep 2 (Step 070655): Train loss 0.908, Val loss 1.793\n",
      "Ep 2 (Step 070660): Train loss 1.108, Val loss 1.792\n",
      "Ep 2 (Step 070665): Train loss 0.825, Val loss 1.791\n",
      "Ep 2 (Step 070670): Train loss 0.902, Val loss 1.789\n",
      "Ep 2 (Step 070675): Train loss 1.003, Val loss 1.790\n",
      "Ep 2 (Step 070680): Train loss 1.084, Val loss 1.790\n",
      "Ep 2 (Step 070685): Train loss 1.255, Val loss 1.789\n",
      "Ep 2 (Step 070690): Train loss 0.956, Val loss 1.789\n",
      "Ep 2 (Step 070695): Train loss 0.977, Val loss 1.789\n",
      "Ep 2 (Step 070700): Train loss 0.848, Val loss 1.788\n",
      "Ep 2 (Step 070705): Train loss 1.102, Val loss 1.787\n",
      "Ep 2 (Step 070710): Train loss 1.033, Val loss 1.787\n",
      "Ep 2 (Step 070715): Train loss 1.219, Val loss 1.787\n",
      "Ep 2 (Step 070720): Train loss 0.994, Val loss 1.788\n",
      "Ep 2 (Step 070725): Train loss 1.201, Val loss 1.788\n",
      "Ep 2 (Step 070730): Train loss 1.066, Val loss 1.788\n",
      "Ep 2 (Step 070735): Train loss 0.861, Val loss 1.787\n",
      "Ep 2 (Step 070740): Train loss 1.079, Val loss 1.787\n",
      "Ep 2 (Step 070745): Train loss 1.060, Val loss 1.787\n",
      "Ep 2 (Step 070750): Train loss 1.113, Val loss 1.788\n",
      "Ep 2 (Step 070755): Train loss 1.111, Val loss 1.790\n",
      "Ep 2 (Step 070760): Train loss 0.908, Val loss 1.790\n",
      "Ep 2 (Step 070765): Train loss 0.919, Val loss 1.790\n",
      "Ep 2 (Step 070770): Train loss 1.013, Val loss 1.790\n",
      "Ep 2 (Step 070775): Train loss 0.898, Val loss 1.789\n",
      "Ep 2 (Step 070780): Train loss 1.277, Val loss 1.789\n",
      "Ep 2 (Step 070785): Train loss 1.136, Val loss 1.790\n",
      "Ep 2 (Step 070790): Train loss 1.177, Val loss 1.790\n",
      "Ep 2 (Step 070795): Train loss 1.027, Val loss 1.790\n",
      "Ep 2 (Step 070800): Train loss 1.009, Val loss 1.790\n",
      "Ep 2 (Step 070805): Train loss 0.890, Val loss 1.790\n",
      "Ep 2 (Step 070810): Train loss 0.810, Val loss 1.790\n",
      "Ep 2 (Step 070815): Train loss 1.427, Val loss 1.790\n",
      "Ep 2 (Step 070820): Train loss 0.662, Val loss 1.791\n",
      "Ep 2 (Step 070825): Train loss 0.856, Val loss 1.791\n",
      "Ep 2 (Step 070830): Train loss 1.152, Val loss 1.791\n",
      "Ep 2 (Step 070835): Train loss 1.037, Val loss 1.792\n",
      "Ep 2 (Step 070840): Train loss 1.038, Val loss 1.792\n",
      "Ep 2 (Step 070845): Train loss 0.774, Val loss 1.793\n",
      "Ep 2 (Step 070850): Train loss 1.164, Val loss 1.793\n",
      "Ep 2 (Step 070855): Train loss 0.852, Val loss 1.794\n",
      "Ep 2 (Step 070860): Train loss 1.230, Val loss 1.794\n",
      "Ep 2 (Step 070865): Train loss 1.219, Val loss 1.794\n",
      "Ep 2 (Step 070870): Train loss 0.932, Val loss 1.793\n",
      "Ep 2 (Step 070875): Train loss 1.119, Val loss 1.793\n",
      "Ep 2 (Step 070880): Train loss 0.942, Val loss 1.792\n",
      "Ep 2 (Step 070885): Train loss 1.247, Val loss 1.791\n",
      "Ep 2 (Step 070890): Train loss 1.133, Val loss 1.791\n",
      "Ep 2 (Step 070895): Train loss 1.092, Val loss 1.789\n",
      "Ep 2 (Step 070900): Train loss 1.129, Val loss 1.789\n",
      "Ep 2 (Step 070905): Train loss 1.128, Val loss 1.789\n",
      "Ep 2 (Step 070910): Train loss 1.341, Val loss 1.788\n",
      "Ep 2 (Step 070915): Train loss 1.041, Val loss 1.789\n",
      "Ep 2 (Step 070920): Train loss 0.882, Val loss 1.790\n",
      "Ep 2 (Step 070925): Train loss 0.932, Val loss 1.789\n",
      "Ep 2 (Step 070930): Train loss 1.364, Val loss 1.789\n",
      "Ep 2 (Step 070935): Train loss 1.083, Val loss 1.789\n",
      "Ep 2 (Step 070940): Train loss 0.796, Val loss 1.788\n",
      "Ep 2 (Step 070945): Train loss 0.972, Val loss 1.788\n",
      "Ep 2 (Step 070950): Train loss 1.023, Val loss 1.789\n",
      "Ep 2 (Step 070955): Train loss 0.900, Val loss 1.791\n",
      "Ep 2 (Step 070960): Train loss 1.019, Val loss 1.792\n",
      "Ep 2 (Step 070965): Train loss 0.750, Val loss 1.792\n",
      "Ep 2 (Step 070970): Train loss 1.259, Val loss 1.791\n",
      "Ep 2 (Step 070975): Train loss 0.841, Val loss 1.790\n",
      "Ep 2 (Step 070980): Train loss 1.141, Val loss 1.789\n",
      "Ep 2 (Step 070985): Train loss 1.074, Val loss 1.783\n",
      "Ep 2 (Step 070990): Train loss 1.096, Val loss 1.781\n",
      "Ep 2 (Step 070995): Train loss 0.936, Val loss 1.779\n",
      "Ep 2 (Step 071000): Train loss 1.078, Val loss 1.777\n",
      "Ep 2 (Step 071005): Train loss 0.959, Val loss 1.775\n",
      "Ep 2 (Step 071010): Train loss 0.865, Val loss 1.774\n",
      "Ep 2 (Step 071015): Train loss 1.185, Val loss 1.774\n",
      "Ep 2 (Step 071020): Train loss 1.128, Val loss 1.773\n",
      "Ep 2 (Step 071025): Train loss 0.833, Val loss 1.773\n",
      "Ep 2 (Step 071030): Train loss 0.925, Val loss 1.773\n",
      "Ep 2 (Step 071035): Train loss 1.273, Val loss 1.774\n",
      "Ep 2 (Step 071040): Train loss 1.185, Val loss 1.774\n",
      "Ep 2 (Step 071045): Train loss 0.977, Val loss 1.774\n",
      "Ep 2 (Step 071050): Train loss 1.075, Val loss 1.774\n",
      "Ep 2 (Step 071055): Train loss 0.933, Val loss 1.775\n",
      "Ep 2 (Step 071060): Train loss 1.128, Val loss 1.775\n",
      "Ep 2 (Step 071065): Train loss 1.000, Val loss 1.776\n",
      "Ep 2 (Step 071070): Train loss 1.089, Val loss 1.775\n",
      "Ep 2 (Step 071075): Train loss 0.828, Val loss 1.776\n",
      "Ep 2 (Step 071080): Train loss 1.112, Val loss 1.774\n",
      "Ep 2 (Step 071085): Train loss 0.978, Val loss 1.773\n",
      "Ep 2 (Step 071090): Train loss 1.254, Val loss 1.772\n",
      "Ep 2 (Step 071095): Train loss 1.149, Val loss 1.768\n",
      "Ep 2 (Step 071100): Train loss 1.208, Val loss 1.765\n",
      "Ep 2 (Step 071105): Train loss 0.803, Val loss 1.763\n",
      "Ep 2 (Step 071110): Train loss 1.175, Val loss 1.762\n",
      "Ep 2 (Step 071115): Train loss 1.043, Val loss 1.762\n",
      "Ep 2 (Step 071120): Train loss 1.039, Val loss 1.760\n",
      "Ep 2 (Step 071125): Train loss 1.033, Val loss 1.761\n",
      "Ep 2 (Step 071130): Train loss 1.013, Val loss 1.762\n",
      "Ep 2 (Step 071135): Train loss 0.844, Val loss 1.763\n",
      "Ep 2 (Step 071140): Train loss 1.004, Val loss 1.765\n",
      "Ep 2 (Step 071145): Train loss 1.068, Val loss 1.766\n",
      "Ep 2 (Step 071150): Train loss 1.170, Val loss 1.767\n",
      "Ep 2 (Step 071155): Train loss 1.206, Val loss 1.768\n",
      "Ep 2 (Step 071160): Train loss 1.080, Val loss 1.768\n",
      "Ep 2 (Step 071165): Train loss 0.832, Val loss 1.770\n",
      "Ep 2 (Step 071170): Train loss 0.763, Val loss 1.771\n",
      "Ep 2 (Step 071175): Train loss 0.950, Val loss 1.771\n",
      "Ep 2 (Step 071180): Train loss 0.811, Val loss 1.770\n",
      "Ep 2 (Step 071185): Train loss 1.019, Val loss 1.770\n",
      "Ep 2 (Step 071190): Train loss 1.270, Val loss 1.769\n",
      "Ep 2 (Step 071195): Train loss 1.212, Val loss 1.768\n",
      "Ep 2 (Step 071200): Train loss 0.820, Val loss 1.768\n",
      "Ep 2 (Step 071205): Train loss 1.159, Val loss 1.768\n",
      "Ep 2 (Step 071210): Train loss 1.024, Val loss 1.769\n",
      "Ep 2 (Step 071215): Train loss 0.943, Val loss 1.769\n",
      "Ep 2 (Step 071220): Train loss 1.098, Val loss 1.770\n",
      "Ep 2 (Step 071225): Train loss 1.364, Val loss 1.770\n",
      "Ep 2 (Step 071230): Train loss 0.824, Val loss 1.769\n",
      "Ep 2 (Step 071235): Train loss 0.950, Val loss 1.767\n",
      "Ep 2 (Step 071240): Train loss 1.321, Val loss 1.767\n",
      "Ep 2 (Step 071245): Train loss 1.303, Val loss 1.768\n",
      "Ep 2 (Step 071250): Train loss 1.049, Val loss 1.768\n",
      "Ep 2 (Step 071255): Train loss 0.963, Val loss 1.768\n",
      "Ep 2 (Step 071260): Train loss 1.123, Val loss 1.769\n",
      "Ep 2 (Step 071265): Train loss 1.009, Val loss 1.770\n",
      "Ep 2 (Step 071270): Train loss 0.985, Val loss 1.771\n",
      "Ep 2 (Step 071275): Train loss 0.989, Val loss 1.773\n",
      "Ep 2 (Step 071280): Train loss 1.022, Val loss 1.775\n",
      "Ep 2 (Step 071285): Train loss 0.734, Val loss 1.778\n",
      "Ep 2 (Step 071290): Train loss 1.161, Val loss 1.779\n",
      "Ep 2 (Step 071295): Train loss 0.697, Val loss 1.779\n",
      "Ep 2 (Step 071300): Train loss 1.194, Val loss 1.779\n",
      "Ep 2 (Step 071305): Train loss 1.046, Val loss 1.779\n",
      "Ep 2 (Step 071310): Train loss 0.907, Val loss 1.779\n",
      "Ep 2 (Step 071315): Train loss 0.657, Val loss 1.778\n",
      "Ep 2 (Step 071320): Train loss 0.923, Val loss 1.777\n",
      "Ep 2 (Step 071325): Train loss 1.048, Val loss 1.776\n",
      "Ep 2 (Step 071330): Train loss 0.717, Val loss 1.775\n",
      "Ep 2 (Step 071335): Train loss 1.049, Val loss 1.776\n",
      "Ep 2 (Step 071340): Train loss 0.946, Val loss 1.776\n",
      "Ep 2 (Step 071345): Train loss 1.174, Val loss 1.776\n",
      "Ep 2 (Step 071350): Train loss 1.232, Val loss 1.776\n",
      "Ep 2 (Step 071355): Train loss 1.284, Val loss 1.775\n",
      "Ep 2 (Step 071360): Train loss 1.305, Val loss 1.775\n",
      "Ep 2 (Step 071365): Train loss 0.947, Val loss 1.775\n",
      "Ep 2 (Step 071370): Train loss 0.925, Val loss 1.773\n",
      "Ep 2 (Step 071375): Train loss 1.164, Val loss 1.772\n",
      "Ep 2 (Step 071380): Train loss 0.792, Val loss 1.773\n",
      "Ep 2 (Step 071385): Train loss 0.909, Val loss 1.774\n",
      "Ep 2 (Step 071390): Train loss 0.770, Val loss 1.775\n",
      "Ep 2 (Step 071395): Train loss 0.992, Val loss 1.777\n",
      "Ep 2 (Step 071400): Train loss 0.837, Val loss 1.778\n",
      "Ep 2 (Step 071405): Train loss 0.937, Val loss 1.780\n",
      "Ep 2 (Step 071410): Train loss 0.915, Val loss 1.782\n",
      "Ep 2 (Step 071415): Train loss 1.115, Val loss 1.782\n",
      "Ep 2 (Step 071420): Train loss 1.062, Val loss 1.781\n",
      "Ep 2 (Step 071425): Train loss 1.220, Val loss 1.781\n",
      "Ep 2 (Step 071430): Train loss 1.027, Val loss 1.779\n",
      "Ep 2 (Step 071435): Train loss 0.974, Val loss 1.776\n",
      "Ep 2 (Step 071440): Train loss 1.228, Val loss 1.774\n",
      "Ep 2 (Step 071445): Train loss 0.861, Val loss 1.772\n",
      "Ep 2 (Step 071450): Train loss 0.911, Val loss 1.772\n",
      "Ep 2 (Step 071455): Train loss 0.971, Val loss 1.772\n",
      "Ep 2 (Step 071460): Train loss 1.100, Val loss 1.773\n",
      "Ep 2 (Step 071465): Train loss 0.908, Val loss 1.774\n",
      "Ep 2 (Step 071470): Train loss 0.911, Val loss 1.775\n",
      "Ep 2 (Step 071475): Train loss 1.054, Val loss 1.776\n",
      "Ep 2 (Step 071480): Train loss 0.932, Val loss 1.775\n",
      "Ep 2 (Step 071485): Train loss 0.947, Val loss 1.773\n",
      "Ep 2 (Step 071490): Train loss 1.052, Val loss 1.773\n",
      "Ep 2 (Step 071495): Train loss 0.920, Val loss 1.771\n",
      "Ep 2 (Step 071500): Train loss 0.856, Val loss 1.770\n",
      "Ep 2 (Step 071505): Train loss 1.228, Val loss 1.770\n",
      "Ep 2 (Step 071510): Train loss 1.154, Val loss 1.769\n",
      "Ep 2 (Step 071515): Train loss 1.039, Val loss 1.769\n",
      "Ep 2 (Step 071520): Train loss 1.016, Val loss 1.769\n",
      "Ep 2 (Step 071525): Train loss 1.137, Val loss 1.767\n",
      "Ep 2 (Step 071530): Train loss 0.851, Val loss 1.767\n",
      "Ep 2 (Step 071535): Train loss 1.034, Val loss 1.768\n",
      "Ep 2 (Step 071540): Train loss 1.089, Val loss 1.768\n",
      "Ep 2 (Step 071545): Train loss 1.300, Val loss 1.769\n",
      "Ep 2 (Step 071550): Train loss 0.912, Val loss 1.770\n",
      "Ep 2 (Step 071555): Train loss 1.257, Val loss 1.771\n",
      "Ep 2 (Step 071560): Train loss 1.225, Val loss 1.771\n",
      "Ep 2 (Step 071565): Train loss 1.004, Val loss 1.772\n",
      "Ep 2 (Step 071570): Train loss 0.992, Val loss 1.774\n",
      "Ep 2 (Step 071575): Train loss 0.825, Val loss 1.775\n",
      "Ep 2 (Step 071580): Train loss 1.290, Val loss 1.776\n",
      "Ep 2 (Step 071585): Train loss 1.060, Val loss 1.776\n",
      "Ep 2 (Step 071590): Train loss 0.824, Val loss 1.775\n",
      "Ep 2 (Step 071595): Train loss 0.963, Val loss 1.774\n",
      "Ep 2 (Step 071600): Train loss 1.375, Val loss 1.773\n",
      "Ep 2 (Step 071605): Train loss 0.813, Val loss 1.771\n",
      "Ep 2 (Step 071610): Train loss 1.351, Val loss 1.769\n",
      "Ep 2 (Step 071615): Train loss 1.056, Val loss 1.768\n",
      "Ep 2 (Step 071620): Train loss 0.964, Val loss 1.769\n",
      "Ep 2 (Step 071625): Train loss 1.073, Val loss 1.769\n",
      "Ep 2 (Step 071630): Train loss 1.209, Val loss 1.770\n",
      "Ep 2 (Step 071635): Train loss 1.086, Val loss 1.770\n",
      "Ep 2 (Step 071640): Train loss 0.939, Val loss 1.769\n",
      "Ep 2 (Step 071645): Train loss 1.107, Val loss 1.768\n",
      "Ep 2 (Step 071650): Train loss 1.007, Val loss 1.767\n",
      "Ep 2 (Step 071655): Train loss 1.071, Val loss 1.766\n",
      "Ep 2 (Step 071660): Train loss 1.199, Val loss 1.765\n",
      "Ep 2 (Step 071665): Train loss 0.846, Val loss 1.764\n",
      "Ep 2 (Step 071670): Train loss 1.092, Val loss 1.764\n",
      "Ep 2 (Step 071675): Train loss 0.950, Val loss 1.764\n",
      "Ep 2 (Step 071680): Train loss 0.937, Val loss 1.765\n",
      "Ep 2 (Step 071685): Train loss 1.054, Val loss 1.767\n",
      "Ep 2 (Step 071690): Train loss 0.995, Val loss 1.770\n",
      "Ep 2 (Step 071695): Train loss 1.119, Val loss 1.769\n",
      "Ep 2 (Step 071700): Train loss 1.022, Val loss 1.768\n",
      "Ep 2 (Step 071705): Train loss 1.061, Val loss 1.769\n",
      "Ep 2 (Step 071710): Train loss 0.800, Val loss 1.770\n",
      "Ep 2 (Step 071715): Train loss 1.090, Val loss 1.771\n",
      "Ep 2 (Step 071720): Train loss 0.900, Val loss 1.770\n",
      "Ep 2 (Step 071725): Train loss 1.271, Val loss 1.770\n",
      "Ep 2 (Step 071730): Train loss 1.058, Val loss 1.771\n",
      "Ep 2 (Step 071735): Train loss 1.275, Val loss 1.770\n",
      "Ep 2 (Step 071740): Train loss 0.842, Val loss 1.768\n",
      "Ep 2 (Step 071745): Train loss 0.966, Val loss 1.766\n",
      "Ep 2 (Step 071750): Train loss 1.132, Val loss 1.764\n",
      "Ep 2 (Step 071755): Train loss 0.728, Val loss 1.762\n",
      "Ep 2 (Step 071760): Train loss 0.956, Val loss 1.761\n",
      "Ep 2 (Step 071765): Train loss 0.889, Val loss 1.760\n",
      "Ep 2 (Step 071770): Train loss 1.085, Val loss 1.759\n",
      "Ep 2 (Step 071775): Train loss 0.866, Val loss 1.759\n",
      "Ep 2 (Step 071780): Train loss 0.987, Val loss 1.759\n",
      "Ep 2 (Step 071785): Train loss 0.843, Val loss 1.758\n",
      "Ep 2 (Step 071790): Train loss 1.224, Val loss 1.757\n",
      "Ep 2 (Step 071795): Train loss 1.211, Val loss 1.756\n",
      "Ep 2 (Step 071800): Train loss 0.789, Val loss 1.751\n",
      "Ep 2 (Step 071805): Train loss 0.947, Val loss 1.747\n",
      "Ep 2 (Step 071810): Train loss 1.074, Val loss 1.746\n",
      "Ep 2 (Step 071815): Train loss 0.848, Val loss 1.745\n",
      "Ep 2 (Step 071820): Train loss 1.163, Val loss 1.744\n",
      "Ep 2 (Step 071825): Train loss 0.953, Val loss 1.744\n",
      "Ep 2 (Step 071830): Train loss 1.093, Val loss 1.744\n",
      "Ep 2 (Step 071835): Train loss 0.891, Val loss 1.744\n",
      "Ep 2 (Step 071840): Train loss 0.802, Val loss 1.743\n",
      "Ep 2 (Step 071845): Train loss 1.142, Val loss 1.741\n",
      "Ep 2 (Step 071850): Train loss 0.920, Val loss 1.741\n",
      "Ep 2 (Step 071855): Train loss 1.217, Val loss 1.741\n",
      "Ep 2 (Step 071860): Train loss 0.635, Val loss 1.743\n",
      "Ep 2 (Step 071865): Train loss 1.137, Val loss 1.746\n",
      "Ep 2 (Step 071870): Train loss 1.105, Val loss 1.748\n",
      "Ep 2 (Step 071875): Train loss 1.009, Val loss 1.748\n",
      "Ep 2 (Step 071880): Train loss 0.737, Val loss 1.749\n",
      "Ep 2 (Step 071885): Train loss 0.847, Val loss 1.751\n",
      "Ep 2 (Step 071890): Train loss 1.037, Val loss 1.752\n",
      "Ep 2 (Step 071895): Train loss 1.107, Val loss 1.753\n",
      "Ep 2 (Step 071900): Train loss 1.162, Val loss 1.754\n",
      "Ep 2 (Step 071905): Train loss 0.990, Val loss 1.756\n",
      "Ep 2 (Step 071910): Train loss 0.807, Val loss 1.758\n",
      "Ep 2 (Step 071915): Train loss 1.082, Val loss 1.758\n",
      "Ep 2 (Step 071920): Train loss 1.343, Val loss 1.758\n",
      "Ep 2 (Step 071925): Train loss 0.952, Val loss 1.758\n",
      "Ep 2 (Step 071930): Train loss 0.828, Val loss 1.758\n",
      "Ep 2 (Step 071935): Train loss 1.051, Val loss 1.760\n",
      "Ep 2 (Step 071940): Train loss 1.182, Val loss 1.762\n",
      "Ep 2 (Step 071945): Train loss 0.928, Val loss 1.763\n",
      "Ep 2 (Step 071950): Train loss 1.102, Val loss 1.763\n",
      "Ep 2 (Step 071955): Train loss 0.906, Val loss 1.764\n",
      "Ep 2 (Step 071960): Train loss 0.865, Val loss 1.766\n",
      "Ep 2 (Step 071965): Train loss 1.103, Val loss 1.767\n",
      "Ep 2 (Step 071970): Train loss 1.266, Val loss 1.768\n",
      "Ep 2 (Step 071975): Train loss 0.940, Val loss 1.769\n",
      "Ep 2 (Step 071980): Train loss 0.878, Val loss 1.769\n",
      "Ep 2 (Step 071985): Train loss 1.285, Val loss 1.770\n",
      "Ep 2 (Step 071990): Train loss 0.718, Val loss 1.771\n",
      "Ep 2 (Step 071995): Train loss 0.987, Val loss 1.770\n",
      "Ep 2 (Step 072000): Train loss 1.338, Val loss 1.769\n",
      "Ep 2 (Step 072005): Train loss 0.933, Val loss 1.769\n",
      "Ep 2 (Step 072010): Train loss 1.118, Val loss 1.769\n",
      "Ep 2 (Step 072015): Train loss 1.178, Val loss 1.769\n",
      "Ep 2 (Step 072020): Train loss 1.060, Val loss 1.770\n",
      "Ep 2 (Step 072025): Train loss 1.107, Val loss 1.770\n",
      "Ep 2 (Step 072030): Train loss 1.145, Val loss 1.768\n",
      "Ep 2 (Step 072035): Train loss 0.913, Val loss 1.767\n",
      "Ep 2 (Step 072040): Train loss 0.794, Val loss 1.765\n",
      "Ep 2 (Step 072045): Train loss 0.966, Val loss 1.765\n",
      "Ep 2 (Step 072050): Train loss 1.189, Val loss 1.767\n",
      "Ep 2 (Step 072055): Train loss 1.121, Val loss 1.768\n",
      "Ep 2 (Step 072060): Train loss 1.060, Val loss 1.769\n",
      "Ep 2 (Step 072065): Train loss 0.851, Val loss 1.769\n",
      "Ep 2 (Step 072070): Train loss 1.085, Val loss 1.770\n",
      "Ep 2 (Step 072075): Train loss 1.068, Val loss 1.770\n",
      "Ep 2 (Step 072080): Train loss 1.095, Val loss 1.770\n",
      "Ep 2 (Step 072085): Train loss 1.030, Val loss 1.769\n",
      "Ep 2 (Step 072090): Train loss 1.013, Val loss 1.768\n",
      "Ep 2 (Step 072095): Train loss 0.857, Val loss 1.767\n",
      "Ep 2 (Step 072100): Train loss 0.913, Val loss 1.766\n",
      "Ep 2 (Step 072105): Train loss 1.396, Val loss 1.767\n",
      "Ep 2 (Step 072110): Train loss 1.106, Val loss 1.768\n",
      "Ep 2 (Step 072115): Train loss 0.803, Val loss 1.767\n",
      "Ep 2 (Step 072120): Train loss 1.024, Val loss 1.766\n",
      "Ep 2 (Step 072125): Train loss 1.258, Val loss 1.766\n",
      "Ep 2 (Step 072130): Train loss 1.151, Val loss 1.767\n",
      "Ep 2 (Step 072135): Train loss 1.148, Val loss 1.767\n",
      "Ep 2 (Step 072140): Train loss 0.840, Val loss 1.768\n",
      "Ep 2 (Step 072145): Train loss 0.880, Val loss 1.767\n",
      "Ep 2 (Step 072150): Train loss 0.944, Val loss 1.767\n",
      "Ep 2 (Step 072155): Train loss 1.005, Val loss 1.768\n",
      "Ep 2 (Step 072160): Train loss 1.104, Val loss 1.767\n",
      "Ep 2 (Step 072165): Train loss 0.737, Val loss 1.766\n",
      "Ep 2 (Step 072170): Train loss 1.171, Val loss 1.764\n",
      "Ep 2 (Step 072175): Train loss 0.989, Val loss 1.763\n",
      "Ep 2 (Step 072180): Train loss 1.000, Val loss 1.762\n",
      "Ep 2 (Step 072185): Train loss 1.071, Val loss 1.762\n",
      "Ep 2 (Step 072190): Train loss 1.589, Val loss 1.763\n",
      "Ep 2 (Step 072195): Train loss 0.813, Val loss 1.764\n",
      "Ep 2 (Step 072200): Train loss 0.945, Val loss 1.766\n",
      "Ep 2 (Step 072205): Train loss 1.007, Val loss 1.767\n",
      "Ep 2 (Step 072210): Train loss 0.754, Val loss 1.769\n",
      "Ep 2 (Step 072215): Train loss 1.126, Val loss 1.770\n",
      "Ep 2 (Step 072220): Train loss 0.991, Val loss 1.770\n",
      "Ep 2 (Step 072225): Train loss 1.111, Val loss 1.771\n",
      "Ep 2 (Step 072230): Train loss 1.088, Val loss 1.772\n",
      "Ep 2 (Step 072235): Train loss 1.023, Val loss 1.772\n",
      "Ep 2 (Step 072240): Train loss 1.085, Val loss 1.771\n",
      "Ep 2 (Step 072245): Train loss 1.037, Val loss 1.769\n",
      "Ep 2 (Step 072250): Train loss 0.617, Val loss 1.767\n",
      "Ep 2 (Step 072255): Train loss 1.154, Val loss 1.766\n",
      "Ep 2 (Step 072260): Train loss 0.916, Val loss 1.765\n",
      "Ep 2 (Step 072265): Train loss 1.016, Val loss 1.765\n",
      "Ep 2 (Step 072270): Train loss 1.169, Val loss 1.765\n",
      "Ep 2 (Step 072275): Train loss 1.019, Val loss 1.766\n",
      "Ep 2 (Step 072280): Train loss 0.834, Val loss 1.768\n",
      "Ep 2 (Step 072285): Train loss 0.903, Val loss 1.767\n",
      "Ep 2 (Step 072290): Train loss 1.075, Val loss 1.765\n",
      "Ep 2 (Step 072295): Train loss 1.023, Val loss 1.763\n",
      "Ep 2 (Step 072300): Train loss 1.156, Val loss 1.763\n",
      "Ep 2 (Step 072305): Train loss 0.847, Val loss 1.762\n",
      "Ep 2 (Step 072310): Train loss 0.890, Val loss 1.761\n",
      "Ep 2 (Step 072315): Train loss 0.771, Val loss 1.760\n",
      "Ep 2 (Step 072320): Train loss 1.164, Val loss 1.760\n",
      "Ep 2 (Step 072325): Train loss 0.908, Val loss 1.761\n",
      "Ep 2 (Step 072330): Train loss 0.899, Val loss 1.763\n",
      "Ep 2 (Step 072335): Train loss 1.064, Val loss 1.763\n",
      "Ep 2 (Step 072340): Train loss 1.133, Val loss 1.762\n",
      "Ep 2 (Step 072345): Train loss 1.159, Val loss 1.761\n",
      "Ep 2 (Step 072350): Train loss 1.102, Val loss 1.761\n",
      "Ep 2 (Step 072355): Train loss 0.861, Val loss 1.760\n",
      "Ep 2 (Step 072360): Train loss 0.924, Val loss 1.762\n",
      "Ep 2 (Step 072365): Train loss 1.079, Val loss 1.763\n",
      "Ep 2 (Step 072370): Train loss 0.918, Val loss 1.765\n",
      "Ep 2 (Step 072375): Train loss 1.052, Val loss 1.766\n",
      "Ep 2 (Step 072380): Train loss 0.841, Val loss 1.768\n",
      "Ep 2 (Step 072385): Train loss 1.027, Val loss 1.768\n",
      "Ep 2 (Step 072390): Train loss 0.978, Val loss 1.767\n",
      "Ep 2 (Step 072395): Train loss 1.071, Val loss 1.766\n",
      "Ep 2 (Step 072400): Train loss 0.984, Val loss 1.764\n",
      "Ep 2 (Step 072405): Train loss 1.194, Val loss 1.763\n",
      "Ep 2 (Step 072410): Train loss 0.887, Val loss 1.761\n",
      "Ep 2 (Step 072415): Train loss 1.182, Val loss 1.761\n",
      "Ep 2 (Step 072420): Train loss 0.986, Val loss 1.761\n",
      "Ep 2 (Step 072425): Train loss 1.091, Val loss 1.762\n",
      "Ep 2 (Step 072430): Train loss 1.051, Val loss 1.761\n",
      "Ep 2 (Step 072435): Train loss 0.925, Val loss 1.761\n",
      "Ep 2 (Step 072440): Train loss 1.367, Val loss 1.760\n",
      "Ep 2 (Step 072445): Train loss 0.890, Val loss 1.759\n",
      "Ep 2 (Step 072450): Train loss 1.145, Val loss 1.758\n",
      "Ep 2 (Step 072455): Train loss 0.917, Val loss 1.758\n",
      "Ep 2 (Step 072460): Train loss 0.925, Val loss 1.758\n",
      "Ep 2 (Step 072465): Train loss 1.119, Val loss 1.757\n",
      "Ep 2 (Step 072470): Train loss 1.068, Val loss 1.757\n",
      "Ep 2 (Step 072475): Train loss 1.117, Val loss 1.757\n",
      "Ep 2 (Step 072480): Train loss 0.797, Val loss 1.757\n",
      "Ep 2 (Step 072485): Train loss 1.069, Val loss 1.758\n",
      "Ep 2 (Step 072490): Train loss 0.992, Val loss 1.759\n",
      "Ep 2 (Step 072495): Train loss 0.838, Val loss 1.759\n",
      "Ep 2 (Step 072500): Train loss 0.844, Val loss 1.759\n",
      "Ep 2 (Step 072505): Train loss 1.169, Val loss 1.760\n",
      "Ep 2 (Step 072510): Train loss 1.111, Val loss 1.760\n",
      "Ep 2 (Step 072515): Train loss 0.753, Val loss 1.761\n",
      "Ep 2 (Step 072520): Train loss 1.097, Val loss 1.762\n",
      "Ep 2 (Step 072525): Train loss 1.114, Val loss 1.763\n",
      "Ep 2 (Step 072530): Train loss 0.870, Val loss 1.764\n",
      "Ep 2 (Step 072535): Train loss 1.234, Val loss 1.763\n",
      "Ep 2 (Step 072540): Train loss 0.961, Val loss 1.763\n",
      "Ep 2 (Step 072545): Train loss 1.244, Val loss 1.763\n",
      "Ep 2 (Step 072550): Train loss 0.843, Val loss 1.762\n",
      "Ep 2 (Step 072555): Train loss 0.958, Val loss 1.762\n",
      "Ep 2 (Step 072560): Train loss 1.011, Val loss 1.762\n",
      "Ep 2 (Step 072565): Train loss 1.406, Val loss 1.762\n",
      "Ep 2 (Step 072570): Train loss 1.089, Val loss 1.762\n",
      "Ep 2 (Step 072575): Train loss 1.113, Val loss 1.762\n",
      "Ep 2 (Step 072580): Train loss 0.654, Val loss 1.762\n",
      "Ep 2 (Step 072585): Train loss 1.026, Val loss 1.760\n",
      "Ep 2 (Step 072590): Train loss 1.025, Val loss 1.760\n",
      "Ep 2 (Step 072595): Train loss 1.124, Val loss 1.759\n",
      "Ep 2 (Step 072600): Train loss 1.026, Val loss 1.760\n",
      "Ep 2 (Step 072605): Train loss 0.876, Val loss 1.762\n",
      "Ep 2 (Step 072610): Train loss 1.188, Val loss 1.762\n",
      "Ep 2 (Step 072615): Train loss 0.837, Val loss 1.763\n",
      "Ep 2 (Step 072620): Train loss 0.833, Val loss 1.763\n",
      "Ep 2 (Step 072625): Train loss 0.979, Val loss 1.763\n",
      "Ep 2 (Step 072630): Train loss 1.125, Val loss 1.764\n",
      "Ep 2 (Step 072635): Train loss 0.890, Val loss 1.764\n",
      "Ep 2 (Step 072640): Train loss 1.035, Val loss 1.764\n",
      "Ep 2 (Step 072645): Train loss 0.755, Val loss 1.766\n",
      "Ep 2 (Step 072650): Train loss 0.934, Val loss 1.767\n",
      "Ep 2 (Step 072655): Train loss 1.026, Val loss 1.768\n",
      "Ep 2 (Step 072660): Train loss 0.772, Val loss 1.768\n",
      "Ep 2 (Step 072665): Train loss 1.037, Val loss 1.770\n",
      "Ep 2 (Step 072670): Train loss 1.107, Val loss 1.771\n",
      "Ep 2 (Step 072675): Train loss 1.104, Val loss 1.771\n",
      "Ep 2 (Step 072680): Train loss 1.306, Val loss 1.770\n",
      "Ep 2 (Step 072685): Train loss 1.178, Val loss 1.769\n",
      "Ep 2 (Step 072690): Train loss 1.155, Val loss 1.768\n",
      "Ep 2 (Step 072695): Train loss 0.951, Val loss 1.766\n",
      "Ep 2 (Step 072700): Train loss 0.954, Val loss 1.765\n",
      "Ep 2 (Step 072705): Train loss 1.256, Val loss 1.764\n",
      "Ep 2 (Step 072710): Train loss 0.929, Val loss 1.764\n",
      "Ep 2 (Step 072715): Train loss 0.995, Val loss 1.764\n",
      "Ep 2 (Step 072720): Train loss 1.073, Val loss 1.764\n",
      "Ep 2 (Step 072725): Train loss 1.189, Val loss 1.764\n",
      "Ep 2 (Step 072730): Train loss 1.131, Val loss 1.761\n",
      "Ep 2 (Step 072735): Train loss 0.816, Val loss 1.760\n",
      "Ep 2 (Step 072740): Train loss 1.148, Val loss 1.760\n",
      "Ep 2 (Step 072745): Train loss 0.971, Val loss 1.757\n",
      "Ep 2 (Step 072750): Train loss 0.904, Val loss 1.756\n",
      "Ep 2 (Step 072755): Train loss 1.250, Val loss 1.756\n",
      "Ep 2 (Step 072760): Train loss 0.882, Val loss 1.757\n",
      "Ep 2 (Step 072765): Train loss 0.942, Val loss 1.757\n",
      "Ep 2 (Step 072770): Train loss 0.941, Val loss 1.756\n",
      "Ep 2 (Step 072775): Train loss 1.004, Val loss 1.757\n",
      "Ep 2 (Step 072780): Train loss 1.095, Val loss 1.758\n",
      "Ep 2 (Step 072785): Train loss 1.139, Val loss 1.758\n",
      "Ep 2 (Step 072790): Train loss 0.985, Val loss 1.756\n",
      "Ep 2 (Step 072795): Train loss 1.129, Val loss 1.756\n",
      "Ep 2 (Step 072800): Train loss 0.627, Val loss 1.755\n",
      "Ep 2 (Step 072805): Train loss 0.863, Val loss 1.756\n",
      "Ep 2 (Step 072810): Train loss 0.929, Val loss 1.758\n",
      "Ep 2 (Step 072815): Train loss 1.111, Val loss 1.759\n",
      "Ep 2 (Step 072820): Train loss 0.911, Val loss 1.760\n",
      "Ep 2 (Step 072825): Train loss 1.110, Val loss 1.761\n",
      "Ep 2 (Step 072830): Train loss 1.099, Val loss 1.762\n",
      "Ep 2 (Step 072835): Train loss 1.106, Val loss 1.761\n",
      "Ep 2 (Step 072840): Train loss 0.798, Val loss 1.762\n",
      "Ep 2 (Step 072845): Train loss 1.060, Val loss 1.762\n",
      "Ep 2 (Step 072850): Train loss 1.015, Val loss 1.763\n",
      "Ep 2 (Step 072855): Train loss 0.868, Val loss 1.763\n",
      "Ep 2 (Step 072860): Train loss 1.108, Val loss 1.762\n",
      "Ep 2 (Step 072865): Train loss 1.182, Val loss 1.761\n",
      "Ep 2 (Step 072870): Train loss 1.036, Val loss 1.761\n",
      "Ep 2 (Step 072875): Train loss 1.011, Val loss 1.760\n",
      "Ep 2 (Step 072880): Train loss 1.223, Val loss 1.761\n",
      "Ep 2 (Step 072885): Train loss 1.056, Val loss 1.762\n",
      "Ep 2 (Step 072890): Train loss 0.927, Val loss 1.762\n",
      "Ep 2 (Step 072895): Train loss 1.086, Val loss 1.762\n",
      "Ep 2 (Step 072900): Train loss 1.276, Val loss 1.762\n",
      "Ep 2 (Step 072905): Train loss 0.963, Val loss 1.761\n",
      "Ep 2 (Step 072910): Train loss 0.916, Val loss 1.759\n",
      "Ep 2 (Step 072915): Train loss 0.912, Val loss 1.758\n",
      "Ep 2 (Step 072920): Train loss 1.070, Val loss 1.758\n",
      "Ep 2 (Step 072925): Train loss 0.928, Val loss 1.757\n",
      "Ep 2 (Step 072930): Train loss 0.978, Val loss 1.757\n",
      "Ep 2 (Step 072935): Train loss 0.982, Val loss 1.756\n",
      "Ep 2 (Step 072940): Train loss 0.942, Val loss 1.755\n",
      "Ep 2 (Step 072945): Train loss 0.920, Val loss 1.756\n",
      "Ep 2 (Step 072950): Train loss 1.129, Val loss 1.757\n",
      "Ep 2 (Step 072955): Train loss 0.918, Val loss 1.758\n",
      "Ep 2 (Step 072960): Train loss 0.871, Val loss 1.760\n",
      "Ep 2 (Step 072965): Train loss 1.055, Val loss 1.759\n",
      "Ep 2 (Step 072970): Train loss 1.154, Val loss 1.759\n",
      "Ep 2 (Step 072975): Train loss 1.189, Val loss 1.758\n",
      "Ep 2 (Step 072980): Train loss 1.211, Val loss 1.758\n",
      "Ep 2 (Step 072985): Train loss 1.122, Val loss 1.757\n",
      "Ep 2 (Step 072990): Train loss 1.192, Val loss 1.757\n",
      "Ep 2 (Step 072995): Train loss 0.880, Val loss 1.755\n",
      "Ep 2 (Step 073000): Train loss 1.182, Val loss 1.754\n",
      "Ep 2 (Step 073005): Train loss 0.878, Val loss 1.754\n",
      "Ep 2 (Step 073010): Train loss 1.249, Val loss 1.755\n",
      "Ep 2 (Step 073015): Train loss 1.038, Val loss 1.755\n",
      "Ep 2 (Step 073020): Train loss 0.910, Val loss 1.755\n",
      "Ep 2 (Step 073025): Train loss 1.047, Val loss 1.754\n",
      "Ep 2 (Step 073030): Train loss 0.976, Val loss 1.755\n",
      "Ep 2 (Step 073035): Train loss 1.107, Val loss 1.755\n",
      "Ep 2 (Step 073040): Train loss 1.358, Val loss 1.754\n",
      "Ep 2 (Step 073045): Train loss 0.927, Val loss 1.751\n",
      "Ep 2 (Step 073050): Train loss 1.299, Val loss 1.750\n",
      "Ep 2 (Step 073055): Train loss 0.681, Val loss 1.748\n",
      "Ep 2 (Step 073060): Train loss 0.993, Val loss 1.747\n",
      "Ep 2 (Step 073065): Train loss 1.252, Val loss 1.748\n",
      "Ep 2 (Step 073070): Train loss 1.308, Val loss 1.748\n",
      "Ep 2 (Step 073075): Train loss 0.944, Val loss 1.748\n",
      "Ep 2 (Step 073080): Train loss 1.003, Val loss 1.748\n",
      "Ep 2 (Step 073085): Train loss 1.126, Val loss 1.747\n",
      "Ep 2 (Step 073090): Train loss 1.166, Val loss 1.747\n",
      "Ep 2 (Step 073095): Train loss 0.909, Val loss 1.746\n",
      "Ep 2 (Step 073100): Train loss 1.004, Val loss 1.747\n",
      "Ep 2 (Step 073105): Train loss 1.294, Val loss 1.747\n",
      "Ep 2 (Step 073110): Train loss 1.035, Val loss 1.748\n",
      "Ep 2 (Step 073115): Train loss 0.956, Val loss 1.748\n",
      "Ep 2 (Step 073120): Train loss 1.187, Val loss 1.748\n",
      "Ep 2 (Step 073125): Train loss 0.890, Val loss 1.748\n",
      "Ep 2 (Step 073130): Train loss 1.069, Val loss 1.749\n",
      "Ep 2 (Step 073135): Train loss 0.951, Val loss 1.751\n",
      "Ep 2 (Step 073140): Train loss 1.074, Val loss 1.753\n",
      "Ep 2 (Step 073145): Train loss 0.999, Val loss 1.754\n",
      "Ep 2 (Step 073150): Train loss 0.915, Val loss 1.755\n",
      "Ep 2 (Step 073155): Train loss 1.311, Val loss 1.755\n",
      "Ep 2 (Step 073160): Train loss 1.022, Val loss 1.756\n",
      "Ep 2 (Step 073165): Train loss 1.091, Val loss 1.758\n",
      "Ep 2 (Step 073170): Train loss 1.025, Val loss 1.758\n",
      "Ep 2 (Step 073175): Train loss 0.797, Val loss 1.760\n",
      "Ep 2 (Step 073180): Train loss 0.949, Val loss 1.760\n",
      "Ep 2 (Step 073185): Train loss 0.935, Val loss 1.760\n",
      "Ep 2 (Step 073190): Train loss 0.651, Val loss 1.758\n",
      "Ep 2 (Step 073195): Train loss 1.010, Val loss 1.757\n",
      "Ep 2 (Step 073200): Train loss 0.950, Val loss 1.756\n",
      "Ep 2 (Step 073205): Train loss 0.970, Val loss 1.756\n",
      "Ep 2 (Step 073210): Train loss 1.202, Val loss 1.758\n",
      "Ep 2 (Step 073215): Train loss 1.094, Val loss 1.759\n",
      "Ep 2 (Step 073220): Train loss 1.074, Val loss 1.760\n",
      "Ep 2 (Step 073225): Train loss 0.858, Val loss 1.762\n",
      "Ep 2 (Step 073230): Train loss 1.109, Val loss 1.763\n",
      "Ep 2 (Step 073235): Train loss 0.949, Val loss 1.762\n",
      "Ep 2 (Step 073240): Train loss 1.009, Val loss 1.762\n",
      "Ep 2 (Step 073245): Train loss 0.886, Val loss 1.762\n",
      "Ep 2 (Step 073250): Train loss 1.176, Val loss 1.762\n",
      "Ep 2 (Step 073255): Train loss 1.012, Val loss 1.762\n",
      "Ep 2 (Step 073260): Train loss 0.927, Val loss 1.761\n",
      "Ep 2 (Step 073265): Train loss 0.945, Val loss 1.761\n",
      "Ep 2 (Step 073270): Train loss 1.107, Val loss 1.762\n",
      "Ep 2 (Step 073275): Train loss 1.007, Val loss 1.764\n",
      "Ep 2 (Step 073280): Train loss 0.929, Val loss 1.765\n",
      "Ep 2 (Step 073285): Train loss 0.931, Val loss 1.766\n",
      "Ep 2 (Step 073290): Train loss 1.139, Val loss 1.766\n",
      "Ep 2 (Step 073295): Train loss 1.023, Val loss 1.767\n",
      "Ep 2 (Step 073300): Train loss 0.901, Val loss 1.767\n",
      "Ep 2 (Step 073305): Train loss 0.999, Val loss 1.769\n",
      "Ep 2 (Step 073310): Train loss 1.113, Val loss 1.770\n",
      "Ep 2 (Step 073315): Train loss 1.052, Val loss 1.772\n",
      "Ep 2 (Step 073320): Train loss 1.131, Val loss 1.773\n",
      "Ep 2 (Step 073325): Train loss 0.766, Val loss 1.774\n",
      "Ep 2 (Step 073330): Train loss 0.809, Val loss 1.773\n",
      "Ep 2 (Step 073335): Train loss 1.229, Val loss 1.772\n",
      "Ep 2 (Step 073340): Train loss 1.063, Val loss 1.772\n",
      "Ep 2 (Step 073345): Train loss 1.047, Val loss 1.770\n",
      "Ep 2 (Step 073350): Train loss 0.806, Val loss 1.767\n",
      "Ep 2 (Step 073355): Train loss 1.089, Val loss 1.765\n",
      "Ep 2 (Step 073360): Train loss 0.813, Val loss 1.764\n",
      "Ep 2 (Step 073365): Train loss 0.972, Val loss 1.763\n",
      "Ep 2 (Step 073370): Train loss 0.990, Val loss 1.763\n",
      "Ep 2 (Step 073375): Train loss 1.107, Val loss 1.763\n",
      "Ep 2 (Step 073380): Train loss 0.910, Val loss 1.763\n",
      "Ep 2 (Step 073385): Train loss 0.885, Val loss 1.762\n",
      "Ep 2 (Step 073390): Train loss 0.926, Val loss 1.760\n",
      "Ep 2 (Step 073395): Train loss 0.960, Val loss 1.759\n",
      "Ep 2 (Step 073400): Train loss 1.068, Val loss 1.758\n",
      "Ep 2 (Step 073405): Train loss 1.103, Val loss 1.757\n",
      "Ep 2 (Step 073410): Train loss 0.912, Val loss 1.757\n",
      "Ep 2 (Step 073415): Train loss 1.103, Val loss 1.757\n",
      "Ep 2 (Step 073420): Train loss 0.852, Val loss 1.756\n",
      "Ep 2 (Step 073425): Train loss 1.568, Val loss 1.754\n",
      "Ep 2 (Step 073430): Train loss 1.148, Val loss 1.753\n",
      "Ep 2 (Step 073435): Train loss 1.090, Val loss 1.752\n",
      "Ep 2 (Step 073440): Train loss 0.966, Val loss 1.752\n",
      "Ep 2 (Step 073445): Train loss 1.235, Val loss 1.751\n",
      "Ep 2 (Step 073450): Train loss 1.097, Val loss 1.750\n",
      "Ep 2 (Step 073455): Train loss 0.854, Val loss 1.750\n",
      "Ep 2 (Step 073460): Train loss 1.172, Val loss 1.752\n",
      "Ep 2 (Step 073465): Train loss 0.977, Val loss 1.753\n",
      "Ep 2 (Step 073470): Train loss 1.276, Val loss 1.754\n",
      "Ep 2 (Step 073475): Train loss 0.723, Val loss 1.755\n",
      "Ep 2 (Step 073480): Train loss 1.225, Val loss 1.756\n",
      "Ep 2 (Step 073485): Train loss 1.231, Val loss 1.758\n",
      "Ep 2 (Step 073490): Train loss 1.059, Val loss 1.759\n",
      "Ep 2 (Step 073495): Train loss 1.050, Val loss 1.759\n",
      "Ep 2 (Step 073500): Train loss 0.967, Val loss 1.759\n",
      "Ep 2 (Step 073505): Train loss 1.035, Val loss 1.759\n",
      "Ep 2 (Step 073510): Train loss 0.755, Val loss 1.759\n",
      "Ep 2 (Step 073515): Train loss 1.248, Val loss 1.759\n",
      "Ep 2 (Step 073520): Train loss 0.790, Val loss 1.760\n",
      "Ep 2 (Step 073525): Train loss 1.084, Val loss 1.760\n",
      "Ep 2 (Step 073530): Train loss 1.017, Val loss 1.761\n",
      "Ep 2 (Step 073535): Train loss 0.933, Val loss 1.763\n",
      "Ep 2 (Step 073540): Train loss 1.158, Val loss 1.765\n",
      "Ep 2 (Step 073545): Train loss 1.191, Val loss 1.765\n",
      "Ep 2 (Step 073550): Train loss 1.233, Val loss 1.765\n",
      "Ep 2 (Step 073555): Train loss 1.003, Val loss 1.765\n",
      "Ep 2 (Step 073560): Train loss 1.015, Val loss 1.767\n",
      "Ep 2 (Step 073565): Train loss 0.957, Val loss 1.769\n",
      "Ep 2 (Step 073570): Train loss 1.333, Val loss 1.769\n",
      "Ep 2 (Step 073575): Train loss 0.895, Val loss 1.770\n",
      "Ep 2 (Step 073580): Train loss 1.124, Val loss 1.770\n",
      "Ep 2 (Step 073585): Train loss 1.084, Val loss 1.770\n",
      "Ep 2 (Step 073590): Train loss 0.906, Val loss 1.770\n",
      "Ep 2 (Step 073595): Train loss 1.219, Val loss 1.768\n",
      "Ep 2 (Step 073600): Train loss 0.897, Val loss 1.765\n",
      "Ep 2 (Step 073605): Train loss 0.852, Val loss 1.764\n",
      "Ep 2 (Step 073610): Train loss 1.079, Val loss 1.765\n",
      "Ep 2 (Step 073615): Train loss 1.081, Val loss 1.765\n",
      "Ep 2 (Step 073620): Train loss 1.144, Val loss 1.763\n",
      "Ep 2 (Step 073625): Train loss 1.195, Val loss 1.762\n",
      "Ep 2 (Step 073630): Train loss 1.175, Val loss 1.761\n",
      "Ep 2 (Step 073635): Train loss 0.944, Val loss 1.761\n",
      "Ep 2 (Step 073640): Train loss 1.284, Val loss 1.762\n",
      "Ep 2 (Step 073645): Train loss 1.062, Val loss 1.762\n",
      "Ep 2 (Step 073650): Train loss 1.082, Val loss 1.763\n",
      "Ep 2 (Step 073655): Train loss 0.929, Val loss 1.763\n",
      "Ep 2 (Step 073660): Train loss 0.846, Val loss 1.764\n",
      "Ep 2 (Step 073665): Train loss 1.029, Val loss 1.766\n",
      "Ep 2 (Step 073670): Train loss 1.038, Val loss 1.768\n",
      "Ep 2 (Step 073675): Train loss 0.890, Val loss 1.770\n",
      "Ep 2 (Step 073680): Train loss 1.123, Val loss 1.771\n",
      "Ep 2 (Step 073685): Train loss 0.850, Val loss 1.771\n",
      "Ep 2 (Step 073690): Train loss 1.070, Val loss 1.772\n",
      "Ep 2 (Step 073695): Train loss 1.018, Val loss 1.772\n",
      "Ep 2 (Step 073700): Train loss 0.909, Val loss 1.772\n",
      "Ep 2 (Step 073705): Train loss 1.186, Val loss 1.771\n",
      "Ep 2 (Step 073710): Train loss 0.964, Val loss 1.770\n",
      "Ep 2 (Step 073715): Train loss 1.100, Val loss 1.769\n",
      "Ep 2 (Step 073720): Train loss 1.055, Val loss 1.768\n",
      "Ep 2 (Step 073725): Train loss 0.881, Val loss 1.767\n",
      "Ep 2 (Step 073730): Train loss 1.066, Val loss 1.766\n",
      "Ep 2 (Step 073735): Train loss 1.126, Val loss 1.765\n",
      "Ep 2 (Step 073740): Train loss 0.920, Val loss 1.765\n",
      "Ep 2 (Step 073745): Train loss 1.194, Val loss 1.764\n",
      "Ep 2 (Step 073750): Train loss 0.848, Val loss 1.765\n",
      "Ep 2 (Step 073755): Train loss 1.116, Val loss 1.765\n",
      "Ep 2 (Step 073760): Train loss 0.923, Val loss 1.765\n",
      "Ep 2 (Step 073765): Train loss 1.056, Val loss 1.766\n",
      "Ep 2 (Step 073770): Train loss 0.836, Val loss 1.767\n",
      "Ep 2 (Step 073775): Train loss 0.935, Val loss 1.768\n",
      "Ep 2 (Step 073780): Train loss 1.229, Val loss 1.769\n",
      "Ep 2 (Step 073785): Train loss 0.844, Val loss 1.769\n",
      "Ep 2 (Step 073790): Train loss 1.153, Val loss 1.769\n",
      "Ep 2 (Step 073795): Train loss 0.984, Val loss 1.769\n",
      "Ep 2 (Step 073800): Train loss 1.057, Val loss 1.770\n",
      "Ep 2 (Step 073805): Train loss 1.035, Val loss 1.771\n",
      "Ep 2 (Step 073810): Train loss 1.084, Val loss 1.771\n",
      "Ep 2 (Step 073815): Train loss 0.789, Val loss 1.771\n",
      "Ep 2 (Step 073820): Train loss 0.994, Val loss 1.770\n",
      "Ep 2 (Step 073825): Train loss 1.186, Val loss 1.768\n",
      "Ep 2 (Step 073830): Train loss 0.968, Val loss 1.767\n",
      "Ep 2 (Step 073835): Train loss 0.947, Val loss 1.766\n",
      "Ep 2 (Step 073840): Train loss 1.028, Val loss 1.765\n",
      "Ep 2 (Step 073845): Train loss 0.960, Val loss 1.765\n",
      "Ep 2 (Step 073850): Train loss 0.891, Val loss 1.764\n",
      "Ep 2 (Step 073855): Train loss 1.032, Val loss 1.763\n",
      "Ep 2 (Step 073860): Train loss 1.169, Val loss 1.762\n",
      "Ep 2 (Step 073865): Train loss 1.046, Val loss 1.763\n",
      "Ep 2 (Step 073870): Train loss 0.984, Val loss 1.765\n",
      "Ep 2 (Step 073875): Train loss 1.385, Val loss 1.764\n",
      "Ep 2 (Step 073880): Train loss 1.072, Val loss 1.763\n",
      "Ep 2 (Step 073885): Train loss 1.148, Val loss 1.761\n",
      "Ep 2 (Step 073890): Train loss 0.850, Val loss 1.759\n",
      "Ep 2 (Step 073895): Train loss 0.899, Val loss 1.759\n",
      "Ep 2 (Step 073900): Train loss 1.185, Val loss 1.759\n",
      "Ep 2 (Step 073905): Train loss 1.056, Val loss 1.758\n",
      "Ep 2 (Step 073910): Train loss 0.947, Val loss 1.758\n",
      "Ep 2 (Step 073915): Train loss 1.010, Val loss 1.758\n",
      "Ep 2 (Step 073920): Train loss 0.913, Val loss 1.757\n",
      "Ep 2 (Step 073925): Train loss 0.971, Val loss 1.756\n",
      "Ep 2 (Step 073930): Train loss 1.065, Val loss 1.756\n",
      "Ep 2 (Step 073935): Train loss 0.987, Val loss 1.757\n",
      "Ep 2 (Step 073940): Train loss 0.804, Val loss 1.758\n",
      "Ep 2 (Step 073945): Train loss 1.025, Val loss 1.759\n",
      "Ep 2 (Step 073950): Train loss 1.079, Val loss 1.760\n",
      "Ep 2 (Step 073955): Train loss 1.026, Val loss 1.760\n",
      "Ep 2 (Step 073960): Train loss 0.939, Val loss 1.761\n",
      "Ep 2 (Step 073965): Train loss 0.794, Val loss 1.762\n",
      "Ep 2 (Step 073970): Train loss 0.847, Val loss 1.763\n",
      "Ep 2 (Step 073975): Train loss 1.049, Val loss 1.763\n",
      "Ep 2 (Step 073980): Train loss 0.961, Val loss 1.764\n",
      "Ep 2 (Step 073985): Train loss 0.911, Val loss 1.764\n",
      "Ep 2 (Step 073990): Train loss 0.995, Val loss 1.765\n",
      "Ep 2 (Step 073995): Train loss 1.058, Val loss 1.765\n",
      "Ep 2 (Step 074000): Train loss 1.050, Val loss 1.764\n",
      "Ep 2 (Step 074005): Train loss 0.720, Val loss 1.762\n",
      "Ep 2 (Step 074010): Train loss 1.001, Val loss 1.760\n",
      "Ep 2 (Step 074015): Train loss 1.036, Val loss 1.759\n",
      "Ep 2 (Step 074020): Train loss 0.809, Val loss 1.759\n",
      "Ep 2 (Step 074025): Train loss 0.830, Val loss 1.759\n",
      "Ep 2 (Step 074030): Train loss 0.859, Val loss 1.758\n",
      "Ep 2 (Step 074035): Train loss 1.032, Val loss 1.757\n",
      "Ep 2 (Step 074040): Train loss 0.785, Val loss 1.756\n",
      "Ep 2 (Step 074045): Train loss 1.159, Val loss 1.755\n",
      "Ep 2 (Step 074050): Train loss 1.216, Val loss 1.755\n",
      "Ep 2 (Step 074055): Train loss 1.182, Val loss 1.756\n",
      "Ep 2 (Step 074060): Train loss 1.043, Val loss 1.757\n",
      "Ep 2 (Step 074065): Train loss 1.241, Val loss 1.759\n",
      "Ep 2 (Step 074070): Train loss 0.961, Val loss 1.760\n",
      "Ep 2 (Step 074075): Train loss 1.009, Val loss 1.760\n",
      "Ep 2 (Step 074080): Train loss 1.043, Val loss 1.760\n",
      "Ep 2 (Step 074085): Train loss 1.177, Val loss 1.760\n",
      "Ep 2 (Step 074090): Train loss 0.878, Val loss 1.759\n",
      "Ep 2 (Step 074095): Train loss 0.896, Val loss 1.760\n",
      "Ep 2 (Step 074100): Train loss 0.972, Val loss 1.760\n",
      "Ep 2 (Step 074105): Train loss 1.090, Val loss 1.760\n",
      "Ep 2 (Step 074110): Train loss 0.780, Val loss 1.760\n",
      "Ep 2 (Step 074115): Train loss 0.934, Val loss 1.761\n",
      "Ep 2 (Step 074120): Train loss 1.069, Val loss 1.761\n",
      "Ep 2 (Step 074125): Train loss 0.811, Val loss 1.761\n",
      "Ep 2 (Step 074130): Train loss 1.259, Val loss 1.760\n",
      "Ep 2 (Step 074135): Train loss 1.052, Val loss 1.761\n",
      "Ep 2 (Step 074140): Train loss 0.865, Val loss 1.761\n",
      "Ep 2 (Step 074145): Train loss 0.731, Val loss 1.761\n",
      "Ep 2 (Step 074150): Train loss 0.951, Val loss 1.762\n",
      "Ep 2 (Step 074155): Train loss 1.069, Val loss 1.762\n",
      "Ep 2 (Step 074160): Train loss 1.296, Val loss 1.762\n",
      "Ep 2 (Step 074165): Train loss 0.948, Val loss 1.763\n",
      "Ep 2 (Step 074170): Train loss 0.992, Val loss 1.764\n",
      "Ep 2 (Step 074175): Train loss 1.022, Val loss 1.766\n",
      "Ep 2 (Step 074180): Train loss 1.110, Val loss 1.768\n",
      "Ep 2 (Step 074185): Train loss 1.367, Val loss 1.769\n",
      "Ep 2 (Step 074190): Train loss 0.927, Val loss 1.769\n",
      "Ep 2 (Step 074195): Train loss 0.985, Val loss 1.769\n",
      "Ep 2 (Step 074200): Train loss 1.171, Val loss 1.769\n",
      "Ep 2 (Step 074205): Train loss 0.985, Val loss 1.768\n",
      "Ep 2 (Step 074210): Train loss 1.036, Val loss 1.767\n",
      "Ep 2 (Step 074215): Train loss 1.040, Val loss 1.764\n",
      "Ep 2 (Step 074220): Train loss 0.983, Val loss 1.763\n",
      "Ep 2 (Step 074225): Train loss 1.086, Val loss 1.763\n",
      "Ep 2 (Step 074230): Train loss 0.987, Val loss 1.762\n",
      "Ep 2 (Step 074235): Train loss 0.842, Val loss 1.762\n",
      "Ep 2 (Step 074240): Train loss 0.902, Val loss 1.761\n",
      "Ep 2 (Step 074245): Train loss 1.348, Val loss 1.761\n",
      "Ep 2 (Step 074250): Train loss 1.157, Val loss 1.763\n",
      "Ep 2 (Step 074255): Train loss 1.013, Val loss 1.764\n",
      "Ep 2 (Step 074260): Train loss 1.058, Val loss 1.766\n",
      "Ep 2 (Step 074265): Train loss 0.836, Val loss 1.768\n",
      "Ep 2 (Step 074270): Train loss 0.922, Val loss 1.769\n",
      "Ep 2 (Step 074275): Train loss 1.162, Val loss 1.769\n",
      "Ep 2 (Step 074280): Train loss 1.214, Val loss 1.769\n",
      "Ep 2 (Step 074285): Train loss 1.102, Val loss 1.768\n",
      "Ep 2 (Step 074290): Train loss 1.035, Val loss 1.768\n",
      "Ep 2 (Step 074295): Train loss 1.458, Val loss 1.768\n",
      "Ep 2 (Step 074300): Train loss 1.253, Val loss 1.768\n",
      "Ep 2 (Step 074305): Train loss 0.808, Val loss 1.769\n",
      "Ep 2 (Step 074310): Train loss 0.986, Val loss 1.769\n",
      "Ep 2 (Step 074315): Train loss 0.858, Val loss 1.769\n",
      "Ep 2 (Step 074320): Train loss 0.929, Val loss 1.769\n",
      "Ep 2 (Step 074325): Train loss 0.922, Val loss 1.769\n",
      "Ep 2 (Step 074330): Train loss 0.765, Val loss 1.768\n",
      "Ep 2 (Step 074335): Train loss 0.866, Val loss 1.767\n",
      "Ep 2 (Step 074340): Train loss 0.843, Val loss 1.766\n",
      "Ep 2 (Step 074345): Train loss 0.985, Val loss 1.766\n",
      "Ep 2 (Step 074350): Train loss 0.775, Val loss 1.767\n",
      "Ep 2 (Step 074355): Train loss 0.751, Val loss 1.768\n",
      "Ep 2 (Step 074360): Train loss 1.050, Val loss 1.769\n",
      "Ep 2 (Step 074365): Train loss 0.870, Val loss 1.771\n",
      "Ep 2 (Step 074370): Train loss 0.924, Val loss 1.773\n",
      "Ep 2 (Step 074375): Train loss 0.624, Val loss 1.774\n",
      "Ep 2 (Step 074380): Train loss 0.983, Val loss 1.772\n",
      "Ep 2 (Step 074385): Train loss 0.967, Val loss 1.772\n",
      "Ep 2 (Step 074390): Train loss 0.952, Val loss 1.771\n",
      "Ep 2 (Step 074395): Train loss 1.371, Val loss 1.772\n",
      "Ep 2 (Step 074400): Train loss 1.040, Val loss 1.772\n",
      "Ep 2 (Step 074405): Train loss 1.038, Val loss 1.771\n",
      "Ep 2 (Step 074410): Train loss 0.972, Val loss 1.770\n",
      "Ep 2 (Step 074415): Train loss 1.214, Val loss 1.769\n",
      "Ep 2 (Step 074420): Train loss 1.206, Val loss 1.769\n",
      "Ep 2 (Step 074425): Train loss 1.270, Val loss 1.769\n",
      "Ep 2 (Step 074430): Train loss 1.280, Val loss 1.769\n",
      "Ep 2 (Step 074435): Train loss 1.114, Val loss 1.769\n",
      "Ep 2 (Step 074440): Train loss 0.717, Val loss 1.769\n",
      "Ep 2 (Step 074445): Train loss 0.978, Val loss 1.771\n",
      "Ep 2 (Step 074450): Train loss 1.039, Val loss 1.771\n",
      "Ep 2 (Step 074455): Train loss 0.999, Val loss 1.772\n",
      "Ep 2 (Step 074460): Train loss 0.915, Val loss 1.774\n",
      "Ep 2 (Step 074465): Train loss 1.008, Val loss 1.776\n",
      "Ep 2 (Step 074470): Train loss 1.124, Val loss 1.778\n",
      "Ep 2 (Step 074475): Train loss 0.935, Val loss 1.778\n",
      "Ep 2 (Step 074480): Train loss 1.058, Val loss 1.779\n",
      "Ep 2 (Step 074485): Train loss 0.950, Val loss 1.781\n",
      "Ep 2 (Step 074490): Train loss 1.029, Val loss 1.782\n",
      "Ep 2 (Step 074495): Train loss 0.973, Val loss 1.784\n",
      "Ep 2 (Step 074500): Train loss 0.915, Val loss 1.783\n",
      "Ep 2 (Step 074505): Train loss 1.241, Val loss 1.783\n",
      "Ep 2 (Step 074510): Train loss 1.053, Val loss 1.782\n",
      "Ep 2 (Step 074515): Train loss 1.118, Val loss 1.782\n",
      "Ep 2 (Step 074520): Train loss 0.980, Val loss 1.783\n",
      "Ep 2 (Step 074525): Train loss 1.116, Val loss 1.783\n",
      "Ep 2 (Step 074530): Train loss 0.940, Val loss 1.783\n",
      "Ep 2 (Step 074535): Train loss 0.838, Val loss 1.783\n",
      "Ep 2 (Step 074540): Train loss 1.109, Val loss 1.782\n",
      "Ep 2 (Step 074545): Train loss 0.908, Val loss 1.781\n",
      "Ep 2 (Step 074550): Train loss 0.982, Val loss 1.782\n",
      "Ep 2 (Step 074555): Train loss 0.881, Val loss 1.783\n",
      "Ep 2 (Step 074560): Train loss 0.924, Val loss 1.784\n",
      "Ep 2 (Step 074565): Train loss 1.100, Val loss 1.784\n",
      "Ep 2 (Step 074570): Train loss 1.074, Val loss 1.784\n",
      "Ep 2 (Step 074575): Train loss 0.659, Val loss 1.783\n",
      "Ep 2 (Step 074580): Train loss 1.014, Val loss 1.782\n",
      "Ep 2 (Step 074585): Train loss 0.985, Val loss 1.782\n",
      "Ep 2 (Step 074590): Train loss 0.893, Val loss 1.782\n",
      "Ep 2 (Step 074595): Train loss 0.955, Val loss 1.782\n",
      "Ep 2 (Step 074600): Train loss 0.978, Val loss 1.780\n",
      "Ep 2 (Step 074605): Train loss 0.775, Val loss 1.778\n",
      "Ep 2 (Step 074610): Train loss 0.968, Val loss 1.775\n",
      "Ep 2 (Step 074615): Train loss 1.311, Val loss 1.773\n",
      "Ep 2 (Step 074620): Train loss 1.045, Val loss 1.772\n",
      "Ep 2 (Step 074625): Train loss 0.762, Val loss 1.771\n",
      "Ep 2 (Step 074630): Train loss 0.818, Val loss 1.769\n",
      "Ep 2 (Step 074635): Train loss 1.034, Val loss 1.768\n",
      "Ep 2 (Step 074640): Train loss 1.281, Val loss 1.766\n",
      "Ep 2 (Step 074645): Train loss 1.001, Val loss 1.765\n",
      "Ep 2 (Step 074650): Train loss 1.032, Val loss 1.765\n",
      "Ep 2 (Step 074655): Train loss 1.034, Val loss 1.765\n",
      "Ep 2 (Step 074660): Train loss 1.004, Val loss 1.766\n",
      "Ep 2 (Step 074665): Train loss 0.912, Val loss 1.768\n",
      "Ep 2 (Step 074670): Train loss 0.977, Val loss 1.769\n",
      "Ep 2 (Step 074675): Train loss 0.841, Val loss 1.769\n",
      "Ep 2 (Step 074680): Train loss 1.096, Val loss 1.769\n",
      "Ep 2 (Step 074685): Train loss 0.917, Val loss 1.767\n",
      "Ep 2 (Step 074690): Train loss 1.167, Val loss 1.766\n",
      "Ep 2 (Step 074695): Train loss 1.159, Val loss 1.764\n",
      "Ep 2 (Step 074700): Train loss 1.075, Val loss 1.763\n",
      "Ep 2 (Step 074705): Train loss 1.215, Val loss 1.762\n",
      "Ep 2 (Step 074710): Train loss 1.199, Val loss 1.763\n",
      "Ep 2 (Step 074715): Train loss 0.965, Val loss 1.765\n",
      "Ep 2 (Step 074720): Train loss 0.872, Val loss 1.769\n",
      "Ep 2 (Step 074725): Train loss 1.147, Val loss 1.771\n",
      "Ep 2 (Step 074730): Train loss 0.902, Val loss 1.773\n",
      "Ep 2 (Step 074735): Train loss 1.240, Val loss 1.773\n",
      "Ep 2 (Step 074740): Train loss 1.172, Val loss 1.772\n",
      "Ep 2 (Step 074745): Train loss 1.065, Val loss 1.773\n",
      "Ep 2 (Step 074750): Train loss 0.922, Val loss 1.774\n",
      "Ep 2 (Step 074755): Train loss 1.208, Val loss 1.773\n",
      "Ep 2 (Step 074760): Train loss 1.210, Val loss 1.773\n",
      "Ep 2 (Step 074765): Train loss 0.932, Val loss 1.772\n",
      "Ep 2 (Step 074770): Train loss 1.069, Val loss 1.772\n",
      "Ep 2 (Step 074775): Train loss 0.967, Val loss 1.771\n",
      "Ep 2 (Step 074780): Train loss 1.028, Val loss 1.770\n",
      "Ep 2 (Step 074785): Train loss 0.795, Val loss 1.768\n",
      "Ep 2 (Step 074790): Train loss 0.925, Val loss 1.767\n",
      "Ep 2 (Step 074795): Train loss 1.276, Val loss 1.766\n",
      "Ep 2 (Step 074800): Train loss 0.838, Val loss 1.766\n",
      "Ep 2 (Step 074805): Train loss 1.042, Val loss 1.765\n",
      "Ep 2 (Step 074810): Train loss 1.041, Val loss 1.765\n",
      "Ep 2 (Step 074815): Train loss 0.994, Val loss 1.766\n",
      "Ep 2 (Step 074820): Train loss 1.125, Val loss 1.766\n",
      "Ep 2 (Step 074825): Train loss 0.687, Val loss 1.766\n",
      "Ep 2 (Step 074830): Train loss 1.113, Val loss 1.767\n",
      "Ep 2 (Step 074835): Train loss 0.961, Val loss 1.768\n",
      "Ep 2 (Step 074840): Train loss 1.025, Val loss 1.768\n",
      "Ep 2 (Step 074845): Train loss 1.137, Val loss 1.769\n",
      "Ep 2 (Step 074850): Train loss 0.979, Val loss 1.769\n",
      "Ep 2 (Step 074855): Train loss 0.814, Val loss 1.768\n",
      "Ep 2 (Step 074860): Train loss 1.254, Val loss 1.769\n",
      "Ep 2 (Step 074865): Train loss 1.115, Val loss 1.768\n",
      "Ep 2 (Step 074870): Train loss 0.930, Val loss 1.768\n",
      "Ep 2 (Step 074875): Train loss 1.535, Val loss 1.767\n",
      "Ep 2 (Step 074880): Train loss 0.964, Val loss 1.766\n",
      "Ep 2 (Step 074885): Train loss 1.024, Val loss 1.766\n",
      "Ep 2 (Step 074890): Train loss 0.951, Val loss 1.766\n",
      "Ep 2 (Step 074895): Train loss 0.989, Val loss 1.766\n",
      "Ep 2 (Step 074900): Train loss 0.905, Val loss 1.767\n",
      "Ep 2 (Step 074905): Train loss 0.717, Val loss 1.768\n",
      "Ep 2 (Step 074910): Train loss 0.830, Val loss 1.767\n",
      "Ep 2 (Step 074915): Train loss 0.920, Val loss 1.767\n",
      "Ep 2 (Step 074920): Train loss 0.971, Val loss 1.767\n",
      "Ep 2 (Step 074925): Train loss 1.020, Val loss 1.766\n",
      "Ep 2 (Step 074930): Train loss 0.786, Val loss 1.764\n",
      "Ep 2 (Step 074935): Train loss 1.073, Val loss 1.762\n",
      "Ep 2 (Step 074940): Train loss 1.005, Val loss 1.761\n",
      "Ep 2 (Step 074945): Train loss 0.807, Val loss 1.759\n",
      "Ep 2 (Step 074950): Train loss 1.169, Val loss 1.757\n",
      "Ep 2 (Step 074955): Train loss 0.983, Val loss 1.756\n",
      "Ep 2 (Step 074960): Train loss 0.837, Val loss 1.753\n",
      "Ep 2 (Step 074965): Train loss 1.063, Val loss 1.751\n",
      "Ep 2 (Step 074970): Train loss 0.892, Val loss 1.749\n",
      "Ep 2 (Step 074975): Train loss 0.919, Val loss 1.748\n",
      "Ep 2 (Step 074980): Train loss 0.872, Val loss 1.747\n",
      "Ep 2 (Step 074985): Train loss 1.274, Val loss 1.748\n",
      "Ep 2 (Step 074990): Train loss 0.887, Val loss 1.749\n",
      "Ep 2 (Step 074995): Train loss 0.944, Val loss 1.750\n",
      "Ep 2 (Step 075000): Train loss 0.843, Val loss 1.752\n",
      "Ep 2 (Step 075005): Train loss 1.258, Val loss 1.754\n",
      "Ep 2 (Step 075010): Train loss 0.899, Val loss 1.756\n",
      "Ep 2 (Step 075015): Train loss 0.901, Val loss 1.759\n",
      "Ep 2 (Step 075020): Train loss 1.308, Val loss 1.761\n",
      "Ep 2 (Step 075025): Train loss 1.077, Val loss 1.763\n",
      "Ep 2 (Step 075030): Train loss 0.979, Val loss 1.764\n",
      "Ep 2 (Step 075035): Train loss 1.128, Val loss 1.764\n",
      "Ep 2 (Step 075040): Train loss 1.087, Val loss 1.764\n",
      "Ep 2 (Step 075045): Train loss 1.115, Val loss 1.763\n",
      "Ep 2 (Step 075050): Train loss 1.187, Val loss 1.762\n",
      "Ep 2 (Step 075055): Train loss 0.868, Val loss 1.761\n",
      "Ep 2 (Step 075060): Train loss 1.100, Val loss 1.762\n",
      "Ep 2 (Step 075065): Train loss 1.044, Val loss 1.762\n",
      "Ep 2 (Step 075070): Train loss 0.880, Val loss 1.761\n",
      "Ep 2 (Step 075075): Train loss 0.903, Val loss 1.761\n",
      "Ep 2 (Step 075080): Train loss 0.909, Val loss 1.759\n",
      "Ep 2 (Step 075085): Train loss 0.984, Val loss 1.756\n",
      "Ep 2 (Step 075090): Train loss 0.877, Val loss 1.757\n",
      "Ep 2 (Step 075095): Train loss 0.964, Val loss 1.758\n",
      "Ep 2 (Step 075100): Train loss 0.887, Val loss 1.757\n",
      "Ep 2 (Step 075105): Train loss 0.881, Val loss 1.758\n",
      "Ep 2 (Step 075110): Train loss 0.777, Val loss 1.760\n",
      "Ep 2 (Step 075115): Train loss 0.804, Val loss 1.761\n",
      "Ep 2 (Step 075120): Train loss 0.874, Val loss 1.761\n",
      "Ep 2 (Step 075125): Train loss 0.835, Val loss 1.761\n",
      "Ep 2 (Step 075130): Train loss 0.777, Val loss 1.760\n",
      "Ep 2 (Step 075135): Train loss 0.864, Val loss 1.759\n",
      "Ep 2 (Step 075140): Train loss 0.669, Val loss 1.760\n",
      "Ep 2 (Step 075145): Train loss 0.997, Val loss 1.761\n",
      "Ep 2 (Step 075150): Train loss 0.968, Val loss 1.763\n",
      "Ep 2 (Step 075155): Train loss 1.101, Val loss 1.764\n",
      "Ep 2 (Step 075160): Train loss 1.015, Val loss 1.765\n",
      "Ep 2 (Step 075165): Train loss 1.164, Val loss 1.765\n",
      "Ep 2 (Step 075170): Train loss 0.993, Val loss 1.767\n",
      "Ep 2 (Step 075175): Train loss 1.225, Val loss 1.767\n",
      "Ep 2 (Step 075180): Train loss 1.257, Val loss 1.767\n",
      "Ep 2 (Step 075185): Train loss 1.226, Val loss 1.768\n",
      "Ep 2 (Step 075190): Train loss 0.997, Val loss 1.770\n",
      "Ep 2 (Step 075195): Train loss 1.274, Val loss 1.772\n",
      "Ep 2 (Step 075200): Train loss 0.923, Val loss 1.772\n",
      "Ep 2 (Step 075205): Train loss 0.978, Val loss 1.772\n",
      "Ep 2 (Step 075210): Train loss 0.957, Val loss 1.774\n",
      "Ep 2 (Step 075215): Train loss 0.908, Val loss 1.776\n",
      "Ep 2 (Step 075220): Train loss 1.248, Val loss 1.778\n",
      "Ep 2 (Step 075225): Train loss 0.725, Val loss 1.780\n",
      "Ep 2 (Step 075230): Train loss 1.033, Val loss 1.780\n",
      "Ep 2 (Step 075235): Train loss 0.945, Val loss 1.781\n",
      "Ep 2 (Step 075240): Train loss 1.014, Val loss 1.783\n",
      "Ep 2 (Step 075245): Train loss 1.236, Val loss 1.784\n",
      "Ep 2 (Step 075250): Train loss 1.220, Val loss 1.787\n",
      "Ep 2 (Step 075255): Train loss 1.282, Val loss 1.788\n",
      "Ep 2 (Step 075260): Train loss 1.099, Val loss 1.788\n",
      "Ep 2 (Step 075265): Train loss 1.172, Val loss 1.786\n",
      "Ep 2 (Step 075270): Train loss 0.896, Val loss 1.785\n",
      "Ep 2 (Step 075275): Train loss 0.891, Val loss 1.783\n",
      "Ep 2 (Step 075280): Train loss 0.921, Val loss 1.783\n",
      "Ep 2 (Step 075285): Train loss 0.866, Val loss 1.781\n",
      "Ep 2 (Step 075290): Train loss 1.077, Val loss 1.779\n",
      "Ep 2 (Step 075295): Train loss 0.974, Val loss 1.778\n",
      "Ep 2 (Step 075300): Train loss 1.043, Val loss 1.777\n",
      "Ep 2 (Step 075305): Train loss 0.941, Val loss 1.776\n",
      "Ep 2 (Step 075310): Train loss 0.761, Val loss 1.775\n",
      "Ep 2 (Step 075315): Train loss 1.185, Val loss 1.775\n",
      "Ep 2 (Step 075320): Train loss 1.080, Val loss 1.775\n",
      "Ep 2 (Step 075325): Train loss 0.723, Val loss 1.775\n",
      "Ep 2 (Step 075330): Train loss 0.857, Val loss 1.776\n",
      "Ep 2 (Step 075335): Train loss 0.985, Val loss 1.776\n",
      "Ep 2 (Step 075340): Train loss 1.044, Val loss 1.776\n",
      "Ep 2 (Step 075345): Train loss 0.979, Val loss 1.776\n",
      "Ep 2 (Step 075350): Train loss 0.935, Val loss 1.775\n",
      "Ep 2 (Step 075355): Train loss 0.885, Val loss 1.776\n",
      "Ep 2 (Step 075360): Train loss 0.962, Val loss 1.776\n",
      "Ep 2 (Step 075365): Train loss 0.891, Val loss 1.776\n",
      "Ep 2 (Step 075370): Train loss 0.986, Val loss 1.774\n",
      "Ep 2 (Step 075375): Train loss 1.213, Val loss 1.773\n",
      "Ep 2 (Step 075380): Train loss 1.192, Val loss 1.772\n",
      "Ep 2 (Step 075385): Train loss 1.238, Val loss 1.772\n",
      "Ep 2 (Step 075390): Train loss 0.790, Val loss 1.772\n",
      "Ep 2 (Step 075395): Train loss 1.089, Val loss 1.771\n",
      "Ep 2 (Step 075400): Train loss 1.316, Val loss 1.769\n",
      "Ep 2 (Step 075405): Train loss 1.184, Val loss 1.768\n",
      "Ep 2 (Step 075410): Train loss 1.248, Val loss 1.766\n",
      "Ep 2 (Step 075415): Train loss 0.982, Val loss 1.765\n",
      "Ep 2 (Step 075420): Train loss 1.005, Val loss 1.765\n",
      "Ep 2 (Step 075425): Train loss 0.969, Val loss 1.765\n",
      "Ep 2 (Step 075430): Train loss 1.115, Val loss 1.764\n",
      "Ep 2 (Step 075435): Train loss 0.717, Val loss 1.764\n",
      "Ep 2 (Step 075440): Train loss 0.908, Val loss 1.764\n",
      "Ep 2 (Step 075445): Train loss 1.075, Val loss 1.763\n",
      "Ep 2 (Step 075450): Train loss 1.099, Val loss 1.763\n",
      "Ep 2 (Step 075455): Train loss 0.994, Val loss 1.761\n",
      "Ep 2 (Step 075460): Train loss 0.821, Val loss 1.761\n",
      "Ep 2 (Step 075465): Train loss 1.165, Val loss 1.761\n",
      "Ep 2 (Step 075470): Train loss 0.904, Val loss 1.761\n",
      "Ep 2 (Step 075475): Train loss 1.191, Val loss 1.762\n",
      "Ep 2 (Step 075480): Train loss 0.926, Val loss 1.763\n",
      "Ep 2 (Step 075485): Train loss 1.075, Val loss 1.763\n",
      "Ep 2 (Step 075490): Train loss 0.949, Val loss 1.764\n",
      "Ep 2 (Step 075495): Train loss 1.299, Val loss 1.765\n",
      "Ep 2 (Step 075500): Train loss 1.085, Val loss 1.766\n",
      "Ep 2 (Step 075505): Train loss 1.014, Val loss 1.767\n",
      "Ep 2 (Step 075510): Train loss 0.813, Val loss 1.768\n",
      "Ep 2 (Step 075515): Train loss 1.221, Val loss 1.769\n",
      "Ep 2 (Step 075520): Train loss 1.131, Val loss 1.768\n",
      "Ep 2 (Step 075525): Train loss 1.326, Val loss 1.766\n",
      "Ep 2 (Step 075530): Train loss 1.209, Val loss 1.765\n",
      "Ep 2 (Step 075535): Train loss 0.968, Val loss 1.764\n",
      "Ep 2 (Step 075540): Train loss 0.795, Val loss 1.763\n",
      "Ep 2 (Step 075545): Train loss 1.079, Val loss 1.763\n",
      "Ep 2 (Step 075550): Train loss 0.983, Val loss 1.764\n",
      "Ep 2 (Step 075555): Train loss 1.089, Val loss 1.763\n",
      "Ep 2 (Step 075560): Train loss 0.824, Val loss 1.761\n",
      "Ep 2 (Step 075565): Train loss 1.045, Val loss 1.761\n",
      "Ep 2 (Step 075570): Train loss 1.051, Val loss 1.761\n",
      "Ep 2 (Step 075575): Train loss 0.775, Val loss 1.761\n",
      "Ep 2 (Step 075580): Train loss 1.141, Val loss 1.761\n",
      "Ep 2 (Step 075585): Train loss 1.134, Val loss 1.760\n",
      "Ep 2 (Step 075590): Train loss 1.122, Val loss 1.760\n",
      "Ep 2 (Step 075595): Train loss 0.999, Val loss 1.760\n",
      "Ep 2 (Step 075600): Train loss 0.793, Val loss 1.761\n",
      "Ep 2 (Step 075605): Train loss 1.208, Val loss 1.761\n",
      "Ep 2 (Step 075610): Train loss 0.908, Val loss 1.761\n",
      "Ep 2 (Step 075615): Train loss 0.868, Val loss 1.761\n",
      "Ep 2 (Step 075620): Train loss 0.822, Val loss 1.759\n",
      "Ep 2 (Step 075625): Train loss 0.721, Val loss 1.759\n",
      "Ep 2 (Step 075630): Train loss 0.786, Val loss 1.760\n",
      "Ep 2 (Step 075635): Train loss 0.907, Val loss 1.760\n",
      "Ep 2 (Step 075640): Train loss 1.059, Val loss 1.761\n",
      "Ep 2 (Step 075645): Train loss 0.755, Val loss 1.760\n",
      "Ep 2 (Step 075650): Train loss 0.802, Val loss 1.760\n",
      "Ep 2 (Step 075655): Train loss 1.313, Val loss 1.760\n",
      "Ep 2 (Step 075660): Train loss 1.110, Val loss 1.760\n",
      "Ep 2 (Step 075665): Train loss 1.013, Val loss 1.760\n",
      "Ep 2 (Step 075670): Train loss 0.875, Val loss 1.759\n",
      "Ep 2 (Step 075675): Train loss 1.115, Val loss 1.757\n",
      "Ep 2 (Step 075680): Train loss 1.026, Val loss 1.755\n",
      "Ep 2 (Step 075685): Train loss 1.121, Val loss 1.752\n",
      "Ep 2 (Step 075690): Train loss 1.312, Val loss 1.752\n",
      "Ep 2 (Step 075695): Train loss 1.099, Val loss 1.751\n",
      "Ep 2 (Step 075700): Train loss 1.102, Val loss 1.751\n",
      "Ep 2 (Step 075705): Train loss 1.031, Val loss 1.751\n",
      "Ep 2 (Step 075710): Train loss 1.303, Val loss 1.750\n",
      "Ep 2 (Step 075715): Train loss 0.963, Val loss 1.752\n",
      "Ep 2 (Step 075720): Train loss 0.891, Val loss 1.754\n",
      "Ep 2 (Step 075725): Train loss 1.043, Val loss 1.754\n",
      "Ep 2 (Step 075730): Train loss 1.120, Val loss 1.755\n",
      "Ep 2 (Step 075735): Train loss 1.028, Val loss 1.756\n",
      "Ep 2 (Step 075740): Train loss 0.975, Val loss 1.758\n",
      "Ep 2 (Step 075745): Train loss 1.009, Val loss 1.759\n",
      "Ep 2 (Step 075750): Train loss 1.130, Val loss 1.759\n",
      "Ep 2 (Step 075755): Train loss 1.036, Val loss 1.759\n",
      "Ep 2 (Step 075760): Train loss 0.819, Val loss 1.760\n",
      "Ep 2 (Step 075765): Train loss 1.044, Val loss 1.759\n",
      "Ep 2 (Step 075770): Train loss 0.965, Val loss 1.760\n",
      "Ep 2 (Step 075775): Train loss 1.050, Val loss 1.760\n",
      "Ep 2 (Step 075780): Train loss 1.102, Val loss 1.761\n",
      "Ep 2 (Step 075785): Train loss 0.873, Val loss 1.760\n",
      "Ep 2 (Step 075790): Train loss 1.135, Val loss 1.760\n",
      "Ep 2 (Step 075795): Train loss 0.858, Val loss 1.761\n",
      "Ep 2 (Step 075800): Train loss 1.201, Val loss 1.762\n",
      "Ep 2 (Step 075805): Train loss 0.795, Val loss 1.764\n",
      "Ep 2 (Step 075810): Train loss 0.949, Val loss 1.766\n",
      "Ep 2 (Step 075815): Train loss 1.077, Val loss 1.767\n",
      "Ep 2 (Step 075820): Train loss 1.186, Val loss 1.768\n",
      "Ep 2 (Step 075825): Train loss 0.803, Val loss 1.768\n",
      "Ep 2 (Step 075830): Train loss 0.871, Val loss 1.768\n",
      "Ep 2 (Step 075835): Train loss 1.020, Val loss 1.769\n",
      "Ep 2 (Step 075840): Train loss 0.929, Val loss 1.768\n",
      "Ep 2 (Step 075845): Train loss 0.951, Val loss 1.769\n",
      "Ep 2 (Step 075850): Train loss 0.929, Val loss 1.769\n",
      "Ep 2 (Step 075855): Train loss 1.099, Val loss 1.769\n",
      "Ep 2 (Step 075860): Train loss 1.013, Val loss 1.770\n",
      "Ep 2 (Step 075865): Train loss 1.030, Val loss 1.771\n",
      "Ep 2 (Step 075870): Train loss 0.862, Val loss 1.772\n",
      "Ep 2 (Step 075875): Train loss 0.824, Val loss 1.773\n",
      "Ep 2 (Step 075880): Train loss 0.988, Val loss 1.773\n",
      "Ep 2 (Step 075885): Train loss 0.846, Val loss 1.773\n",
      "Ep 2 (Step 075890): Train loss 1.183, Val loss 1.773\n",
      "Ep 2 (Step 075895): Train loss 1.357, Val loss 1.772\n",
      "Ep 2 (Step 075900): Train loss 1.111, Val loss 1.771\n",
      "Ep 2 (Step 075905): Train loss 1.030, Val loss 1.770\n",
      "Ep 2 (Step 075910): Train loss 0.831, Val loss 1.768\n",
      "Ep 2 (Step 075915): Train loss 1.097, Val loss 1.768\n",
      "Ep 2 (Step 075920): Train loss 0.920, Val loss 1.768\n",
      "Ep 2 (Step 075925): Train loss 1.149, Val loss 1.767\n",
      "Ep 2 (Step 075930): Train loss 0.937, Val loss 1.766\n",
      "Ep 2 (Step 075935): Train loss 1.082, Val loss 1.767\n",
      "Ep 2 (Step 075940): Train loss 0.722, Val loss 1.765\n",
      "Ep 2 (Step 075945): Train loss 0.964, Val loss 1.764\n",
      "Ep 2 (Step 075950): Train loss 1.058, Val loss 1.762\n",
      "Ep 2 (Step 075955): Train loss 0.759, Val loss 1.761\n",
      "Ep 2 (Step 075960): Train loss 1.051, Val loss 1.762\n",
      "Ep 2 (Step 075965): Train loss 1.002, Val loss 1.762\n",
      "Ep 2 (Step 075970): Train loss 0.803, Val loss 1.762\n",
      "Ep 2 (Step 075975): Train loss 0.849, Val loss 1.761\n",
      "Ep 2 (Step 075980): Train loss 1.118, Val loss 1.759\n",
      "Ep 2 (Step 075985): Train loss 0.763, Val loss 1.757\n",
      "Ep 2 (Step 075990): Train loss 0.660, Val loss 1.757\n",
      "Ep 2 (Step 075995): Train loss 1.060, Val loss 1.757\n",
      "Ep 2 (Step 076000): Train loss 0.866, Val loss 1.758\n",
      "Ep 2 (Step 076005): Train loss 0.753, Val loss 1.757\n",
      "Ep 2 (Step 076010): Train loss 0.851, Val loss 1.758\n",
      "Ep 2 (Step 076015): Train loss 1.027, Val loss 1.758\n",
      "Ep 2 (Step 076020): Train loss 1.086, Val loss 1.756\n",
      "Ep 2 (Step 076025): Train loss 1.000, Val loss 1.755\n",
      "Ep 2 (Step 076030): Train loss 1.155, Val loss 1.754\n",
      "Ep 2 (Step 076035): Train loss 0.929, Val loss 1.753\n",
      "Ep 2 (Step 076040): Train loss 1.404, Val loss 1.753\n",
      "Ep 2 (Step 076045): Train loss 1.061, Val loss 1.754\n",
      "Ep 2 (Step 076050): Train loss 1.124, Val loss 1.753\n",
      "Ep 2 (Step 076055): Train loss 0.863, Val loss 1.751\n",
      "Ep 2 (Step 076060): Train loss 0.996, Val loss 1.750\n",
      "Ep 2 (Step 076065): Train loss 0.870, Val loss 1.750\n",
      "Ep 2 (Step 076070): Train loss 1.027, Val loss 1.750\n",
      "Ep 2 (Step 076075): Train loss 0.997, Val loss 1.750\n",
      "Ep 2 (Step 076080): Train loss 1.089, Val loss 1.751\n",
      "Ep 2 (Step 076085): Train loss 1.057, Val loss 1.751\n",
      "Ep 2 (Step 076090): Train loss 0.978, Val loss 1.752\n",
      "Ep 2 (Step 076095): Train loss 0.779, Val loss 1.753\n",
      "Ep 2 (Step 076100): Train loss 0.875, Val loss 1.755\n",
      "Ep 2 (Step 076105): Train loss 0.892, Val loss 1.756\n",
      "Ep 2 (Step 076110): Train loss 1.161, Val loss 1.759\n",
      "Ep 2 (Step 076115): Train loss 0.990, Val loss 1.759\n",
      "Ep 2 (Step 076120): Train loss 0.948, Val loss 1.760\n",
      "Ep 2 (Step 076125): Train loss 1.134, Val loss 1.759\n",
      "Ep 2 (Step 076130): Train loss 0.906, Val loss 1.758\n",
      "Ep 2 (Step 076135): Train loss 0.993, Val loss 1.756\n",
      "Ep 2 (Step 076140): Train loss 1.041, Val loss 1.755\n",
      "Ep 2 (Step 076145): Train loss 1.019, Val loss 1.755\n",
      "Ep 2 (Step 076150): Train loss 1.044, Val loss 1.755\n",
      "Ep 2 (Step 076155): Train loss 0.923, Val loss 1.754\n",
      "Ep 2 (Step 076160): Train loss 1.107, Val loss 1.752\n",
      "Ep 2 (Step 076165): Train loss 1.004, Val loss 1.751\n",
      "Ep 2 (Step 076170): Train loss 1.091, Val loss 1.750\n",
      "Ep 2 (Step 076175): Train loss 0.892, Val loss 1.750\n",
      "Ep 2 (Step 076180): Train loss 0.876, Val loss 1.751\n",
      "Ep 2 (Step 076185): Train loss 0.748, Val loss 1.752\n",
      "Ep 2 (Step 076190): Train loss 1.259, Val loss 1.752\n",
      "Ep 2 (Step 076195): Train loss 0.902, Val loss 1.752\n",
      "Ep 2 (Step 076200): Train loss 0.952, Val loss 1.754\n",
      "Ep 2 (Step 076205): Train loss 1.194, Val loss 1.756\n",
      "Ep 2 (Step 076210): Train loss 0.886, Val loss 1.758\n",
      "Ep 2 (Step 076215): Train loss 1.169, Val loss 1.758\n",
      "Ep 2 (Step 076220): Train loss 0.890, Val loss 1.758\n",
      "Ep 2 (Step 076225): Train loss 1.142, Val loss 1.757\n",
      "Ep 2 (Step 076230): Train loss 1.159, Val loss 1.758\n",
      "Ep 2 (Step 076235): Train loss 1.072, Val loss 1.758\n",
      "Ep 2 (Step 076240): Train loss 1.064, Val loss 1.757\n",
      "Ep 2 (Step 076245): Train loss 0.742, Val loss 1.757\n",
      "Ep 2 (Step 076250): Train loss 0.897, Val loss 1.757\n",
      "Ep 2 (Step 076255): Train loss 1.362, Val loss 1.756\n",
      "Ep 2 (Step 076260): Train loss 1.171, Val loss 1.755\n",
      "Ep 2 (Step 076265): Train loss 1.044, Val loss 1.755\n",
      "Ep 2 (Step 076270): Train loss 0.990, Val loss 1.755\n",
      "Ep 2 (Step 076275): Train loss 0.972, Val loss 1.755\n",
      "Ep 2 (Step 076280): Train loss 1.057, Val loss 1.755\n",
      "Ep 2 (Step 076285): Train loss 1.001, Val loss 1.756\n",
      "Ep 2 (Step 076290): Train loss 1.012, Val loss 1.758\n",
      "Ep 2 (Step 076295): Train loss 1.017, Val loss 1.758\n",
      "Ep 2 (Step 076300): Train loss 0.764, Val loss 1.757\n",
      "Ep 2 (Step 076305): Train loss 1.352, Val loss 1.758\n",
      "Ep 2 (Step 076310): Train loss 1.171, Val loss 1.757\n",
      "Ep 2 (Step 076315): Train loss 0.947, Val loss 1.756\n",
      "Ep 2 (Step 076320): Train loss 1.095, Val loss 1.756\n",
      "Ep 2 (Step 076325): Train loss 0.921, Val loss 1.757\n",
      "Ep 2 (Step 076330): Train loss 0.982, Val loss 1.756\n",
      "Ep 2 (Step 076335): Train loss 1.119, Val loss 1.756\n",
      "Ep 2 (Step 076340): Train loss 1.036, Val loss 1.756\n",
      "Ep 2 (Step 076345): Train loss 0.823, Val loss 1.756\n",
      "Ep 2 (Step 076350): Train loss 1.251, Val loss 1.756\n",
      "Ep 2 (Step 076355): Train loss 0.964, Val loss 1.756\n",
      "Ep 2 (Step 076360): Train loss 1.037, Val loss 1.756\n",
      "Ep 2 (Step 076365): Train loss 1.037, Val loss 1.756\n",
      "Ep 2 (Step 076370): Train loss 0.899, Val loss 1.757\n",
      "Ep 2 (Step 076375): Train loss 1.073, Val loss 1.758\n",
      "Ep 2 (Step 076380): Train loss 1.081, Val loss 1.758\n",
      "Ep 2 (Step 076385): Train loss 0.912, Val loss 1.757\n",
      "Ep 2 (Step 076390): Train loss 1.069, Val loss 1.757\n",
      "Ep 2 (Step 076395): Train loss 0.914, Val loss 1.757\n",
      "Ep 2 (Step 076400): Train loss 0.850, Val loss 1.757\n",
      "Ep 2 (Step 076405): Train loss 1.152, Val loss 1.757\n",
      "Ep 2 (Step 076410): Train loss 0.942, Val loss 1.757\n",
      "Ep 2 (Step 076415): Train loss 0.922, Val loss 1.756\n",
      "Ep 2 (Step 076420): Train loss 0.902, Val loss 1.755\n",
      "Ep 2 (Step 076425): Train loss 0.858, Val loss 1.753\n",
      "Ep 2 (Step 076430): Train loss 0.963, Val loss 1.753\n",
      "Ep 2 (Step 076435): Train loss 0.927, Val loss 1.752\n",
      "Ep 2 (Step 076440): Train loss 1.077, Val loss 1.753\n",
      "Ep 2 (Step 076445): Train loss 0.886, Val loss 1.752\n",
      "Ep 2 (Step 076450): Train loss 0.964, Val loss 1.752\n",
      "Ep 2 (Step 076455): Train loss 0.833, Val loss 1.753\n",
      "Ep 2 (Step 076460): Train loss 0.810, Val loss 1.754\n",
      "Ep 2 (Step 076465): Train loss 1.221, Val loss 1.753\n",
      "Ep 2 (Step 076470): Train loss 1.100, Val loss 1.751\n",
      "Ep 2 (Step 076475): Train loss 1.111, Val loss 1.751\n",
      "Ep 2 (Step 076480): Train loss 0.824, Val loss 1.751\n",
      "Ep 2 (Step 076485): Train loss 1.226, Val loss 1.752\n",
      "Ep 2 (Step 076490): Train loss 0.753, Val loss 1.753\n",
      "Ep 2 (Step 076495): Train loss 0.849, Val loss 1.754\n",
      "Ep 2 (Step 076500): Train loss 1.017, Val loss 1.754\n",
      "Ep 2 (Step 076505): Train loss 0.942, Val loss 1.755\n",
      "Ep 2 (Step 076510): Train loss 0.994, Val loss 1.757\n",
      "Ep 2 (Step 076515): Train loss 1.176, Val loss 1.758\n",
      "Ep 2 (Step 076520): Train loss 1.224, Val loss 1.758\n",
      "Ep 2 (Step 076525): Train loss 0.948, Val loss 1.756\n",
      "Ep 2 (Step 076530): Train loss 1.191, Val loss 1.753\n",
      "Ep 2 (Step 076535): Train loss 0.956, Val loss 1.751\n",
      "Ep 2 (Step 076540): Train loss 1.397, Val loss 1.750\n",
      "Ep 2 (Step 076545): Train loss 1.230, Val loss 1.750\n",
      "Ep 2 (Step 076550): Train loss 0.910, Val loss 1.752\n",
      "Ep 2 (Step 076555): Train loss 0.954, Val loss 1.753\n",
      "Ep 2 (Step 076560): Train loss 1.011, Val loss 1.753\n",
      "Ep 2 (Step 076565): Train loss 1.162, Val loss 1.753\n",
      "Ep 2 (Step 076570): Train loss 0.869, Val loss 1.752\n",
      "Ep 2 (Step 076575): Train loss 0.960, Val loss 1.754\n",
      "Ep 2 (Step 076580): Train loss 0.934, Val loss 1.756\n",
      "Ep 2 (Step 076585): Train loss 0.993, Val loss 1.758\n",
      "Ep 2 (Step 076590): Train loss 1.126, Val loss 1.758\n",
      "Ep 2 (Step 076595): Train loss 1.014, Val loss 1.758\n",
      "Ep 2 (Step 076600): Train loss 1.023, Val loss 1.758\n",
      "Ep 2 (Step 076605): Train loss 0.848, Val loss 1.759\n",
      "Ep 2 (Step 076610): Train loss 1.170, Val loss 1.760\n",
      "Ep 2 (Step 076615): Train loss 0.866, Val loss 1.760\n",
      "Ep 2 (Step 076620): Train loss 0.831, Val loss 1.761\n",
      "Ep 2 (Step 076625): Train loss 1.019, Val loss 1.762\n",
      "Ep 2 (Step 076630): Train loss 0.856, Val loss 1.763\n",
      "Ep 2 (Step 076635): Train loss 1.064, Val loss 1.763\n",
      "Ep 2 (Step 076640): Train loss 0.827, Val loss 1.763\n",
      "Ep 2 (Step 076645): Train loss 0.982, Val loss 1.763\n",
      "Ep 2 (Step 076650): Train loss 1.021, Val loss 1.762\n",
      "Ep 2 (Step 076655): Train loss 1.080, Val loss 1.762\n",
      "Ep 2 (Step 076660): Train loss 1.012, Val loss 1.761\n",
      "Ep 2 (Step 076665): Train loss 1.046, Val loss 1.759\n",
      "Ep 2 (Step 076670): Train loss 1.295, Val loss 1.758\n",
      "Ep 2 (Step 076675): Train loss 0.920, Val loss 1.759\n",
      "Ep 2 (Step 076680): Train loss 0.918, Val loss 1.759\n",
      "Ep 2 (Step 076685): Train loss 0.973, Val loss 1.758\n",
      "Ep 2 (Step 076690): Train loss 1.004, Val loss 1.756\n",
      "Ep 2 (Step 076695): Train loss 1.056, Val loss 1.756\n",
      "Ep 2 (Step 076700): Train loss 0.837, Val loss 1.756\n",
      "Ep 2 (Step 076705): Train loss 0.873, Val loss 1.757\n",
      "Ep 2 (Step 076710): Train loss 1.185, Val loss 1.756\n",
      "Ep 2 (Step 076715): Train loss 0.916, Val loss 1.755\n",
      "Ep 2 (Step 076720): Train loss 1.087, Val loss 1.753\n",
      "Ep 2 (Step 076725): Train loss 1.070, Val loss 1.753\n",
      "Ep 2 (Step 076730): Train loss 1.156, Val loss 1.754\n",
      "Ep 2 (Step 076735): Train loss 1.341, Val loss 1.754\n",
      "Ep 2 (Step 076740): Train loss 0.815, Val loss 1.753\n",
      "Ep 2 (Step 076745): Train loss 0.893, Val loss 1.752\n",
      "Ep 2 (Step 076750): Train loss 1.114, Val loss 1.753\n",
      "Ep 2 (Step 076755): Train loss 1.138, Val loss 1.754\n",
      "Ep 2 (Step 076760): Train loss 0.914, Val loss 1.756\n",
      "Ep 2 (Step 076765): Train loss 0.960, Val loss 1.757\n",
      "Ep 2 (Step 076770): Train loss 0.976, Val loss 1.756\n",
      "Ep 2 (Step 076775): Train loss 1.116, Val loss 1.755\n",
      "Ep 2 (Step 076780): Train loss 1.070, Val loss 1.755\n",
      "Ep 2 (Step 076785): Train loss 1.202, Val loss 1.756\n",
      "Ep 2 (Step 076790): Train loss 0.923, Val loss 1.755\n",
      "Ep 2 (Step 076795): Train loss 1.050, Val loss 1.754\n",
      "Ep 2 (Step 076800): Train loss 1.068, Val loss 1.753\n",
      "Ep 2 (Step 076805): Train loss 0.997, Val loss 1.751\n",
      "Ep 2 (Step 076810): Train loss 1.165, Val loss 1.751\n",
      "Ep 2 (Step 076815): Train loss 0.881, Val loss 1.752\n",
      "Ep 2 (Step 076820): Train loss 1.106, Val loss 1.752\n",
      "Ep 2 (Step 076825): Train loss 0.987, Val loss 1.753\n",
      "Ep 2 (Step 076830): Train loss 1.086, Val loss 1.753\n",
      "Ep 2 (Step 076835): Train loss 1.117, Val loss 1.751\n",
      "Ep 2 (Step 076840): Train loss 1.149, Val loss 1.750\n",
      "Ep 2 (Step 076845): Train loss 1.145, Val loss 1.749\n",
      "Ep 2 (Step 076850): Train loss 1.128, Val loss 1.749\n",
      "Ep 2 (Step 076855): Train loss 1.187, Val loss 1.747\n",
      "Ep 2 (Step 076860): Train loss 0.861, Val loss 1.747\n",
      "Ep 2 (Step 076865): Train loss 1.067, Val loss 1.747\n",
      "Ep 2 (Step 076870): Train loss 1.176, Val loss 1.747\n",
      "Ep 2 (Step 076875): Train loss 1.025, Val loss 1.748\n",
      "Ep 2 (Step 076880): Train loss 1.216, Val loss 1.749\n",
      "Ep 2 (Step 076885): Train loss 0.910, Val loss 1.751\n",
      "Ep 2 (Step 076890): Train loss 0.878, Val loss 1.752\n",
      "Ep 2 (Step 076895): Train loss 1.017, Val loss 1.752\n",
      "Ep 2 (Step 076900): Train loss 1.010, Val loss 1.749\n",
      "Ep 2 (Step 076905): Train loss 0.831, Val loss 1.747\n",
      "Ep 2 (Step 076910): Train loss 1.118, Val loss 1.746\n",
      "Ep 2 (Step 076915): Train loss 1.007, Val loss 1.746\n",
      "Ep 2 (Step 076920): Train loss 1.178, Val loss 1.746\n",
      "Ep 2 (Step 076925): Train loss 1.469, Val loss 1.746\n",
      "Ep 2 (Step 076930): Train loss 1.023, Val loss 1.746\n",
      "Ep 2 (Step 076935): Train loss 0.946, Val loss 1.746\n",
      "Ep 2 (Step 076940): Train loss 1.119, Val loss 1.748\n",
      "Ep 2 (Step 076945): Train loss 0.997, Val loss 1.749\n",
      "Ep 2 (Step 076950): Train loss 1.136, Val loss 1.749\n",
      "Ep 2 (Step 076955): Train loss 1.154, Val loss 1.750\n",
      "Ep 2 (Step 076960): Train loss 0.794, Val loss 1.750\n",
      "Ep 2 (Step 076965): Train loss 1.003, Val loss 1.750\n",
      "Ep 2 (Step 076970): Train loss 0.783, Val loss 1.750\n",
      "Ep 2 (Step 076975): Train loss 1.175, Val loss 1.750\n",
      "Ep 2 (Step 076980): Train loss 1.253, Val loss 1.751\n",
      "Ep 2 (Step 076985): Train loss 1.057, Val loss 1.752\n",
      "Ep 2 (Step 076990): Train loss 0.992, Val loss 1.752\n",
      "Ep 2 (Step 076995): Train loss 1.150, Val loss 1.754\n",
      "Ep 2 (Step 077000): Train loss 0.964, Val loss 1.756\n",
      "Ep 2 (Step 077005): Train loss 0.808, Val loss 1.758\n",
      "Ep 2 (Step 077010): Train loss 0.900, Val loss 1.758\n",
      "Ep 2 (Step 077015): Train loss 0.906, Val loss 1.757\n",
      "Ep 2 (Step 077020): Train loss 0.952, Val loss 1.757\n",
      "Ep 2 (Step 077025): Train loss 1.311, Val loss 1.756\n",
      "Ep 2 (Step 077030): Train loss 1.010, Val loss 1.755\n",
      "Ep 2 (Step 077035): Train loss 0.703, Val loss 1.755\n",
      "Ep 2 (Step 077040): Train loss 0.939, Val loss 1.755\n",
      "Ep 2 (Step 077045): Train loss 0.898, Val loss 1.756\n",
      "Ep 2 (Step 077050): Train loss 0.841, Val loss 1.757\n",
      "Ep 2 (Step 077055): Train loss 1.018, Val loss 1.757\n",
      "Ep 2 (Step 077060): Train loss 0.847, Val loss 1.758\n",
      "Ep 2 (Step 077065): Train loss 0.973, Val loss 1.758\n",
      "Ep 2 (Step 077070): Train loss 1.049, Val loss 1.758\n",
      "Ep 2 (Step 077075): Train loss 1.090, Val loss 1.759\n",
      "Ep 2 (Step 077080): Train loss 0.817, Val loss 1.760\n",
      "Ep 2 (Step 077085): Train loss 1.217, Val loss 1.761\n",
      "Ep 2 (Step 077090): Train loss 0.709, Val loss 1.761\n",
      "Ep 2 (Step 077095): Train loss 1.250, Val loss 1.761\n",
      "Ep 2 (Step 077100): Train loss 1.094, Val loss 1.760\n",
      "Ep 2 (Step 077105): Train loss 1.192, Val loss 1.761\n",
      "Ep 2 (Step 077110): Train loss 1.003, Val loss 1.762\n",
      "Ep 2 (Step 077115): Train loss 1.025, Val loss 1.764\n",
      "Ep 2 (Step 077120): Train loss 1.061, Val loss 1.765\n",
      "Ep 2 (Step 077125): Train loss 0.941, Val loss 1.766\n",
      "Ep 2 (Step 077130): Train loss 1.241, Val loss 1.767\n",
      "Ep 2 (Step 077135): Train loss 0.992, Val loss 1.767\n",
      "Ep 2 (Step 077140): Train loss 0.842, Val loss 1.768\n",
      "Ep 2 (Step 077145): Train loss 0.925, Val loss 1.769\n",
      "Ep 2 (Step 077150): Train loss 1.122, Val loss 1.768\n",
      "Ep 2 (Step 077155): Train loss 0.959, Val loss 1.767\n",
      "Ep 2 (Step 077160): Train loss 1.002, Val loss 1.766\n",
      "Ep 2 (Step 077165): Train loss 0.910, Val loss 1.767\n",
      "Ep 2 (Step 077170): Train loss 0.969, Val loss 1.768\n",
      "Ep 2 (Step 077175): Train loss 1.197, Val loss 1.767\n",
      "Ep 2 (Step 077180): Train loss 1.010, Val loss 1.766\n",
      "Ep 2 (Step 077185): Train loss 1.097, Val loss 1.765\n",
      "Ep 2 (Step 077190): Train loss 0.988, Val loss 1.765\n",
      "Ep 2 (Step 077195): Train loss 1.006, Val loss 1.765\n",
      "Ep 2 (Step 077200): Train loss 1.016, Val loss 1.765\n",
      "Ep 2 (Step 077205): Train loss 1.118, Val loss 1.765\n",
      "Ep 2 (Step 077210): Train loss 0.946, Val loss 1.766\n",
      "Ep 2 (Step 077215): Train loss 0.722, Val loss 1.767\n",
      "Ep 2 (Step 077220): Train loss 0.993, Val loss 1.769\n",
      "Ep 2 (Step 077225): Train loss 0.839, Val loss 1.769\n",
      "Ep 2 (Step 077230): Train loss 1.038, Val loss 1.769\n",
      "Ep 2 (Step 077235): Train loss 0.839, Val loss 1.770\n",
      "Ep 2 (Step 077240): Train loss 1.137, Val loss 1.772\n",
      "Ep 2 (Step 077245): Train loss 1.003, Val loss 1.773\n",
      "Ep 2 (Step 077250): Train loss 1.036, Val loss 1.774\n",
      "Ep 2 (Step 077255): Train loss 0.999, Val loss 1.775\n",
      "Ep 2 (Step 077260): Train loss 1.159, Val loss 1.775\n",
      "Ep 2 (Step 077265): Train loss 0.914, Val loss 1.775\n",
      "Ep 2 (Step 077270): Train loss 1.289, Val loss 1.776\n",
      "Ep 2 (Step 077275): Train loss 1.006, Val loss 1.774\n",
      "Ep 2 (Step 077280): Train loss 0.972, Val loss 1.774\n",
      "Ep 2 (Step 077285): Train loss 1.102, Val loss 1.774\n",
      "Ep 2 (Step 077290): Train loss 1.031, Val loss 1.773\n",
      "Ep 2 (Step 077295): Train loss 1.149, Val loss 1.773\n",
      "Ep 2 (Step 077300): Train loss 1.112, Val loss 1.772\n",
      "Ep 2 (Step 077305): Train loss 0.993, Val loss 1.771\n",
      "Ep 2 (Step 077310): Train loss 0.748, Val loss 1.771\n",
      "Ep 2 (Step 077315): Train loss 1.061, Val loss 1.771\n",
      "Ep 2 (Step 077320): Train loss 0.814, Val loss 1.771\n",
      "Ep 2 (Step 077325): Train loss 0.925, Val loss 1.770\n",
      "Ep 2 (Step 077330): Train loss 0.706, Val loss 1.770\n",
      "Ep 2 (Step 077335): Train loss 0.846, Val loss 1.769\n",
      "Ep 2 (Step 077340): Train loss 0.760, Val loss 1.769\n",
      "Ep 2 (Step 077345): Train loss 0.994, Val loss 1.769\n",
      "Ep 2 (Step 077350): Train loss 1.097, Val loss 1.768\n",
      "Ep 2 (Step 077355): Train loss 1.139, Val loss 1.766\n",
      "Ep 2 (Step 077360): Train loss 1.166, Val loss 1.765\n",
      "Ep 2 (Step 077365): Train loss 0.974, Val loss 1.764\n",
      "Ep 2 (Step 077370): Train loss 1.090, Val loss 1.764\n",
      "Ep 2 (Step 077375): Train loss 0.928, Val loss 1.763\n",
      "Ep 2 (Step 077380): Train loss 1.040, Val loss 1.764\n",
      "Ep 2 (Step 077385): Train loss 0.985, Val loss 1.763\n",
      "Ep 2 (Step 077390): Train loss 0.994, Val loss 1.763\n",
      "Ep 2 (Step 077395): Train loss 0.880, Val loss 1.762\n",
      "Ep 2 (Step 077400): Train loss 0.970, Val loss 1.761\n",
      "Ep 2 (Step 077405): Train loss 0.876, Val loss 1.761\n",
      "Ep 2 (Step 077410): Train loss 0.948, Val loss 1.761\n",
      "Ep 2 (Step 077415): Train loss 0.989, Val loss 1.762\n",
      "Ep 2 (Step 077420): Train loss 0.897, Val loss 1.763\n",
      "Ep 2 (Step 077425): Train loss 0.855, Val loss 1.763\n",
      "Ep 2 (Step 077430): Train loss 1.176, Val loss 1.764\n",
      "Ep 2 (Step 077435): Train loss 0.948, Val loss 1.765\n",
      "Ep 2 (Step 077440): Train loss 0.997, Val loss 1.765\n",
      "Ep 2 (Step 077445): Train loss 0.976, Val loss 1.765\n",
      "Ep 2 (Step 077450): Train loss 1.014, Val loss 1.766\n",
      "Ep 2 (Step 077455): Train loss 0.724, Val loss 1.766\n",
      "Ep 2 (Step 077460): Train loss 1.103, Val loss 1.765\n",
      "Ep 2 (Step 077465): Train loss 0.722, Val loss 1.764\n",
      "Ep 2 (Step 077470): Train loss 1.197, Val loss 1.764\n",
      "Ep 2 (Step 077475): Train loss 1.022, Val loss 1.763\n",
      "Ep 2 (Step 077480): Train loss 1.141, Val loss 1.762\n",
      "Ep 2 (Step 077485): Train loss 0.912, Val loss 1.762\n",
      "Ep 2 (Step 077490): Train loss 0.930, Val loss 1.762\n",
      "Ep 2 (Step 077495): Train loss 0.998, Val loss 1.762\n",
      "Ep 2 (Step 077500): Train loss 0.953, Val loss 1.763\n",
      "Ep 2 (Step 077505): Train loss 1.113, Val loss 1.763\n",
      "Ep 2 (Step 077510): Train loss 0.960, Val loss 1.763\n",
      "Ep 2 (Step 077515): Train loss 0.921, Val loss 1.763\n",
      "Ep 2 (Step 077520): Train loss 0.790, Val loss 1.764\n",
      "Ep 2 (Step 077525): Train loss 0.964, Val loss 1.766\n",
      "Ep 2 (Step 077530): Train loss 0.940, Val loss 1.766\n",
      "Ep 2 (Step 077535): Train loss 0.865, Val loss 1.766\n",
      "Ep 2 (Step 077540): Train loss 1.188, Val loss 1.765\n",
      "Ep 2 (Step 077545): Train loss 1.218, Val loss 1.764\n",
      "Ep 2 (Step 077550): Train loss 1.005, Val loss 1.764\n",
      "Ep 2 (Step 077555): Train loss 1.057, Val loss 1.764\n",
      "Ep 2 (Step 077560): Train loss 1.209, Val loss 1.764\n",
      "Ep 2 (Step 077565): Train loss 0.985, Val loss 1.763\n",
      "Ep 2 (Step 077570): Train loss 1.313, Val loss 1.762\n",
      "Ep 2 (Step 077575): Train loss 1.067, Val loss 1.760\n",
      "Ep 2 (Step 077580): Train loss 1.041, Val loss 1.758\n",
      "Ep 2 (Step 077585): Train loss 1.094, Val loss 1.758\n",
      "Ep 2 (Step 077590): Train loss 1.078, Val loss 1.758\n",
      "Ep 2 (Step 077595): Train loss 1.147, Val loss 1.759\n",
      "Ep 2 (Step 077600): Train loss 1.088, Val loss 1.758\n",
      "Ep 2 (Step 077605): Train loss 1.040, Val loss 1.758\n",
      "Ep 2 (Step 077610): Train loss 0.783, Val loss 1.759\n",
      "Ep 2 (Step 077615): Train loss 1.108, Val loss 1.762\n",
      "Ep 2 (Step 077620): Train loss 0.963, Val loss 1.764\n",
      "Ep 2 (Step 077625): Train loss 0.923, Val loss 1.765\n",
      "Ep 2 (Step 077630): Train loss 1.141, Val loss 1.765\n",
      "Ep 2 (Step 077635): Train loss 0.881, Val loss 1.764\n",
      "Ep 2 (Step 077640): Train loss 0.816, Val loss 1.764\n",
      "Ep 2 (Step 077645): Train loss 1.084, Val loss 1.762\n",
      "Ep 2 (Step 077650): Train loss 0.927, Val loss 1.761\n",
      "Ep 2 (Step 077655): Train loss 0.849, Val loss 1.760\n",
      "Ep 2 (Step 077660): Train loss 0.845, Val loss 1.760\n",
      "Ep 2 (Step 077665): Train loss 1.057, Val loss 1.761\n",
      "Ep 2 (Step 077670): Train loss 1.110, Val loss 1.761\n",
      "Ep 2 (Step 077675): Train loss 1.243, Val loss 1.761\n",
      "Ep 2 (Step 077680): Train loss 1.085, Val loss 1.761\n",
      "Ep 2 (Step 077685): Train loss 1.020, Val loss 1.762\n",
      "Ep 2 (Step 077690): Train loss 0.969, Val loss 1.762\n",
      "Ep 2 (Step 077695): Train loss 0.906, Val loss 1.760\n",
      "Ep 2 (Step 077700): Train loss 0.906, Val loss 1.760\n",
      "Ep 2 (Step 077705): Train loss 1.045, Val loss 1.759\n",
      "Ep 2 (Step 077710): Train loss 1.038, Val loss 1.759\n",
      "Ep 2 (Step 077715): Train loss 1.123, Val loss 1.758\n",
      "Ep 2 (Step 077720): Train loss 0.857, Val loss 1.758\n",
      "Ep 2 (Step 077725): Train loss 0.997, Val loss 1.759\n",
      "Ep 2 (Step 077730): Train loss 1.011, Val loss 1.760\n",
      "Ep 2 (Step 077735): Train loss 1.137, Val loss 1.761\n",
      "Ep 2 (Step 077740): Train loss 1.103, Val loss 1.761\n",
      "Ep 2 (Step 077745): Train loss 1.034, Val loss 1.762\n",
      "Ep 2 (Step 077750): Train loss 1.293, Val loss 1.762\n",
      "Ep 2 (Step 077755): Train loss 0.907, Val loss 1.761\n",
      "Ep 2 (Step 077760): Train loss 0.840, Val loss 1.762\n",
      "Ep 2 (Step 077765): Train loss 1.030, Val loss 1.762\n",
      "Ep 2 (Step 077770): Train loss 1.009, Val loss 1.761\n",
      "Ep 2 (Step 077775): Train loss 1.014, Val loss 1.760\n",
      "Ep 2 (Step 077780): Train loss 0.881, Val loss 1.761\n",
      "Ep 2 (Step 077785): Train loss 1.068, Val loss 1.761\n",
      "Ep 2 (Step 077790): Train loss 1.137, Val loss 1.761\n",
      "Ep 2 (Step 077795): Train loss 1.065, Val loss 1.762\n",
      "Ep 2 (Step 077800): Train loss 0.967, Val loss 1.762\n",
      "Ep 2 (Step 077805): Train loss 0.883, Val loss 1.762\n",
      "Ep 2 (Step 077810): Train loss 0.941, Val loss 1.762\n",
      "Ep 2 (Step 077815): Train loss 1.053, Val loss 1.762\n",
      "Ep 2 (Step 077820): Train loss 1.120, Val loss 1.764\n",
      "Ep 2 (Step 077825): Train loss 0.863, Val loss 1.764\n",
      "Ep 2 (Step 077830): Train loss 0.994, Val loss 1.765\n",
      "Ep 2 (Step 077835): Train loss 0.895, Val loss 1.765\n",
      "Ep 2 (Step 077840): Train loss 0.873, Val loss 1.764\n",
      "Ep 2 (Step 077845): Train loss 0.965, Val loss 1.764\n",
      "Ep 2 (Step 077850): Train loss 1.156, Val loss 1.764\n",
      "Ep 2 (Step 077855): Train loss 1.301, Val loss 1.763\n",
      "Ep 2 (Step 077860): Train loss 0.959, Val loss 1.764\n",
      "Ep 2 (Step 077865): Train loss 0.945, Val loss 1.764\n",
      "Ep 2 (Step 077870): Train loss 0.955, Val loss 1.765\n",
      "Ep 2 (Step 077875): Train loss 1.030, Val loss 1.763\n",
      "Ep 2 (Step 077880): Train loss 0.845, Val loss 1.762\n",
      "Ep 2 (Step 077885): Train loss 0.914, Val loss 1.761\n",
      "Ep 2 (Step 077890): Train loss 1.021, Val loss 1.761\n",
      "Ep 2 (Step 077895): Train loss 0.760, Val loss 1.759\n",
      "Ep 2 (Step 077900): Train loss 0.893, Val loss 1.757\n",
      "Ep 2 (Step 077905): Train loss 1.036, Val loss 1.756\n",
      "Ep 2 (Step 077910): Train loss 1.158, Val loss 1.757\n",
      "Ep 2 (Step 077915): Train loss 0.967, Val loss 1.758\n",
      "Ep 2 (Step 077920): Train loss 1.314, Val loss 1.758\n",
      "Ep 2 (Step 077925): Train loss 1.068, Val loss 1.758\n",
      "Ep 2 (Step 077930): Train loss 1.210, Val loss 1.757\n",
      "Ep 2 (Step 077935): Train loss 0.806, Val loss 1.757\n",
      "Ep 2 (Step 077940): Train loss 1.127, Val loss 1.757\n",
      "Ep 2 (Step 077945): Train loss 1.153, Val loss 1.757\n",
      "Ep 2 (Step 077950): Train loss 1.023, Val loss 1.756\n",
      "Ep 2 (Step 077955): Train loss 0.858, Val loss 1.755\n",
      "Ep 2 (Step 077960): Train loss 0.992, Val loss 1.755\n",
      "Ep 2 (Step 077965): Train loss 1.199, Val loss 1.756\n",
      "Ep 2 (Step 077970): Train loss 1.534, Val loss 1.757\n",
      "Ep 2 (Step 077975): Train loss 1.122, Val loss 1.757\n",
      "Ep 2 (Step 077980): Train loss 0.934, Val loss 1.758\n",
      "Ep 2 (Step 077985): Train loss 1.188, Val loss 1.758\n",
      "Ep 2 (Step 077990): Train loss 1.192, Val loss 1.759\n",
      "Ep 2 (Step 077995): Train loss 1.035, Val loss 1.758\n",
      "Ep 2 (Step 078000): Train loss 0.962, Val loss 1.758\n",
      "Ep 2 (Step 078005): Train loss 0.725, Val loss 1.760\n",
      "Ep 2 (Step 078010): Train loss 0.831, Val loss 1.763\n",
      "Ep 2 (Step 078015): Train loss 0.918, Val loss 1.763\n",
      "Ep 2 (Step 078020): Train loss 1.084, Val loss 1.763\n",
      "Ep 2 (Step 078025): Train loss 0.967, Val loss 1.763\n",
      "Ep 2 (Step 078030): Train loss 1.039, Val loss 1.765\n",
      "Ep 2 (Step 078035): Train loss 1.066, Val loss 1.767\n",
      "Ep 2 (Step 078040): Train loss 0.977, Val loss 1.769\n",
      "Ep 2 (Step 078045): Train loss 1.105, Val loss 1.771\n",
      "Ep 2 (Step 078050): Train loss 0.969, Val loss 1.770\n",
      "Ep 2 (Step 078055): Train loss 1.009, Val loss 1.770\n",
      "Ep 2 (Step 078060): Train loss 1.034, Val loss 1.768\n",
      "Ep 2 (Step 078065): Train loss 0.973, Val loss 1.766\n",
      "Ep 2 (Step 078070): Train loss 0.845, Val loss 1.765\n",
      "Ep 2 (Step 078075): Train loss 1.186, Val loss 1.766\n",
      "Ep 2 (Step 078080): Train loss 1.255, Val loss 1.767\n",
      "Ep 2 (Step 078085): Train loss 0.778, Val loss 1.768\n",
      "Ep 2 (Step 078090): Train loss 1.124, Val loss 1.769\n",
      "Ep 2 (Step 078095): Train loss 0.786, Val loss 1.770\n",
      "Ep 2 (Step 078100): Train loss 1.068, Val loss 1.769\n",
      "Ep 2 (Step 078105): Train loss 1.093, Val loss 1.768\n",
      "Ep 2 (Step 078110): Train loss 0.970, Val loss 1.765\n",
      "Ep 2 (Step 078115): Train loss 1.070, Val loss 1.763\n",
      "Ep 2 (Step 078120): Train loss 0.897, Val loss 1.760\n",
      "Ep 2 (Step 078125): Train loss 0.924, Val loss 1.761\n",
      "Ep 2 (Step 078130): Train loss 1.132, Val loss 1.760\n",
      "Ep 2 (Step 078135): Train loss 1.073, Val loss 1.759\n",
      "Ep 2 (Step 078140): Train loss 0.969, Val loss 1.758\n",
      "Ep 2 (Step 078145): Train loss 0.947, Val loss 1.758\n",
      "Ep 2 (Step 078150): Train loss 0.903, Val loss 1.759\n",
      "Ep 2 (Step 078155): Train loss 1.022, Val loss 1.761\n",
      "Ep 2 (Step 078160): Train loss 1.055, Val loss 1.763\n",
      "Ep 2 (Step 078165): Train loss 1.179, Val loss 1.764\n",
      "Ep 2 (Step 078170): Train loss 1.035, Val loss 1.765\n",
      "Ep 2 (Step 078175): Train loss 1.021, Val loss 1.766\n",
      "Ep 2 (Step 078180): Train loss 1.087, Val loss 1.768\n",
      "Ep 2 (Step 078185): Train loss 1.133, Val loss 1.766\n",
      "Ep 2 (Step 078190): Train loss 1.024, Val loss 1.763\n",
      "Ep 2 (Step 078195): Train loss 0.789, Val loss 1.760\n",
      "Ep 2 (Step 078200): Train loss 0.949, Val loss 1.758\n",
      "Ep 2 (Step 078205): Train loss 0.938, Val loss 1.757\n",
      "Ep 2 (Step 078210): Train loss 1.294, Val loss 1.756\n",
      "Ep 2 (Step 078215): Train loss 1.332, Val loss 1.755\n",
      "Ep 2 (Step 078220): Train loss 0.995, Val loss 1.754\n",
      "Ep 2 (Step 078225): Train loss 1.058, Val loss 1.754\n",
      "Ep 2 (Step 078230): Train loss 0.870, Val loss 1.754\n",
      "Ep 2 (Step 078235): Train loss 1.287, Val loss 1.755\n",
      "Ep 2 (Step 078240): Train loss 0.797, Val loss 1.755\n",
      "Ep 2 (Step 078245): Train loss 1.269, Val loss 1.756\n",
      "Ep 2 (Step 078250): Train loss 1.009, Val loss 1.755\n",
      "Ep 2 (Step 078255): Train loss 1.382, Val loss 1.754\n",
      "Ep 2 (Step 078260): Train loss 0.784, Val loss 1.754\n",
      "Ep 2 (Step 078265): Train loss 0.911, Val loss 1.754\n",
      "Ep 2 (Step 078270): Train loss 0.976, Val loss 1.755\n",
      "Ep 2 (Step 078275): Train loss 0.924, Val loss 1.757\n",
      "Ep 2 (Step 078280): Train loss 0.802, Val loss 1.758\n",
      "Ep 2 (Step 078285): Train loss 1.226, Val loss 1.758\n",
      "Ep 2 (Step 078290): Train loss 1.162, Val loss 1.759\n",
      "Ep 2 (Step 078295): Train loss 0.926, Val loss 1.759\n",
      "Ep 2 (Step 078300): Train loss 0.971, Val loss 1.758\n",
      "Ep 2 (Step 078305): Train loss 1.044, Val loss 1.757\n",
      "Ep 2 (Step 078310): Train loss 0.997, Val loss 1.756\n",
      "Ep 2 (Step 078315): Train loss 1.000, Val loss 1.757\n",
      "Ep 2 (Step 078320): Train loss 0.974, Val loss 1.757\n",
      "Ep 2 (Step 078325): Train loss 1.084, Val loss 1.758\n",
      "Ep 2 (Step 078330): Train loss 1.002, Val loss 1.758\n",
      "Ep 2 (Step 078335): Train loss 1.013, Val loss 1.757\n",
      "Ep 2 (Step 078340): Train loss 1.060, Val loss 1.756\n",
      "Ep 2 (Step 078345): Train loss 1.092, Val loss 1.755\n",
      "Ep 2 (Step 078350): Train loss 0.741, Val loss 1.755\n",
      "Ep 2 (Step 078355): Train loss 1.187, Val loss 1.755\n",
      "Ep 2 (Step 078360): Train loss 1.021, Val loss 1.755\n",
      "Ep 2 (Step 078365): Train loss 1.088, Val loss 1.755\n",
      "Ep 2 (Step 078370): Train loss 0.942, Val loss 1.755\n",
      "Ep 2 (Step 078375): Train loss 0.891, Val loss 1.757\n",
      "Ep 2 (Step 078380): Train loss 1.027, Val loss 1.759\n",
      "Ep 2 (Step 078385): Train loss 0.744, Val loss 1.760\n",
      "Ep 2 (Step 078390): Train loss 1.351, Val loss 1.760\n",
      "Ep 2 (Step 078395): Train loss 1.137, Val loss 1.760\n",
      "Ep 2 (Step 078400): Train loss 1.128, Val loss 1.760\n",
      "Ep 2 (Step 078405): Train loss 0.883, Val loss 1.760\n",
      "Ep 2 (Step 078410): Train loss 1.099, Val loss 1.761\n",
      "Ep 2 (Step 078415): Train loss 1.010, Val loss 1.762\n",
      "Ep 2 (Step 078420): Train loss 1.077, Val loss 1.763\n",
      "Ep 2 (Step 078425): Train loss 1.080, Val loss 1.763\n",
      "Ep 2 (Step 078430): Train loss 1.011, Val loss 1.762\n",
      "Ep 2 (Step 078435): Train loss 0.972, Val loss 1.763\n",
      "Ep 2 (Step 078440): Train loss 1.133, Val loss 1.765\n",
      "Ep 2 (Step 078445): Train loss 0.939, Val loss 1.766\n",
      "Ep 2 (Step 078450): Train loss 1.054, Val loss 1.765\n",
      "Ep 2 (Step 078455): Train loss 0.765, Val loss 1.764\n",
      "Ep 2 (Step 078460): Train loss 1.031, Val loss 1.764\n",
      "Ep 2 (Step 078465): Train loss 0.995, Val loss 1.765\n",
      "Ep 2 (Step 078470): Train loss 0.998, Val loss 1.767\n",
      "Ep 2 (Step 078475): Train loss 0.958, Val loss 1.768\n",
      "Ep 2 (Step 078480): Train loss 0.969, Val loss 1.769\n",
      "Ep 2 (Step 078485): Train loss 0.911, Val loss 1.769\n",
      "Ep 2 (Step 078490): Train loss 1.036, Val loss 1.771\n",
      "Ep 2 (Step 078495): Train loss 0.977, Val loss 1.771\n",
      "Ep 2 (Step 078500): Train loss 0.752, Val loss 1.772\n",
      "Ep 2 (Step 078505): Train loss 1.139, Val loss 1.774\n",
      "Ep 2 (Step 078510): Train loss 1.089, Val loss 1.776\n",
      "Ep 2 (Step 078515): Train loss 1.043, Val loss 1.777\n",
      "Ep 2 (Step 078520): Train loss 1.005, Val loss 1.777\n",
      "Ep 2 (Step 078525): Train loss 0.774, Val loss 1.777\n",
      "Ep 2 (Step 078530): Train loss 0.874, Val loss 1.778\n",
      "Ep 2 (Step 078535): Train loss 1.001, Val loss 1.779\n",
      "Ep 2 (Step 078540): Train loss 0.964, Val loss 1.780\n",
      "Ep 2 (Step 078545): Train loss 0.951, Val loss 1.779\n",
      "Ep 2 (Step 078550): Train loss 1.138, Val loss 1.778\n",
      "Ep 2 (Step 078555): Train loss 1.056, Val loss 1.777\n",
      "Ep 2 (Step 078560): Train loss 1.246, Val loss 1.774\n",
      "Ep 2 (Step 078565): Train loss 1.327, Val loss 1.773\n",
      "Ep 2 (Step 078570): Train loss 1.051, Val loss 1.773\n",
      "Ep 2 (Step 078575): Train loss 0.904, Val loss 1.774\n",
      "Ep 2 (Step 078580): Train loss 0.827, Val loss 1.773\n",
      "Ep 2 (Step 078585): Train loss 1.125, Val loss 1.773\n",
      "Ep 2 (Step 078590): Train loss 0.851, Val loss 1.773\n",
      "Ep 2 (Step 078595): Train loss 0.853, Val loss 1.773\n",
      "Ep 2 (Step 078600): Train loss 0.908, Val loss 1.775\n",
      "Ep 2 (Step 078605): Train loss 1.249, Val loss 1.777\n",
      "Ep 2 (Step 078610): Train loss 1.199, Val loss 1.777\n",
      "Ep 2 (Step 078615): Train loss 1.014, Val loss 1.777\n",
      "Ep 2 (Step 078620): Train loss 0.930, Val loss 1.777\n",
      "Ep 2 (Step 078625): Train loss 0.988, Val loss 1.777\n",
      "Ep 2 (Step 078630): Train loss 0.934, Val loss 1.777\n",
      "Ep 2 (Step 078635): Train loss 1.185, Val loss 1.777\n",
      "Ep 2 (Step 078640): Train loss 0.999, Val loss 1.776\n",
      "Ep 2 (Step 078645): Train loss 0.836, Val loss 1.775\n",
      "Ep 2 (Step 078650): Train loss 0.973, Val loss 1.772\n",
      "Ep 2 (Step 078655): Train loss 0.804, Val loss 1.770\n",
      "Ep 2 (Step 078660): Train loss 0.890, Val loss 1.771\n",
      "Ep 2 (Step 078665): Train loss 1.022, Val loss 1.771\n",
      "Ep 2 (Step 078670): Train loss 1.076, Val loss 1.772\n",
      "Ep 2 (Step 078675): Train loss 1.066, Val loss 1.771\n",
      "Ep 2 (Step 078680): Train loss 0.895, Val loss 1.771\n",
      "Ep 2 (Step 078685): Train loss 0.977, Val loss 1.770\n",
      "Ep 2 (Step 078690): Train loss 1.011, Val loss 1.769\n",
      "Ep 2 (Step 078695): Train loss 1.003, Val loss 1.769\n",
      "Ep 2 (Step 078700): Train loss 0.933, Val loss 1.770\n",
      "Ep 2 (Step 078705): Train loss 1.114, Val loss 1.772\n",
      "Ep 2 (Step 078710): Train loss 1.053, Val loss 1.773\n",
      "Ep 2 (Step 078715): Train loss 1.278, Val loss 1.773\n",
      "Ep 2 (Step 078720): Train loss 0.927, Val loss 1.774\n",
      "Ep 2 (Step 078725): Train loss 1.021, Val loss 1.775\n",
      "Ep 2 (Step 078730): Train loss 1.150, Val loss 1.776\n",
      "Ep 2 (Step 078735): Train loss 0.995, Val loss 1.777\n",
      "Ep 2 (Step 078740): Train loss 1.274, Val loss 1.777\n",
      "Ep 2 (Step 078745): Train loss 0.900, Val loss 1.775\n",
      "Ep 2 (Step 078750): Train loss 1.163, Val loss 1.775\n",
      "Ep 2 (Step 078755): Train loss 0.905, Val loss 1.775\n",
      "Ep 2 (Step 078760): Train loss 0.834, Val loss 1.773\n",
      "Ep 2 (Step 078765): Train loss 0.849, Val loss 1.771\n",
      "Ep 2 (Step 078770): Train loss 0.885, Val loss 1.769\n",
      "Ep 2 (Step 078775): Train loss 0.844, Val loss 1.768\n",
      "Ep 2 (Step 078780): Train loss 1.135, Val loss 1.769\n",
      "Ep 2 (Step 078785): Train loss 0.936, Val loss 1.770\n",
      "Ep 2 (Step 078790): Train loss 1.048, Val loss 1.771\n",
      "Ep 2 (Step 078795): Train loss 1.103, Val loss 1.773\n",
      "Ep 2 (Step 078800): Train loss 0.924, Val loss 1.773\n",
      "Ep 2 (Step 078805): Train loss 1.108, Val loss 1.773\n",
      "Ep 2 (Step 078810): Train loss 0.984, Val loss 1.773\n",
      "Ep 2 (Step 078815): Train loss 1.281, Val loss 1.773\n",
      "Ep 2 (Step 078820): Train loss 0.974, Val loss 1.772\n",
      "Ep 2 (Step 078825): Train loss 0.847, Val loss 1.772\n",
      "Ep 2 (Step 078830): Train loss 0.979, Val loss 1.771\n",
      "Ep 2 (Step 078835): Train loss 0.993, Val loss 1.772\n",
      "Ep 2 (Step 078840): Train loss 1.034, Val loss 1.772\n",
      "Ep 2 (Step 078845): Train loss 1.066, Val loss 1.775\n",
      "Ep 2 (Step 078850): Train loss 1.214, Val loss 1.776\n",
      "Ep 2 (Step 078855): Train loss 0.846, Val loss 1.778\n",
      "Ep 2 (Step 078860): Train loss 0.976, Val loss 1.779\n",
      "Ep 2 (Step 078865): Train loss 0.819, Val loss 1.780\n",
      "Ep 2 (Step 078870): Train loss 1.355, Val loss 1.782\n",
      "Ep 2 (Step 078875): Train loss 0.957, Val loss 1.783\n",
      "Ep 2 (Step 078880): Train loss 0.903, Val loss 1.784\n",
      "Ep 2 (Step 078885): Train loss 0.978, Val loss 1.783\n",
      "Ep 2 (Step 078890): Train loss 1.107, Val loss 1.784\n",
      "Ep 2 (Step 078895): Train loss 1.051, Val loss 1.786\n",
      "Ep 2 (Step 078900): Train loss 1.034, Val loss 1.787\n",
      "Ep 2 (Step 078905): Train loss 0.738, Val loss 1.788\n",
      "Ep 2 (Step 078910): Train loss 1.129, Val loss 1.788\n",
      "Ep 2 (Step 078915): Train loss 0.983, Val loss 1.786\n",
      "Ep 2 (Step 078920): Train loss 0.959, Val loss 1.784\n",
      "Ep 2 (Step 078925): Train loss 0.975, Val loss 1.782\n",
      "Ep 2 (Step 078930): Train loss 1.105, Val loss 1.782\n",
      "Ep 2 (Step 078935): Train loss 0.745, Val loss 1.782\n",
      "Ep 2 (Step 078940): Train loss 1.084, Val loss 1.781\n",
      "Ep 2 (Step 078945): Train loss 0.992, Val loss 1.779\n",
      "Ep 2 (Step 078950): Train loss 0.816, Val loss 1.779\n",
      "Ep 2 (Step 078955): Train loss 0.785, Val loss 1.779\n",
      "Ep 2 (Step 078960): Train loss 0.982, Val loss 1.779\n",
      "Ep 2 (Step 078965): Train loss 1.291, Val loss 1.778\n",
      "Ep 2 (Step 078970): Train loss 1.156, Val loss 1.777\n",
      "Ep 2 (Step 078975): Train loss 1.153, Val loss 1.776\n",
      "Ep 2 (Step 078980): Train loss 0.953, Val loss 1.775\n",
      "Ep 2 (Step 078985): Train loss 1.124, Val loss 1.775\n",
      "Ep 2 (Step 078990): Train loss 1.319, Val loss 1.775\n",
      "Ep 2 (Step 078995): Train loss 0.942, Val loss 1.776\n",
      "Ep 2 (Step 079000): Train loss 0.851, Val loss 1.777\n",
      "Ep 2 (Step 079005): Train loss 1.079, Val loss 1.778\n",
      "Ep 2 (Step 079010): Train loss 1.031, Val loss 1.778\n",
      "Ep 2 (Step 079015): Train loss 1.194, Val loss 1.779\n",
      "Ep 2 (Step 079020): Train loss 1.132, Val loss 1.780\n",
      "Ep 2 (Step 079025): Train loss 1.124, Val loss 1.780\n",
      "Ep 2 (Step 079030): Train loss 1.062, Val loss 1.780\n",
      "Ep 2 (Step 079035): Train loss 1.283, Val loss 1.779\n",
      "Ep 2 (Step 079040): Train loss 1.081, Val loss 1.779\n",
      "Ep 2 (Step 079045): Train loss 0.960, Val loss 1.778\n",
      "Ep 2 (Step 079050): Train loss 1.046, Val loss 1.778\n",
      "Ep 2 (Step 079055): Train loss 1.142, Val loss 1.775\n",
      "Ep 2 (Step 079060): Train loss 0.803, Val loss 1.775\n",
      "Ep 2 (Step 079065): Train loss 0.658, Val loss 1.775\n",
      "Ep 2 (Step 079070): Train loss 1.179, Val loss 1.775\n",
      "Ep 2 (Step 079075): Train loss 0.854, Val loss 1.775\n",
      "Ep 2 (Step 079080): Train loss 0.990, Val loss 1.774\n",
      "Ep 2 (Step 079085): Train loss 1.058, Val loss 1.772\n",
      "Ep 2 (Step 079090): Train loss 0.859, Val loss 1.771\n",
      "Ep 2 (Step 079095): Train loss 1.058, Val loss 1.771\n",
      "Ep 2 (Step 079100): Train loss 0.908, Val loss 1.772\n",
      "Ep 2 (Step 079105): Train loss 1.196, Val loss 1.773\n",
      "Ep 2 (Step 079110): Train loss 0.963, Val loss 1.772\n",
      "Ep 2 (Step 079115): Train loss 0.793, Val loss 1.773\n",
      "Ep 2 (Step 079120): Train loss 0.900, Val loss 1.773\n",
      "Ep 2 (Step 079125): Train loss 0.946, Val loss 1.773\n",
      "Ep 2 (Step 079130): Train loss 0.933, Val loss 1.773\n",
      "Ep 2 (Step 079135): Train loss 1.209, Val loss 1.774\n",
      "Ep 2 (Step 079140): Train loss 1.276, Val loss 1.774\n",
      "Ep 2 (Step 079145): Train loss 1.005, Val loss 1.773\n",
      "Ep 2 (Step 079150): Train loss 1.059, Val loss 1.772\n",
      "Ep 2 (Step 079155): Train loss 1.058, Val loss 1.773\n",
      "Ep 2 (Step 079160): Train loss 1.052, Val loss 1.772\n",
      "Ep 2 (Step 079165): Train loss 0.940, Val loss 1.770\n",
      "Ep 2 (Step 079170): Train loss 0.903, Val loss 1.770\n",
      "Ep 2 (Step 079175): Train loss 0.976, Val loss 1.770\n",
      "Ep 2 (Step 079180): Train loss 0.981, Val loss 1.770\n",
      "Ep 2 (Step 079185): Train loss 0.946, Val loss 1.770\n",
      "Ep 2 (Step 079190): Train loss 1.164, Val loss 1.770\n",
      "Ep 2 (Step 079195): Train loss 1.165, Val loss 1.768\n",
      "Ep 2 (Step 079200): Train loss 1.278, Val loss 1.766\n",
      "Ep 2 (Step 079205): Train loss 0.743, Val loss 1.766\n",
      "Ep 2 (Step 079210): Train loss 1.168, Val loss 1.765\n",
      "Ep 2 (Step 079215): Train loss 0.687, Val loss 1.765\n",
      "Ep 2 (Step 079220): Train loss 0.746, Val loss 1.765\n",
      "Ep 2 (Step 079225): Train loss 0.864, Val loss 1.766\n",
      "Ep 2 (Step 079230): Train loss 1.002, Val loss 1.767\n",
      "Ep 2 (Step 079235): Train loss 0.840, Val loss 1.767\n",
      "Ep 2 (Step 079240): Train loss 1.071, Val loss 1.768\n",
      "Ep 2 (Step 079245): Train loss 1.047, Val loss 1.767\n",
      "Ep 2 (Step 079250): Train loss 0.947, Val loss 1.766\n",
      "Ep 2 (Step 079255): Train loss 1.123, Val loss 1.767\n",
      "Ep 2 (Step 079260): Train loss 1.010, Val loss 1.768\n",
      "Ep 2 (Step 079265): Train loss 1.147, Val loss 1.770\n",
      "Ep 2 (Step 079270): Train loss 1.326, Val loss 1.771\n",
      "Ep 2 (Step 079275): Train loss 0.929, Val loss 1.773\n",
      "Ep 2 (Step 079280): Train loss 1.149, Val loss 1.775\n",
      "Ep 2 (Step 079285): Train loss 1.119, Val loss 1.776\n",
      "Ep 2 (Step 079290): Train loss 1.050, Val loss 1.778\n",
      "Ep 2 (Step 079295): Train loss 1.009, Val loss 1.779\n",
      "Ep 2 (Step 079300): Train loss 0.915, Val loss 1.779\n",
      "Ep 2 (Step 079305): Train loss 0.870, Val loss 1.778\n",
      "Ep 2 (Step 079310): Train loss 1.306, Val loss 1.778\n",
      "Ep 2 (Step 079315): Train loss 1.026, Val loss 1.778\n",
      "Ep 2 (Step 079320): Train loss 0.765, Val loss 1.778\n",
      "Ep 2 (Step 079325): Train loss 0.830, Val loss 1.779\n",
      "Ep 2 (Step 079330): Train loss 0.951, Val loss 1.779\n",
      "Ep 2 (Step 079335): Train loss 0.871, Val loss 1.779\n",
      "Ep 2 (Step 079340): Train loss 0.923, Val loss 1.779\n",
      "Ep 2 (Step 079345): Train loss 0.937, Val loss 1.776\n",
      "Ep 2 (Step 079350): Train loss 0.769, Val loss 1.775\n",
      "Ep 2 (Step 079355): Train loss 1.016, Val loss 1.774\n",
      "Ep 2 (Step 079360): Train loss 1.044, Val loss 1.774\n",
      "Ep 2 (Step 079365): Train loss 1.155, Val loss 1.774\n",
      "Ep 2 (Step 079370): Train loss 0.962, Val loss 1.775\n",
      "Ep 2 (Step 079375): Train loss 1.050, Val loss 1.774\n",
      "Ep 2 (Step 079380): Train loss 1.061, Val loss 1.772\n",
      "Ep 2 (Step 079385): Train loss 1.113, Val loss 1.771\n",
      "Ep 2 (Step 079390): Train loss 1.419, Val loss 1.770\n",
      "Ep 2 (Step 079395): Train loss 0.913, Val loss 1.769\n",
      "Ep 2 (Step 079400): Train loss 1.123, Val loss 1.768\n",
      "Ep 2 (Step 079405): Train loss 1.266, Val loss 1.766\n",
      "Ep 2 (Step 079410): Train loss 0.760, Val loss 1.766\n",
      "Ep 2 (Step 079415): Train loss 1.144, Val loss 1.767\n",
      "Ep 2 (Step 079420): Train loss 0.995, Val loss 1.767\n",
      "Ep 2 (Step 079425): Train loss 0.970, Val loss 1.766\n",
      "Ep 2 (Step 079430): Train loss 0.873, Val loss 1.767\n",
      "Ep 2 (Step 079435): Train loss 0.854, Val loss 1.768\n",
      "Ep 2 (Step 079440): Train loss 1.122, Val loss 1.768\n",
      "Ep 2 (Step 079445): Train loss 1.082, Val loss 1.769\n",
      "Ep 2 (Step 079450): Train loss 0.950, Val loss 1.770\n",
      "Ep 2 (Step 079455): Train loss 0.913, Val loss 1.771\n",
      "Ep 2 (Step 079460): Train loss 1.120, Val loss 1.773\n",
      "Ep 2 (Step 079465): Train loss 0.861, Val loss 1.773\n",
      "Ep 2 (Step 079470): Train loss 1.245, Val loss 1.774\n",
      "Ep 2 (Step 079475): Train loss 1.361, Val loss 1.773\n",
      "Ep 2 (Step 079480): Train loss 0.774, Val loss 1.772\n",
      "Ep 2 (Step 079485): Train loss 1.049, Val loss 1.773\n",
      "Ep 2 (Step 079490): Train loss 0.853, Val loss 1.774\n",
      "Ep 2 (Step 079495): Train loss 0.919, Val loss 1.775\n",
      "Ep 2 (Step 079500): Train loss 1.302, Val loss 1.775\n",
      "Ep 2 (Step 079505): Train loss 1.197, Val loss 1.774\n",
      "Ep 2 (Step 079510): Train loss 0.955, Val loss 1.773\n",
      "Ep 2 (Step 079515): Train loss 0.870, Val loss 1.774\n",
      "Ep 2 (Step 079520): Train loss 1.078, Val loss 1.774\n",
      "Ep 2 (Step 079525): Train loss 0.952, Val loss 1.775\n",
      "Ep 2 (Step 079530): Train loss 0.939, Val loss 1.776\n",
      "Ep 2 (Step 079535): Train loss 1.093, Val loss 1.775\n",
      "Ep 2 (Step 079540): Train loss 0.809, Val loss 1.774\n",
      "Ep 2 (Step 079545): Train loss 0.966, Val loss 1.773\n",
      "Ep 2 (Step 079550): Train loss 1.091, Val loss 1.772\n",
      "Ep 2 (Step 079555): Train loss 1.130, Val loss 1.772\n",
      "Ep 2 (Step 079560): Train loss 1.150, Val loss 1.773\n",
      "Ep 2 (Step 079565): Train loss 1.072, Val loss 1.774\n",
      "Ep 2 (Step 079570): Train loss 1.124, Val loss 1.776\n",
      "Ep 2 (Step 079575): Train loss 1.109, Val loss 1.776\n",
      "Ep 2 (Step 079580): Train loss 0.808, Val loss 1.778\n",
      "Ep 2 (Step 079585): Train loss 1.218, Val loss 1.779\n",
      "Ep 2 (Step 079590): Train loss 1.048, Val loss 1.780\n",
      "Ep 2 (Step 079595): Train loss 0.911, Val loss 1.780\n",
      "Ep 2 (Step 079600): Train loss 0.732, Val loss 1.779\n",
      "Ep 2 (Step 079605): Train loss 0.889, Val loss 1.778\n",
      "Ep 2 (Step 079610): Train loss 1.099, Val loss 1.778\n",
      "Ep 2 (Step 079615): Train loss 0.769, Val loss 1.777\n",
      "Ep 2 (Step 079620): Train loss 0.991, Val loss 1.777\n",
      "Ep 2 (Step 079625): Train loss 1.111, Val loss 1.776\n",
      "Ep 2 (Step 079630): Train loss 1.183, Val loss 1.775\n",
      "Ep 2 (Step 079635): Train loss 1.106, Val loss 1.773\n",
      "Ep 2 (Step 079640): Train loss 1.108, Val loss 1.772\n",
      "Ep 2 (Step 079645): Train loss 1.082, Val loss 1.771\n",
      "Ep 2 (Step 079650): Train loss 0.788, Val loss 1.770\n",
      "Ep 2 (Step 079655): Train loss 0.955, Val loss 1.770\n",
      "Ep 2 (Step 079660): Train loss 1.310, Val loss 1.770\n",
      "Ep 2 (Step 079665): Train loss 1.105, Val loss 1.769\n",
      "Ep 2 (Step 079670): Train loss 1.110, Val loss 1.768\n",
      "Ep 2 (Step 079675): Train loss 1.077, Val loss 1.769\n",
      "Ep 2 (Step 079680): Train loss 0.969, Val loss 1.770\n",
      "Ep 2 (Step 079685): Train loss 1.348, Val loss 1.771\n",
      "Ep 2 (Step 079690): Train loss 0.953, Val loss 1.773\n",
      "Ep 2 (Step 079695): Train loss 1.076, Val loss 1.774\n",
      "Ep 2 (Step 079700): Train loss 1.039, Val loss 1.775\n",
      "Ep 2 (Step 079705): Train loss 0.993, Val loss 1.777\n",
      "Ep 2 (Step 079710): Train loss 0.754, Val loss 1.779\n",
      "Ep 2 (Step 079715): Train loss 0.990, Val loss 1.780\n",
      "Ep 2 (Step 079720): Train loss 1.156, Val loss 1.782\n",
      "Ep 2 (Step 079725): Train loss 1.051, Val loss 1.784\n",
      "Ep 2 (Step 079730): Train loss 0.716, Val loss 1.785\n",
      "Ep 2 (Step 079735): Train loss 1.118, Val loss 1.786\n",
      "Ep 2 (Step 079740): Train loss 1.423, Val loss 1.786\n",
      "Ep 2 (Step 079745): Train loss 0.933, Val loss 1.787\n",
      "Ep 2 (Step 079750): Train loss 0.966, Val loss 1.788\n",
      "Ep 2 (Step 079755): Train loss 0.953, Val loss 1.789\n",
      "Ep 2 (Step 079760): Train loss 1.085, Val loss 1.790\n",
      "Ep 2 (Step 079765): Train loss 1.195, Val loss 1.790\n",
      "Ep 2 (Step 079770): Train loss 1.050, Val loss 1.791\n",
      "Ep 2 (Step 079775): Train loss 0.918, Val loss 1.792\n",
      "Ep 2 (Step 079780): Train loss 1.054, Val loss 1.793\n",
      "Ep 2 (Step 079785): Train loss 1.100, Val loss 1.793\n",
      "Ep 2 (Step 079790): Train loss 1.151, Val loss 1.794\n",
      "Ep 2 (Step 079795): Train loss 1.080, Val loss 1.796\n",
      "Ep 2 (Step 079800): Train loss 0.826, Val loss 1.796\n",
      "Ep 2 (Step 079805): Train loss 1.187, Val loss 1.796\n",
      "Ep 2 (Step 079810): Train loss 0.865, Val loss 1.797\n",
      "Ep 2 (Step 079815): Train loss 1.001, Val loss 1.798\n",
      "Ep 2 (Step 079820): Train loss 1.165, Val loss 1.799\n",
      "Ep 2 (Step 079825): Train loss 0.982, Val loss 1.799\n",
      "Ep 2 (Step 079830): Train loss 1.031, Val loss 1.799\n",
      "Ep 2 (Step 079835): Train loss 0.953, Val loss 1.799\n",
      "Ep 2 (Step 079840): Train loss 0.732, Val loss 1.800\n",
      "Ep 2 (Step 079845): Train loss 1.052, Val loss 1.798\n",
      "Ep 2 (Step 079850): Train loss 0.904, Val loss 1.798\n",
      "Ep 2 (Step 079855): Train loss 0.671, Val loss 1.797\n",
      "Ep 2 (Step 079860): Train loss 1.257, Val loss 1.796\n",
      "Ep 2 (Step 079865): Train loss 0.933, Val loss 1.794\n",
      "Ep 2 (Step 079870): Train loss 0.937, Val loss 1.793\n",
      "Ep 2 (Step 079875): Train loss 1.125, Val loss 1.792\n",
      "Ep 2 (Step 079880): Train loss 1.044, Val loss 1.790\n",
      "Ep 2 (Step 079885): Train loss 0.994, Val loss 1.790\n",
      "Ep 2 (Step 079890): Train loss 1.044, Val loss 1.791\n",
      "Ep 2 (Step 079895): Train loss 1.150, Val loss 1.792\n",
      "Ep 2 (Step 079900): Train loss 0.821, Val loss 1.794\n",
      "Ep 2 (Step 079905): Train loss 1.297, Val loss 1.795\n",
      "Ep 2 (Step 079910): Train loss 0.893, Val loss 1.795\n",
      "Ep 2 (Step 079915): Train loss 1.126, Val loss 1.795\n",
      "Ep 2 (Step 079920): Train loss 1.032, Val loss 1.794\n",
      "Ep 2 (Step 079925): Train loss 0.893, Val loss 1.793\n",
      "Ep 2 (Step 079930): Train loss 0.752, Val loss 1.793\n",
      "Ep 2 (Step 079935): Train loss 1.107, Val loss 1.792\n",
      "Ep 2 (Step 079940): Train loss 1.182, Val loss 1.791\n",
      "Ep 2 (Step 079945): Train loss 1.113, Val loss 1.791\n",
      "Ep 2 (Step 079950): Train loss 0.945, Val loss 1.792\n",
      "Ep 2 (Step 079955): Train loss 1.397, Val loss 1.792\n",
      "Ep 2 (Step 079960): Train loss 1.134, Val loss 1.792\n",
      "Ep 2 (Step 079965): Train loss 1.058, Val loss 1.792\n",
      "Ep 2 (Step 079970): Train loss 0.834, Val loss 1.794\n",
      "Ep 2 (Step 079975): Train loss 1.075, Val loss 1.795\n",
      "Ep 2 (Step 079980): Train loss 1.007, Val loss 1.797\n",
      "Ep 2 (Step 079985): Train loss 1.174, Val loss 1.798\n",
      "Ep 2 (Step 079990): Train loss 0.996, Val loss 1.797\n",
      "Ep 2 (Step 079995): Train loss 0.914, Val loss 1.794\n",
      "Ep 2 (Step 080000): Train loss 1.030, Val loss 1.792\n",
      "Ep 2 (Step 080005): Train loss 1.022, Val loss 1.790\n",
      "Ep 2 (Step 080010): Train loss 0.963, Val loss 1.788\n",
      "Ep 2 (Step 080015): Train loss 1.046, Val loss 1.786\n",
      "Ep 2 (Step 080020): Train loss 0.988, Val loss 1.785\n",
      "Ep 2 (Step 080025): Train loss 1.252, Val loss 1.784\n",
      "Ep 2 (Step 080030): Train loss 0.692, Val loss 1.784\n",
      "Ep 2 (Step 080035): Train loss 1.091, Val loss 1.784\n",
      "Ep 2 (Step 080040): Train loss 0.934, Val loss 1.785\n",
      "Ep 2 (Step 080045): Train loss 1.108, Val loss 1.787\n",
      "Ep 2 (Step 080050): Train loss 1.331, Val loss 1.788\n",
      "Ep 2 (Step 080055): Train loss 0.790, Val loss 1.789\n",
      "Ep 2 (Step 080060): Train loss 1.031, Val loss 1.790\n",
      "Ep 2 (Step 080065): Train loss 1.186, Val loss 1.791\n",
      "Ep 2 (Step 080070): Train loss 1.066, Val loss 1.792\n",
      "Ep 2 (Step 080075): Train loss 1.108, Val loss 1.791\n",
      "Ep 2 (Step 080080): Train loss 0.831, Val loss 1.790\n",
      "Ep 2 (Step 080085): Train loss 0.965, Val loss 1.787\n",
      "Ep 2 (Step 080090): Train loss 1.089, Val loss 1.786\n",
      "Ep 2 (Step 080095): Train loss 1.179, Val loss 1.785\n",
      "Ep 2 (Step 080100): Train loss 1.288, Val loss 1.785\n",
      "Ep 2 (Step 080105): Train loss 1.016, Val loss 1.784\n",
      "Ep 2 (Step 080110): Train loss 0.969, Val loss 1.784\n",
      "Ep 2 (Step 080115): Train loss 0.991, Val loss 1.785\n",
      "Ep 2 (Step 080120): Train loss 0.805, Val loss 1.786\n",
      "Ep 2 (Step 080125): Train loss 1.080, Val loss 1.787\n",
      "Ep 2 (Step 080130): Train loss 1.063, Val loss 1.788\n",
      "Ep 2 (Step 080135): Train loss 0.952, Val loss 1.789\n",
      "Ep 2 (Step 080140): Train loss 1.046, Val loss 1.790\n",
      "Ep 2 (Step 080145): Train loss 0.959, Val loss 1.790\n",
      "Ep 2 (Step 080150): Train loss 1.108, Val loss 1.790\n",
      "Ep 2 (Step 080155): Train loss 0.818, Val loss 1.788\n",
      "Ep 2 (Step 080160): Train loss 0.889, Val loss 1.788\n",
      "Ep 2 (Step 080165): Train loss 1.020, Val loss 1.788\n",
      "Ep 2 (Step 080170): Train loss 0.867, Val loss 1.788\n",
      "Ep 2 (Step 080175): Train loss 0.956, Val loss 1.790\n",
      "Ep 2 (Step 080180): Train loss 1.060, Val loss 1.791\n",
      "Ep 2 (Step 080185): Train loss 0.868, Val loss 1.791\n",
      "Ep 2 (Step 080190): Train loss 1.019, Val loss 1.792\n",
      "Ep 2 (Step 080195): Train loss 0.949, Val loss 1.792\n",
      "Ep 2 (Step 080200): Train loss 0.765, Val loss 1.792\n",
      "Ep 2 (Step 080205): Train loss 0.811, Val loss 1.793\n",
      "Ep 2 (Step 080210): Train loss 1.107, Val loss 1.795\n",
      "Ep 2 (Step 080215): Train loss 0.971, Val loss 1.795\n",
      "Ep 2 (Step 080220): Train loss 1.067, Val loss 1.795\n",
      "Ep 2 (Step 080225): Train loss 1.108, Val loss 1.795\n",
      "Ep 2 (Step 080230): Train loss 1.019, Val loss 1.795\n",
      "Ep 2 (Step 080235): Train loss 1.025, Val loss 1.793\n",
      "Ep 2 (Step 080240): Train loss 1.080, Val loss 1.792\n",
      "Ep 2 (Step 080245): Train loss 1.268, Val loss 1.791\n",
      "Ep 2 (Step 080250): Train loss 0.893, Val loss 1.792\n",
      "Ep 2 (Step 080255): Train loss 1.027, Val loss 1.794\n",
      "Ep 2 (Step 080260): Train loss 0.951, Val loss 1.796\n",
      "Ep 2 (Step 080265): Train loss 0.899, Val loss 1.797\n",
      "Ep 2 (Step 080270): Train loss 0.953, Val loss 1.798\n",
      "Ep 2 (Step 080275): Train loss 1.093, Val loss 1.797\n",
      "Ep 2 (Step 080280): Train loss 0.930, Val loss 1.797\n",
      "Ep 2 (Step 080285): Train loss 0.972, Val loss 1.796\n",
      "Ep 2 (Step 080290): Train loss 1.019, Val loss 1.797\n",
      "Ep 2 (Step 080295): Train loss 0.921, Val loss 1.798\n",
      "Ep 2 (Step 080300): Train loss 0.950, Val loss 1.799\n",
      "Ep 2 (Step 080305): Train loss 0.899, Val loss 1.800\n",
      "Ep 2 (Step 080310): Train loss 1.088, Val loss 1.800\n",
      "Ep 2 (Step 080315): Train loss 0.782, Val loss 1.799\n",
      "Ep 2 (Step 080320): Train loss 0.849, Val loss 1.799\n",
      "Ep 2 (Step 080325): Train loss 1.002, Val loss 1.798\n",
      "Ep 2 (Step 080330): Train loss 1.105, Val loss 1.798\n",
      "Ep 2 (Step 080335): Train loss 0.996, Val loss 1.798\n",
      "Ep 2 (Step 080340): Train loss 0.872, Val loss 1.797\n",
      "Ep 2 (Step 080345): Train loss 0.990, Val loss 1.795\n",
      "Ep 2 (Step 080350): Train loss 0.812, Val loss 1.794\n",
      "Ep 2 (Step 080355): Train loss 0.873, Val loss 1.794\n",
      "Ep 2 (Step 080360): Train loss 1.013, Val loss 1.795\n",
      "Ep 2 (Step 080365): Train loss 1.091, Val loss 1.796\n",
      "Ep 2 (Step 080370): Train loss 0.856, Val loss 1.796\n",
      "Ep 2 (Step 080375): Train loss 1.256, Val loss 1.796\n",
      "Ep 2 (Step 080380): Train loss 1.159, Val loss 1.795\n",
      "Ep 2 (Step 080385): Train loss 1.075, Val loss 1.794\n",
      "Ep 2 (Step 080390): Train loss 0.996, Val loss 1.793\n",
      "Ep 2 (Step 080395): Train loss 1.040, Val loss 1.793\n",
      "Ep 2 (Step 080400): Train loss 1.153, Val loss 1.794\n",
      "Ep 2 (Step 080405): Train loss 1.138, Val loss 1.796\n",
      "Ep 2 (Step 080410): Train loss 0.803, Val loss 1.798\n",
      "Ep 2 (Step 080415): Train loss 0.862, Val loss 1.800\n",
      "Ep 2 (Step 080420): Train loss 1.198, Val loss 1.801\n",
      "Ep 2 (Step 080425): Train loss 0.775, Val loss 1.802\n",
      "Ep 2 (Step 080430): Train loss 0.692, Val loss 1.803\n",
      "Ep 2 (Step 080435): Train loss 1.176, Val loss 1.804\n",
      "Ep 2 (Step 080440): Train loss 1.070, Val loss 1.804\n",
      "Ep 2 (Step 080445): Train loss 1.551, Val loss 1.805\n",
      "Ep 2 (Step 080450): Train loss 1.276, Val loss 1.805\n",
      "Ep 2 (Step 080455): Train loss 0.841, Val loss 1.803\n",
      "Ep 2 (Step 080460): Train loss 0.932, Val loss 1.802\n",
      "Ep 2 (Step 080465): Train loss 1.086, Val loss 1.804\n",
      "Ep 2 (Step 080470): Train loss 1.208, Val loss 1.805\n",
      "Ep 2 (Step 080475): Train loss 1.297, Val loss 1.804\n",
      "Ep 2 (Step 080480): Train loss 0.929, Val loss 1.804\n",
      "Ep 2 (Step 080485): Train loss 0.925, Val loss 1.803\n",
      "Ep 2 (Step 080490): Train loss 0.990, Val loss 1.803\n",
      "Ep 2 (Step 080495): Train loss 0.798, Val loss 1.804\n",
      "Ep 2 (Step 080500): Train loss 1.078, Val loss 1.805\n",
      "Ep 2 (Step 080505): Train loss 1.105, Val loss 1.808\n",
      "Ep 2 (Step 080510): Train loss 1.116, Val loss 1.809\n",
      "Ep 2 (Step 080515): Train loss 0.900, Val loss 1.807\n",
      "Ep 2 (Step 080520): Train loss 0.958, Val loss 1.805\n",
      "Ep 2 (Step 080525): Train loss 0.979, Val loss 1.803\n",
      "Ep 2 (Step 080530): Train loss 0.818, Val loss 1.801\n",
      "Ep 2 (Step 080535): Train loss 0.932, Val loss 1.799\n",
      "Ep 2 (Step 080540): Train loss 0.969, Val loss 1.798\n",
      "Ep 2 (Step 080545): Train loss 1.169, Val loss 1.797\n",
      "Ep 2 (Step 080550): Train loss 0.853, Val loss 1.796\n",
      "Ep 2 (Step 080555): Train loss 1.098, Val loss 1.797\n",
      "Ep 2 (Step 080560): Train loss 0.886, Val loss 1.797\n",
      "Ep 2 (Step 080565): Train loss 1.111, Val loss 1.796\n",
      "Ep 2 (Step 080570): Train loss 1.139, Val loss 1.794\n",
      "Ep 2 (Step 080575): Train loss 0.962, Val loss 1.793\n",
      "Ep 2 (Step 080580): Train loss 1.273, Val loss 1.794\n",
      "Ep 2 (Step 080585): Train loss 0.901, Val loss 1.794\n",
      "Ep 2 (Step 080590): Train loss 0.956, Val loss 1.794\n",
      "Ep 2 (Step 080595): Train loss 0.875, Val loss 1.793\n",
      "Ep 2 (Step 080600): Train loss 1.011, Val loss 1.793\n",
      "Ep 2 (Step 080605): Train loss 0.757, Val loss 1.793\n",
      "Ep 2 (Step 080610): Train loss 1.152, Val loss 1.792\n",
      "Ep 2 (Step 080615): Train loss 0.958, Val loss 1.793\n",
      "Ep 2 (Step 080620): Train loss 1.117, Val loss 1.793\n",
      "Ep 2 (Step 080625): Train loss 1.150, Val loss 1.795\n",
      "Ep 2 (Step 080630): Train loss 1.041, Val loss 1.797\n",
      "Ep 2 (Step 080635): Train loss 1.145, Val loss 1.799\n",
      "Ep 2 (Step 080640): Train loss 0.987, Val loss 1.801\n",
      "Ep 2 (Step 080645): Train loss 1.158, Val loss 1.802\n",
      "Ep 2 (Step 080650): Train loss 0.730, Val loss 1.804\n",
      "Ep 2 (Step 080655): Train loss 0.990, Val loss 1.805\n",
      "Ep 2 (Step 080660): Train loss 0.879, Val loss 1.805\n",
      "Ep 2 (Step 080665): Train loss 1.155, Val loss 1.802\n",
      "Ep 2 (Step 080670): Train loss 1.180, Val loss 1.800\n",
      "Ep 2 (Step 080675): Train loss 1.086, Val loss 1.799\n",
      "Ep 2 (Step 080680): Train loss 0.815, Val loss 1.799\n",
      "Ep 2 (Step 080685): Train loss 1.181, Val loss 1.799\n",
      "Ep 2 (Step 080690): Train loss 1.191, Val loss 1.800\n",
      "Ep 2 (Step 080695): Train loss 0.986, Val loss 1.800\n",
      "Ep 2 (Step 080700): Train loss 1.032, Val loss 1.798\n",
      "Ep 2 (Step 080705): Train loss 1.259, Val loss 1.797\n",
      "Ep 2 (Step 080710): Train loss 0.923, Val loss 1.795\n",
      "Ep 2 (Step 080715): Train loss 0.987, Val loss 1.794\n",
      "Ep 2 (Step 080720): Train loss 1.110, Val loss 1.793\n",
      "Ep 2 (Step 080725): Train loss 0.887, Val loss 1.792\n",
      "Ep 2 (Step 080730): Train loss 1.233, Val loss 1.791\n",
      "Ep 2 (Step 080735): Train loss 1.248, Val loss 1.789\n",
      "Ep 2 (Step 080740): Train loss 1.026, Val loss 1.789\n",
      "Ep 2 (Step 080745): Train loss 0.997, Val loss 1.789\n",
      "Ep 2 (Step 080750): Train loss 1.081, Val loss 1.790\n",
      "Ep 2 (Step 080755): Train loss 0.966, Val loss 1.792\n",
      "Ep 2 (Step 080760): Train loss 0.873, Val loss 1.794\n",
      "Ep 2 (Step 080765): Train loss 1.062, Val loss 1.797\n",
      "Ep 2 (Step 080770): Train loss 0.874, Val loss 1.798\n",
      "Ep 2 (Step 080775): Train loss 0.953, Val loss 1.800\n",
      "Ep 2 (Step 080780): Train loss 0.786, Val loss 1.801\n",
      "Ep 2 (Step 080785): Train loss 1.077, Val loss 1.802\n",
      "Ep 2 (Step 080790): Train loss 1.125, Val loss 1.804\n",
      "Ep 2 (Step 080795): Train loss 1.213, Val loss 1.806\n",
      "Ep 2 (Step 080800): Train loss 0.973, Val loss 1.808\n",
      "Ep 2 (Step 080805): Train loss 1.283, Val loss 1.809\n",
      "Ep 2 (Step 080810): Train loss 0.818, Val loss 1.810\n",
      "Ep 2 (Step 080815): Train loss 0.738, Val loss 1.808\n",
      "Ep 2 (Step 080820): Train loss 1.143, Val loss 1.807\n",
      "Ep 2 (Step 080825): Train loss 1.056, Val loss 1.805\n",
      "Ep 2 (Step 080830): Train loss 1.039, Val loss 1.804\n",
      "Ep 2 (Step 080835): Train loss 1.135, Val loss 1.804\n",
      "Ep 2 (Step 080840): Train loss 1.119, Val loss 1.803\n",
      "Ep 2 (Step 080845): Train loss 0.784, Val loss 1.802\n",
      "Ep 2 (Step 080850): Train loss 0.944, Val loss 1.800\n",
      "Ep 2 (Step 080855): Train loss 0.806, Val loss 1.798\n",
      "Ep 2 (Step 080860): Train loss 0.907, Val loss 1.797\n",
      "Ep 2 (Step 080865): Train loss 0.922, Val loss 1.796\n",
      "Ep 2 (Step 080870): Train loss 0.797, Val loss 1.795\n",
      "Ep 2 (Step 080875): Train loss 0.896, Val loss 1.794\n",
      "Ep 2 (Step 080880): Train loss 0.979, Val loss 1.795\n",
      "Ep 2 (Step 080885): Train loss 1.007, Val loss 1.794\n",
      "Ep 2 (Step 080890): Train loss 1.202, Val loss 1.794\n",
      "Ep 2 (Step 080895): Train loss 1.347, Val loss 1.795\n",
      "Ep 2 (Step 080900): Train loss 0.921, Val loss 1.795\n",
      "Ep 2 (Step 080905): Train loss 0.939, Val loss 1.795\n",
      "Ep 2 (Step 080910): Train loss 1.139, Val loss 1.796\n",
      "Ep 2 (Step 080915): Train loss 1.229, Val loss 1.797\n",
      "Ep 2 (Step 080920): Train loss 0.816, Val loss 1.797\n",
      "Ep 2 (Step 080925): Train loss 1.153, Val loss 1.797\n",
      "Ep 2 (Step 080930): Train loss 1.053, Val loss 1.798\n",
      "Ep 2 (Step 080935): Train loss 0.932, Val loss 1.798\n",
      "Ep 2 (Step 080940): Train loss 0.897, Val loss 1.798\n",
      "Ep 2 (Step 080945): Train loss 1.033, Val loss 1.798\n",
      "Ep 2 (Step 080950): Train loss 1.004, Val loss 1.796\n",
      "Ep 2 (Step 080955): Train loss 0.922, Val loss 1.795\n",
      "Ep 2 (Step 080960): Train loss 1.286, Val loss 1.795\n",
      "Ep 2 (Step 080965): Train loss 0.906, Val loss 1.796\n",
      "Ep 2 (Step 080970): Train loss 0.997, Val loss 1.798\n",
      "Ep 2 (Step 080975): Train loss 1.185, Val loss 1.799\n",
      "Ep 2 (Step 080980): Train loss 0.775, Val loss 1.800\n",
      "Ep 2 (Step 080985): Train loss 0.864, Val loss 1.799\n",
      "Ep 2 (Step 080990): Train loss 1.054, Val loss 1.798\n",
      "Ep 2 (Step 080995): Train loss 1.015, Val loss 1.798\n",
      "Ep 2 (Step 081000): Train loss 0.877, Val loss 1.797\n",
      "Ep 2 (Step 081005): Train loss 0.774, Val loss 1.796\n",
      "Ep 2 (Step 081010): Train loss 0.628, Val loss 1.795\n",
      "Ep 2 (Step 081015): Train loss 1.012, Val loss 1.794\n",
      "Ep 2 (Step 081020): Train loss 1.173, Val loss 1.794\n",
      "Ep 2 (Step 081025): Train loss 1.114, Val loss 1.792\n",
      "Ep 2 (Step 081030): Train loss 0.869, Val loss 1.791\n",
      "Ep 2 (Step 081035): Train loss 1.230, Val loss 1.791\n",
      "Ep 2 (Step 081040): Train loss 1.066, Val loss 1.788\n",
      "Ep 2 (Step 081045): Train loss 0.887, Val loss 1.786\n",
      "Ep 2 (Step 081050): Train loss 0.968, Val loss 1.786\n",
      "Ep 2 (Step 081055): Train loss 1.122, Val loss 1.785\n",
      "Ep 2 (Step 081060): Train loss 0.713, Val loss 1.784\n",
      "Ep 2 (Step 081065): Train loss 1.048, Val loss 1.784\n",
      "Ep 2 (Step 081070): Train loss 0.982, Val loss 1.785\n",
      "Ep 2 (Step 081075): Train loss 1.009, Val loss 1.785\n",
      "Ep 2 (Step 081080): Train loss 0.876, Val loss 1.785\n",
      "Ep 2 (Step 081085): Train loss 1.055, Val loss 1.785\n",
      "Ep 2 (Step 081090): Train loss 0.860, Val loss 1.785\n",
      "Ep 2 (Step 081095): Train loss 0.818, Val loss 1.785\n",
      "Ep 2 (Step 081100): Train loss 0.993, Val loss 1.786\n",
      "Ep 2 (Step 081105): Train loss 1.017, Val loss 1.785\n",
      "Ep 2 (Step 081110): Train loss 1.031, Val loss 1.785\n",
      "Ep 2 (Step 081115): Train loss 0.962, Val loss 1.785\n",
      "Ep 2 (Step 081120): Train loss 1.046, Val loss 1.784\n",
      "Ep 2 (Step 081125): Train loss 1.125, Val loss 1.784\n",
      "Ep 2 (Step 081130): Train loss 0.795, Val loss 1.783\n",
      "Ep 2 (Step 081135): Train loss 0.932, Val loss 1.783\n",
      "Ep 2 (Step 081140): Train loss 1.050, Val loss 1.785\n",
      "Ep 2 (Step 081145): Train loss 1.099, Val loss 1.786\n",
      "Ep 2 (Step 081150): Train loss 1.178, Val loss 1.787\n",
      "Ep 2 (Step 081155): Train loss 0.982, Val loss 1.787\n",
      "Ep 2 (Step 081160): Train loss 0.917, Val loss 1.787\n",
      "Ep 2 (Step 081165): Train loss 0.746, Val loss 1.787\n",
      "Ep 2 (Step 081170): Train loss 0.900, Val loss 1.786\n",
      "Ep 2 (Step 081175): Train loss 1.068, Val loss 1.784\n",
      "Ep 2 (Step 081180): Train loss 1.046, Val loss 1.784\n",
      "Ep 2 (Step 081185): Train loss 0.882, Val loss 1.784\n",
      "Ep 2 (Step 081190): Train loss 1.156, Val loss 1.785\n",
      "Ep 2 (Step 081195): Train loss 0.847, Val loss 1.786\n",
      "Ep 2 (Step 081200): Train loss 0.932, Val loss 1.787\n",
      "Ep 2 (Step 081205): Train loss 0.955, Val loss 1.787\n",
      "Ep 2 (Step 081210): Train loss 1.011, Val loss 1.785\n",
      "Ep 2 (Step 081215): Train loss 0.944, Val loss 1.784\n",
      "Ep 2 (Step 081220): Train loss 1.341, Val loss 1.782\n",
      "Ep 2 (Step 081225): Train loss 0.771, Val loss 1.782\n",
      "Ep 2 (Step 081230): Train loss 1.088, Val loss 1.781\n",
      "Ep 2 (Step 081235): Train loss 1.152, Val loss 1.781\n",
      "Ep 2 (Step 081240): Train loss 1.185, Val loss 1.782\n",
      "Ep 2 (Step 081245): Train loss 0.971, Val loss 1.781\n",
      "Ep 2 (Step 081250): Train loss 0.855, Val loss 1.781\n",
      "Ep 2 (Step 081255): Train loss 0.966, Val loss 1.782\n",
      "Ep 2 (Step 081260): Train loss 1.106, Val loss 1.782\n",
      "Ep 2 (Step 081265): Train loss 1.017, Val loss 1.783\n",
      "Ep 2 (Step 081270): Train loss 1.310, Val loss 1.782\n",
      "Ep 2 (Step 081275): Train loss 1.115, Val loss 1.781\n",
      "Ep 2 (Step 081280): Train loss 1.006, Val loss 1.781\n",
      "Ep 2 (Step 081285): Train loss 1.184, Val loss 1.781\n",
      "Ep 2 (Step 081290): Train loss 1.085, Val loss 1.780\n",
      "Ep 2 (Step 081295): Train loss 1.038, Val loss 1.778\n",
      "Ep 2 (Step 081300): Train loss 1.058, Val loss 1.776\n",
      "Ep 2 (Step 081305): Train loss 1.059, Val loss 1.776\n",
      "Ep 2 (Step 081310): Train loss 1.120, Val loss 1.777\n",
      "Ep 2 (Step 081315): Train loss 0.935, Val loss 1.778\n",
      "Ep 2 (Step 081320): Train loss 1.109, Val loss 1.779\n",
      "Ep 2 (Step 081325): Train loss 1.069, Val loss 1.780\n",
      "Ep 2 (Step 081330): Train loss 0.919, Val loss 1.781\n",
      "Ep 2 (Step 081335): Train loss 1.184, Val loss 1.782\n",
      "Ep 2 (Step 081340): Train loss 1.098, Val loss 1.782\n",
      "Ep 2 (Step 081345): Train loss 1.074, Val loss 1.781\n",
      "Ep 2 (Step 081350): Train loss 1.113, Val loss 1.782\n",
      "Ep 2 (Step 081355): Train loss 1.423, Val loss 1.783\n",
      "Ep 2 (Step 081360): Train loss 0.913, Val loss 1.784\n",
      "Ep 2 (Step 081365): Train loss 0.929, Val loss 1.784\n",
      "Ep 2 (Step 081370): Train loss 1.165, Val loss 1.783\n",
      "Ep 2 (Step 081375): Train loss 0.920, Val loss 1.784\n",
      "Ep 2 (Step 081380): Train loss 1.211, Val loss 1.783\n",
      "Ep 2 (Step 081385): Train loss 1.115, Val loss 1.783\n",
      "Ep 2 (Step 081390): Train loss 1.160, Val loss 1.783\n",
      "Ep 2 (Step 081395): Train loss 1.217, Val loss 1.784\n",
      "Ep 2 (Step 081400): Train loss 0.872, Val loss 1.783\n",
      "Ep 2 (Step 081405): Train loss 1.113, Val loss 1.783\n",
      "Ep 2 (Step 081410): Train loss 1.058, Val loss 1.783\n",
      "Ep 2 (Step 081415): Train loss 1.160, Val loss 1.784\n",
      "Ep 2 (Step 081420): Train loss 0.901, Val loss 1.784\n",
      "Ep 2 (Step 081425): Train loss 1.016, Val loss 1.785\n",
      "Ep 2 (Step 081430): Train loss 0.969, Val loss 1.783\n",
      "Ep 2 (Step 081435): Train loss 1.214, Val loss 1.782\n",
      "Ep 2 (Step 081440): Train loss 0.937, Val loss 1.780\n",
      "Ep 2 (Step 081445): Train loss 0.976, Val loss 1.778\n",
      "Ep 2 (Step 081450): Train loss 0.986, Val loss 1.777\n",
      "Ep 2 (Step 081455): Train loss 1.055, Val loss 1.775\n",
      "Ep 2 (Step 081460): Train loss 1.111, Val loss 1.775\n",
      "Ep 2 (Step 081465): Train loss 1.091, Val loss 1.775\n",
      "Ep 2 (Step 081470): Train loss 1.063, Val loss 1.774\n",
      "Ep 2 (Step 081475): Train loss 0.917, Val loss 1.775\n",
      "Ep 2 (Step 081480): Train loss 1.166, Val loss 1.775\n",
      "Ep 2 (Step 081485): Train loss 1.089, Val loss 1.776\n",
      "Ep 2 (Step 081490): Train loss 1.150, Val loss 1.776\n",
      "Ep 2 (Step 081495): Train loss 1.122, Val loss 1.776\n",
      "Ep 2 (Step 081500): Train loss 1.072, Val loss 1.776\n",
      "Ep 2 (Step 081505): Train loss 1.132, Val loss 1.777\n",
      "Ep 2 (Step 081510): Train loss 1.087, Val loss 1.778\n",
      "Ep 2 (Step 081515): Train loss 0.661, Val loss 1.779\n",
      "Ep 2 (Step 081520): Train loss 0.896, Val loss 1.781\n",
      "Ep 2 (Step 081525): Train loss 1.162, Val loss 1.784\n",
      "Ep 2 (Step 081530): Train loss 0.931, Val loss 1.785\n",
      "Ep 2 (Step 081535): Train loss 1.096, Val loss 1.787\n",
      "Ep 2 (Step 081540): Train loss 1.250, Val loss 1.787\n",
      "Ep 2 (Step 081545): Train loss 0.972, Val loss 1.788\n",
      "Ep 2 (Step 081550): Train loss 0.997, Val loss 1.790\n",
      "Ep 2 (Step 081555): Train loss 1.033, Val loss 1.792\n",
      "Ep 2 (Step 081560): Train loss 0.835, Val loss 1.792\n",
      "Ep 2 (Step 081565): Train loss 0.844, Val loss 1.792\n",
      "Ep 2 (Step 081570): Train loss 0.863, Val loss 1.793\n",
      "Ep 2 (Step 081575): Train loss 0.996, Val loss 1.794\n",
      "Ep 2 (Step 081580): Train loss 0.964, Val loss 1.795\n",
      "Ep 2 (Step 081585): Train loss 0.888, Val loss 1.796\n",
      "Ep 2 (Step 081590): Train loss 0.943, Val loss 1.796\n",
      "Ep 2 (Step 081595): Train loss 0.962, Val loss 1.795\n",
      "Ep 2 (Step 081600): Train loss 1.036, Val loss 1.796\n",
      "Ep 2 (Step 081605): Train loss 0.790, Val loss 1.796\n",
      "Ep 2 (Step 081610): Train loss 0.917, Val loss 1.795\n",
      "Ep 2 (Step 081615): Train loss 1.312, Val loss 1.796\n",
      "Ep 2 (Step 081620): Train loss 0.970, Val loss 1.796\n",
      "Ep 2 (Step 081625): Train loss 0.706, Val loss 1.797\n",
      "Ep 2 (Step 081630): Train loss 1.002, Val loss 1.796\n",
      "Ep 2 (Step 081635): Train loss 1.096, Val loss 1.796\n",
      "Ep 2 (Step 081640): Train loss 0.846, Val loss 1.795\n",
      "Ep 2 (Step 081645): Train loss 0.720, Val loss 1.794\n",
      "Ep 2 (Step 081650): Train loss 1.010, Val loss 1.793\n",
      "Ep 2 (Step 081655): Train loss 1.121, Val loss 1.793\n",
      "Ep 2 (Step 081660): Train loss 0.890, Val loss 1.792\n",
      "Ep 2 (Step 081665): Train loss 0.994, Val loss 1.791\n",
      "Ep 2 (Step 081670): Train loss 1.101, Val loss 1.790\n",
      "Ep 2 (Step 081675): Train loss 1.080, Val loss 1.790\n",
      "Ep 2 (Step 081680): Train loss 1.150, Val loss 1.790\n",
      "Ep 2 (Step 081685): Train loss 0.970, Val loss 1.789\n",
      "Ep 2 (Step 081690): Train loss 1.021, Val loss 1.789\n",
      "Ep 2 (Step 081695): Train loss 1.141, Val loss 1.789\n",
      "Ep 2 (Step 081700): Train loss 1.194, Val loss 1.789\n",
      "Ep 2 (Step 081705): Train loss 0.994, Val loss 1.789\n",
      "Ep 2 (Step 081710): Train loss 1.154, Val loss 1.788\n",
      "Ep 2 (Step 081715): Train loss 0.899, Val loss 1.788\n",
      "Ep 2 (Step 081720): Train loss 0.923, Val loss 1.786\n",
      "Ep 2 (Step 081725): Train loss 1.029, Val loss 1.785\n",
      "Ep 2 (Step 081730): Train loss 1.008, Val loss 1.785\n",
      "Ep 2 (Step 081735): Train loss 1.038, Val loss 1.785\n",
      "Ep 2 (Step 081740): Train loss 0.961, Val loss 1.786\n",
      "Ep 2 (Step 081745): Train loss 0.971, Val loss 1.787\n",
      "Ep 2 (Step 081750): Train loss 1.075, Val loss 1.787\n",
      "Ep 2 (Step 081755): Train loss 1.330, Val loss 1.787\n",
      "Ep 2 (Step 081760): Train loss 1.082, Val loss 1.787\n",
      "Ep 2 (Step 081765): Train loss 0.814, Val loss 1.790\n",
      "Ep 2 (Step 081770): Train loss 0.949, Val loss 1.791\n",
      "Ep 2 (Step 081775): Train loss 1.092, Val loss 1.790\n",
      "Ep 2 (Step 081780): Train loss 1.042, Val loss 1.788\n",
      "Ep 2 (Step 081785): Train loss 1.062, Val loss 1.787\n",
      "Ep 2 (Step 081790): Train loss 1.041, Val loss 1.785\n",
      "Ep 2 (Step 081795): Train loss 0.929, Val loss 1.783\n",
      "Ep 2 (Step 081800): Train loss 0.635, Val loss 1.783\n",
      "Ep 2 (Step 081805): Train loss 0.861, Val loss 1.783\n",
      "Ep 2 (Step 081810): Train loss 1.038, Val loss 1.783\n",
      "Ep 2 (Step 081815): Train loss 0.956, Val loss 1.783\n",
      "Ep 2 (Step 081820): Train loss 1.078, Val loss 1.785\n",
      "Ep 2 (Step 081825): Train loss 0.874, Val loss 1.785\n",
      "Ep 2 (Step 081830): Train loss 1.254, Val loss 1.786\n",
      "Ep 2 (Step 081835): Train loss 0.912, Val loss 1.787\n",
      "Ep 2 (Step 081840): Train loss 0.804, Val loss 1.789\n",
      "Ep 2 (Step 081845): Train loss 0.799, Val loss 1.790\n",
      "Ep 2 (Step 081850): Train loss 1.102, Val loss 1.792\n",
      "Ep 2 (Step 081855): Train loss 0.829, Val loss 1.793\n",
      "Ep 2 (Step 081860): Train loss 1.088, Val loss 1.791\n",
      "Ep 2 (Step 081865): Train loss 0.889, Val loss 1.790\n",
      "Ep 2 (Step 081870): Train loss 0.826, Val loss 1.791\n",
      "Ep 2 (Step 081875): Train loss 1.130, Val loss 1.791\n",
      "Ep 2 (Step 081880): Train loss 1.137, Val loss 1.792\n",
      "Ep 2 (Step 081885): Train loss 1.149, Val loss 1.794\n",
      "Ep 2 (Step 081890): Train loss 0.847, Val loss 1.796\n",
      "Ep 2 (Step 081895): Train loss 1.109, Val loss 1.796\n",
      "Ep 2 (Step 081900): Train loss 0.988, Val loss 1.795\n",
      "Ep 2 (Step 081905): Train loss 0.995, Val loss 1.795\n",
      "Ep 2 (Step 081910): Train loss 0.911, Val loss 1.795\n",
      "Ep 2 (Step 081915): Train loss 1.091, Val loss 1.796\n",
      "Ep 2 (Step 081920): Train loss 1.087, Val loss 1.796\n",
      "Ep 2 (Step 081925): Train loss 1.024, Val loss 1.798\n",
      "Ep 2 (Step 081930): Train loss 1.034, Val loss 1.798\n",
      "Ep 2 (Step 081935): Train loss 0.786, Val loss 1.796\n",
      "Ep 2 (Step 081940): Train loss 1.129, Val loss 1.795\n",
      "Ep 2 (Step 081945): Train loss 1.134, Val loss 1.793\n",
      "Ep 2 (Step 081950): Train loss 0.904, Val loss 1.792\n",
      "Ep 2 (Step 081955): Train loss 1.177, Val loss 1.792\n",
      "Ep 2 (Step 081960): Train loss 1.071, Val loss 1.792\n",
      "Ep 2 (Step 081965): Train loss 1.081, Val loss 1.792\n",
      "Ep 2 (Step 081970): Train loss 1.063, Val loss 1.791\n",
      "Ep 2 (Step 081975): Train loss 0.973, Val loss 1.791\n",
      "Ep 2 (Step 081980): Train loss 1.088, Val loss 1.790\n",
      "Ep 2 (Step 081985): Train loss 1.183, Val loss 1.789\n",
      "Ep 2 (Step 081990): Train loss 0.863, Val loss 1.789\n",
      "Ep 2 (Step 081995): Train loss 0.870, Val loss 1.789\n",
      "Ep 2 (Step 082000): Train loss 1.164, Val loss 1.788\n",
      "Ep 2 (Step 082005): Train loss 1.217, Val loss 1.788\n",
      "Ep 2 (Step 082010): Train loss 0.951, Val loss 1.789\n",
      "Ep 2 (Step 082015): Train loss 0.705, Val loss 1.787\n",
      "Ep 2 (Step 082020): Train loss 0.997, Val loss 1.785\n",
      "Ep 2 (Step 082025): Train loss 1.164, Val loss 1.784\n",
      "Ep 2 (Step 082030): Train loss 1.023, Val loss 1.785\n",
      "Ep 2 (Step 082035): Train loss 0.914, Val loss 1.785\n",
      "Ep 2 (Step 082040): Train loss 0.861, Val loss 1.786\n",
      "Ep 2 (Step 082045): Train loss 1.010, Val loss 1.785\n",
      "Ep 2 (Step 082050): Train loss 0.848, Val loss 1.785\n",
      "Ep 2 (Step 082055): Train loss 1.133, Val loss 1.784\n",
      "Ep 2 (Step 082060): Train loss 0.910, Val loss 1.784\n",
      "Ep 2 (Step 082065): Train loss 0.841, Val loss 1.784\n",
      "Ep 2 (Step 082070): Train loss 1.089, Val loss 1.784\n",
      "Ep 2 (Step 082075): Train loss 0.890, Val loss 1.784\n",
      "Ep 2 (Step 082080): Train loss 1.140, Val loss 1.784\n",
      "Ep 2 (Step 082085): Train loss 0.850, Val loss 1.783\n",
      "Ep 2 (Step 082090): Train loss 0.809, Val loss 1.783\n",
      "Ep 2 (Step 082095): Train loss 1.004, Val loss 1.782\n",
      "Ep 2 (Step 082100): Train loss 1.038, Val loss 1.782\n",
      "Ep 2 (Step 082105): Train loss 1.051, Val loss 1.780\n",
      "Ep 2 (Step 082110): Train loss 1.286, Val loss 1.780\n",
      "Ep 2 (Step 082115): Train loss 0.711, Val loss 1.779\n",
      "Ep 2 (Step 082120): Train loss 0.797, Val loss 1.779\n",
      "Ep 2 (Step 082125): Train loss 1.317, Val loss 1.780\n",
      "Ep 2 (Step 082130): Train loss 1.157, Val loss 1.782\n",
      "Ep 2 (Step 082135): Train loss 0.906, Val loss 1.783\n",
      "Ep 2 (Step 082140): Train loss 1.130, Val loss 1.784\n",
      "Ep 2 (Step 082145): Train loss 0.814, Val loss 1.786\n",
      "Ep 2 (Step 082150): Train loss 0.715, Val loss 1.788\n",
      "Ep 2 (Step 082155): Train loss 0.944, Val loss 1.790\n",
      "Ep 2 (Step 082160): Train loss 0.745, Val loss 1.792\n",
      "Ep 2 (Step 082165): Train loss 0.998, Val loss 1.793\n",
      "Ep 2 (Step 082170): Train loss 1.055, Val loss 1.792\n",
      "Ep 2 (Step 082175): Train loss 1.033, Val loss 1.792\n",
      "Ep 2 (Step 082180): Train loss 0.938, Val loss 1.790\n",
      "Ep 2 (Step 082185): Train loss 1.007, Val loss 1.789\n",
      "Ep 2 (Step 082190): Train loss 0.933, Val loss 1.788\n",
      "Ep 2 (Step 082195): Train loss 0.844, Val loss 1.788\n",
      "Ep 2 (Step 082200): Train loss 1.043, Val loss 1.788\n",
      "Ep 2 (Step 082205): Train loss 1.114, Val loss 1.786\n",
      "Ep 2 (Step 082210): Train loss 1.038, Val loss 1.785\n",
      "Ep 2 (Step 082215): Train loss 0.905, Val loss 1.786\n",
      "Ep 2 (Step 082220): Train loss 1.291, Val loss 1.786\n",
      "Ep 2 (Step 082225): Train loss 0.639, Val loss 1.786\n",
      "Ep 2 (Step 082230): Train loss 0.859, Val loss 1.786\n",
      "Ep 2 (Step 082235): Train loss 1.178, Val loss 1.786\n",
      "Ep 2 (Step 082240): Train loss 1.325, Val loss 1.785\n",
      "Ep 2 (Step 082245): Train loss 1.137, Val loss 1.784\n",
      "Ep 2 (Step 082250): Train loss 1.082, Val loss 1.784\n",
      "Ep 2 (Step 082255): Train loss 1.121, Val loss 1.783\n",
      "Ep 2 (Step 082260): Train loss 1.081, Val loss 1.782\n",
      "Ep 2 (Step 082265): Train loss 1.136, Val loss 1.781\n",
      "Ep 2 (Step 082270): Train loss 0.988, Val loss 1.782\n",
      "Ep 2 (Step 082275): Train loss 1.153, Val loss 1.782\n",
      "Ep 2 (Step 082280): Train loss 0.945, Val loss 1.784\n",
      "Ep 2 (Step 082285): Train loss 1.118, Val loss 1.785\n",
      "Ep 2 (Step 082290): Train loss 1.272, Val loss 1.788\n",
      "Ep 2 (Step 082295): Train loss 1.043, Val loss 1.790\n",
      "Ep 2 (Step 082300): Train loss 0.922, Val loss 1.790\n",
      "Ep 2 (Step 082305): Train loss 0.755, Val loss 1.790\n",
      "Ep 2 (Step 082310): Train loss 0.943, Val loss 1.790\n",
      "Ep 2 (Step 082315): Train loss 0.914, Val loss 1.789\n",
      "Ep 2 (Step 082320): Train loss 0.834, Val loss 1.789\n",
      "Ep 2 (Step 082325): Train loss 1.134, Val loss 1.787\n",
      "Ep 2 (Step 082330): Train loss 0.999, Val loss 1.784\n",
      "Ep 2 (Step 082335): Train loss 1.010, Val loss 1.784\n",
      "Ep 2 (Step 082340): Train loss 0.955, Val loss 1.783\n",
      "Ep 2 (Step 082345): Train loss 1.037, Val loss 1.782\n",
      "Ep 2 (Step 082350): Train loss 0.977, Val loss 1.781\n",
      "Ep 2 (Step 082355): Train loss 0.877, Val loss 1.781\n",
      "Ep 2 (Step 082360): Train loss 0.883, Val loss 1.780\n",
      "Ep 2 (Step 082365): Train loss 0.933, Val loss 1.780\n",
      "Ep 2 (Step 082370): Train loss 1.049, Val loss 1.781\n",
      "Ep 2 (Step 082375): Train loss 0.901, Val loss 1.783\n",
      "Ep 2 (Step 082380): Train loss 0.879, Val loss 1.785\n",
      "Ep 2 (Step 082385): Train loss 1.137, Val loss 1.786\n",
      "Ep 2 (Step 082390): Train loss 0.907, Val loss 1.786\n",
      "Ep 2 (Step 082395): Train loss 1.124, Val loss 1.787\n",
      "Ep 2 (Step 082400): Train loss 0.879, Val loss 1.787\n",
      "Ep 2 (Step 082405): Train loss 1.174, Val loss 1.786\n",
      "Ep 2 (Step 082410): Train loss 0.906, Val loss 1.786\n",
      "Ep 2 (Step 082415): Train loss 0.863, Val loss 1.784\n",
      "Ep 2 (Step 082420): Train loss 0.844, Val loss 1.783\n",
      "Ep 2 (Step 082425): Train loss 1.426, Val loss 1.781\n",
      "Ep 2 (Step 082430): Train loss 1.215, Val loss 1.780\n",
      "Ep 2 (Step 082435): Train loss 0.870, Val loss 1.778\n",
      "Ep 2 (Step 082440): Train loss 1.105, Val loss 1.778\n",
      "Ep 2 (Step 082445): Train loss 1.078, Val loss 1.777\n",
      "Ep 2 (Step 082450): Train loss 0.865, Val loss 1.775\n",
      "Ep 2 (Step 082455): Train loss 1.113, Val loss 1.776\n",
      "Ep 2 (Step 082460): Train loss 1.041, Val loss 1.775\n",
      "Ep 2 (Step 082465): Train loss 0.759, Val loss 1.776\n",
      "Ep 2 (Step 082470): Train loss 1.007, Val loss 1.777\n",
      "Ep 2 (Step 082475): Train loss 1.044, Val loss 1.778\n",
      "Ep 2 (Step 082480): Train loss 0.929, Val loss 1.779\n",
      "Ep 2 (Step 082485): Train loss 0.972, Val loss 1.780\n",
      "Ep 2 (Step 082490): Train loss 0.858, Val loss 1.779\n",
      "Ep 2 (Step 082495): Train loss 1.008, Val loss 1.779\n",
      "Ep 2 (Step 082500): Train loss 0.891, Val loss 1.779\n",
      "Ep 2 (Step 082505): Train loss 1.090, Val loss 1.779\n",
      "Ep 2 (Step 082510): Train loss 1.063, Val loss 1.781\n",
      "Ep 2 (Step 082515): Train loss 0.891, Val loss 1.782\n",
      "Ep 2 (Step 082520): Train loss 0.909, Val loss 1.784\n",
      "Ep 2 (Step 082525): Train loss 1.214, Val loss 1.784\n",
      "Ep 2 (Step 082530): Train loss 1.075, Val loss 1.784\n",
      "Ep 2 (Step 082535): Train loss 1.177, Val loss 1.784\n",
      "Ep 2 (Step 082540): Train loss 0.879, Val loss 1.784\n",
      "Ep 2 (Step 082545): Train loss 1.125, Val loss 1.785\n",
      "Ep 2 (Step 082550): Train loss 0.912, Val loss 1.785\n",
      "Ep 2 (Step 082555): Train loss 1.159, Val loss 1.785\n",
      "Ep 2 (Step 082560): Train loss 0.769, Val loss 1.785\n",
      "Ep 2 (Step 082565): Train loss 0.856, Val loss 1.785\n",
      "Ep 2 (Step 082570): Train loss 1.090, Val loss 1.785\n",
      "Ep 2 (Step 082575): Train loss 0.882, Val loss 1.785\n",
      "Ep 2 (Step 082580): Train loss 1.320, Val loss 1.784\n",
      "Ep 2 (Step 082585): Train loss 0.896, Val loss 1.783\n",
      "Ep 2 (Step 082590): Train loss 0.974, Val loss 1.782\n",
      "Ep 2 (Step 082595): Train loss 0.885, Val loss 1.782\n",
      "Ep 2 (Step 082600): Train loss 1.176, Val loss 1.782\n",
      "Ep 2 (Step 082605): Train loss 1.176, Val loss 1.783\n",
      "Ep 2 (Step 082610): Train loss 0.849, Val loss 1.783\n",
      "Ep 2 (Step 082615): Train loss 0.825, Val loss 1.784\n",
      "Ep 2 (Step 082620): Train loss 0.826, Val loss 1.784\n",
      "Ep 2 (Step 082625): Train loss 1.153, Val loss 1.784\n",
      "Ep 2 (Step 082630): Train loss 0.734, Val loss 1.783\n",
      "Ep 2 (Step 082635): Train loss 1.018, Val loss 1.782\n",
      "Ep 2 (Step 082640): Train loss 1.193, Val loss 1.781\n",
      "Ep 2 (Step 082645): Train loss 0.724, Val loss 1.779\n",
      "Ep 2 (Step 082650): Train loss 0.967, Val loss 1.777\n",
      "Ep 2 (Step 082655): Train loss 1.103, Val loss 1.777\n",
      "Ep 2 (Step 082660): Train loss 0.866, Val loss 1.777\n",
      "Ep 2 (Step 082665): Train loss 0.809, Val loss 1.777\n",
      "Ep 2 (Step 082670): Train loss 1.093, Val loss 1.776\n",
      "Ep 2 (Step 082675): Train loss 0.980, Val loss 1.776\n",
      "Ep 2 (Step 082680): Train loss 1.183, Val loss 1.775\n",
      "Ep 2 (Step 082685): Train loss 1.448, Val loss 1.775\n",
      "Ep 2 (Step 082690): Train loss 0.842, Val loss 1.775\n",
      "Ep 2 (Step 082695): Train loss 0.963, Val loss 1.774\n",
      "Ep 2 (Step 082700): Train loss 0.808, Val loss 1.773\n",
      "Ep 2 (Step 082705): Train loss 1.202, Val loss 1.774\n",
      "Ep 2 (Step 082710): Train loss 1.087, Val loss 1.775\n",
      "Ep 2 (Step 082715): Train loss 0.973, Val loss 1.776\n",
      "Ep 2 (Step 082720): Train loss 1.107, Val loss 1.777\n",
      "Ep 2 (Step 082725): Train loss 0.995, Val loss 1.779\n",
      "Ep 2 (Step 082730): Train loss 1.097, Val loss 1.781\n",
      "Ep 2 (Step 082735): Train loss 1.027, Val loss 1.782\n",
      "Ep 2 (Step 082740): Train loss 1.009, Val loss 1.783\n",
      "Ep 2 (Step 082745): Train loss 0.902, Val loss 1.783\n",
      "Ep 2 (Step 082750): Train loss 1.383, Val loss 1.784\n",
      "Ep 2 (Step 082755): Train loss 1.126, Val loss 1.784\n",
      "Ep 2 (Step 082760): Train loss 1.006, Val loss 1.784\n",
      "Ep 2 (Step 082765): Train loss 0.816, Val loss 1.786\n",
      "Ep 2 (Step 082770): Train loss 0.961, Val loss 1.786\n",
      "Ep 2 (Step 082775): Train loss 0.718, Val loss 1.786\n",
      "Ep 2 (Step 082780): Train loss 0.985, Val loss 1.785\n",
      "Ep 2 (Step 082785): Train loss 1.013, Val loss 1.784\n",
      "Ep 2 (Step 082790): Train loss 1.084, Val loss 1.784\n",
      "Ep 2 (Step 082795): Train loss 1.061, Val loss 1.782\n",
      "Ep 2 (Step 082800): Train loss 0.967, Val loss 1.780\n",
      "Ep 2 (Step 082805): Train loss 1.231, Val loss 1.778\n",
      "Ep 2 (Step 082810): Train loss 1.059, Val loss 1.776\n",
      "Ep 2 (Step 082815): Train loss 1.109, Val loss 1.775\n",
      "Ep 2 (Step 082820): Train loss 1.150, Val loss 1.775\n",
      "Ep 2 (Step 082825): Train loss 1.116, Val loss 1.776\n",
      "Ep 2 (Step 082830): Train loss 1.113, Val loss 1.778\n",
      "Ep 2 (Step 082835): Train loss 0.883, Val loss 1.779\n",
      "Ep 2 (Step 082840): Train loss 1.092, Val loss 1.779\n",
      "Ep 2 (Step 082845): Train loss 1.133, Val loss 1.779\n",
      "Ep 2 (Step 082850): Train loss 1.025, Val loss 1.781\n",
      "Ep 2 (Step 082855): Train loss 0.884, Val loss 1.783\n",
      "Ep 2 (Step 082860): Train loss 1.035, Val loss 1.784\n",
      "Ep 2 (Step 082865): Train loss 0.800, Val loss 1.785\n",
      "Ep 2 (Step 082870): Train loss 0.680, Val loss 1.785\n",
      "Ep 2 (Step 082875): Train loss 1.092, Val loss 1.785\n",
      "Ep 2 (Step 082880): Train loss 1.039, Val loss 1.786\n",
      "Ep 2 (Step 082885): Train loss 1.154, Val loss 1.786\n",
      "Ep 2 (Step 082890): Train loss 1.126, Val loss 1.785\n",
      "Ep 2 (Step 082895): Train loss 0.747, Val loss 1.785\n",
      "Ep 2 (Step 082900): Train loss 1.104, Val loss 1.785\n",
      "Ep 2 (Step 082905): Train loss 1.025, Val loss 1.784\n",
      "Ep 2 (Step 082910): Train loss 1.020, Val loss 1.785\n",
      "Ep 2 (Step 082915): Train loss 1.424, Val loss 1.785\n",
      "Ep 2 (Step 082920): Train loss 0.929, Val loss 1.784\n",
      "Ep 2 (Step 082925): Train loss 0.744, Val loss 1.783\n",
      "Ep 2 (Step 082930): Train loss 1.101, Val loss 1.783\n",
      "Ep 2 (Step 082935): Train loss 1.067, Val loss 1.784\n",
      "Ep 2 (Step 082940): Train loss 0.947, Val loss 1.783\n",
      "Ep 2 (Step 082945): Train loss 0.777, Val loss 1.782\n",
      "Ep 2 (Step 082950): Train loss 1.003, Val loss 1.780\n",
      "Ep 2 (Step 082955): Train loss 0.900, Val loss 1.779\n",
      "Ep 2 (Step 082960): Train loss 1.104, Val loss 1.779\n",
      "Ep 2 (Step 082965): Train loss 1.114, Val loss 1.779\n",
      "Ep 2 (Step 082970): Train loss 0.899, Val loss 1.777\n",
      "Ep 2 (Step 082975): Train loss 1.037, Val loss 1.777\n",
      "Ep 2 (Step 082980): Train loss 0.907, Val loss 1.778\n",
      "Ep 2 (Step 082985): Train loss 0.926, Val loss 1.780\n",
      "Ep 2 (Step 082990): Train loss 1.019, Val loss 1.782\n",
      "Ep 2 (Step 082995): Train loss 1.048, Val loss 1.783\n",
      "Ep 2 (Step 083000): Train loss 1.121, Val loss 1.784\n",
      "Ep 2 (Step 083005): Train loss 0.984, Val loss 1.784\n",
      "Ep 2 (Step 083010): Train loss 1.069, Val loss 1.783\n",
      "Ep 2 (Step 083015): Train loss 0.908, Val loss 1.783\n",
      "Ep 2 (Step 083020): Train loss 0.705, Val loss 1.781\n",
      "Ep 2 (Step 083025): Train loss 1.148, Val loss 1.781\n",
      "Ep 2 (Step 083030): Train loss 0.840, Val loss 1.781\n",
      "Ep 2 (Step 083035): Train loss 1.096, Val loss 1.783\n",
      "Ep 2 (Step 083040): Train loss 1.088, Val loss 1.785\n",
      "Ep 2 (Step 083045): Train loss 1.048, Val loss 1.787\n",
      "Ep 2 (Step 083050): Train loss 1.167, Val loss 1.788\n",
      "Ep 2 (Step 083055): Train loss 1.011, Val loss 1.788\n",
      "Ep 2 (Step 083060): Train loss 0.894, Val loss 1.789\n",
      "Ep 2 (Step 083065): Train loss 0.989, Val loss 1.789\n",
      "Ep 2 (Step 083070): Train loss 1.143, Val loss 1.786\n",
      "Ep 2 (Step 083075): Train loss 0.998, Val loss 1.784\n",
      "Ep 2 (Step 083080): Train loss 0.903, Val loss 1.781\n",
      "Ep 2 (Step 083085): Train loss 0.796, Val loss 1.780\n",
      "Ep 2 (Step 083090): Train loss 0.980, Val loss 1.780\n",
      "Ep 2 (Step 083095): Train loss 1.104, Val loss 1.779\n",
      "Ep 2 (Step 083100): Train loss 0.875, Val loss 1.777\n",
      "Ep 2 (Step 083105): Train loss 1.066, Val loss 1.775\n",
      "Ep 2 (Step 083110): Train loss 1.112, Val loss 1.774\n",
      "Ep 2 (Step 083115): Train loss 1.066, Val loss 1.773\n",
      "Ep 2 (Step 083120): Train loss 1.201, Val loss 1.772\n",
      "Ep 2 (Step 083125): Train loss 1.110, Val loss 1.771\n",
      "Ep 2 (Step 083130): Train loss 1.025, Val loss 1.770\n",
      "Ep 2 (Step 083135): Train loss 0.952, Val loss 1.770\n",
      "Ep 2 (Step 083140): Train loss 1.111, Val loss 1.771\n",
      "Ep 2 (Step 083145): Train loss 1.110, Val loss 1.771\n",
      "Ep 2 (Step 083150): Train loss 0.986, Val loss 1.771\n",
      "Ep 2 (Step 083155): Train loss 0.925, Val loss 1.772\n",
      "Ep 2 (Step 083160): Train loss 0.986, Val loss 1.772\n",
      "Ep 2 (Step 083165): Train loss 1.075, Val loss 1.772\n",
      "Ep 2 (Step 083170): Train loss 0.875, Val loss 1.772\n",
      "Ep 2 (Step 083175): Train loss 1.133, Val loss 1.772\n",
      "Ep 2 (Step 083180): Train loss 0.959, Val loss 1.772\n",
      "Ep 2 (Step 083185): Train loss 1.191, Val loss 1.772\n",
      "Ep 2 (Step 083190): Train loss 1.031, Val loss 1.774\n",
      "Ep 2 (Step 083195): Train loss 0.979, Val loss 1.773\n",
      "Ep 2 (Step 083200): Train loss 1.103, Val loss 1.774\n",
      "Ep 2 (Step 083205): Train loss 1.063, Val loss 1.775\n",
      "Ep 2 (Step 083210): Train loss 0.782, Val loss 1.774\n",
      "Ep 2 (Step 083215): Train loss 0.883, Val loss 1.776\n",
      "Ep 2 (Step 083220): Train loss 0.777, Val loss 1.778\n",
      "Ep 2 (Step 083225): Train loss 1.046, Val loss 1.778\n",
      "Ep 2 (Step 083230): Train loss 0.981, Val loss 1.779\n",
      "Ep 2 (Step 083235): Train loss 1.119, Val loss 1.780\n",
      "Ep 2 (Step 083240): Train loss 1.126, Val loss 1.781\n",
      "Ep 2 (Step 083245): Train loss 1.059, Val loss 1.781\n",
      "Ep 2 (Step 083250): Train loss 0.973, Val loss 1.781\n",
      "Ep 2 (Step 083255): Train loss 1.094, Val loss 1.782\n",
      "Ep 2 (Step 083260): Train loss 1.069, Val loss 1.782\n",
      "Ep 2 (Step 083265): Train loss 0.926, Val loss 1.783\n",
      "Ep 2 (Step 083270): Train loss 1.025, Val loss 1.784\n",
      "Ep 2 (Step 083275): Train loss 0.859, Val loss 1.783\n",
      "Ep 2 (Step 083280): Train loss 1.286, Val loss 1.783\n",
      "Ep 2 (Step 083285): Train loss 1.097, Val loss 1.783\n",
      "Ep 2 (Step 083290): Train loss 0.835, Val loss 1.782\n",
      "Ep 2 (Step 083295): Train loss 0.992, Val loss 1.781\n",
      "Ep 2 (Step 083300): Train loss 1.056, Val loss 1.781\n",
      "Ep 2 (Step 083305): Train loss 0.965, Val loss 1.779\n",
      "Ep 2 (Step 083310): Train loss 1.046, Val loss 1.777\n",
      "Ep 2 (Step 083315): Train loss 0.869, Val loss 1.776\n",
      "Ep 2 (Step 083320): Train loss 1.021, Val loss 1.777\n",
      "Ep 2 (Step 083325): Train loss 1.069, Val loss 1.778\n",
      "Ep 2 (Step 083330): Train loss 1.271, Val loss 1.778\n",
      "Ep 2 (Step 083335): Train loss 1.117, Val loss 1.778\n",
      "Ep 2 (Step 083340): Train loss 1.129, Val loss 1.778\n",
      "Ep 2 (Step 083345): Train loss 1.024, Val loss 1.778\n",
      "Ep 2 (Step 083350): Train loss 0.926, Val loss 1.778\n",
      "Ep 2 (Step 083355): Train loss 1.015, Val loss 1.778\n",
      "Ep 2 (Step 083360): Train loss 1.029, Val loss 1.779\n",
      "Ep 2 (Step 083365): Train loss 0.991, Val loss 1.779\n",
      "Ep 2 (Step 083370): Train loss 0.985, Val loss 1.779\n",
      "Ep 2 (Step 083375): Train loss 1.035, Val loss 1.780\n",
      "Ep 2 (Step 083380): Train loss 1.160, Val loss 1.780\n",
      "Ep 2 (Step 083385): Train loss 1.162, Val loss 1.780\n",
      "Ep 2 (Step 083390): Train loss 0.909, Val loss 1.782\n",
      "Ep 2 (Step 083395): Train loss 1.139, Val loss 1.783\n",
      "Ep 2 (Step 083400): Train loss 1.085, Val loss 1.784\n",
      "Ep 2 (Step 083405): Train loss 0.900, Val loss 1.785\n",
      "Ep 2 (Step 083410): Train loss 0.919, Val loss 1.785\n",
      "Ep 2 (Step 083415): Train loss 0.883, Val loss 1.787\n",
      "Ep 2 (Step 083420): Train loss 1.133, Val loss 1.787\n",
      "Ep 2 (Step 083425): Train loss 1.077, Val loss 1.786\n",
      "Ep 2 (Step 083430): Train loss 0.952, Val loss 1.786\n",
      "Ep 2 (Step 083435): Train loss 0.903, Val loss 1.785\n",
      "Ep 2 (Step 083440): Train loss 0.843, Val loss 1.785\n",
      "Ep 2 (Step 083445): Train loss 0.738, Val loss 1.785\n",
      "Ep 2 (Step 083450): Train loss 1.050, Val loss 1.784\n",
      "Ep 2 (Step 083455): Train loss 0.830, Val loss 1.783\n",
      "Ep 2 (Step 083460): Train loss 1.012, Val loss 1.783\n",
      "Ep 2 (Step 083465): Train loss 1.099, Val loss 1.784\n",
      "Ep 2 (Step 083470): Train loss 1.045, Val loss 1.784\n",
      "Ep 2 (Step 083475): Train loss 1.068, Val loss 1.783\n",
      "Ep 2 (Step 083480): Train loss 0.855, Val loss 1.783\n",
      "Ep 2 (Step 083485): Train loss 0.929, Val loss 1.784\n",
      "Ep 2 (Step 083490): Train loss 0.902, Val loss 1.785\n",
      "Ep 2 (Step 083495): Train loss 0.806, Val loss 1.785\n",
      "Ep 2 (Step 083500): Train loss 0.983, Val loss 1.784\n",
      "Ep 2 (Step 083505): Train loss 1.115, Val loss 1.783\n",
      "Ep 2 (Step 083510): Train loss 0.864, Val loss 1.782\n",
      "Ep 2 (Step 083515): Train loss 0.974, Val loss 1.782\n",
      "Ep 2 (Step 083520): Train loss 1.073, Val loss 1.783\n",
      "Ep 2 (Step 083525): Train loss 0.876, Val loss 1.783\n",
      "Ep 2 (Step 083530): Train loss 0.956, Val loss 1.783\n",
      "Ep 2 (Step 083535): Train loss 0.853, Val loss 1.780\n",
      "Ep 2 (Step 083540): Train loss 0.838, Val loss 1.777\n",
      "Ep 2 (Step 083545): Train loss 0.965, Val loss 1.775\n",
      "Ep 2 (Step 083550): Train loss 1.041, Val loss 1.774\n",
      "Ep 2 (Step 083555): Train loss 0.920, Val loss 1.773\n",
      "Ep 2 (Step 083560): Train loss 0.933, Val loss 1.773\n",
      "Ep 2 (Step 083565): Train loss 0.889, Val loss 1.771\n",
      "Ep 2 (Step 083570): Train loss 1.246, Val loss 1.771\n",
      "Ep 2 (Step 083575): Train loss 0.925, Val loss 1.772\n",
      "Ep 2 (Step 083580): Train loss 1.084, Val loss 1.773\n",
      "Ep 2 (Step 083585): Train loss 0.895, Val loss 1.776\n",
      "Ep 2 (Step 083590): Train loss 0.965, Val loss 1.777\n",
      "Ep 2 (Step 083595): Train loss 1.025, Val loss 1.778\n",
      "Ep 2 (Step 083600): Train loss 1.111, Val loss 1.779\n",
      "Ep 2 (Step 083605): Train loss 0.980, Val loss 1.779\n",
      "Ep 2 (Step 083610): Train loss 1.141, Val loss 1.780\n",
      "Ep 2 (Step 083615): Train loss 1.012, Val loss 1.780\n",
      "Ep 2 (Step 083620): Train loss 0.912, Val loss 1.779\n",
      "Ep 2 (Step 083625): Train loss 0.801, Val loss 1.780\n",
      "Ep 2 (Step 083630): Train loss 1.069, Val loss 1.781\n",
      "Ep 2 (Step 083635): Train loss 0.946, Val loss 1.782\n",
      "Ep 2 (Step 083640): Train loss 0.848, Val loss 1.782\n",
      "Ep 2 (Step 083645): Train loss 1.205, Val loss 1.783\n",
      "Ep 2 (Step 083650): Train loss 0.962, Val loss 1.783\n",
      "Ep 2 (Step 083655): Train loss 0.907, Val loss 1.782\n",
      "Ep 2 (Step 083660): Train loss 1.008, Val loss 1.782\n",
      "Ep 2 (Step 083665): Train loss 0.786, Val loss 1.781\n",
      "Ep 2 (Step 083670): Train loss 1.046, Val loss 1.781\n",
      "Ep 2 (Step 083675): Train loss 0.828, Val loss 1.779\n",
      "Ep 2 (Step 083680): Train loss 1.146, Val loss 1.777\n",
      "Ep 2 (Step 083685): Train loss 0.948, Val loss 1.776\n",
      "Ep 2 (Step 083690): Train loss 1.087, Val loss 1.777\n",
      "Ep 2 (Step 083695): Train loss 0.980, Val loss 1.777\n",
      "Ep 2 (Step 083700): Train loss 1.184, Val loss 1.777\n",
      "Ep 2 (Step 083705): Train loss 1.148, Val loss 1.776\n",
      "Ep 2 (Step 083710): Train loss 1.281, Val loss 1.777\n",
      "Ep 2 (Step 083715): Train loss 1.044, Val loss 1.777\n",
      "Ep 2 (Step 083720): Train loss 0.877, Val loss 1.776\n",
      "Ep 2 (Step 083725): Train loss 1.004, Val loss 1.775\n",
      "Ep 2 (Step 083730): Train loss 0.998, Val loss 1.772\n",
      "Ep 2 (Step 083735): Train loss 0.965, Val loss 1.770\n",
      "Ep 2 (Step 083740): Train loss 0.883, Val loss 1.771\n",
      "Ep 2 (Step 083745): Train loss 0.770, Val loss 1.771\n",
      "Ep 2 (Step 083750): Train loss 0.896, Val loss 1.772\n",
      "Ep 2 (Step 083755): Train loss 0.856, Val loss 1.772\n",
      "Ep 2 (Step 083760): Train loss 1.095, Val loss 1.773\n",
      "Ep 2 (Step 083765): Train loss 1.124, Val loss 1.775\n",
      "Ep 2 (Step 083770): Train loss 1.222, Val loss 1.777\n",
      "Ep 2 (Step 083775): Train loss 1.150, Val loss 1.780\n",
      "Ep 2 (Step 083780): Train loss 1.024, Val loss 1.782\n",
      "Ep 2 (Step 083785): Train loss 0.834, Val loss 1.782\n",
      "Ep 2 (Step 083790): Train loss 1.124, Val loss 1.780\n",
      "Ep 2 (Step 083795): Train loss 0.872, Val loss 1.780\n",
      "Ep 2 (Step 083800): Train loss 1.152, Val loss 1.781\n",
      "Ep 2 (Step 083805): Train loss 0.820, Val loss 1.781\n",
      "Ep 2 (Step 083810): Train loss 1.153, Val loss 1.781\n",
      "Ep 2 (Step 083815): Train loss 0.838, Val loss 1.781\n",
      "Ep 2 (Step 083820): Train loss 1.230, Val loss 1.781\n",
      "Ep 2 (Step 083825): Train loss 1.076, Val loss 1.782\n",
      "Ep 2 (Step 083830): Train loss 1.090, Val loss 1.783\n",
      "Ep 2 (Step 083835): Train loss 0.796, Val loss 1.785\n",
      "Ep 2 (Step 083840): Train loss 0.953, Val loss 1.786\n",
      "Ep 2 (Step 083845): Train loss 0.954, Val loss 1.786\n",
      "Ep 2 (Step 083850): Train loss 0.911, Val loss 1.786\n",
      "Ep 2 (Step 083855): Train loss 1.119, Val loss 1.787\n",
      "Ep 2 (Step 083860): Train loss 0.952, Val loss 1.788\n",
      "Ep 2 (Step 083865): Train loss 1.075, Val loss 1.787\n",
      "Ep 2 (Step 083870): Train loss 1.001, Val loss 1.786\n",
      "Ep 2 (Step 083875): Train loss 0.798, Val loss 1.784\n",
      "Ep 2 (Step 083880): Train loss 0.811, Val loss 1.784\n",
      "Ep 2 (Step 083885): Train loss 1.027, Val loss 1.783\n",
      "Ep 2 (Step 083890): Train loss 0.802, Val loss 1.783\n",
      "Ep 2 (Step 083895): Train loss 0.827, Val loss 1.783\n",
      "Ep 2 (Step 083900): Train loss 0.773, Val loss 1.782\n",
      "Ep 2 (Step 083905): Train loss 0.824, Val loss 1.781\n",
      "Ep 2 (Step 083910): Train loss 1.033, Val loss 1.781\n",
      "Ep 2 (Step 083915): Train loss 0.879, Val loss 1.781\n",
      "Ep 2 (Step 083920): Train loss 1.293, Val loss 1.780\n",
      "Ep 2 (Step 083925): Train loss 0.992, Val loss 1.778\n",
      "Ep 2 (Step 083930): Train loss 0.940, Val loss 1.777\n",
      "Ep 2 (Step 083935): Train loss 1.147, Val loss 1.777\n",
      "Ep 2 (Step 083940): Train loss 1.174, Val loss 1.776\n",
      "Ep 2 (Step 083945): Train loss 0.748, Val loss 1.775\n",
      "Ep 2 (Step 083950): Train loss 0.956, Val loss 1.774\n",
      "Ep 2 (Step 083955): Train loss 0.914, Val loss 1.773\n",
      "Ep 2 (Step 083960): Train loss 1.100, Val loss 1.773\n",
      "Ep 2 (Step 083965): Train loss 0.760, Val loss 1.774\n",
      "Ep 2 (Step 083970): Train loss 1.009, Val loss 1.775\n",
      "Ep 2 (Step 083975): Train loss 0.983, Val loss 1.777\n",
      "Ep 2 (Step 083980): Train loss 0.779, Val loss 1.779\n",
      "Ep 2 (Step 083985): Train loss 1.121, Val loss 1.779\n",
      "Ep 2 (Step 083990): Train loss 1.270, Val loss 1.780\n",
      "Ep 2 (Step 083995): Train loss 1.041, Val loss 1.782\n",
      "Ep 2 (Step 084000): Train loss 0.934, Val loss 1.783\n",
      "Ep 2 (Step 084005): Train loss 1.068, Val loss 1.783\n",
      "Ep 2 (Step 084010): Train loss 0.896, Val loss 1.784\n",
      "Ep 2 (Step 084015): Train loss 1.035, Val loss 1.784\n",
      "Ep 2 (Step 084020): Train loss 0.979, Val loss 1.784\n",
      "Ep 2 (Step 084025): Train loss 1.175, Val loss 1.782\n",
      "Ep 2 (Step 084030): Train loss 1.011, Val loss 1.782\n",
      "Ep 2 (Step 084035): Train loss 0.943, Val loss 1.782\n",
      "Ep 2 (Step 084040): Train loss 1.084, Val loss 1.783\n",
      "Ep 2 (Step 084045): Train loss 1.104, Val loss 1.784\n",
      "Ep 2 (Step 084050): Train loss 1.047, Val loss 1.784\n",
      "Ep 2 (Step 084055): Train loss 1.258, Val loss 1.785\n",
      "Ep 2 (Step 084060): Train loss 0.890, Val loss 1.784\n",
      "Ep 2 (Step 084065): Train loss 0.832, Val loss 1.784\n",
      "Ep 2 (Step 084070): Train loss 1.148, Val loss 1.783\n",
      "Ep 2 (Step 084075): Train loss 0.817, Val loss 1.784\n",
      "Ep 2 (Step 084080): Train loss 1.020, Val loss 1.785\n",
      "Ep 2 (Step 084085): Train loss 1.093, Val loss 1.785\n",
      "Ep 2 (Step 084090): Train loss 0.976, Val loss 1.785\n",
      "Ep 2 (Step 084095): Train loss 0.922, Val loss 1.786\n",
      "Ep 2 (Step 084100): Train loss 1.015, Val loss 1.787\n",
      "Ep 2 (Step 084105): Train loss 0.829, Val loss 1.787\n",
      "Ep 2 (Step 084110): Train loss 0.961, Val loss 1.788\n",
      "Ep 2 (Step 084115): Train loss 0.991, Val loss 1.788\n",
      "Ep 2 (Step 084120): Train loss 0.922, Val loss 1.789\n",
      "Ep 2 (Step 084125): Train loss 1.209, Val loss 1.790\n",
      "Ep 2 (Step 084130): Train loss 1.027, Val loss 1.789\n",
      "Ep 2 (Step 084135): Train loss 0.811, Val loss 1.787\n",
      "Ep 2 (Step 084140): Train loss 1.081, Val loss 1.787\n",
      "Ep 2 (Step 084145): Train loss 1.268, Val loss 1.787\n",
      "Ep 2 (Step 084150): Train loss 0.870, Val loss 1.787\n",
      "Ep 2 (Step 084155): Train loss 0.996, Val loss 1.787\n",
      "Ep 2 (Step 084160): Train loss 1.039, Val loss 1.787\n",
      "Ep 2 (Step 084165): Train loss 0.623, Val loss 1.786\n",
      "Ep 2 (Step 084170): Train loss 1.124, Val loss 1.786\n",
      "Ep 2 (Step 084175): Train loss 1.011, Val loss 1.787\n",
      "Ep 2 (Step 084180): Train loss 1.096, Val loss 1.789\n",
      "Ep 2 (Step 084185): Train loss 0.960, Val loss 1.788\n",
      "Ep 2 (Step 084190): Train loss 0.941, Val loss 1.786\n",
      "Ep 2 (Step 084195): Train loss 0.947, Val loss 1.785\n",
      "Ep 2 (Step 084200): Train loss 0.951, Val loss 1.786\n",
      "Ep 2 (Step 084205): Train loss 0.799, Val loss 1.785\n",
      "Ep 2 (Step 084210): Train loss 1.022, Val loss 1.785\n",
      "Ep 2 (Step 084215): Train loss 0.975, Val loss 1.786\n",
      "Ep 2 (Step 084220): Train loss 1.027, Val loss 1.788\n",
      "Ep 2 (Step 084225): Train loss 0.854, Val loss 1.789\n",
      "Ep 2 (Step 084230): Train loss 0.876, Val loss 1.790\n",
      "Ep 2 (Step 084235): Train loss 1.123, Val loss 1.790\n",
      "Ep 2 (Step 084240): Train loss 1.077, Val loss 1.789\n",
      "Ep 2 (Step 084245): Train loss 0.957, Val loss 1.789\n",
      "Ep 2 (Step 084250): Train loss 1.021, Val loss 1.789\n",
      "Ep 2 (Step 084255): Train loss 1.027, Val loss 1.791\n",
      "Ep 2 (Step 084260): Train loss 0.974, Val loss 1.791\n",
      "Ep 2 (Step 084265): Train loss 0.965, Val loss 1.791\n",
      "Ep 2 (Step 084270): Train loss 1.240, Val loss 1.791\n",
      "Ep 2 (Step 084275): Train loss 0.859, Val loss 1.792\n",
      "Ep 2 (Step 084280): Train loss 0.943, Val loss 1.792\n",
      "Ep 2 (Step 084285): Train loss 1.078, Val loss 1.792\n",
      "Ep 2 (Step 084290): Train loss 1.062, Val loss 1.792\n",
      "Ep 2 (Step 084295): Train loss 1.130, Val loss 1.792\n",
      "Ep 2 (Step 084300): Train loss 0.935, Val loss 1.792\n",
      "Ep 2 (Step 084305): Train loss 1.084, Val loss 1.792\n",
      "Ep 2 (Step 084310): Train loss 0.890, Val loss 1.792\n",
      "Ep 2 (Step 084315): Train loss 1.027, Val loss 1.791\n",
      "Ep 2 (Step 084320): Train loss 0.956, Val loss 1.790\n",
      "Ep 2 (Step 084325): Train loss 1.116, Val loss 1.788\n",
      "Ep 2 (Step 084330): Train loss 0.868, Val loss 1.784\n",
      "Ep 2 (Step 084335): Train loss 0.941, Val loss 1.781\n",
      "Ep 2 (Step 084340): Train loss 0.831, Val loss 1.779\n",
      "Ep 2 (Step 084345): Train loss 0.913, Val loss 1.778\n",
      "Ep 2 (Step 084350): Train loss 1.061, Val loss 1.777\n",
      "Ep 2 (Step 084355): Train loss 1.154, Val loss 1.777\n",
      "Ep 2 (Step 084360): Train loss 1.085, Val loss 1.777\n",
      "Ep 2 (Step 084365): Train loss 1.088, Val loss 1.777\n",
      "Ep 2 (Step 084370): Train loss 0.956, Val loss 1.778\n",
      "Ep 2 (Step 084375): Train loss 1.111, Val loss 1.777\n",
      "Ep 2 (Step 084380): Train loss 1.074, Val loss 1.776\n",
      "Ep 2 (Step 084385): Train loss 0.801, Val loss 1.775\n",
      "Ep 2 (Step 084390): Train loss 0.921, Val loss 1.772\n",
      "Ep 2 (Step 084395): Train loss 0.985, Val loss 1.771\n",
      "Ep 2 (Step 084400): Train loss 1.079, Val loss 1.771\n",
      "Ep 2 (Step 084405): Train loss 1.071, Val loss 1.771\n",
      "Ep 2 (Step 084410): Train loss 0.785, Val loss 1.771\n",
      "Ep 2 (Step 084415): Train loss 1.050, Val loss 1.772\n",
      "Ep 2 (Step 084420): Train loss 1.222, Val loss 1.774\n",
      "Ep 2 (Step 084425): Train loss 0.998, Val loss 1.775\n",
      "Ep 2 (Step 084430): Train loss 0.948, Val loss 1.775\n",
      "Ep 2 (Step 084435): Train loss 0.914, Val loss 1.777\n",
      "Ep 2 (Step 084440): Train loss 1.004, Val loss 1.778\n",
      "Ep 2 (Step 084445): Train loss 0.970, Val loss 1.778\n",
      "Ep 2 (Step 084450): Train loss 1.168, Val loss 1.779\n",
      "Ep 2 (Step 084455): Train loss 0.926, Val loss 1.780\n",
      "Ep 2 (Step 084460): Train loss 0.815, Val loss 1.781\n",
      "Ep 2 (Step 084465): Train loss 0.996, Val loss 1.782\n",
      "Ep 2 (Step 084470): Train loss 1.246, Val loss 1.782\n",
      "Ep 2 (Step 084475): Train loss 0.959, Val loss 1.782\n",
      "Ep 2 (Step 084480): Train loss 0.783, Val loss 1.782\n",
      "Ep 2 (Step 084485): Train loss 0.843, Val loss 1.783\n",
      "Ep 2 (Step 084490): Train loss 0.961, Val loss 1.782\n",
      "Ep 2 (Step 084495): Train loss 0.933, Val loss 1.782\n",
      "Ep 2 (Step 084500): Train loss 0.769, Val loss 1.781\n",
      "Ep 2 (Step 084505): Train loss 0.748, Val loss 1.780\n",
      "Ep 2 (Step 084510): Train loss 1.091, Val loss 1.778\n",
      "Ep 2 (Step 084515): Train loss 1.011, Val loss 1.775\n",
      "Ep 2 (Step 084520): Train loss 0.923, Val loss 1.773\n",
      "Ep 2 (Step 084525): Train loss 0.929, Val loss 1.772\n",
      "Ep 2 (Step 084530): Train loss 1.013, Val loss 1.772\n",
      "Ep 2 (Step 084535): Train loss 0.778, Val loss 1.771\n",
      "Ep 2 (Step 084540): Train loss 0.787, Val loss 1.771\n",
      "Ep 2 (Step 084545): Train loss 0.946, Val loss 1.770\n",
      "Ep 2 (Step 084550): Train loss 0.836, Val loss 1.770\n",
      "Ep 2 (Step 084555): Train loss 1.317, Val loss 1.770\n",
      "Ep 2 (Step 084560): Train loss 0.802, Val loss 1.771\n",
      "Ep 2 (Step 084565): Train loss 0.802, Val loss 1.771\n",
      "Ep 2 (Step 084570): Train loss 0.980, Val loss 1.771\n",
      "Ep 2 (Step 084575): Train loss 1.150, Val loss 1.773\n",
      "Ep 2 (Step 084580): Train loss 0.963, Val loss 1.774\n",
      "Ep 2 (Step 084585): Train loss 1.061, Val loss 1.776\n",
      "Ep 2 (Step 084590): Train loss 0.964, Val loss 1.776\n",
      "Ep 2 (Step 084595): Train loss 1.105, Val loss 1.776\n",
      "Ep 2 (Step 084600): Train loss 1.053, Val loss 1.777\n",
      "Ep 2 (Step 084605): Train loss 0.942, Val loss 1.778\n",
      "Ep 2 (Step 084610): Train loss 0.857, Val loss 1.778\n",
      "Ep 2 (Step 084615): Train loss 0.942, Val loss 1.779\n",
      "Ep 2 (Step 084620): Train loss 1.013, Val loss 1.779\n",
      "Ep 2 (Step 084625): Train loss 1.036, Val loss 1.780\n",
      "Ep 2 (Step 084630): Train loss 1.279, Val loss 1.780\n",
      "Ep 2 (Step 084635): Train loss 0.952, Val loss 1.780\n",
      "Ep 2 (Step 084640): Train loss 1.165, Val loss 1.779\n",
      "Ep 2 (Step 084645): Train loss 1.084, Val loss 1.778\n",
      "Ep 2 (Step 084650): Train loss 1.059, Val loss 1.777\n",
      "Ep 2 (Step 084655): Train loss 1.190, Val loss 1.778\n",
      "Ep 2 (Step 084660): Train loss 0.981, Val loss 1.779\n",
      "Ep 2 (Step 084665): Train loss 1.222, Val loss 1.779\n",
      "Ep 2 (Step 084670): Train loss 1.088, Val loss 1.779\n",
      "Ep 2 (Step 084675): Train loss 0.799, Val loss 1.779\n",
      "Ep 2 (Step 084680): Train loss 0.783, Val loss 1.781\n",
      "Ep 2 (Step 084685): Train loss 1.150, Val loss 1.781\n",
      "Ep 2 (Step 084690): Train loss 0.848, Val loss 1.780\n",
      "Ep 2 (Step 084695): Train loss 0.855, Val loss 1.778\n",
      "Ep 2 (Step 084700): Train loss 1.057, Val loss 1.777\n",
      "Ep 2 (Step 084705): Train loss 0.899, Val loss 1.777\n",
      "Ep 2 (Step 084710): Train loss 0.947, Val loss 1.777\n",
      "Ep 2 (Step 084715): Train loss 0.756, Val loss 1.778\n",
      "Ep 2 (Step 084720): Train loss 1.017, Val loss 1.779\n",
      "Ep 2 (Step 084725): Train loss 1.074, Val loss 1.780\n",
      "Ep 2 (Step 084730): Train loss 0.982, Val loss 1.782\n",
      "Ep 2 (Step 084735): Train loss 0.694, Val loss 1.785\n",
      "Ep 2 (Step 084740): Train loss 1.038, Val loss 1.786\n",
      "Ep 2 (Step 084745): Train loss 1.041, Val loss 1.787\n",
      "Ep 2 (Step 084750): Train loss 1.073, Val loss 1.786\n",
      "Ep 2 (Step 084755): Train loss 1.108, Val loss 1.786\n",
      "Ep 2 (Step 084760): Train loss 0.879, Val loss 1.786\n",
      "Ep 2 (Step 084765): Train loss 0.981, Val loss 1.786\n",
      "Ep 2 (Step 084770): Train loss 1.277, Val loss 1.786\n",
      "Ep 2 (Step 084775): Train loss 0.906, Val loss 1.787\n",
      "Ep 2 (Step 084780): Train loss 0.979, Val loss 1.787\n",
      "Ep 2 (Step 084785): Train loss 1.221, Val loss 1.788\n",
      "Ep 2 (Step 084790): Train loss 0.738, Val loss 1.789\n",
      "Ep 2 (Step 084795): Train loss 0.810, Val loss 1.790\n",
      "Ep 2 (Step 084800): Train loss 0.776, Val loss 1.789\n",
      "Ep 2 (Step 084805): Train loss 1.181, Val loss 1.787\n",
      "Ep 2 (Step 084810): Train loss 0.917, Val loss 1.785\n",
      "Ep 2 (Step 084815): Train loss 0.891, Val loss 1.782\n",
      "Ep 2 (Step 084820): Train loss 0.942, Val loss 1.781\n",
      "Ep 2 (Step 084825): Train loss 1.006, Val loss 1.778\n",
      "Ep 2 (Step 084830): Train loss 0.605, Val loss 1.777\n",
      "Ep 2 (Step 084835): Train loss 0.799, Val loss 1.778\n",
      "Ep 2 (Step 084840): Train loss 0.743, Val loss 1.778\n",
      "Ep 2 (Step 084845): Train loss 1.041, Val loss 1.777\n",
      "Ep 2 (Step 084850): Train loss 0.763, Val loss 1.777\n",
      "Ep 2 (Step 084855): Train loss 0.839, Val loss 1.779\n",
      "Ep 2 (Step 084860): Train loss 0.936, Val loss 1.780\n",
      "Ep 2 (Step 084865): Train loss 0.956, Val loss 1.779\n",
      "Ep 2 (Step 084870): Train loss 0.953, Val loss 1.779\n",
      "Ep 2 (Step 084875): Train loss 1.116, Val loss 1.779\n",
      "Ep 2 (Step 084880): Train loss 1.130, Val loss 1.779\n",
      "Ep 2 (Step 084885): Train loss 1.166, Val loss 1.779\n",
      "Ep 2 (Step 084890): Train loss 1.020, Val loss 1.780\n",
      "Ep 2 (Step 084895): Train loss 1.100, Val loss 1.780\n",
      "Ep 2 (Step 084900): Train loss 0.815, Val loss 1.781\n",
      "Ep 2 (Step 084905): Train loss 1.213, Val loss 1.782\n",
      "Ep 2 (Step 084910): Train loss 1.057, Val loss 1.783\n",
      "Ep 2 (Step 084915): Train loss 0.912, Val loss 1.785\n",
      "Ep 2 (Step 084920): Train loss 0.780, Val loss 1.786\n",
      "Ep 2 (Step 084925): Train loss 1.042, Val loss 1.786\n",
      "Ep 2 (Step 084930): Train loss 0.940, Val loss 1.787\n",
      "Ep 2 (Step 084935): Train loss 1.502, Val loss 1.787\n",
      "Ep 2 (Step 084940): Train loss 0.939, Val loss 1.785\n",
      "Ep 2 (Step 084945): Train loss 1.051, Val loss 1.783\n",
      "Ep 2 (Step 084950): Train loss 0.766, Val loss 1.782\n",
      "Ep 2 (Step 084955): Train loss 0.748, Val loss 1.781\n",
      "Ep 2 (Step 084960): Train loss 1.190, Val loss 1.780\n",
      "Ep 2 (Step 084965): Train loss 0.922, Val loss 1.780\n",
      "Ep 2 (Step 084970): Train loss 0.887, Val loss 1.779\n",
      "Ep 2 (Step 084975): Train loss 0.889, Val loss 1.777\n",
      "Ep 2 (Step 084980): Train loss 1.418, Val loss 1.776\n",
      "Ep 2 (Step 084985): Train loss 0.815, Val loss 1.776\n",
      "Ep 2 (Step 084990): Train loss 1.124, Val loss 1.776\n",
      "Ep 2 (Step 084995): Train loss 1.030, Val loss 1.776\n",
      "Ep 2 (Step 085000): Train loss 0.895, Val loss 1.775\n",
      "Ep 2 (Step 085005): Train loss 1.129, Val loss 1.776\n",
      "Ep 2 (Step 085010): Train loss 1.040, Val loss 1.776\n",
      "Ep 2 (Step 085015): Train loss 1.281, Val loss 1.776\n",
      "Ep 2 (Step 085020): Train loss 0.981, Val loss 1.776\n",
      "Ep 2 (Step 085025): Train loss 1.043, Val loss 1.778\n",
      "Ep 2 (Step 085030): Train loss 0.913, Val loss 1.778\n",
      "Ep 2 (Step 085035): Train loss 0.954, Val loss 1.779\n",
      "Ep 2 (Step 085040): Train loss 1.160, Val loss 1.781\n",
      "Ep 2 (Step 085045): Train loss 0.925, Val loss 1.784\n",
      "Ep 2 (Step 085050): Train loss 0.948, Val loss 1.786\n",
      "Ep 2 (Step 085055): Train loss 0.812, Val loss 1.785\n",
      "Ep 2 (Step 085060): Train loss 1.021, Val loss 1.782\n",
      "Ep 2 (Step 085065): Train loss 0.987, Val loss 1.781\n",
      "Ep 2 (Step 085070): Train loss 0.692, Val loss 1.780\n",
      "Ep 2 (Step 085075): Train loss 0.782, Val loss 1.780\n",
      "Ep 2 (Step 085080): Train loss 0.863, Val loss 1.780\n",
      "Ep 2 (Step 085085): Train loss 1.007, Val loss 1.780\n",
      "Ep 2 (Step 085090): Train loss 1.094, Val loss 1.781\n",
      "Ep 2 (Step 085095): Train loss 1.202, Val loss 1.782\n",
      "Ep 2 (Step 085100): Train loss 1.008, Val loss 1.782\n",
      "Ep 2 (Step 085105): Train loss 0.748, Val loss 1.783\n",
      "Ep 2 (Step 085110): Train loss 1.122, Val loss 1.782\n",
      "Ep 2 (Step 085115): Train loss 0.991, Val loss 1.781\n",
      "Ep 2 (Step 085120): Train loss 1.025, Val loss 1.780\n",
      "Ep 2 (Step 085125): Train loss 1.044, Val loss 1.779\n",
      "Ep 2 (Step 085130): Train loss 1.099, Val loss 1.779\n",
      "Ep 2 (Step 085135): Train loss 1.127, Val loss 1.778\n",
      "Ep 2 (Step 085140): Train loss 1.132, Val loss 1.777\n",
      "Ep 2 (Step 085145): Train loss 0.980, Val loss 1.777\n",
      "Ep 2 (Step 085150): Train loss 1.244, Val loss 1.778\n",
      "Ep 2 (Step 085155): Train loss 1.144, Val loss 1.779\n",
      "Ep 2 (Step 085160): Train loss 0.883, Val loss 1.781\n",
      "Ep 2 (Step 085165): Train loss 0.920, Val loss 1.783\n",
      "Ep 2 (Step 085170): Train loss 1.119, Val loss 1.784\n",
      "Ep 2 (Step 085175): Train loss 1.134, Val loss 1.783\n",
      "Ep 2 (Step 085180): Train loss 0.827, Val loss 1.782\n",
      "Ep 2 (Step 085185): Train loss 0.862, Val loss 1.780\n",
      "Ep 2 (Step 085190): Train loss 1.100, Val loss 1.777\n",
      "Ep 2 (Step 085195): Train loss 1.184, Val loss 1.775\n",
      "Ep 2 (Step 085200): Train loss 1.412, Val loss 1.774\n",
      "Ep 2 (Step 085205): Train loss 0.999, Val loss 1.774\n",
      "Ep 2 (Step 085210): Train loss 1.171, Val loss 1.773\n",
      "Ep 2 (Step 085215): Train loss 0.913, Val loss 1.772\n",
      "Ep 2 (Step 085220): Train loss 0.932, Val loss 1.771\n",
      "Ep 2 (Step 085225): Train loss 0.993, Val loss 1.771\n",
      "Ep 2 (Step 085230): Train loss 1.092, Val loss 1.771\n",
      "Ep 2 (Step 085235): Train loss 0.877, Val loss 1.773\n",
      "Ep 2 (Step 085240): Train loss 1.155, Val loss 1.775\n",
      "Ep 2 (Step 085245): Train loss 1.072, Val loss 1.778\n",
      "Ep 2 (Step 085250): Train loss 1.057, Val loss 1.778\n",
      "Ep 2 (Step 085255): Train loss 1.240, Val loss 1.778\n",
      "Ep 2 (Step 085260): Train loss 1.042, Val loss 1.778\n",
      "Ep 2 (Step 085265): Train loss 0.994, Val loss 1.777\n",
      "Ep 2 (Step 085270): Train loss 0.865, Val loss 1.776\n",
      "Ep 2 (Step 085275): Train loss 0.889, Val loss 1.774\n",
      "Ep 2 (Step 085280): Train loss 0.872, Val loss 1.773\n",
      "Ep 2 (Step 085285): Train loss 1.007, Val loss 1.773\n",
      "Ep 2 (Step 085290): Train loss 0.794, Val loss 1.773\n",
      "Ep 2 (Step 085295): Train loss 1.209, Val loss 1.775\n",
      "Ep 2 (Step 085300): Train loss 0.906, Val loss 1.776\n",
      "Ep 2 (Step 085305): Train loss 1.078, Val loss 1.777\n",
      "Ep 2 (Step 085310): Train loss 0.887, Val loss 1.778\n",
      "Ep 2 (Step 085315): Train loss 0.971, Val loss 1.779\n",
      "Ep 2 (Step 085320): Train loss 0.937, Val loss 1.781\n",
      "Ep 2 (Step 085325): Train loss 0.878, Val loss 1.781\n",
      "Ep 2 (Step 085330): Train loss 1.286, Val loss 1.781\n",
      "Ep 2 (Step 085335): Train loss 1.253, Val loss 1.781\n",
      "Ep 2 (Step 085340): Train loss 0.977, Val loss 1.781\n",
      "Ep 2 (Step 085345): Train loss 1.107, Val loss 1.782\n",
      "Ep 2 (Step 085350): Train loss 1.022, Val loss 1.781\n",
      "Ep 2 (Step 085355): Train loss 1.010, Val loss 1.780\n",
      "Ep 2 (Step 085360): Train loss 1.107, Val loss 1.779\n",
      "Ep 2 (Step 085365): Train loss 0.638, Val loss 1.778\n",
      "Ep 2 (Step 085370): Train loss 0.914, Val loss 1.778\n",
      "Ep 2 (Step 085375): Train loss 0.835, Val loss 1.779\n",
      "Ep 2 (Step 085380): Train loss 1.066, Val loss 1.780\n",
      "Ep 2 (Step 085385): Train loss 1.199, Val loss 1.780\n",
      "Ep 2 (Step 085390): Train loss 0.934, Val loss 1.780\n",
      "Ep 2 (Step 085395): Train loss 0.854, Val loss 1.777\n",
      "Ep 2 (Step 085400): Train loss 1.185, Val loss 1.775\n",
      "Ep 2 (Step 085405): Train loss 0.975, Val loss 1.775\n",
      "Ep 2 (Step 085410): Train loss 0.967, Val loss 1.775\n",
      "Ep 2 (Step 085415): Train loss 1.257, Val loss 1.775\n",
      "Ep 2 (Step 085420): Train loss 0.989, Val loss 1.774\n",
      "Ep 2 (Step 085425): Train loss 0.830, Val loss 1.774\n",
      "Ep 2 (Step 085430): Train loss 1.029, Val loss 1.775\n",
      "Ep 2 (Step 085435): Train loss 1.154, Val loss 1.777\n",
      "Ep 2 (Step 085440): Train loss 0.881, Val loss 1.779\n",
      "Ep 2 (Step 085445): Train loss 1.181, Val loss 1.781\n",
      "Ep 2 (Step 085450): Train loss 0.958, Val loss 1.782\n",
      "Ep 2 (Step 085455): Train loss 0.832, Val loss 1.781\n",
      "Ep 2 (Step 085460): Train loss 1.118, Val loss 1.781\n",
      "Ep 2 (Step 085465): Train loss 0.886, Val loss 1.781\n",
      "Ep 2 (Step 085470): Train loss 0.817, Val loss 1.780\n",
      "Ep 2 (Step 085475): Train loss 1.159, Val loss 1.780\n",
      "Ep 2 (Step 085480): Train loss 1.005, Val loss 1.780\n",
      "Ep 2 (Step 085485): Train loss 1.081, Val loss 1.778\n",
      "Ep 2 (Step 085490): Train loss 0.982, Val loss 1.776\n",
      "Ep 2 (Step 085495): Train loss 1.066, Val loss 1.776\n",
      "Ep 2 (Step 085500): Train loss 1.088, Val loss 1.776\n",
      "Ep 2 (Step 085505): Train loss 0.980, Val loss 1.775\n",
      "Ep 2 (Step 085510): Train loss 0.757, Val loss 1.775\n",
      "Ep 2 (Step 085515): Train loss 0.880, Val loss 1.775\n",
      "Ep 2 (Step 085520): Train loss 1.199, Val loss 1.776\n",
      "Ep 2 (Step 085525): Train loss 0.938, Val loss 1.777\n",
      "Ep 2 (Step 085530): Train loss 1.070, Val loss 1.778\n",
      "Ep 2 (Step 085535): Train loss 0.999, Val loss 1.777\n",
      "Ep 2 (Step 085540): Train loss 0.858, Val loss 1.776\n",
      "Ep 2 (Step 085545): Train loss 1.271, Val loss 1.773\n",
      "Ep 2 (Step 085550): Train loss 0.764, Val loss 1.773\n",
      "Ep 2 (Step 085555): Train loss 0.886, Val loss 1.773\n",
      "Ep 2 (Step 085560): Train loss 1.261, Val loss 1.772\n",
      "Ep 2 (Step 085565): Train loss 1.092, Val loss 1.772\n",
      "Ep 2 (Step 085570): Train loss 1.186, Val loss 1.772\n",
      "Ep 2 (Step 085575): Train loss 1.078, Val loss 1.771\n",
      "Ep 2 (Step 085580): Train loss 0.936, Val loss 1.771\n",
      "Ep 2 (Step 085585): Train loss 1.260, Val loss 1.771\n",
      "Ep 2 (Step 085590): Train loss 1.160, Val loss 1.771\n",
      "Ep 2 (Step 085595): Train loss 0.937, Val loss 1.772\n",
      "Ep 2 (Step 085600): Train loss 0.827, Val loss 1.773\n",
      "Ep 2 (Step 085605): Train loss 0.996, Val loss 1.773\n",
      "Ep 2 (Step 085610): Train loss 1.109, Val loss 1.773\n",
      "Ep 2 (Step 085615): Train loss 1.035, Val loss 1.772\n",
      "Ep 2 (Step 085620): Train loss 0.995, Val loss 1.772\n",
      "Ep 2 (Step 085625): Train loss 1.171, Val loss 1.772\n",
      "Ep 2 (Step 085630): Train loss 1.205, Val loss 1.772\n",
      "Ep 2 (Step 085635): Train loss 0.776, Val loss 1.771\n",
      "Ep 2 (Step 085640): Train loss 0.991, Val loss 1.771\n",
      "Ep 2 (Step 085645): Train loss 1.038, Val loss 1.772\n",
      "Ep 2 (Step 085650): Train loss 1.079, Val loss 1.773\n",
      "Ep 2 (Step 085655): Train loss 0.982, Val loss 1.773\n",
      "Ep 2 (Step 085660): Train loss 0.970, Val loss 1.773\n",
      "Ep 2 (Step 085665): Train loss 1.207, Val loss 1.773\n",
      "Ep 2 (Step 085670): Train loss 0.904, Val loss 1.774\n",
      "Ep 2 (Step 085675): Train loss 1.211, Val loss 1.774\n",
      "Ep 2 (Step 085680): Train loss 0.824, Val loss 1.774\n",
      "Ep 2 (Step 085685): Train loss 1.118, Val loss 1.775\n",
      "Ep 2 (Step 085690): Train loss 0.902, Val loss 1.778\n",
      "Ep 2 (Step 085695): Train loss 0.997, Val loss 1.779\n",
      "Ep 2 (Step 085700): Train loss 1.139, Val loss 1.779\n",
      "Ep 2 (Step 085705): Train loss 1.117, Val loss 1.778\n",
      "Ep 2 (Step 085710): Train loss 0.776, Val loss 1.778\n",
      "Ep 2 (Step 085715): Train loss 1.069, Val loss 1.778\n",
      "Ep 2 (Step 085720): Train loss 0.965, Val loss 1.778\n",
      "Ep 2 (Step 085725): Train loss 1.069, Val loss 1.777\n",
      "Ep 2 (Step 085730): Train loss 0.990, Val loss 1.776\n",
      "Ep 2 (Step 085735): Train loss 1.148, Val loss 1.775\n",
      "Ep 2 (Step 085740): Train loss 0.962, Val loss 1.773\n",
      "Ep 2 (Step 085745): Train loss 0.843, Val loss 1.771\n",
      "Ep 2 (Step 085750): Train loss 1.454, Val loss 1.770\n",
      "Ep 2 (Step 085755): Train loss 1.017, Val loss 1.769\n",
      "Ep 2 (Step 085760): Train loss 1.077, Val loss 1.768\n",
      "Ep 2 (Step 085765): Train loss 1.163, Val loss 1.768\n",
      "Ep 2 (Step 085770): Train loss 0.863, Val loss 1.769\n",
      "Ep 2 (Step 085775): Train loss 0.861, Val loss 1.770\n",
      "Ep 2 (Step 085780): Train loss 0.953, Val loss 1.771\n",
      "Ep 2 (Step 085785): Train loss 1.069, Val loss 1.773\n",
      "Ep 2 (Step 085790): Train loss 0.973, Val loss 1.773\n",
      "Ep 2 (Step 085795): Train loss 0.893, Val loss 1.773\n",
      "Ep 2 (Step 085800): Train loss 1.109, Val loss 1.771\n",
      "Ep 2 (Step 085805): Train loss 1.295, Val loss 1.769\n",
      "Ep 2 (Step 085810): Train loss 0.916, Val loss 1.768\n",
      "Ep 2 (Step 085815): Train loss 1.157, Val loss 1.769\n",
      "Ep 2 (Step 085820): Train loss 1.050, Val loss 1.770\n",
      "Ep 2 (Step 085825): Train loss 0.966, Val loss 1.771\n",
      "Ep 2 (Step 085830): Train loss 0.996, Val loss 1.773\n",
      "Ep 2 (Step 085835): Train loss 1.099, Val loss 1.774\n",
      "Ep 2 (Step 085840): Train loss 1.242, Val loss 1.774\n",
      "Ep 2 (Step 085845): Train loss 1.145, Val loss 1.775\n",
      "Ep 2 (Step 085850): Train loss 0.988, Val loss 1.774\n",
      "Ep 2 (Step 085855): Train loss 1.107, Val loss 1.775\n",
      "Ep 2 (Step 085860): Train loss 0.827, Val loss 1.774\n",
      "Ep 2 (Step 085865): Train loss 0.932, Val loss 1.774\n",
      "Ep 2 (Step 085870): Train loss 0.937, Val loss 1.775\n",
      "Ep 2 (Step 085875): Train loss 1.024, Val loss 1.776\n",
      "Ep 2 (Step 085880): Train loss 0.756, Val loss 1.776\n",
      "Ep 2 (Step 085885): Train loss 1.056, Val loss 1.776\n",
      "Ep 2 (Step 085890): Train loss 0.982, Val loss 1.777\n",
      "Ep 2 (Step 085895): Train loss 0.855, Val loss 1.777\n",
      "Ep 2 (Step 085900): Train loss 1.378, Val loss 1.778\n",
      "Ep 2 (Step 085905): Train loss 1.108, Val loss 1.778\n",
      "Ep 2 (Step 085910): Train loss 1.037, Val loss 1.777\n",
      "Ep 2 (Step 085915): Train loss 0.850, Val loss 1.777\n",
      "Ep 2 (Step 085920): Train loss 0.980, Val loss 1.777\n",
      "Ep 2 (Step 085925): Train loss 1.081, Val loss 1.777\n",
      "Ep 2 (Step 085930): Train loss 0.791, Val loss 1.776\n",
      "Ep 2 (Step 085935): Train loss 1.250, Val loss 1.773\n",
      "Ep 2 (Step 085940): Train loss 1.390, Val loss 1.770\n",
      "Ep 2 (Step 085945): Train loss 1.088, Val loss 1.768\n",
      "Ep 2 (Step 085950): Train loss 0.907, Val loss 1.766\n",
      "Ep 2 (Step 085955): Train loss 1.154, Val loss 1.765\n",
      "Ep 2 (Step 085960): Train loss 1.012, Val loss 1.766\n",
      "Ep 2 (Step 085965): Train loss 0.959, Val loss 1.767\n",
      "Ep 2 (Step 085970): Train loss 0.896, Val loss 1.769\n",
      "Ep 2 (Step 085975): Train loss 0.773, Val loss 1.770\n",
      "Ep 2 (Step 085980): Train loss 0.883, Val loss 1.771\n",
      "Ep 2 (Step 085985): Train loss 1.062, Val loss 1.771\n",
      "Ep 2 (Step 085990): Train loss 0.798, Val loss 1.770\n",
      "Ep 2 (Step 085995): Train loss 0.836, Val loss 1.768\n",
      "Ep 2 (Step 086000): Train loss 0.947, Val loss 1.769\n",
      "Ep 2 (Step 086005): Train loss 0.781, Val loss 1.769\n",
      "Ep 2 (Step 086010): Train loss 0.960, Val loss 1.769\n",
      "Ep 2 (Step 086015): Train loss 0.954, Val loss 1.770\n",
      "Ep 2 (Step 086020): Train loss 1.083, Val loss 1.772\n",
      "Ep 2 (Step 086025): Train loss 0.924, Val loss 1.774\n",
      "Ep 2 (Step 086030): Train loss 0.785, Val loss 1.776\n",
      "Ep 2 (Step 086035): Train loss 1.161, Val loss 1.778\n",
      "Ep 2 (Step 086040): Train loss 1.122, Val loss 1.779\n",
      "Ep 2 (Step 086045): Train loss 1.144, Val loss 1.779\n",
      "Ep 2 (Step 086050): Train loss 1.109, Val loss 1.778\n",
      "Ep 2 (Step 086055): Train loss 1.028, Val loss 1.778\n",
      "Ep 2 (Step 086060): Train loss 1.025, Val loss 1.776\n",
      "Ep 2 (Step 086065): Train loss 1.029, Val loss 1.775\n",
      "Ep 2 (Step 086070): Train loss 1.007, Val loss 1.775\n",
      "Ep 2 (Step 086075): Train loss 1.355, Val loss 1.776\n",
      "Ep 2 (Step 086080): Train loss 1.014, Val loss 1.774\n",
      "Ep 2 (Step 086085): Train loss 1.049, Val loss 1.773\n",
      "Ep 2 (Step 086090): Train loss 1.230, Val loss 1.771\n",
      "Ep 2 (Step 086095): Train loss 1.102, Val loss 1.770\n",
      "Ep 2 (Step 086100): Train loss 1.052, Val loss 1.769\n",
      "Ep 2 (Step 086105): Train loss 1.171, Val loss 1.769\n",
      "Ep 2 (Step 086110): Train loss 0.899, Val loss 1.770\n",
      "Ep 2 (Step 086115): Train loss 1.133, Val loss 1.770\n",
      "Ep 2 (Step 086120): Train loss 0.765, Val loss 1.769\n",
      "Ep 2 (Step 086125): Train loss 1.003, Val loss 1.769\n",
      "Ep 2 (Step 086130): Train loss 0.823, Val loss 1.769\n",
      "Ep 2 (Step 086135): Train loss 0.995, Val loss 1.771\n",
      "Ep 2 (Step 086140): Train loss 0.760, Val loss 1.773\n",
      "Ep 2 (Step 086145): Train loss 1.109, Val loss 1.776\n",
      "Ep 2 (Step 086150): Train loss 0.886, Val loss 1.776\n",
      "Ep 2 (Step 086155): Train loss 0.915, Val loss 1.775\n",
      "Ep 2 (Step 086160): Train loss 0.994, Val loss 1.774\n",
      "Ep 2 (Step 086165): Train loss 1.005, Val loss 1.774\n",
      "Ep 2 (Step 086170): Train loss 1.084, Val loss 1.774\n",
      "Ep 2 (Step 086175): Train loss 1.082, Val loss 1.773\n",
      "Ep 2 (Step 086180): Train loss 0.746, Val loss 1.772\n",
      "Ep 2 (Step 086185): Train loss 0.960, Val loss 1.772\n",
      "Ep 2 (Step 086190): Train loss 0.927, Val loss 1.773\n",
      "Ep 2 (Step 086195): Train loss 0.893, Val loss 1.774\n",
      "Ep 2 (Step 086200): Train loss 1.002, Val loss 1.775\n",
      "Ep 2 (Step 086205): Train loss 0.890, Val loss 1.775\n",
      "Ep 2 (Step 086210): Train loss 1.124, Val loss 1.776\n",
      "Ep 2 (Step 086215): Train loss 0.826, Val loss 1.778\n",
      "Ep 2 (Step 086220): Train loss 0.941, Val loss 1.779\n",
      "Ep 2 (Step 086225): Train loss 0.963, Val loss 1.779\n",
      "Ep 2 (Step 086230): Train loss 0.988, Val loss 1.781\n",
      "Ep 2 (Step 086235): Train loss 0.803, Val loss 1.783\n",
      "Ep 2 (Step 086240): Train loss 0.997, Val loss 1.786\n",
      "Ep 2 (Step 086245): Train loss 1.134, Val loss 1.788\n",
      "Ep 2 (Step 086250): Train loss 0.877, Val loss 1.789\n",
      "Ep 2 (Step 086255): Train loss 1.082, Val loss 1.789\n",
      "Ep 2 (Step 086260): Train loss 0.737, Val loss 1.789\n",
      "Ep 2 (Step 086265): Train loss 1.177, Val loss 1.789\n",
      "Ep 2 (Step 086270): Train loss 0.841, Val loss 1.789\n",
      "Ep 2 (Step 086275): Train loss 1.491, Val loss 1.790\n",
      "Ep 2 (Step 086280): Train loss 0.734, Val loss 1.790\n",
      "Ep 2 (Step 086285): Train loss 0.874, Val loss 1.790\n",
      "Ep 2 (Step 086290): Train loss 0.956, Val loss 1.789\n",
      "Ep 2 (Step 086295): Train loss 1.183, Val loss 1.788\n",
      "Ep 2 (Step 086300): Train loss 0.903, Val loss 1.787\n",
      "Ep 2 (Step 086305): Train loss 1.124, Val loss 1.787\n",
      "Ep 2 (Step 086310): Train loss 0.867, Val loss 1.786\n",
      "Ep 2 (Step 086315): Train loss 0.926, Val loss 1.785\n",
      "Ep 2 (Step 086320): Train loss 0.946, Val loss 1.785\n",
      "Ep 2 (Step 086325): Train loss 0.789, Val loss 1.785\n",
      "Ep 2 (Step 086330): Train loss 0.827, Val loss 1.785\n",
      "Ep 2 (Step 086335): Train loss 0.978, Val loss 1.786\n",
      "Ep 2 (Step 086340): Train loss 0.995, Val loss 1.786\n",
      "Ep 2 (Step 086345): Train loss 1.198, Val loss 1.786\n",
      "Ep 2 (Step 086350): Train loss 0.993, Val loss 1.787\n",
      "Ep 2 (Step 086355): Train loss 0.941, Val loss 1.788\n",
      "Ep 2 (Step 086360): Train loss 1.102, Val loss 1.787\n",
      "Ep 2 (Step 086365): Train loss 0.950, Val loss 1.787\n",
      "Ep 2 (Step 086370): Train loss 0.943, Val loss 1.787\n",
      "Ep 2 (Step 086375): Train loss 0.870, Val loss 1.786\n",
      "Ep 2 (Step 086380): Train loss 0.753, Val loss 1.785\n",
      "Ep 2 (Step 086385): Train loss 0.894, Val loss 1.786\n",
      "Ep 2 (Step 086390): Train loss 0.928, Val loss 1.787\n",
      "Ep 2 (Step 086395): Train loss 0.864, Val loss 1.787\n",
      "Ep 2 (Step 086400): Train loss 0.947, Val loss 1.786\n",
      "Ep 2 (Step 086405): Train loss 0.893, Val loss 1.786\n",
      "Ep 2 (Step 086410): Train loss 0.879, Val loss 1.785\n",
      "Ep 2 (Step 086415): Train loss 1.457, Val loss 1.784\n",
      "Ep 2 (Step 086420): Train loss 0.712, Val loss 1.783\n",
      "Ep 2 (Step 086425): Train loss 1.299, Val loss 1.781\n",
      "Ep 2 (Step 086430): Train loss 1.085, Val loss 1.780\n",
      "Ep 2 (Step 086435): Train loss 1.083, Val loss 1.779\n",
      "Ep 2 (Step 086440): Train loss 0.991, Val loss 1.779\n",
      "Ep 2 (Step 086445): Train loss 1.252, Val loss 1.779\n",
      "Ep 2 (Step 086450): Train loss 0.931, Val loss 1.780\n",
      "Ep 2 (Step 086455): Train loss 0.769, Val loss 1.778\n",
      "Ep 2 (Step 086460): Train loss 1.032, Val loss 1.778\n",
      "Ep 2 (Step 086465): Train loss 1.064, Val loss 1.778\n",
      "Ep 2 (Step 086470): Train loss 0.976, Val loss 1.777\n",
      "Ep 2 (Step 086475): Train loss 0.871, Val loss 1.777\n",
      "Ep 2 (Step 086480): Train loss 0.671, Val loss 1.777\n",
      "Ep 2 (Step 086485): Train loss 1.289, Val loss 1.776\n",
      "Ep 2 (Step 086490): Train loss 1.044, Val loss 1.775\n",
      "Ep 2 (Step 086495): Train loss 0.921, Val loss 1.775\n",
      "Ep 2 (Step 086500): Train loss 1.088, Val loss 1.775\n",
      "Ep 2 (Step 086505): Train loss 0.860, Val loss 1.774\n",
      "Ep 2 (Step 086510): Train loss 1.111, Val loss 1.773\n",
      "Ep 2 (Step 086515): Train loss 1.004, Val loss 1.773\n",
      "Ep 2 (Step 086520): Train loss 1.036, Val loss 1.774\n",
      "Ep 2 (Step 086525): Train loss 0.884, Val loss 1.776\n",
      "Ep 2 (Step 086530): Train loss 0.899, Val loss 1.777\n",
      "Ep 2 (Step 086535): Train loss 1.197, Val loss 1.778\n",
      "Ep 2 (Step 086540): Train loss 1.007, Val loss 1.779\n",
      "Ep 2 (Step 086545): Train loss 1.063, Val loss 1.779\n",
      "Ep 2 (Step 086550): Train loss 1.087, Val loss 1.777\n",
      "Ep 2 (Step 086555): Train loss 0.806, Val loss 1.776\n",
      "Ep 2 (Step 086560): Train loss 0.955, Val loss 1.776\n",
      "Ep 2 (Step 086565): Train loss 0.874, Val loss 1.777\n",
      "Ep 2 (Step 086570): Train loss 0.860, Val loss 1.778\n",
      "Ep 2 (Step 086575): Train loss 1.095, Val loss 1.778\n",
      "Ep 2 (Step 086580): Train loss 0.799, Val loss 1.780\n",
      "Ep 2 (Step 086585): Train loss 0.727, Val loss 1.782\n",
      "Ep 2 (Step 086590): Train loss 0.795, Val loss 1.782\n",
      "Ep 2 (Step 086595): Train loss 0.794, Val loss 1.782\n",
      "Ep 2 (Step 086600): Train loss 1.043, Val loss 1.781\n",
      "Ep 2 (Step 086605): Train loss 1.089, Val loss 1.780\n",
      "Ep 2 (Step 086610): Train loss 1.089, Val loss 1.779\n",
      "Ep 2 (Step 086615): Train loss 1.101, Val loss 1.779\n",
      "Ep 2 (Step 086620): Train loss 0.806, Val loss 1.781\n",
      "Ep 2 (Step 086625): Train loss 0.792, Val loss 1.782\n",
      "Ep 2 (Step 086630): Train loss 0.878, Val loss 1.783\n",
      "Ep 2 (Step 086635): Train loss 1.254, Val loss 1.783\n",
      "Ep 2 (Step 086640): Train loss 0.752, Val loss 1.782\n",
      "Ep 2 (Step 086645): Train loss 1.344, Val loss 1.783\n",
      "Ep 2 (Step 086650): Train loss 0.977, Val loss 1.783\n",
      "Ep 2 (Step 086655): Train loss 1.084, Val loss 1.785\n",
      "Ep 2 (Step 086660): Train loss 1.123, Val loss 1.786\n",
      "Ep 2 (Step 086665): Train loss 1.001, Val loss 1.786\n",
      "Ep 2 (Step 086670): Train loss 0.872, Val loss 1.786\n",
      "Ep 2 (Step 086675): Train loss 1.307, Val loss 1.786\n",
      "Ep 2 (Step 086680): Train loss 0.856, Val loss 1.783\n",
      "Ep 2 (Step 086685): Train loss 1.093, Val loss 1.782\n",
      "Ep 2 (Step 086690): Train loss 1.028, Val loss 1.781\n",
      "Ep 2 (Step 086695): Train loss 0.877, Val loss 1.781\n",
      "Ep 2 (Step 086700): Train loss 0.889, Val loss 1.781\n",
      "Ep 2 (Step 086705): Train loss 1.145, Val loss 1.782\n",
      "Ep 2 (Step 086710): Train loss 0.916, Val loss 1.782\n",
      "Ep 2 (Step 086715): Train loss 1.100, Val loss 1.784\n",
      "Ep 2 (Step 086720): Train loss 0.837, Val loss 1.785\n",
      "Ep 2 (Step 086725): Train loss 0.948, Val loss 1.785\n",
      "Ep 2 (Step 086730): Train loss 1.107, Val loss 1.786\n",
      "Ep 2 (Step 086735): Train loss 1.003, Val loss 1.786\n",
      "Ep 2 (Step 086740): Train loss 1.240, Val loss 1.786\n",
      "Ep 2 (Step 086745): Train loss 0.841, Val loss 1.785\n",
      "Ep 2 (Step 086750): Train loss 0.941, Val loss 1.784\n",
      "Ep 2 (Step 086755): Train loss 0.868, Val loss 1.785\n",
      "Ep 2 (Step 086760): Train loss 0.918, Val loss 1.789\n",
      "Ep 2 (Step 086765): Train loss 0.918, Val loss 1.792\n",
      "Ep 2 (Step 086770): Train loss 0.657, Val loss 1.793\n",
      "Ep 2 (Step 086775): Train loss 1.195, Val loss 1.794\n",
      "Ep 2 (Step 086780): Train loss 0.987, Val loss 1.795\n",
      "Ep 2 (Step 086785): Train loss 1.141, Val loss 1.796\n",
      "Ep 2 (Step 086790): Train loss 0.931, Val loss 1.797\n",
      "Ep 2 (Step 086795): Train loss 1.227, Val loss 1.798\n",
      "Ep 2 (Step 086800): Train loss 0.895, Val loss 1.798\n",
      "Ep 2 (Step 086805): Train loss 0.662, Val loss 1.797\n",
      "Ep 2 (Step 086810): Train loss 1.134, Val loss 1.796\n",
      "Ep 2 (Step 086815): Train loss 0.823, Val loss 1.795\n",
      "Ep 2 (Step 086820): Train loss 0.806, Val loss 1.795\n",
      "Ep 2 (Step 086825): Train loss 0.794, Val loss 1.793\n",
      "Ep 2 (Step 086830): Train loss 1.006, Val loss 1.791\n",
      "Ep 2 (Step 086835): Train loss 1.146, Val loss 1.791\n",
      "Ep 2 (Step 086840): Train loss 1.004, Val loss 1.791\n",
      "Ep 2 (Step 086845): Train loss 1.048, Val loss 1.790\n",
      "Ep 2 (Step 086850): Train loss 1.076, Val loss 1.790\n",
      "Ep 2 (Step 086855): Train loss 0.963, Val loss 1.791\n",
      "Ep 2 (Step 086860): Train loss 0.936, Val loss 1.791\n",
      "Ep 2 (Step 086865): Train loss 0.907, Val loss 1.790\n",
      "Ep 2 (Step 086870): Train loss 1.175, Val loss 1.788\n",
      "Ep 2 (Step 086875): Train loss 1.009, Val loss 1.785\n",
      "Ep 2 (Step 086880): Train loss 1.098, Val loss 1.783\n",
      "Ep 2 (Step 086885): Train loss 0.754, Val loss 1.783\n",
      "Ep 2 (Step 086890): Train loss 1.031, Val loss 1.783\n",
      "Ep 2 (Step 086895): Train loss 1.098, Val loss 1.783\n",
      "Ep 2 (Step 086900): Train loss 0.976, Val loss 1.784\n",
      "Ep 2 (Step 086905): Train loss 0.816, Val loss 1.784\n",
      "Ep 2 (Step 086910): Train loss 0.766, Val loss 1.783\n",
      "Ep 2 (Step 086915): Train loss 0.932, Val loss 1.782\n",
      "Ep 2 (Step 086920): Train loss 0.837, Val loss 1.781\n",
      "Ep 2 (Step 086925): Train loss 1.063, Val loss 1.781\n",
      "Ep 2 (Step 086930): Train loss 0.890, Val loss 1.781\n",
      "Ep 2 (Step 086935): Train loss 0.984, Val loss 1.780\n",
      "Ep 2 (Step 086940): Train loss 0.969, Val loss 1.778\n",
      "Ep 2 (Step 086945): Train loss 1.018, Val loss 1.778\n",
      "Ep 2 (Step 086950): Train loss 0.932, Val loss 1.777\n",
      "Ep 2 (Step 086955): Train loss 0.919, Val loss 1.778\n",
      "Ep 2 (Step 086960): Train loss 0.900, Val loss 1.779\n",
      "Ep 2 (Step 086965): Train loss 0.741, Val loss 1.780\n",
      "Ep 2 (Step 086970): Train loss 1.085, Val loss 1.779\n",
      "Ep 2 (Step 086975): Train loss 0.984, Val loss 1.778\n",
      "Ep 2 (Step 086980): Train loss 1.000, Val loss 1.776\n",
      "Ep 2 (Step 086985): Train loss 0.990, Val loss 1.774\n",
      "Ep 2 (Step 086990): Train loss 0.874, Val loss 1.774\n",
      "Ep 2 (Step 086995): Train loss 0.846, Val loss 1.775\n",
      "Ep 2 (Step 087000): Train loss 0.960, Val loss 1.776\n",
      "Ep 2 (Step 087005): Train loss 1.045, Val loss 1.776\n",
      "Ep 2 (Step 087010): Train loss 1.179, Val loss 1.777\n",
      "Ep 2 (Step 087015): Train loss 0.781, Val loss 1.777\n",
      "Ep 2 (Step 087020): Train loss 1.387, Val loss 1.777\n",
      "Ep 2 (Step 087025): Train loss 1.017, Val loss 1.777\n",
      "Ep 2 (Step 087030): Train loss 1.064, Val loss 1.776\n",
      "Ep 2 (Step 087035): Train loss 0.853, Val loss 1.776\n",
      "Ep 2 (Step 087040): Train loss 0.976, Val loss 1.775\n",
      "Ep 2 (Step 087045): Train loss 0.899, Val loss 1.775\n",
      "Ep 2 (Step 087050): Train loss 0.904, Val loss 1.775\n",
      "Ep 2 (Step 087055): Train loss 0.936, Val loss 1.774\n",
      "Ep 2 (Step 087060): Train loss 0.980, Val loss 1.775\n",
      "Ep 2 (Step 087065): Train loss 0.621, Val loss 1.776\n",
      "Ep 2 (Step 087070): Train loss 0.876, Val loss 1.777\n",
      "Ep 2 (Step 087075): Train loss 0.913, Val loss 1.779\n",
      "Ep 2 (Step 087080): Train loss 1.086, Val loss 1.781\n",
      "Ep 2 (Step 087085): Train loss 0.740, Val loss 1.783\n",
      "Ep 2 (Step 087090): Train loss 0.972, Val loss 1.785\n",
      "Ep 2 (Step 087095): Train loss 0.862, Val loss 1.785\n",
      "Ep 2 (Step 087100): Train loss 1.024, Val loss 1.784\n",
      "Ep 2 (Step 087105): Train loss 1.077, Val loss 1.785\n",
      "Ep 2 (Step 087110): Train loss 0.879, Val loss 1.787\n",
      "Ep 2 (Step 087115): Train loss 0.894, Val loss 1.788\n",
      "Ep 2 (Step 087120): Train loss 0.956, Val loss 1.788\n",
      "Ep 2 (Step 087125): Train loss 1.011, Val loss 1.788\n",
      "Ep 2 (Step 087130): Train loss 0.835, Val loss 1.786\n",
      "Ep 2 (Step 087135): Train loss 0.961, Val loss 1.785\n",
      "Ep 2 (Step 087140): Train loss 0.811, Val loss 1.785\n",
      "Ep 2 (Step 087145): Train loss 0.791, Val loss 1.786\n",
      "Ep 2 (Step 087150): Train loss 1.028, Val loss 1.785\n",
      "Ep 2 (Step 087155): Train loss 1.012, Val loss 1.782\n",
      "Ep 2 (Step 087160): Train loss 1.029, Val loss 1.781\n",
      "Ep 2 (Step 087165): Train loss 0.912, Val loss 1.780\n",
      "Ep 2 (Step 087170): Train loss 0.700, Val loss 1.780\n",
      "Ep 2 (Step 087175): Train loss 1.139, Val loss 1.780\n",
      "Ep 2 (Step 087180): Train loss 1.164, Val loss 1.781\n",
      "Ep 2 (Step 087185): Train loss 1.304, Val loss 1.780\n",
      "Ep 2 (Step 087190): Train loss 1.022, Val loss 1.782\n",
      "Ep 2 (Step 087195): Train loss 0.969, Val loss 1.783\n",
      "Ep 2 (Step 087200): Train loss 0.987, Val loss 1.784\n",
      "Ep 2 (Step 087205): Train loss 1.089, Val loss 1.785\n",
      "Ep 2 (Step 087210): Train loss 0.987, Val loss 1.785\n",
      "Ep 2 (Step 087215): Train loss 1.015, Val loss 1.786\n",
      "Ep 2 (Step 087220): Train loss 1.191, Val loss 1.785\n",
      "Ep 2 (Step 087225): Train loss 1.194, Val loss 1.784\n",
      "Ep 2 (Step 087230): Train loss 1.139, Val loss 1.784\n",
      "Ep 2 (Step 087235): Train loss 0.954, Val loss 1.783\n",
      "Ep 2 (Step 087240): Train loss 0.994, Val loss 1.782\n",
      "Ep 2 (Step 087245): Train loss 1.108, Val loss 1.781\n",
      "Ep 2 (Step 087250): Train loss 0.675, Val loss 1.780\n",
      "Ep 2 (Step 087255): Train loss 0.706, Val loss 1.778\n",
      "Ep 2 (Step 087260): Train loss 1.120, Val loss 1.777\n",
      "Ep 2 (Step 087265): Train loss 1.025, Val loss 1.777\n",
      "Ep 2 (Step 087270): Train loss 0.959, Val loss 1.776\n",
      "Ep 2 (Step 087275): Train loss 1.248, Val loss 1.776\n",
      "Ep 2 (Step 087280): Train loss 0.798, Val loss 1.777\n",
      "Ep 2 (Step 087285): Train loss 1.172, Val loss 1.777\n",
      "Ep 2 (Step 087290): Train loss 0.918, Val loss 1.776\n",
      "Ep 2 (Step 087295): Train loss 0.682, Val loss 1.774\n",
      "Ep 2 (Step 087300): Train loss 0.922, Val loss 1.774\n",
      "Ep 2 (Step 087305): Train loss 1.081, Val loss 1.773\n",
      "Ep 2 (Step 087310): Train loss 0.833, Val loss 1.774\n",
      "Ep 2 (Step 087315): Train loss 0.861, Val loss 1.774\n",
      "Ep 2 (Step 087320): Train loss 1.051, Val loss 1.774\n",
      "Ep 2 (Step 087325): Train loss 0.740, Val loss 1.774\n",
      "Ep 2 (Step 087330): Train loss 1.028, Val loss 1.773\n",
      "Ep 2 (Step 087335): Train loss 1.029, Val loss 1.773\n",
      "Ep 2 (Step 087340): Train loss 1.116, Val loss 1.773\n",
      "Ep 2 (Step 087345): Train loss 0.877, Val loss 1.775\n",
      "Ep 2 (Step 087350): Train loss 1.020, Val loss 1.775\n",
      "Ep 2 (Step 087355): Train loss 1.061, Val loss 1.776\n",
      "Ep 2 (Step 087360): Train loss 0.940, Val loss 1.776\n",
      "Ep 2 (Step 087365): Train loss 1.104, Val loss 1.776\n",
      "Ep 2 (Step 087370): Train loss 1.068, Val loss 1.777\n",
      "Ep 2 (Step 087375): Train loss 1.211, Val loss 1.778\n",
      "Ep 2 (Step 087380): Train loss 0.930, Val loss 1.779\n",
      "Ep 2 (Step 087385): Train loss 0.920, Val loss 1.780\n",
      "Ep 2 (Step 087390): Train loss 0.892, Val loss 1.779\n",
      "Ep 2 (Step 087395): Train loss 1.314, Val loss 1.778\n",
      "Ep 2 (Step 087400): Train loss 1.003, Val loss 1.777\n",
      "Ep 2 (Step 087405): Train loss 1.019, Val loss 1.776\n",
      "Ep 2 (Step 087410): Train loss 0.920, Val loss 1.775\n",
      "Ep 2 (Step 087415): Train loss 0.693, Val loss 1.775\n",
      "Ep 2 (Step 087420): Train loss 0.921, Val loss 1.775\n",
      "Ep 2 (Step 087425): Train loss 0.877, Val loss 1.776\n",
      "Ep 2 (Step 087430): Train loss 1.039, Val loss 1.777\n",
      "Ep 2 (Step 087435): Train loss 1.286, Val loss 1.778\n",
      "Ep 2 (Step 087440): Train loss 1.111, Val loss 1.778\n",
      "Ep 2 (Step 087445): Train loss 1.147, Val loss 1.778\n",
      "Ep 2 (Step 087450): Train loss 0.842, Val loss 1.779\n",
      "Ep 2 (Step 087455): Train loss 0.902, Val loss 1.779\n",
      "Ep 2 (Step 087460): Train loss 1.054, Val loss 1.779\n",
      "Ep 2 (Step 087465): Train loss 0.964, Val loss 1.778\n",
      "Ep 2 (Step 087470): Train loss 1.085, Val loss 1.778\n",
      "Ep 2 (Step 087475): Train loss 1.066, Val loss 1.780\n",
      "Ep 2 (Step 087480): Train loss 0.938, Val loss 1.781\n",
      "Ep 2 (Step 087485): Train loss 0.987, Val loss 1.781\n",
      "Ep 2 (Step 087490): Train loss 0.761, Val loss 1.782\n",
      "Ep 2 (Step 087495): Train loss 1.063, Val loss 1.783\n",
      "Ep 2 (Step 087500): Train loss 0.689, Val loss 1.784\n",
      "Ep 2 (Step 087505): Train loss 1.107, Val loss 1.784\n",
      "Ep 2 (Step 087510): Train loss 0.918, Val loss 1.783\n",
      "Ep 2 (Step 087515): Train loss 0.953, Val loss 1.783\n",
      "Ep 2 (Step 087520): Train loss 0.872, Val loss 1.782\n",
      "Ep 2 (Step 087525): Train loss 1.136, Val loss 1.782\n",
      "Ep 2 (Step 087530): Train loss 0.848, Val loss 1.782\n",
      "Ep 2 (Step 087535): Train loss 0.885, Val loss 1.783\n",
      "Ep 2 (Step 087540): Train loss 1.141, Val loss 1.783\n",
      "Ep 2 (Step 087545): Train loss 1.410, Val loss 1.783\n",
      "Ep 2 (Step 087550): Train loss 0.780, Val loss 1.785\n",
      "Ep 2 (Step 087555): Train loss 0.918, Val loss 1.786\n",
      "Ep 2 (Step 087560): Train loss 0.994, Val loss 1.788\n",
      "Ep 2 (Step 087565): Train loss 0.809, Val loss 1.789\n",
      "Ep 2 (Step 087570): Train loss 1.070, Val loss 1.789\n",
      "Ep 2 (Step 087575): Train loss 0.805, Val loss 1.789\n",
      "Ep 2 (Step 087580): Train loss 0.830, Val loss 1.789\n",
      "Ep 2 (Step 087585): Train loss 0.943, Val loss 1.788\n",
      "Ep 2 (Step 087590): Train loss 0.971, Val loss 1.788\n",
      "Ep 2 (Step 087595): Train loss 1.027, Val loss 1.788\n",
      "Ep 2 (Step 087600): Train loss 1.099, Val loss 1.788\n",
      "Ep 2 (Step 087605): Train loss 0.966, Val loss 1.789\n",
      "Ep 2 (Step 087610): Train loss 0.963, Val loss 1.790\n",
      "Ep 2 (Step 087615): Train loss 1.090, Val loss 1.790\n",
      "Ep 2 (Step 087620): Train loss 1.114, Val loss 1.790\n",
      "Ep 2 (Step 087625): Train loss 0.882, Val loss 1.790\n",
      "Ep 2 (Step 087630): Train loss 1.108, Val loss 1.790\n",
      "Ep 2 (Step 087635): Train loss 0.838, Val loss 1.791\n",
      "Ep 2 (Step 087640): Train loss 0.933, Val loss 1.791\n",
      "Ep 2 (Step 087645): Train loss 1.080, Val loss 1.790\n",
      "Ep 2 (Step 087650): Train loss 0.878, Val loss 1.789\n",
      "Ep 2 (Step 087655): Train loss 1.082, Val loss 1.788\n",
      "Ep 2 (Step 087660): Train loss 1.355, Val loss 1.787\n",
      "Ep 2 (Step 087665): Train loss 0.982, Val loss 1.785\n",
      "Ep 2 (Step 087670): Train loss 1.027, Val loss 1.784\n",
      "Ep 2 (Step 087675): Train loss 0.927, Val loss 1.783\n",
      "Ep 2 (Step 087680): Train loss 0.952, Val loss 1.781\n",
      "Ep 2 (Step 087685): Train loss 0.930, Val loss 1.782\n",
      "Ep 2 (Step 087690): Train loss 0.840, Val loss 1.781\n",
      "Ep 2 (Step 087695): Train loss 0.846, Val loss 1.780\n",
      "Ep 2 (Step 087700): Train loss 0.891, Val loss 1.778\n",
      "Ep 2 (Step 087705): Train loss 0.987, Val loss 1.779\n",
      "Ep 2 (Step 087710): Train loss 1.072, Val loss 1.778\n",
      "Ep 2 (Step 087715): Train loss 0.945, Val loss 1.779\n",
      "Ep 2 (Step 087720): Train loss 1.104, Val loss 1.779\n",
      "Ep 2 (Step 087725): Train loss 0.922, Val loss 1.779\n",
      "Ep 2 (Step 087730): Train loss 1.001, Val loss 1.779\n",
      "Ep 2 (Step 087735): Train loss 0.874, Val loss 1.779\n",
      "Ep 2 (Step 087740): Train loss 0.806, Val loss 1.777\n",
      "Ep 2 (Step 087745): Train loss 0.891, Val loss 1.776\n",
      "Ep 2 (Step 087750): Train loss 0.901, Val loss 1.776\n",
      "Ep 2 (Step 087755): Train loss 0.796, Val loss 1.777\n",
      "Ep 2 (Step 087760): Train loss 1.080, Val loss 1.776\n",
      "Ep 2 (Step 087765): Train loss 0.948, Val loss 1.775\n",
      "Ep 2 (Step 087770): Train loss 1.115, Val loss 1.776\n",
      "Ep 2 (Step 087775): Train loss 0.851, Val loss 1.776\n",
      "Ep 2 (Step 087780): Train loss 1.209, Val loss 1.776\n",
      "Ep 2 (Step 087785): Train loss 1.286, Val loss 1.775\n",
      "Ep 2 (Step 087790): Train loss 0.960, Val loss 1.775\n",
      "Ep 2 (Step 087795): Train loss 1.054, Val loss 1.776\n",
      "Ep 2 (Step 087800): Train loss 0.990, Val loss 1.778\n",
      "Ep 2 (Step 087805): Train loss 1.084, Val loss 1.780\n",
      "Ep 2 (Step 087810): Train loss 0.986, Val loss 1.781\n",
      "Ep 2 (Step 087815): Train loss 1.083, Val loss 1.783\n",
      "Ep 2 (Step 087820): Train loss 0.961, Val loss 1.784\n",
      "Ep 2 (Step 087825): Train loss 1.165, Val loss 1.784\n",
      "Ep 2 (Step 087830): Train loss 0.944, Val loss 1.785\n",
      "Ep 2 (Step 087835): Train loss 1.182, Val loss 1.785\n",
      "Ep 2 (Step 087840): Train loss 0.997, Val loss 1.784\n",
      "Ep 2 (Step 087845): Train loss 1.154, Val loss 1.783\n",
      "Ep 2 (Step 087850): Train loss 0.976, Val loss 1.783\n",
      "Ep 2 (Step 087855): Train loss 0.984, Val loss 1.782\n",
      "Ep 2 (Step 087860): Train loss 1.083, Val loss 1.781\n",
      "Ep 2 (Step 087865): Train loss 0.904, Val loss 1.782\n",
      "Ep 2 (Step 087870): Train loss 0.923, Val loss 1.782\n",
      "Ep 2 (Step 087875): Train loss 0.862, Val loss 1.782\n",
      "Ep 2 (Step 087880): Train loss 0.764, Val loss 1.783\n",
      "Ep 2 (Step 087885): Train loss 0.788, Val loss 1.782\n",
      "Ep 2 (Step 087890): Train loss 0.897, Val loss 1.782\n",
      "Ep 2 (Step 087895): Train loss 1.063, Val loss 1.782\n",
      "Ep 2 (Step 087900): Train loss 0.938, Val loss 1.781\n",
      "Ep 2 (Step 087905): Train loss 1.035, Val loss 1.781\n",
      "Ep 2 (Step 087910): Train loss 1.120, Val loss 1.782\n",
      "Ep 2 (Step 087915): Train loss 0.952, Val loss 1.783\n",
      "Ep 2 (Step 087920): Train loss 1.038, Val loss 1.785\n",
      "Ep 2 (Step 087925): Train loss 0.759, Val loss 1.786\n",
      "Ep 2 (Step 087930): Train loss 1.045, Val loss 1.788\n",
      "Ep 2 (Step 087935): Train loss 0.909, Val loss 1.789\n",
      "Ep 2 (Step 087940): Train loss 0.949, Val loss 1.789\n",
      "Ep 2 (Step 087945): Train loss 1.094, Val loss 1.789\n",
      "Ep 2 (Step 087950): Train loss 0.808, Val loss 1.789\n",
      "Ep 2 (Step 087955): Train loss 0.912, Val loss 1.788\n",
      "Ep 2 (Step 087960): Train loss 0.913, Val loss 1.786\n",
      "Ep 2 (Step 087965): Train loss 1.113, Val loss 1.784\n",
      "Ep 2 (Step 087970): Train loss 0.955, Val loss 1.784\n",
      "Ep 2 (Step 087975): Train loss 1.009, Val loss 1.784\n",
      "Ep 2 (Step 087980): Train loss 0.840, Val loss 1.785\n",
      "Ep 2 (Step 087985): Train loss 1.059, Val loss 1.787\n",
      "Ep 2 (Step 087990): Train loss 1.057, Val loss 1.787\n",
      "Ep 2 (Step 087995): Train loss 0.887, Val loss 1.786\n",
      "Ep 2 (Step 088000): Train loss 0.958, Val loss 1.786\n",
      "Ep 2 (Step 088005): Train loss 1.149, Val loss 1.787\n",
      "Ep 2 (Step 088010): Train loss 0.940, Val loss 1.787\n",
      "Ep 2 (Step 088015): Train loss 1.180, Val loss 1.787\n",
      "Ep 2 (Step 088020): Train loss 1.099, Val loss 1.787\n",
      "Ep 2 (Step 088025): Train loss 0.828, Val loss 1.787\n",
      "Ep 2 (Step 088030): Train loss 1.173, Val loss 1.785\n",
      "Ep 2 (Step 088035): Train loss 1.080, Val loss 1.784\n",
      "Ep 2 (Step 088040): Train loss 0.663, Val loss 1.784\n",
      "Ep 2 (Step 088045): Train loss 1.261, Val loss 1.785\n",
      "Ep 2 (Step 088050): Train loss 1.027, Val loss 1.786\n",
      "Ep 2 (Step 088055): Train loss 1.032, Val loss 1.785\n",
      "Ep 2 (Step 088060): Train loss 0.849, Val loss 1.784\n",
      "Ep 2 (Step 088065): Train loss 1.259, Val loss 1.783\n",
      "Ep 2 (Step 088070): Train loss 0.925, Val loss 1.782\n",
      "Ep 2 (Step 088075): Train loss 0.953, Val loss 1.783\n",
      "Ep 2 (Step 088080): Train loss 1.023, Val loss 1.784\n",
      "Ep 2 (Step 088085): Train loss 0.868, Val loss 1.785\n",
      "Ep 2 (Step 088090): Train loss 0.880, Val loss 1.786\n",
      "Ep 2 (Step 088095): Train loss 1.046, Val loss 1.788\n",
      "Ep 2 (Step 088100): Train loss 0.959, Val loss 1.787\n",
      "Ep 2 (Step 088105): Train loss 0.765, Val loss 1.787\n",
      "Ep 2 (Step 088110): Train loss 1.029, Val loss 1.787\n",
      "Ep 2 (Step 088115): Train loss 0.795, Val loss 1.787\n",
      "Ep 2 (Step 088120): Train loss 0.851, Val loss 1.786\n",
      "Ep 2 (Step 088125): Train loss 0.957, Val loss 1.786\n",
      "Ep 2 (Step 088130): Train loss 0.942, Val loss 1.786\n",
      "Ep 2 (Step 088135): Train loss 1.041, Val loss 1.787\n",
      "Ep 2 (Step 088140): Train loss 0.936, Val loss 1.788\n",
      "Ep 2 (Step 088145): Train loss 0.946, Val loss 1.788\n",
      "Ep 2 (Step 088150): Train loss 0.980, Val loss 1.786\n",
      "Ep 2 (Step 088155): Train loss 1.308, Val loss 1.787\n",
      "Ep 2 (Step 088160): Train loss 1.160, Val loss 1.788\n",
      "Ep 2 (Step 088165): Train loss 0.893, Val loss 1.788\n",
      "Ep 2 (Step 088170): Train loss 0.745, Val loss 1.788\n",
      "Ep 2 (Step 088175): Train loss 0.742, Val loss 1.788\n",
      "Ep 2 (Step 088180): Train loss 1.098, Val loss 1.787\n",
      "Ep 2 (Step 088185): Train loss 0.802, Val loss 1.787\n",
      "Ep 2 (Step 088190): Train loss 1.028, Val loss 1.786\n",
      "Ep 2 (Step 088195): Train loss 0.683, Val loss 1.786\n",
      "Ep 2 (Step 088200): Train loss 0.815, Val loss 1.788\n",
      "Ep 2 (Step 088205): Train loss 0.922, Val loss 1.788\n",
      "Ep 2 (Step 088210): Train loss 0.852, Val loss 1.788\n",
      "Ep 2 (Step 088215): Train loss 0.980, Val loss 1.788\n",
      "Ep 2 (Step 088220): Train loss 0.807, Val loss 1.786\n",
      "Ep 2 (Step 088225): Train loss 0.738, Val loss 1.784\n",
      "Ep 2 (Step 088230): Train loss 1.010, Val loss 1.783\n",
      "Ep 2 (Step 088235): Train loss 0.732, Val loss 1.782\n",
      "Ep 2 (Step 088240): Train loss 0.816, Val loss 1.780\n",
      "Ep 2 (Step 088245): Train loss 1.019, Val loss 1.780\n",
      "Ep 2 (Step 088250): Train loss 0.806, Val loss 1.778\n",
      "Ep 2 (Step 088255): Train loss 0.857, Val loss 1.776\n",
      "Ep 2 (Step 088260): Train loss 1.092, Val loss 1.775\n",
      "Ep 2 (Step 088265): Train loss 0.950, Val loss 1.774\n",
      "Ep 2 (Step 088270): Train loss 1.068, Val loss 1.773\n",
      "Ep 2 (Step 088275): Train loss 0.939, Val loss 1.772\n",
      "Ep 2 (Step 088280): Train loss 1.091, Val loss 1.772\n",
      "Ep 2 (Step 088285): Train loss 1.001, Val loss 1.772\n",
      "Ep 2 (Step 088290): Train loss 0.837, Val loss 1.772\n",
      "Ep 2 (Step 088295): Train loss 0.950, Val loss 1.773\n",
      "Ep 2 (Step 088300): Train loss 1.083, Val loss 1.775\n",
      "Ep 2 (Step 088305): Train loss 0.980, Val loss 1.776\n",
      "Ep 2 (Step 088310): Train loss 0.933, Val loss 1.778\n",
      "Ep 2 (Step 088315): Train loss 0.748, Val loss 1.778\n",
      "Ep 2 (Step 088320): Train loss 1.063, Val loss 1.778\n",
      "Ep 2 (Step 088325): Train loss 0.981, Val loss 1.776\n",
      "Ep 2 (Step 088330): Train loss 1.196, Val loss 1.775\n",
      "Ep 2 (Step 088335): Train loss 1.030, Val loss 1.773\n",
      "Ep 2 (Step 088340): Train loss 1.068, Val loss 1.773\n",
      "Ep 2 (Step 088345): Train loss 0.831, Val loss 1.773\n",
      "Ep 2 (Step 088350): Train loss 0.993, Val loss 1.774\n",
      "Ep 2 (Step 088355): Train loss 0.913, Val loss 1.775\n",
      "Ep 2 (Step 088360): Train loss 0.871, Val loss 1.776\n",
      "Ep 2 (Step 088365): Train loss 1.092, Val loss 1.776\n",
      "Ep 2 (Step 088370): Train loss 0.950, Val loss 1.777\n",
      "Ep 2 (Step 088375): Train loss 0.889, Val loss 1.779\n",
      "Ep 2 (Step 088380): Train loss 0.905, Val loss 1.779\n",
      "Ep 2 (Step 088385): Train loss 1.075, Val loss 1.778\n",
      "Ep 2 (Step 088390): Train loss 1.158, Val loss 1.777\n",
      "Ep 2 (Step 088395): Train loss 0.906, Val loss 1.777\n",
      "Ep 2 (Step 088400): Train loss 0.847, Val loss 1.776\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Explain how using transitional words help in writing  ### Input: \"<noinput>\"  ### Response: Using transitional words helps in writing by allowing the writer to move from one word to another without changing the meaning of the original word. Transitional words can help the writer to create a more vivid and vivid writing experience.<|endoftext|>\n",
      "Training completed in 692.56 minutes.\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")\n",
    "\n",
    "# 模型训练\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5 / 8.0, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABaqUlEQVR4nO2dB3hTZRuGH1pKyyhlj7L33nuDbBBZCirKUFCWgqD+IlNkqMhwIIgDVEDAwd57I3vvvfdoy2hLm/96vvSkSZqkSZM0afveXOdqzskZX8LJeb93p9LpdDoIgiAIguCV+Hh6AIIgCIIgWEcEtSAIgiB4MSKoBUEQBMGLEUEtCIIgCF6MCGpBEARB8GJEUAuCIAiCFyOCWhAEQRC8GBHUgiAIguDFiKAWBEEQBC9GBLUgCIIgeDEiqAVBEATBjC1btqBNmzYIDg5GqlSpsGjRIjgKK3R//fXXKF68OPz9/ZEnTx6MHTvW4fOIoBaEZMTFixfVQ+XgwYOeHoogJGkeP36MChUqYOrUqQk+x4ABA/Dzzz8rYX3y5EksWbIE1atXd/g8qRM8AkEQ3AIFrS1GjhyJUaNGJdp4BCEl0rJlS7VYIzw8HEOHDsWff/6Jhw8fomzZsvjyyy/RsGFD9f6JEycwbdo0HD16FCVKlFDbChUqlKCxiKAWBC/jxo0bhtfz58/HiBEjcOrUKcO2DBkyeGhkgiBo9O/fH8ePH8e8efOUeXzhwoVo0aIFjhw5gmLFimHp0qUoXLgwli1bprbTDN6kSRN89dVXyJIlCxxBTN+C4GXkypXLsAQFBSkNW1vPkSMHJk2ahLx58yqfV8WKFbFq1Sqr54qKisJbb72FkiVL4vLly2rb4sWLUblyZQQEBKgHyWeffYbnz58bjuH1aK5r37490qVLpx46NNlpPHjwAF26dEH27NmRNm1a9f7MmTOtjuHvv/9GuXLl1L5Zs2ZVDyuaFTV4rVKlSqnxcJw//PCDyfFXrlxBp06dkClTJvWAa9u2rTLxa3Tv3h3t2rVT5sXcuXOra/Tr1w+RkZEJ+PYFIX74W+I9/9dff6FevXooUqQIPvzwQ9StW9fwWzh//jwuXbqk9vn9998xa9Ys7Nu3Dy+//DIchv2oBUHwTmbOnKkLCgoyrE+aNEmXMWNG3Z9//qk7efKk7uOPP9b5+fnpTp8+rd6/cOEC+8vrDhw4oHv27Jmuffv2ukqVKulu376t3t+yZYs6ftasWbpz587p1qxZoytYsKBu1KhRhmvw+Lx58+rmzp2rO3PmjO7999/XZciQQXfv3j31fr9+/XQVK1bU7dmzR11v7dq1uiVLllgc//Xr13WpU6dW4+a+hw8f1k2dOlUXGhqq3p89e7Yud+7cun/++Ud3/vx59TdLlixqfCQiIkJXqlQp3VtvvaWOPX78uO7111/XlShRQhceHq726datm/pMvXv31p04cUK3dOlSXbp06XQzZsxw2/+LkLIAoFu4cKFhfdmyZWpb+vTpTRbe6506dVL79OrVS+1z6tQpw3H79u1T2/jbdej6LvwsgiC4WVAHBwfrxo4da7JPtWrVdH379jUR1Fu3btU1btxYV7duXd3Dhw8N+3LbuHHjTI7/448/lLDU4PHDhg0zrIeFhaltK1euVOtt2rTR9ejRw67xaw+mixcvWny/SJEiakJgzOeff66rVauWYWwUytHR0Yb3KaDTpk2rW716tUFQFyhQQPf8+XPDPq+88oquc+fOdo1REBwV1PPmzdP5+voqgcvJrPFy48YNtc+IESOU4DbmyZMn6lycIDuC+KgFIYkQEhKC69evo06dOibbuX7o0CGTba+99poyj2/YsEGZnDW43/bt201SRGgef/bsGZ48eaJM3aR8+fKG99OnT4+MGTPi9u3bar1Pnz7o2LEj9u/fj2bNmimzc+3atS2OmVGzjRs3Vqbv5s2bq/1p+sucObMyf587dw5vv/02evXqZTiGZnia/LXxnj17FoGBgSbn5Xh5rEaZMmXg6+trWKcJnL5CQXAHlSpVUr8b/iZo+rYEf5e8l3mf0jROTp8+rf4WKFDAoeuJoBaEZEirVq0we/Zs7Ny5Ey+88IJhe1hYmPJJd+jQIc4x9BFr+Pn5mbxHv3V0dLR6zUhY+t5WrFiBtWvXKkFMnzB9xOZQeHKfHTt2YM2aNfjuu+9UpOx///1nmBT89NNPqFGjRpzjtPFWqVIFc+bMiXNu+sjtGa8gJATee5wkaly4cEGlPTJOgnnRjNPo2rUrJk6cqAT3nTt3sH79ejXJbd26tYrFYCwIY0SmTJmi7kf+Tpo2baqOdwiH9G9BELzS9E2/sbmP+ttvv1V+s02bNhn2rV27tvL3OmLmIxwDx2KJ6dOn6wIDA+36PDRP58mTRzdx4kTD5xk9erTV/elnzpw5s+7Ro0dW96Hpu23btibbBgwYoGvQoIFdYxIES2zcuFH9FswX3m9a/ATN24zxYJwI3UeMCWEshca1a9d0HTp0UDEeOXPm1HXv3t0Q6+EIolELQhLio48+UnnUNKUx4psRppzlW9I433vvPWWee/HFF7Fy5UoVkcpUL67nz59fmaB9fHyUeZm5nmPGjLFrDDwHtVyam5lLyvQTRm1bgpoztQyavBmxznVqHtr+1O7ff/99ZepmCgvPt3fvXhVZPmjQIKW1TJgwQUV6jx49Wpnzqc3/+++/+Pjjj9W6ILgD5kPr562WoRWH9y8XazBt659//nF6LCKoBSEJQaH26NEjDB48WPnHSpcurVKnmCJliYEDByqTG03hTOOin5iClUKPxRn4sGFKVM+ePe0eQ5o0aTBkyBCVIkX/N310zCW1BH3bLMVI0x997PTN0VSoFZLgdWkCpzDmJIT+cPqzOW7C93j8//73P2WuDw0NVWUYaW7nuQUhJZCKarWnByEIgiAIgmWk4IkgCIIgeDEiqAVBEATBixFBLQiCIAhejAhqQRAEQfBiRFALgiAIghcjgtoJ2FC8YMGCqqITKyvt3r3b00NKlowfPx7VqlVTZSSZi8uSlcZtHwX38sUXX6hKX1rKlOAerl27hjfeeEN1/2LaG9PUmFMuuAfWGBg+fLjqEc3vm7UJPv/8c5u5055CBHUCYZ9gFmRg8QnWPGZNY+aoavWQBdexefNmVXpv165dqhwl2xeygIZxq0TBPezZswc//vijSe1vwfWwwAtrQzOvncVp2OeY+easiS64B9YRmDZtGr7//nucOHFCrbNXNMvcehuSR51AqEFTy+N/MmFRiXz58qlqUJ988omnh5esYWUratYU4PXr1/f0cJJ1rWPWKmZ/aFYtYyU0Fi4RXA+fGWyWsnXrVk8PJcXw4osvImfOnPjll18M29hshto16+R7E6JRJ4CIiAjVAJxF1zVYipHrbIIguBdW5iIsji+4D1oxtOYCgnthdbmqVavilVdeUZNQNnlgsxLBfbDjG8vbah2tWEp327Zthqp53oSUEE0Ad+/eVf4NzsaM4frJkyc9Nq6UAC0X9JXSTFi2bFlPDyfZwpKgdOnQ9C24n/PnzyszLN1pn376qfreWS6W5Vq7devm6eElWytGSEiIKqHLbm18prP9K+vLexsiqIUkp+WxgQRnvoJ7uHLlCgYMGKDiAYxbXwrunYBSox43bpxap0bN+3z69OkiqN3EggULVDObuXPnqgYzbG5DJYCNNLztOxdBnQCyZcumZmC3bt0y2c71XLlyeWxcyZ3+/furhhJs0iBdk9wH3ToMiqR/WoPaBr93xmSww5XWL1pwDblz51YNVoxhhzFXdF4SLMMmMNSqX331VbXOKHt2ZmOWibcJavFRJwCao9jmj/4N4xkx12vVquXRsSVHGO9IIb1w4UJs2LBBpVMI7oOdqY4cOaI0DG2htkeTIF+LkHY9dOWYpxzSd8puY4J7ePLkiYotMob3Np/l3oZo1AmEviTOuvgAq169uoqGZbpQjx49PD20ZGnupnlq8eLFKpf65s2bajt7GDNCU3At/I7N/f9sP8n8XokLcA8ffPCBCm6i6btTp06qJsOMGTPUIriHNm3aKJ80e7PT9H3gwAFMmjQJb731FrwOpmcJCeO7777T5c+fX5cmTRpd9erVdbt27fL0kJIlvE0tLTNnzvT00FIMDRo00A0YMMDTw0jWLF26VFe2bFmdv7+/rmTJkroZM2Z4ekjJmpCQEHVP8xkeEBCgK1y4sG7o0KG68PBwnbchedSCIAiC4MWIj1oQBEEQvBgR1IIgCILgxYigFgRBEAQvRgS1IAiCIHgxIqgFQRAEwYsRQS0IgiAIXowIakEQBEHwYkRQOwFrHo8aNUr9FRIH+c4TF/m+Ex/5zhOX8CTwfUvBEydgizSWsWR/5IwZM3p6OCkC+c4TF/m+Ex/5zhOXkCTwfYtGLQiCIAhejAhqQRAEQfBiUlz3rOfPn6suKTlz5ozT4sxRQkND1d9r164p84ngfuQ7T1zk+0585DtPGd93dHQ0bt26hUqVKiF1atuiOMX5qPfs2aPaUgqCIAiCp2FL02rVqtncJ8Vp1NSktS8nd+7cnh6OIAiCkAK5ceOGUho1mWSLFCeoNXM3hXTevHk9PRxBEAQhBeNjhwtWgskEQRAEwYsRQS0IgiAIXowIakEQBEHwYlKcj1oQBMEWUVFRiIyM9PQwhCSOn58ffH19XXIuEdSCIAgAmKl68+ZNPHz40NNDEZIJmTJlQq5cuZAqVSqnziOC2hnWjwZuHAbqDAAK1fP0aARBcAJNSOfIkQPp0qVz+uEqpOxJ35MnT3D79m217mwqsAhqZ7i2Dzi/CSjfydMjEQTBSXO3JqSzZs3q6eEIyYC0adOqvxTWvK+cMYNLMJlTyIxbEJIDmk+amrQguArtfnI25kEEtStIWVVYBSHZIuZuwRvvJxHUziA/akEQkiEFCxbElClT7N5/06ZNSii5OxBv1qxZKkArpSGC2iWIRi0IQuJD4WhrGTVqVIKbF73zzjt271+7dm1VuzooKChB1xNsI8FkThGjUYvpWxAED0DhqDF//nyMGDECp06dMmzLkCGDSSQyg+bia6lIsmfP7tA40qRJo9KQBPcgGrUziOlbEAQPQuGoLdRmqUVr6ydPnkRgYCBWrlyJKlWqwN/fH9u2bcO5c+fQtm1b1bWJgpwtFtetW2fT9M3z/vzzz2jfvr0KkCpWrBiWLFli1fStmahXr16NUqVKqeu0aNHCZGLx/PlzvP/++2o/Rtr/73//Q7du3dCuXTuHvoNp06ahSJEiarJQokQJ/PHHHyaTE1oV8ufPrz5/cHCwuqbGDz/8oD5LQECA+j5efvlleCMiqF2CaNSCIHgnn3zyCb744gucOHEC5cuXR1hYGFq1aoX169fjwIEDSoC2adMGly9ftnmezz77DJ06dcLhw4fV8V26dMH9+/et7s884q+//loJzi1btqjzf/jhh4b3v/zyS8yZMwczZ87E9u3bERISgkWLFjn02RYuXIgBAwZg8ODBOHr0KN5991306NEDGzduVO//888/mDx5Mn788UecOXNGnb9cuXLqvb179yqhPXr0aGWFWLVqFerXrw9vREzfTiEatSAkV6iNPY2M8si10/r5uiximIKoadOmhvUsWbKgQoUKhvXPP/9cCTxqyP3797d6nu7du+O1115Tr8eNG4dvv/0Wu3fvVoLeEkxJmj59utJ2Cc/NsWh89913GDJkiNLSyffff48VK1Y49Nm+/vprNa6+ffuq9UGDBmHXrl1qe6NGjdTkgNaFJk2aqJKe1KzZA5rwvfTp0+PFF19UlocCBQqgUqVK8EZEULsC8VELQrKDQrr0iNUeufbx0c2RLo1rHs9Vq1Y1WadGTXPw8uXLlSmaJuinT5/Gq1FTG9eggMuYMaOh8pYlaCLXhLRWnUvb/9GjR7h165ZBaBIWBKGJPjo62u7PduLEiThBb3Xq1ME333yjXr/yyivKhF+4cGE1oaAlgNYD+uk5eaFw1t7jopn2vQ0xfTuDYcYrgloQBO+EQtUYmp+pQVMr3rp1Kw4ePKjMwRERETbPQ43UGGr8toSqpf1ppUhM8uXLp8za9EWzUhg1b5q3qe1Ti96/fz/+/PNPNYlgIB4tDd5Y6100aqcQ07cgJFdofqZm66lruwv6g2ku1kzO1LAvXryIxISBbwzeYhqY5hdmRDoFZ8WKFe0+T6lSpdTnYRCaBtdLly5tWKeAphbNpV+/fihZsiSOHDmCypUrK82aZnEuI0eOVIFtGzZsQIcOHeBNiKB2hsINgfTZgCyFPT0SQRBcDDVAV5mfvQlGOf/7779KcPEzDh8+3CFzs6t47733MH78eBQtWlQJT/qsHzx44JBv/qOPPlIBbvQtU9guXbpUfTYtip3R55wA1KhRQ5m0Z8+erQQ3Td7Lli3D+fPn1UQhc+bMyj/O74GR495G8rsLE5Na+gAGQRCEpMKkSZPw1ltvqSIl2bJlU2lRjLhObHhddizr2rWr8k/T19y8eXOHmle0a9dO+aMZPMbo70KFCqko8oYNG6r3qSEz4p1BZhTYNPFTmDMdjO9RqNNf/+zZMzWBoRm8TJky8Dp0HuSHH37QlStXThcYGKiWmjVr6lasWGHzmAULFuhKlCih8/f315UtW1a3fPlyh6555coVOknUX0EQBPL06VPd8ePH1V/BM0RFRemKFy+uGzZsmC4l3FdXHJBFHg0my5s3r5rt7Nu3T+W0vfDCCyoR/9ixYxb337Fjh0oPePvtt1X+H2dTXJg/5xEinwHhYUCUc51RBEEQUhqXLl3CTz/9hNOnTyufcZ8+fXDhwgW8/vrrnh6a1+FRQU0fCcPlaXIoXrw4xo4dqyrYMA/OEjRxMISefgkGETD/jwEBzL/zCH/3AMbnAQ7O9cz1BUEQkig+Pj7Kh8zKaEyporCmb5nPdsFLfdT0H/z11194/PgxatWqZXGfnTt3Kl+DMfRp2KpmEx4erhaN0NBQF45aEARBSGjqFCO0hSQgqDmLomCmM5/aNPP7jEPrjWHgAUP6jeE6t1uDUYUsfecO7rWcjogmzxEUmB7elyIvCIIgJAc8XvCEofBMuP/vv/+Uj4L5cMePH3fZ+VmijlVwtMWV537/7xOoNXEX1p6857JzCoIgCIJXadTseMI8OsLycUyApy+aRdTNYc1Wlp0zhuu22quxYwoXDXekIUgFUUEQBCHZatTmMOHc2KdsDE3k7PhizNq1a636tN1N89BFmOg3Ddnu/OeR6wuCIAjJH49q1DRLt2zZUnU0YZDX3LlzVV9T9jAlTITPkyeP8jMTJrQ3aNAAEydOROvWrTFv3jyV1jVjxgyPjL9U+GFU892OA49f8Mj1BUEQhOSPRwU1O6lQGLODC2u/sjsLhbTWko3dXBjCr8FKOhTmw4YNw6effqrSuhjxXbZsWc98AENPDrF9C4IgCMnQ9P3LL7+oYvA0dVNoM4fOuG8qtWvm2RnDtmXshsJjWOiEedieI0ZSi6AWBCEJw5KbAwcONKwXLFhQtYe0BWty20qNtRdXnccWLBPqSLMPb8PrfNRJCRHPgiB4umgUi0BZgi0sKQQPHz7s8HkZ1Gve59ldwpIWVbpABeuIoHYJIrIFQUh8WE6ZAbVXr16N8x6bU1StWlW5FB0le/bsqttUYsCsHePMHCEuIqidQvpRC4LgOV588UUlVM1dhOwxzUqPFOT37t1TPRIYmEvhyw5S7BJlC3PT95kzZ1Q7yICAAFWQipMDS92wWAqa1yhcuLBqnxkZqe+DwPGx8NShQ4eUls9FG7O56ZtFsNj3ge0o2eXqnXfeUZ9Hg7202eOBHbNy586t9mGfae1a9mYXjR49WvWb4CSBmv6qVasM70dERKB///7q/PzMbIupBTXrdDplHWAQNI8NDg7G+++/j2SdR50sEB+1ICRfIh47foyvP+Ab83iNeg5EhQOpfAC/tPGfN016uy+TOnVqFZBLoTd06FBDL2cKaZZlpoCmkGONCgrSjBkzYvny5XjzzTdRpEgRVK9e3S6h1qFDB1UFkoWpWDjK2J+tERgYqMZBwUVh26tXL7Xt448/RufOnVVMEYWh1iuaAcTmsIQ0y0Iz5Zbmd8Yu9ezZUwlN48nIxo0blRDl37Nnz6rzU9jymvbAWh3MHmK9Dvay/vXXX/HSSy+phlAMUv7222+xZMkSLFiwQAnkK1euqIX8888/mDx5sso6YktMVsbkBMSdiKB2BkODcxHUgpBsGRfs+DGvzALKtNe/PrkU+Ks7UKAu0GN57D5TygFPLFQ1HPXIoUuxt/SECROwefNmQx9mmr07duyohCGXDz/80LD/e++9p7JrKITsEdQUrCdPnlTHUAiTcePGxfErMxvHWCPnNSnMKKipHbNENCcWtgpUMauH5aR///13pE+vn7B8//33yhf/5ZdfGkpIZ86cWW1n7+qSJUuqdF3W2LBXUFMb58Tl1VdfVes8N4U+rQhTp05VGUcU2HXr1lWTH2rUGnyPn6FJkybw8/NTgtye79EZxPTtBLoY07co1IIgeAoKKqauUisk1DAZSEazN6FmzU6DNHlnyZJFCUwKXQocezhx4oRqoKEJaWKpyNT8+fNVFywKMV6Dgtveaxhfq0KFCgYhTerUqaO0emb7aFCTpZDWoHZN7dseWJ3y+vXr6rzGcJ3X18zrLG3NEtc0a69Zs8Yk8+jp06fKvM+JAftTPH/+HO5ENGoXkEo0akFIvnx6PWGmb42SbfTnoOnbmIFH4CoolKkpUxukNk2zNotDEWrbNPVSW6SwphCk6Zp+WFfBzoZdunRRfmiarqnFU5umedkd+Pn5maxT66UwdxVsn8ze2CtXrlQWhU6dOikN+u+//1aTFk4auJ2++r59+xosGubjchWiUTtFjEbt6WEIguA+6DN2dNH804Svuc3YP23rvAmAgoTFoWg6ptmY5nDNX81Wkm3btsUbb7yhtFVqgqdPn7b73OwPTf8s06g0du3aZbLPjh07lHmYfnJGmtNsfOnSJdOPmyaN0u7juxb9vfRVa2zfvl19Nmq3roB+eloHzFtsct24cyP3o+/7p59+UtYC+qbv37+v3qMpn+Z4+rJZ74MTFfrl3YVo1C5ANGpBEDwJTc0UKizLTNMuTbcaFJrUBClM6dudNGmSamZkrZ2wOdQkGc3NzobUHHl+CmRjeA2aualFV6tWTQWs0SRsDP3W1FJpUma0NQPNzNOyqJWPHDlSXYuR1Xfu3FGWAga/mbc4doaPPvpIXYeWBwah0QrBcc2ZM0e9z++I5nQGmnGSwOA8mvQzZcqkgto44ahRo4aKcJ89e7YS3MZ+bFcjGrUTXPYvig1RFfE4bR5PD0UQhBQOzd8PHjxQpmdjfzJ9xTTlcjuDzShwmN5kLxRUFLr0yzJoilHYY8eONdmHEdMffPCBis6m4OOkgOlZxjC4jcVZGjVqpFLKLKWIUfDRf07NlQL/5ZdfRuPGjVXgmCuh33nQoEEYPHiwcgcwGp1R3pxwEE4ivvrqK2Ud4DhYQXPFihXqu6CwppZNnzZz1GkCX7p0qUoTcxepdEwKS0GwMAB9DDTlcFbnDG/N2oMNJ2/jq47l0alaPpeNURCExIWRxtT2ChUqpPJmBcHd95Ujskg0aieITc5KUXMdQRAEIRERQe0CUpZNQhAEQUhMRFA7QY+7E3DCvzuKXdQHIAiCIAiCqxFB7QSpdc+RNlUEUulspxwIgiAIQkIRQe0E87L2RZ1n3+Bc3g6eHoogCIKQTJE8aicI882EawhHZOoMnh6KIAguIIUlwQhJ5H4SjdoVPTkEQUjSaKUfnzx54umhCMmIJzH3k7OlRUWjdoJqYRtQLfVBZL9H03d+Tw9HEIQEwgYPLGShNXZg4Q2tBKcgJESTppDm/cT7yriBSEIQQe0EZZ/sRu3Ua7H/EWvQvuzp4QiC4ARa+0V7uzAJQnxQSNtq65kkBPX48ePx77//ql6nrJXKVm3sC2qr+DrrrPbo0cNkG+vFsgKM5xC/liAkdahBs75zjhw5EBkZ6enhCEkcPz8/pzVprxDUbAvWr18/VUuV/Tw//fRTNGvWDMePHzfpR2oOu5oY9yb1mIlKu64EoAhCsoEPV1c9YAUhyQtqFkI315Y5m923bx/q169v9TgKZleYE5xH2lwKgiAI7sWror4fPXqk/mbJksXmfmFhYaqlGAuas8/qsWPHrO4bHh6u2rJpS2hoqMvHLSEngiAIQrIX1NHR0Rg4cKBqHVa2bFmr+9F//euvv2Lx4sWqDyiPo2+bnUis+cGDgoIMi709WO1DTN+CIAhCChHU9FUfPXpUNR63Ra1atdC1a1fV87RBgwYqGI29TX/88UeL+7OROjV1baH/2/WIoBYEQRDcg1ekZ7HZ+LJly7BlyxaHe0Qzsq5SpUo4e/asxfcZEc5Fg+ZvlyF5loIgCEJy1qiZFE4hvXDhQmzYsEE113aUqKgoHDlyRKVVJDY68U4LgiAIyVmjprl77ty5yt8cGBiImzdvqu30JTOvmtDMnSdPHuVrJqNHj0bNmjVRtGhRPHz4EBMmTMClS5fQs2dPz30Q8VELgiAIyVFQT5s2Tf1t2LChyfaZM2eie/fu6vXly5fh4xOr+D948AC9evVSQj1z5syoUqUKduzY4eIgMXsRjVoQBEFIxoLans4imzZtMlmfPHmyWrwKXbSnRyAIgiAkU7wm6jspcs8vFw5GF8YT/+yeHoogCIKQTBFB7QRrsr6BdhFjcCZPO08PRRAEQUimiKB2ARJLJgiCILgLEdROIP1qBUEQBHcjgtoJXrzzM7b5v4/Sl+d4eiiCIAhCMkUEtROkjwpB3lR3kea56xt9CIIgCAIRQe0E67K+hpfCP8fJ4PaeHoogCIKQTBFB7QQP0uTGYV0RPPHP4emhCIIgCMkUEdROIKFkgiAIQoronpVUKf54L9713YecD58BKOzp4QiCIAjJEBHUTlAubDvq+S3E/nsZAHT09HAEQRCEZIiYvl2CVDwRBEEQ3IMIakEQBEHwYkRQuyKcTGqICoIgCG5CBLUzSNi3IAiC4I2C+sqVK7h69aphfffu3Rg4cCBmzJiBlITOIKlFoxYEQRC8SFC//vrr2Lhxo3p98+ZNNG3aVAnroUOHYvTo0Ug5iEotCIIgeKGgPnr0KKpXr65eL1iwAGXLlsWOHTswZ84czJo1CylOoxYftSAIguBNgjoyMhL+/v7q9bp16/DSSy+p1yVLlsSNGzeQUtCl8lV/fXTPPT0UQRAEIZmSIEFdpkwZTJ8+HVu3bsXatWvRokULtf369evImjUrUgpRqfT1YnyiRVALgiAIXiSov/zyS/z4449o2LAhXnvtNVSoUEFtX7JkicEkbg/jx49HtWrVEBgYiBw5cqBdu3Y4depUvMf99ddfSnsPCAhAuXLlsGLFCngCP12E+ps99LhHri8IgiAkfxIkqCmg7969q5Zff/3VsP2dd95Rmra9bN68Gf369cOuXbuUZk6TerNmzfD48WOrx9AXzsnB22+/jQMHDijhzoV+88Sm/r0F6m/ukEOJfm1BEAQhZZBKp3M8Eurp06fgYenSpVPrly5dwsKFC1GqVCk0b948wYO5c+eO0qwpwOvXr29xn86dOytBvmzZMsO2mjVromLFinZNEphWli9fPpViljdvXjjD6THVUPz5af3KqEdOnUsQBEFIOVx1QBYlqClH27Zt0aFDB/Tu3RsPHz5EjRo14OfnpzTsSZMmoU+fPgka+KNHemGXJUsWq/vs3LkTgwYNMtnGycGiRYss7h8eHq4WjdDQULiKlmHD4YtoPIcvzrvsrIIgCILgpOl7//79qFevnnr9999/I2fOnEqr/v333/Htt98m5JSIjo5WRVPq1Kmj0r2swbxtXs8YrnO7NT94UFCQYSldujRcRRR8EQE/REs+tSAIguBNgvrJkycqAIysWbNGadc+Pj7KBE2BnRDoq6afed68eXAlQ4YMUZq6thw/7trAr76+i3ExoAtwYqlLzysIgiAICRbURYsWVaZm2tZXr16tAsDI7du3kTFjRofP179/f+VzZrWz+Gz1uXLlwq1bt0y2cZ3bLcF8b45JW7QJhqv42G++/sX8N1x6XkEQBEFIsKAeMWIEPvzwQxQsWFClY9WqVcugXVeqVMnu8zAgjUKagWgbNmxAoUKF4j2G11q/fr3JNkaMa2MQBEEQBKR0Qf3yyy/j8uXL2Lt3r9KoNRo3bozJkyc7ZO6ePXs25s6dqzRd+pm5MKpco2vXrsp8rTFgwACsWrUKEydOxMmTJzFq1Cg1Dgp8T/BBRGzg3L/7LmPB3iseGYcgCIKQPElQ1DehqZmL1kWLJmtHip2QadOmGfKyjZk5cya6d++uXnNCQP+3Ru3atZVgHzZsGD799FMUK1ZMmeFtBaC5kzO6PIbXHZaWQ8Fnc9G8dC4EpfPzyHgEQRCE5EXqhEZojxkzRmm1YWFhahs14sGDB6sOWsaC1Rb2pHBv2rQpzrZXXnlFLd7AUV1hk3VfROFpZBSCIIJaEARB8JDpm8L4+++/xxdffKGqg3EZN24cvvvuOwwfPhwpjbPRwYbX5wLeRIbt44An94HrBywf8OASsHUiMLEU8PRh4g1UEARBSBmVyYKDg1UVMK1rlsbixYvRt29fXLt2Dd6KKyuTFfxkufqbAU9wNKCn5Z3y1wZeGAZsGg9c3ApkKwHcNatnLlXNBEEQUhRXHZBFCdKo79+/r5pimMNtfC+lEYZ0COlz0PKbl3cAs1rphTQxF9IA3py2EdHRMfOl40uAm0fcOVxBEAQhCZEgQc1uWTR9m8Nt5cuXR0pElzEvCj6bk6BjL12+AJ/RmYBRQcCCN4HpdYELW4DIZ8B/PwJbJwFPHwC3jrl83ILgFmioO/I3cPukp0ciCCkzmOyrr75C69atsW7dOkP+MmtwU4X3VMtJ7yAV2oaPxh+tAlBzeTYcD3jL9O3X5gMbxwI3D2NQRG8EFq+Hzy52wRb/D+Ke6rc2puvrP4t9/cJwoM4AwNcPeHwPOL4QKP8q4J/B9JjoKGDTF0DZjkCOuBaQRH9wrx0BXN4FdPkLSJvJs+MR3MOxRXo3T7biwIklydu18zwc8PED7Aye9TqeRwA+qb13/BFPgOfP9M85vr7yH7BvFvDiJCBzQaQkEiSoGzRogNOnT2Pq1Kkql5mwjCjbXDIaXKsDnhI5pCuKe6Ua4snyTSpVq2mJrPjp1ZKALhpIlwUo3ACVhy/CfWREh9SZEKnzhV+qKMcusuFz/VKrP7AzxrKxfDAQlB9oPhb4bzoQXCn2vS1f6f9W6wlkLwms/BjouR7IUzlm0POBhe/oz1dvMLDjO2DbJP17lbsBLb8EVg8F/AOBpjEThuhoIPQGMLczcOsIEJAJKFRPHygXXBGoMxDIWgTY/o3enH9tb+z4vywAFG8BNB8HZMwDXNkF5KmiN/lHPgF80wBht4FyLwOXdgKH/gT2/wbkKAM0Gw1kKgBkyAlc2gFkK6a/juBebhzSx1f4BZhu5wN02QfAyeWAb2qgdFvgzingjpEm/U8v/f9t9V7Ak3tA5a76e2nbFODyTv09xonbxnFA7gpAmyn64+6dAxb2Bip1Aaro0zUdmhguHwRUeA3IZ5Y2ykBP/4zAg4tA6HWgUH39byBdVmBOR/0+A48AmfIDkU8Bv7Sxf3m/n15ler4PjuvvR37mnGWAPT8DZ9fF7jf4NBAY05+AE9U1w4HWE4Hcdlgf+Vs8+g9Q/V39+PJW1f++s5cACjcC/nwNCLkOdF8GZC6g/0zpcwDhIcCzECB7ceD8ZuDSdqBaLyBDdv15t00G1o2KvU7BekC3mDLIqRzoXcDvkp+pREu91e/8RqB0O8DHFw7De4T3R+GG+knQuNyW9/umApC3GnB1j369z05g51QgRymgfGdgw2igQF39GPhsOLte/73lrwVEPwfuXwCOLACqv6P/f+NEQCPqOXB+k/4+3Po1UKwZcG6DftL58DLQ5hvH70VPBZNZ49ChQ6hcuTKiohwUPEk8mIwcGtEMFUavUa+rFsiMvZceqNe1CmfFn+/UtHhc+bxByHh9G4JT3cXovHsRcOcI0H2F/gc81nJJVJfi669/iGn+c3ugkHyYsHrubocPG05E+KDZPQMo10kvFPjw5CSJt/qzR4BfOiB1Gv0Plg9U7hMVqbdI/NJMP3Pv8DNQPoEpgA+v6B9YfAjcOKyfcPChwe+bAokTHE0j4MOUD3YKlIxWHkzaT3TOK0DoTeD1eUCQnfcuLS4UrnSj3D6mF0ra9gkxqYW13wcafqIsQkijb12r9uek6fRqYFHv2PNRWLefDhyerxcY7qDiG8DB2XG3F6gDFG+u16rux/Sro+beaChw4yBQ5AX9hG/hu3Fr79fsC+z6QX+PmN/vqdMCz2OLLMWh7qDYias1SrQGTsU+D+LQexswo6FeUFiCn6FkayBtFv19cHAusMjBLoRZigD3z9ne57V5wJ+v2t7nzUX6cfLZ8Ogq8OgaULyZfsLNSRhfU5DxvvzMyDKWozRwm70UUgEj7lvW1Fd8pJ9I1P/QdELAid681+FWshYFHt8FnlnJtCn7MnD0b9vnoAXlrdVA3iqJKotEULtIUB8c0RQVR6+Ns0+p3BmxckA9q8dprBtUH0VzBOLqgyfo9utu9KscgA73ZuhnwzQV88dFIX7sX7353BGoaT+6jBRNq6/1DwnE3O4UmlGx7U8V1FAoYM356Bxw/SBQtLH+4UJNng9UzZ1AzS/isV7gUvNPCHRnXN2rP/+peNxH72zWWy3IL831FglNkFFzavCJXrM6bKHBDcf99L5+QmOs9WrQqlKmPfBzYySYjr/o3S0/1LR8DVukCQQiXNeK1rVQsCTwcRkQpJ8kxgcnd+Gh+kmSO8leCrhzIuETD42gfMAjC9UY63+st+Qx42XPL/rJqSXy1dTfv2kz6zVyS3SerU91pUa8Zljce0q7VnzQ2nN8MZz+fjr9rj+Xt/ejFuLyz37LKWknboSYrD+PirZyBv3scsyyEzh35zEGrX6MDl/8qrZ9u/4M0lZfhV7ZCgMNPkaRlSWRBpEYnPov9OzYBijWFNgwRq+Jc951bKH+gU1ozqIWxXxtmpytzTSpodA8r/G/S8DJZcDifnH3z1Ve+dkN9NkBpMmg12oWdDXdt+EQoNKb+gcc/WGBuYAza4GoCP1MnVqsuebw7hb9dc2j37k97I5eO9030/4fHVnxoem6uZAmFG6WmBCPaZ2fixpI8/FIMHRl2MtvLwHhFh762v/55i+sH0shTawJULpLNJeJIzT9HAjIqH/A5a+h39bvP32MBH3W1HY1jZimX+MJkX8Q8AHdJ0H6uIpfm8eaNRMLTlA4SSr5ov76lv5/sxQC+u3Wm0ADc+utD5z8cYJ3YbPen0ptX4PuH5r2aaHRhHS7acD6z/Umd0vXCDNtOKRo+4P+90ONmb/Re2eApQPi7td+ht5FUaad/junydYcPg94/WpvAxViNOuFfYBDc03342e0RSof/Vjoppgd4y4wtrpogpMxC9aENNEmmaXaAPt/j/v+e/v15mu+T/isoxuC3zOteyVbxT5nyM1Denfb5f+A0i/prT5UVDr+rP8OOeZa7wE7v4trdaH7rko3/d/U/vr/Nz6j+H3ePArU/0j/3MnrWAVOVyAatRNY0owtcfGL1obXny09hpnb4wqE9xsXw6CmxdFj5m5sPHXHcNytkGeoMU7fhOT0mJZIk9rH5LrG59YIfx6FT/89ioYlsqNNhdhiLAZCbgC3juKLs/kwffM5fNWxPDpVy2d1/MWHrkREVDTKpLqA5QMb6U3JtuDDlj9ke31dnCTs+00vmN/4x+S4aZvO4ctVJ61+1i2n72DC8kMY06EiKhTIptdK6YOnOfPSNgtatanAXhdVCSGFWqPD43n6ByL9fH+/HfdYe6AmykmSMTQHcoLBBzmFliUBlCGX3i/PyYOlBzWhxjHgsF7bmmylpzp9ZyrYZrLeb2yJxiP1WQTdluiFDX3F42PL4Crom+WDWrMw0E/X6Q/9BInbsxTWP/AS4oe0B7oDaGLNVy1m/RFwcoXeokGLAH3h9I0yLoMWJs18T/M2H7DMlKjQWT9OQmsHJ4acEERH6v2gd0/rzaBlOugnnZx8WjLV8t7kg5uuE3uh28Dcl0+fKyfjdLlo56VbpGBdIH02vTa5+Su9cNUmUb026E35rgjIsjV+xptc26e30tDfTa2V46VLiL+lEDMlhAKLpmreq/TP87v9vS1wfb9egy7RSn8/f1/VsluBFpd/3tZbtTiBO70SaPkVUOPd2PF4a4CbC3Gb6ZsBY7Z4+PAhNm/eLILaDE3AXL73BPUnWDCtUqlI7YNTY1qi+8zd2GQkqC/de4wGE/RlVE+NaYEr95+iyaTNcc5tzMztF/DZ0uNW37c0flfs5w7iE9Ta2ILS+uHQSH27VYsPfs7CGdDD2337FNw/th61LryNcKRBt1oF8Flbo1rx9FcfXgAUaaR/IFHL2j4F60/dReOHFnxY1JwY1MbIewoRW/D6nIhQ+NCcTmsH1ylMKPz80uvXH1zQm0HTpDc9np/DPH6BD8ugPHEnSxTqSkt9bhowY41TK4HdPwFd/k4RD0qvhPceJ0OeCpDkfUOtsVAD/T2twYkRBTItYtaOo3DXYhzivB8NXN2t10bl3nKv6TsoKCje99ntSojL3/uu4sO/Dll9317lk4LbnHXHb2HQgoN4o2YBpZXfDbNg1nUjt0Oe4ej1R2hYPAd8fByIGHUhIc8irb9JkywXkioV7lToi2rLilvf39cPz8q+qiZPqRipTFp+iX8f7MfbNzsgM0Jw4LVUessCH6xa9Lw9aP/RFKCFG8RuNxfImjZojl9a/NH8EH5fshpndcG48IVZGp8GtV0tBc4eIU0YuctF8Bz8v/JkFgPvG2aHmBOfRYHHWRPS6n0fIL9pUK1gPw4Jana1EhImRCauiVuRzJhUMT5qY+bvuYwahbIa1tt8tw1v1ozrZ+75uz716YdN55Rm6SyPw58jWqdDYIB956r31UaEP4/G5M4V0L6Sc1aKxODeY9sTmfN3wvDCxM1oVzEYU16N21/9ATICFRLXsmDOGZ33f88JZfWxm3j4JAKdq+X39FAEwSsQG0QiUH7UGtx49MzmPuy4te3MXTyJiHUb/O+fIzh2PTYY7fStMAxfbLs62drjtywKfbLyyA28MHETjhud0xx6QsqMXI1yo9bgWaR9LgwKabLl9F2b+/HcETH7uhrXRVoAv2y7oP4uOmga8OMZW0HCeWp0L7kS/j+euhmKKK3srYt594996t6nq0gQBBHUXsUbv/yH3RdMa6X3m7vfoXMwf/v7jWctvtdnzn6cv/MY/W2cMzIq9uF7M57JhaMMnH8QZUauUqZyc45cfaQmGeYR8qduWp9UuIpUjhR48AbsGO+Gk7dQasQqTFl32uWXn7HlPJpP2WLTlZMYlg9BSCmIoE6BUHu3B0v60vcbzuDotfjzQXeeu4fRS4+baOWLD15XE4F5e66o7YsPXsOmU7eVlt3m+23o9ftenL4Vip6/7VVBYkWHroyj1bqCi3fj+vmN8Xq5bYf5gFH/ZMq6My6//Pcb9BPBhQfc2yXPTQq7VWghWH74Bm48slH8RBA8gORRJ3HsNQ/a+/AxFlKWEgK+XnNaLal9UmH30CbIkj4m3cSM137S50fSZz6gSTE8iXhuMlEoOdysDGMMJ2+GYt0JKylKLmDjqdvoPdvUomD+Oa25DpISuoQW5vAiXJg5ahdzd1/G8EVHkcbXB6fHSlCd4D2IRp3EsZbupfHHzouo++UG1BpvofiBBYxF1OR1Z6w+LJ9H61Bz/HqEGkVbH7oatzTfpft67bVhTIoZ+X2HlcIiieAHnr/bQhUl8zGYDeLRk0i3CI05/13Ci99txe1QB10MVlR+tko1tEt1IXRBWC/U4z6sfRTm/jPQ0tVsPa1Pi2TNAHN+2HQWP22JKVuaAuD9znoMgncgGnUyx1Lwmb0yZ+mh6+hSw3rkLU3WfYy0U/q/r9x/gg0nb8fZ93ZorL/xsY0gJx832Z2p0adLk9pm4NW1h09UGVdrOeSty1mpxZ1Ahi7Um6cnrTmNLzo60B7Wwn8gH6wvfrdNReuveL+eyS77Lz9AxbyZ4qTO0dTrG086HV0RzSZvUa/PjWul39+NsynjiQY/iyW3xRcr9Xn1ro4Kt/azuBcWjq9W6bM2utTMb/M+ssW7f+zFs8hozOpRzevjIt7+ba8qJmTLaiYkHqJRC4p/9l1Vf80fIMYC1hLbzt6Nk6o1ckns5ODf/ddw9rb9tZvtTcOesPok/txtWatiXnmnH3eqSYOW7lN6xGrlX7cEP3PLb7agyaQt2G72eYy57kLf5ZB/jzgcM2CLh08icfxGiHId3HscYfJehx924I9dpo1UDlx+gFLDV+Hnrba1xK6/7DYRWO6E/19aYxtrgjr0mZWmFm5Ey2owD7Z0NOVx9bFb2Hz6Dq499H4fOCfbtJotPxJb/vPY9Ud4JyaOREhBgnrLli1o06YNgoOD1cNy0aJFNvfftGmT2s98uXnzZqKNOTlwM+QZasaUJdUY/NchVB2zNo6y9P6fB5y+Xqtv7S/Hycj0+GAw29SN50yEnXleOaPntfe1v/Strzpm+V65GOPrX3b4OtafiGsRIK7UgaxNMixp+nHM7nZoY+bixHjyRD7554gy8Y5ZfiLhhWRczOR1p00EcSK7qE34fedFJVyJK5TfpB8xoJ/wrTl+C6//9F+iXjfkWaTKVOEEPKXiUUH9+PFjVKhQQfW1doRTp07hxo0bhiVHjhxuG2NyFtbm3A0z1cRchatyp9lZzBGtylyztBdrGo89D1uak8/cCnXIp21pV5qAOSFhitWAeQfjP8COc1rDVr68sYzSTplYRltLGrXdx0brMGntaWW+TQgjFh/D58v0ZXhNMBoSi7LQd309CWjI1txBjL9w1LJgXvkwNEaQ0nLlCLO2X1D/R/ExZe0ZLDt8w1DYKSXiUUHdsmVLjBkzBu3bt3foOArmXLlyGRYfqR2bIhi84JBNDYcRuwyeM+5cxiYn9x0S2NbF0HMjs+coMw1V43//HEbTyVvw09bz2HHuriHoiaZma2Z1PujM3QNdfv5P+ZzJkkPXHY74jq+MrHFUOCPwNWFjPqkyd4VQqIckwPzMNLwVRmZUe7AnLo6TmYNX4gYxLj18XXWd6/prrOk+PsznBTRTW8sCYMGXVt9sVb7rV6bvhKehOZr3pCOBiXQH0dWgWQ4SCq1bFKQsVOMIo5YeV/9HF+JJl7zlaLBlMiRJBpNVrFgR4eHhKFu2LEaNGoU6depY3Zf7cdEIDRX/ii084QO0F7b/ZIBbWj/LXZvM/bBE60RmC+PgN1tmziNG+eOzdlxUC6lXLBt+f6u6Emqs6U6+W38WoTEPQPYkb//DDvV6UqcK6FA5b5wxcln2Xl2UzaOvp7/z/D2LY6AgNQ4Q5PdRJjgjMqWLDfh5Eh6/z/vRU1NN6q+9V5EvS1oMWnAIUzpXRLtKeZS2lcpK1TZj6Lu8ExqOhiUsW7ZoXeg+U981bPfQxsgRaNZZClCTGsYzmGvU/LzsGEc4mThwJbZncWRUtGEyc3hUM2QM8EPVMevUJKVrLcstXSnIMqdLAz9fS5N7nUXBbZKyyH86nSr4ouENPucWU7aoic3Z22GY3TOmzaidMAi0XF7bfRxs4XDWghnOThQSkm0R8vQ5+jQ0ranO/1dvDfJLUoI6d+7cmD59OqpWraqE788//4yGDRviv//+U+01LTF+/Hh89tlniT7WpEpoeOL5JB2FD+D3/jwQb6Syo1yOCTojc/9zPO1n65m76mG942yscNWENHl5WqzGRUFYq0hs/XZjNp68bRDU5oxZdhwdq+TFiMX6SHENfh9k//Cmdo+Xn/FWSFyNm2PTKsgxKI1pUOZMWH0qzph7zNIL4fWDG6BI9gxKWLAL3HsvFEVqHx/D5IVQ+FsS1Jb8noPmH8SDJ5EomiMDfulW1dBFzlhQazx8HKkEtWZJ+H1n3EnbyZshaDFlq5rYLH+/HpJKIRZHxsTmOImN2+sO6OCWbIsXy+dGvizpDH7w1t9uRZNSOTGyTTxtfD1AkhLUJUqUUItG7dq1ce7cOUyePBl//PGHxWOGDBmCQYMGGdavXbuG0qWt9PMV8I0bKlm5GnfVmHYGmnXHrYjpI2yGeV6utdKbFEpMPzKPpCc/b7ugFmehqfjThZaD8IyxJKQfPInrQtCEtNbZjYL6k38O4+qDp6petzmO/M/x+yAU/BSwtgSEPQVeWBmPGNfPJ+fuhKmAw8NmdQA05cpcDDmbUx+f1kYN9eLdJ6heyIEe2DGExWMRY51/msg/ahH7HHUGe78LliOmP79rrYJq4uUNPDYqwsT6CmwhPHP7RRHU7qB69erYts16VLG/v79aNEJC3F87Oinzl5H2I9iPNSFtie1Gmrcxv26/oJaEYKxdNvzaVPM0puM0vQneUsR1fFgSlvGlM5mTUBlnKYXNUSultWv3n3tAxTOYw1Qsc5/97F2X8GbNgnAn1cfqMzLm9KyBOkWzOXQsU6psQZ89rQ6u8Kt/vfqUKiNbMpdp7QFL9JmzDwcuP1TWleOjWzh2oVT2m9D5/1g5f2a72+2ysuNXq0+aTJ44mbVm2fIUST4K6+DBg8okLggpmRpm6XYJEaKu4N/9V018+eZQ86Wv+bOlx+Ktue5ItLqtCYC2300refDWAu+4vcqYtSbfGVP8hpm5HywRZuZ3vW+UUWGuhbICmKUcdbpUbJEQzd7SZ42MTtg9weY/dPmst1DgyFLTHWLcHdBudPbt9vpPu/Dy9J3KB23vd/bOH3tVIBxjPTQcDYpL9oI6LCxMCVou5MKFC+r15cuXDWbrrl27GvafMmUKFi9ejLNnz+Lo0aMYOHAgNmzYgH79+nnsMwiCoIfPe83PbQs2X6GJsfOMnSqy2vgh6QgVR681vL5kFGdgDgPO2JnNuMHL+JUnUHjIclXwhYFwtoIrz5hF5K89bjsN6bcdF1F25GqTXPlpRq4E86Y2LK9bZcw6u9O8ePy0TefU52c2gTWYJkg3BOvbN5m0WVWos5YfrRUHcheOTCnY0OfLVSdVnIN5Rz2tb8Hbs/aotsAa360/g0MxkwEGW2qpnPG5CRhIF5/1JrFrznudoN67dy8qVaqkFkJfMl+PGDFCrTNHWhPaJCIiAoMHD0a5cuXQoEEDHDp0COvWrUPjxo099hkEIamw56JpC1VXY0+e6+PwKIOPmAFt3X7dbQiIc4axyy3kPMdAH/e0TaatX3/cfF4FYMVX8CW+ND1LaMVl6Pd+ZbpeCBrXSr9k1khH61W/9YxphoIlvztT0TjxoCBj5D7N9pagcGk3dbvqVNdj5h71HVAgW8NSNL+W8rjqqGNpdYQug69WnVSFhxhA6EhcCRv6cCKy6dQdNakzh7EP1OLZFlhjolk+thYwZo6xzKX2bamuuzH0q7OngT2CP9n6qBmxbWu2MmvWLJP1jz/+WC2CIDiON+T7WvOROwuFfmIFGdryA8/YYhqEt+fiAyWwcwfFjXSPD2YRsHAL/a3/nb+HwtkzYJuZMCfmzTMoIBnrwLriCcFcu2a3uYtftDbZxha18VkVfth0Ti325IDb8gkvN8u/vxUzsXFnRT3WLgiPjDLUeGe2wzev6hVKT5Dkg8kEQRCoXb79W2wEujn7L8ctimIv+y89dCqokNHyxpH89k4n6Oufv/cKgjOlVZYHpiUy5c0Y+ojLjFhtss0e4WgONcvzd8IwfPFRuwR8nGp5Zpy/G2b3tekuYW57o5LeUWEy8nl0nNLJnrZ+i6AWBCFZQFOpO2DQlDOYP+TZ/Yud2LQcXmOT7ovlg022Gdezp8VgioX0yfgive3NrbdVQ6Df3Phr8Buz6IBjcQf04bPkb5b0fvGa1HV2THWsCVZ7vilLwXV73ew2ig8R1IIgCC6A6Ur2wi5z09+ojBZlc1ssk+ttLD/smJ/a0Y5w9GVziY/iw1aarNP3n9pilbmEY8micN0Oc7s7SfLpWYIgCN6ANc2b7UctQd+vOda6uyUVGJHO+vrx4apI6pLDV8WJC7CWY0+LBIPqkiKiUQuCIDhJwU+We3oIXoFWe90WM7dfwHgHCgTZgmZ/e4sNfTD/oMMNboxhp6+8mdKiU7V8SGxEUAuCIAiJxmdLrafSuYozt8JUXnqAny+GLTqCTlXzOSWkCTt9ERHUSYwKeYMMSfaCIAiO8qsL6rcLlivD1f5ig2F9xZGk7VIQH7UTVC6Q2dNDEAQhCTN6mfu1SyHpI4Lam9u7CYIgCCkeEdROkNpXBLUgCILgXkRQO8FbdQp5egiCIAhCMkcEtRNkjqeKjiAIgpC82BZP+1F3IILaCcRHLQiCkLJ4w6hrV2IhgloQBEEQvBgR1E5gqUydIAiCILgSEdROIHJaEARBcDciqAVBEATBixFB7QSpxPYtCIIguBkR1E4gYloQBEFwNyKonUAUakEQBCFZC+otW7agTZs2CA4OVmbkRYsWxXvMpk2bULlyZfj7+6No0aKYNWtWooxVEARBEFKcoH78+DEqVKiAqVOn2rX/hQsX0Lp1azRq1AgHDx7EwIED0bNnT6xevRqegJOLI6OaYccnL3jk+oIgCELyx6P9qFu2bKkWe5k+fToKFSqEiRMnqvVSpUph27ZtmDx5Mpo3bw5PEBjgpxZBEARBQEr3Ue/cuRNNmjQx2UYBze3WCA8PR0hIiGEJDQ11y9i+ebUifH1SoWDWdG45vyAIgpAySVKC+ubNm8iZM6fJNq5TAD99+tTiMePHj0dQUJBhKV26tFvG1rZiHpz6vAVerZ7fLecXBEEQUiZJSlAnhCFDhuDRo0eG5fjx4267VmpfH7xRs4DJtn3DTC0AgiAIgpBsBXWuXLlw69Ytk21cz5gxI9KmTWvxGEaH831tCQwMdOsYM/inxsg2sVp71gz+br2eIAiCkLxJUoK6Vq1aWL9+vcm2tWvXqu3ehHl69fZPXsDU1yt7ZCw1C2exe9+gtBIUJwiC4G14VFCHhYWpNCsuWvoVX1++fNlgtu7atath/969e+P8+fP4+OOPcfLkSfzwww9YsGABPvjgA3gTuYICTNbzZEqL1uVzW93/gybFkSmdH16v4Xr/ds+6he3el8FwgiAIgnfhUUG9d+9eVKpUSS1k0KBB6vWIESPU+o0bNwxCmzA1a/ny5UqLZv4107R+/vlnj6VmWaNZ6Vx4t0FhTH/DPi16QJNi2D+sKca1L4fDo5qZvJc1fRqrxzHCnNHmtsgW6I9OVfPaOXJBEATB2/BoHnXDhg2h0+msvm+p6hiPOXDgALwZH59UGNKylF37vlO/sOEYS5TIFYgd5+4Z1oODAnD90TP1umbhrCrafMA8vUWCFMmeHusHN0TBT5YbtjUplRML9l6Nf9wpUKFO7ZMKz6Ot34OCIAieJkn5qJML+bLEBr4NaVnS7uMW9q2NVR/Ux6J+ddC9dkEMaVXKro5eTUvnxNxeNdC/UVGb5y+SPYNd47j4RWucGN0CZfNkRFLjw2bFTdYndbZtkRAEQTDHloLpDkRQJyI0bb9SJS+WvVcPL1fJi1k9qtlslUmB3K12QfW6btFsqJQ/MzIG+KFivkwY9VIZQ/DX371jg+malzHNM8+WIY26Ru0i2fB+42J4s2YB/Ny1KqoX1AeZFc6eHov71cFXHcura2gEBtg2tqRN46vGER+j25aBu5jSuaKaMNhLq3K50KJsLpNtzhoRaIVgpL8gCIK7kCdMIsJgMS1g7OtXKlg1xWrkz5JOCcPNHzVEcCbL6WekaozQJfWKZVd//3i7Oh4+iUTezLGV0tKk9sHn7cqq1xXyZcKc/y6hU9V86txcX3s8NvVty0eNUOnztcgR6I9vXq2EhQeuYtf5+yiYLb1Dn7lrrYIYsfiYYb1y/kzYf/khEoKx2Z+0rahv5sLPFfE8Ot7jc2WM+x1ynpQujS+eREQlaEzl82ZCoWzpsfDAtXj3ff+Fovh2w1mTbS3L5sLKozcTdG1BEDwDFerE7J4ogtrLSJcmNYa1LqX8plliAskKZHVMOBoLbGtkD/THwCamZuAmpXLgiw7lUDZPEDKnT6Majvin9lWCsFaRrHHMPQ2L58DsXbHBfvb65HvP3h9H011xJK6wGtikGH7ZegGh4c8NJvzfdl4yuAE0a8QLJXJg1bGECztbgpoWibthEXG2l8sThPuPIzDh5fLInSktqhTIjGGLjtq8zgdNi6sJ0du/7TVso8VEBLUgJC10iXw9MX17IT3rFUbvBkUcOqZwtvQqvap83qAEX5eCjyVQKagJm41QSBu/b2yqb1wqh13n1XzZpXLb79Me2qqUmkjMequ6mlR891olq24C43H8+GYVrB/cwO7r5FMWB+tT492fmlaWO/l5C+WjX/peXZUfXyxnoDJ9m1ekM4eaM8ffuFROwwSMJNRs3rikfd99fEx/o4qaqAiCAK9NZRVBnUxYO6gBjn3WXGnkiQUFD7Xh+PilWzWlHdMnb8xLFYLV3171CpuY+5f0r4O36xZS69RUd3/aGG1i9jW+tkbHynnx21vVsXdYEzQvk8tqUFyPOgVNXAH0y1PDLR0cdwLRt2ERbP24kUk0foW8QQjws0+o/dKtqknqXJcalgV5meCMSojbCzX4gyOaYlBMUFy7isHY9GFDm358W9Bnv+y9uspaIQiCdyKCOhnN8OwVIq5kxItlULVAZnz7WiUlXC1FsefMGKC0Y/411l4pyJg3ziA5jeBMAcrvaywgNaHcuVo+i2Pgvg2KZ0c2o3KtM7tXw9j2en88ea16PuTLkk59R4dGNlOTmk4x5/v6lfJxup593KKk2p/Me6cmXq2WD7+/XcPmd/H96/p6AHQfUHNm6lzsZ4jdr0dMgCC1Yn62aW9UidPTnBaFAjFjYoQ/NV/m5nNSkildGpQJDsLRz5pjcueKVuMGeDwnQxvisTAUzp4B07pUtmqpcRROHmh5MP7+tc9kCwY4Niph22VjD9oEUBCSC+KjFpyuwvZ3n9omD8jxK0/a5d2hkGIUO6FfftqmcxgTE+xmCWPTeXwm40YxpuGhC/V+41eq5rNaKjVHYIAKsnvzl91qvVstU+2X+epc4uPF8sFKM6Vf3xxjLb9vo6KoXTSb0qY1GNDHCnbXHj5V2nyv+oXRrlIebD59By+Wz60mGOYR67a+A1oYahfJqiYxFMQaGz9siCUHr2PyutNxGsocH90cDSdswu3QcLWNk67DVx/h/N3HFq/Bsri3Qp6poMRzd2L3mfJqJYMVYf6eK+ocTEm0ZLkwhm6LrWfvYuOpO3Heo5th06nbOHTlYbw1ARqWyI4lh67b3EcQkhIiqAWXQ+2sz5z9mNzZcmS7Nb88NXJb6Wrk87ZlcCcsAkVz2Jfz/W/f2rh07zEqG2ntliieM7ZZi59vwg1N5kKaAib0WaRJWVlaP2jSt5SOt+PcXYNApm+eaXz2mtqNg9RoYTBmzQf18eBxhIpQD/Cz/PnoNjH2V/P/5P0/rRcX0srilssbhFemW+4J/0+f2thw8jaqFcyCUzdt94JnAKMlaP1I759aCf77FgL7zHEkFsIYToiWHb7hcC19ZkO4m9xBAbhhlPEgpCzE9C24nJblcuPM2JZoX8mSkLEuiOMT0uTNWgUxqKlptLotKKAtj8MUvVne9VBLLpnLPsFBwUxzuSWNPD5oameOvq2JSI0YqwC1Z2toQXEUQJxQ6MziW2sUyoKvXi6P5e/XNWzLZKOZCyc9jBtgAF21grYnS+bQZE8TOoW0Rrp4LCnMXKCgDozZz98oGFLjfy1KYuWAeibbqhfKgomdKig3h7X6AGksfG/fvWbqMujT0LEgUHtJ6gF/mlvIGxlr5qLxRkRQC27BmlbK5iPeTmLmR3oC+tspzCxVqnurTiFlhZjVo7paz27k96dp+uduVVXuPX3kjn5ftiYIlqDJ3jzuokuN/MqPzRz6zBbupR519EGI/w1trGIgGLVvnglBYcrPz6BFjR+6VFYTJLo4/HzjfiDGCewb3kTtZyv6l5MA+ugtwYJDZ8e2tOuzx1fDPymxc8gL8QYr/tq9qsuv27p8bsx/p2acIlDm0FLDMdqLLfecuxDTt5CoUCN7t35hu03XiQlroq87cQuvW4nQ9nbs1bqooZprlBr0aRu7CQY1LYGbIc/QoXJepRm7CqbsHb0W4nBpRgrumTGTCGJc097cjK8F8y3pX9fifgxanNuzhjLyGAciWhoCKwGS+mYuBUexZ7JyYXwrZV06cPkhZu24qLYxaPCl77c7fD2mNUZGRaNZmVwqpkH7Hjjh6DvHtJ6BvbAS4tzdl/Hv/viL/JDcQWnjFCTiZOjv3rVx6OpDVS2Rn5eur1+2XYCrmBrTWnj+3it2jdFePPHsEo1aSFT4g2SNcuPgLm/hp65VVEAV/bhJEUaqu5qgdH748c2qdgtpLVLdnop1FEjG5W8TGg/BiP740NIIO1SOjcQnDOpjeV17Mdaf6xXL5nQJWovXiDFR0HpgXGBnVJvSyrT/aavY/+dDI5rZ7NJHvzsnWVrg4U9dq6q2uo6kBFqqhDipU0WrwYznxrWKs431GErExIEw6HTf8KYqNZL3gfZ5jb/LhPwGixoJUHvKG5tjXELZ2xCNWhBi4AMjMfPQXQ015fPjWlntxOY+Yq/HnGy7j0qVShXXoSXAVonc+OIhuBy7HoLzd6wHDU58pSI6Vr6LOk4+jI3N/CPbxNbbN8aaTUBrN/tnr5pKc6T1xpYJ2Pw83esUUstvMVq2NpFqUTY3PmpeAiHPIrHl9F2cuBFiENLmcR80Qbs7Z95aMZDVH9S3+xzT3qiMFlO2GsoOVyuYRdVSePG7bXYd/2XH8nAUViG0h4QGKzpD0n0qCYIQh8QX0kDezGkTXGmNpuwDI5oitY/euJc2gbUAFvWto8ruGlfSM28iw4A7e7DX585ANUvft6UANgpSrdgOy/HmzOhvENT7hzdF5c/Xqtf5s1jWJI0FbruKeTBp7WnULRY76egXE28wpGWsOyC+/wsG0O2+4JqIdWqwj55GGiZKzInfffG+Q3n4xpkRDMBk0OjWM3fwx9s17KoRwW9o15DGylXD9sDWAjbvxKQfmkNL3/oTtw0lizVYibD9D9uVK4KTLUuTM3cjgloQBKfgQ5TV0qhJxRe5z8jpiKho1CwUm5duHOX+QskcSuNjFThHoMBM46ZJirFp3XisWilYpsIx313jw2YlcPDKQ7xePT9GLT2utpkLGh+j78k4Xc6eSQK16H3DmljVXL/sWA5zd1/B4GYlbJ5n9ts1VDogm/fQojFz+wU8jYzCJgt57MYdAC0x480qSghq//9Tu1Q2NP2xF5rBL957rHoIaMF3XCzRr1ERTN14zmQbBSiFvbHANydVPJkfRz5rjpe+36Zy/wlrGmh1CXadu4cGLijIkxBEUAuC4DSslmYPjJymYNCqvlkKtqIf1VvQArs0KBzZWS4yOtqQNsaqfI0nblKTDJIjYwDWfKCvBqcJauPiNuYCOSGtjW0FpXWull8t8UHrQ8MSOUw07DO3Qq0KavrDOUnQYGe9sBjtk585vqY/9oxnTDvLEwFz6hbNrlIOB8w7qAJUOfmz1pFQS9Uzp3TujCiZO67mbRx8qE2GWJiJAXmeQgS1IAiJBhu9cPFmGP2/5+IDFQluyUKQ3yxgjpoc08AsmcHXflAf5+6E2axsR+26WemcePg0EoWMOuWx7n1iwyYzLDAzcskx/L3vqqoYGPrsuRLixkKazOhaBZ8uPIqBVrRedzCnZw01maD7gND6El9cCScMJ26EomOVPPh2fWyb2RVWMh80jX3poRsqXdEbSKWLLx8imXH16lXky5cPV65cQd689lV9EgQh5fA8Klr1Zmd0MzVDd/A4/DnKjFytXjO32pqGzKp29N/Tx56YREfrlBma0df2FCJKKhy68hC9ft+LIa1K2lUIyVtkkQhqQRAED0DN0NesFrvgfnQ6nVdMPhyRRV6RRz116lQULFgQAQEBqFGjBnbv1jdHsMSsWbMMfZG1hccJgiAkJWhmFiGd+KTyAiHtKB4X1PPnz8egQYMwcuRI7N+/HxUqVEDz5s1x+/Ztq8dkzJgRN27cMCyXLl1K1DELgiAIQooR1JMmTUKvXr3Qo0cPlC5dGtOnT0e6dOnw66+/2pwR5cqVy7DkzClN7wVBEITkiUcFdUREBPbt24cmTZrEDsjHR63v3Gm5bR4JCwtDgQIFlH2/bdu2OHbsmNV9w8PDERISYlhCQ2232hMEQRAEb8Kjgvru3buIioqKoxFz/ebNmxaPKVGihNK2Fy9ejNmzZyM6Ohq1a9dWjnlLjB8/HkFBQYaFWrsgCIIgJBU8bvp2lFq1aqFr166oWLEiGjRogH///RfZs2fHjz/+aHH/IUOG4NGjR4bl+HF9AQJBEARBSAp4tOBJtmzZ4Ovri1u3TIvTc52+Z3vw8/NDpUqVcPZsbCK7Mf7+/mrRePjwofrLIDRBEARB8ASaDKJV2KsFdZo0aVClShWsX78e7dq1Mwya6/3797frHDSdHzlyBK1axW2tZgltUlC9emxPW0EQBEHwBJRJ+fPn9+4SokzN6tatG6pWraqE55QpU/D48WMVBU5o5s6TJ4/yNZPRo0ejZs2aKFq0qNKOJ0yYoNKzevbsadf1qH0zT5t+cAauOQMD0+jzpjk9MNBytxZBSI7IvS+kREJdeN9TKaWQpkyKD48L6s6dO+POnTsYMWKECiCj73nVqlWGALPLly+bCNQHDx6odC7umzlzZqWR79ixw+4gsdSpU6NatWouGTujyAknEsztFoSUgtz7QkokxMX3fXyadIotIerq/zRGkjNITR5WQkpC7n0hJRLiofs+yUV9C4IgCEJKQgS1EzCanKVPjaPKBSElIPe+kBLx99B9L6ZvQRAEQfBiRKMWBEEQBC9GBLUgCIIgeDEiqAVBEATBixFB7QRTp05FwYIFERAQgBo1aqhCKoKQnNmyZQvatGmD4OBg1W520aJFnh6SILgdFtxi/Q0WOcmRI4eqpHnq1CkkFiKoE8j8+fNVVTVGAO7fvx8VKlRA8+bNcfv2bU8PTRDcBqsG8l7nJFUQUgqbN29Gv379sGvXLqxduxaRkZFo1qyZ+j0kBhL1nUCoQXOG9f333xvKwbE/9nvvvYdPPvnE08MTBLdDjXrhwoWGOv2CkFK4c+eO0qwpwOvXr+/264lGnQAiIiKwb98+NGnSxLCNZU65vnPnTo+OTRAEQXAvrExGsmTJgsRABHUCuHv3rurapdUj1+A6a5ALgiAIyZPo6GgMHDgQderUQdmyZRPlmh5vyiEIgiAISYV+/frh6NGj2LZtW6JdUwR1AsiWLRt8fX0Nva01uJ4rVy6PjUsQBEFwH/3798eyZctU9kPevHmRWIjpOwGkSZNGtddcv369iTmE67Vq1fLo2ARBEATXwphrCmkGT27YsAGFChVCYiIadQJhala3bt1QtWpVVK9eHVOmTFGh+j169PD00ATBbYSFheHs2bOG9QsXLuDgwYMqqMbe3rqCkBTN3XPnzsXixYtVLrUWi8SWl2nTpnX79SU9ywmYmjVhwgT1n1axYkV8++23Km1LEJIrmzZtQqNGjeJs56R11qxZHhmTICRGKqIlZs6cie7du7v/+iKoBUEQBMF7ER+1IAiCIHgxIqgFQRAEwYsRQS0IgiAIXowIakEQBEHwYkRQC4IgCIIXI4JaEARBELwYEdSCIAiC4MWIoBYEQRAEL0YEtSAIbq3otGjRIk8PQxCSNCKoBSGZwtKGFJTmS4sWLTw9NEEQHECacghCMoZCmfWIjfH39/fYeARBcBzRqAUhGUOhzB7pxkvmzJnVe9Sup02bhpYtW6oOQIULF8bff/9tcvyRI0fwwgsvqPezZs2Kd955R3XQMubXX39FmTJl1LVy586t2gEac/fuXbRv3x7p0qVDsWLFsGTJEsN7Dx48QJcuXZA9e3Z1Db5vPrEQhJSOCGpBSMEMHz4cHTt2xKFDh5TAfPXVV3HixAn1Htu2Nm/eXAn2PXv24K+//sK6detMBDEFPVsAUoBTqFMIFy1a1OQan332GTp16oTDhw+jVatW6jr37983XP/48eNYuXKlui7Ply1btkT+FgTBy2H3LEEQkh/dunXT+fr66tKnT2+yjB07Vr3Pn3/v3r1NjqlRo4auT58+6vWMGTN0mTNn1oWFhRneX758uc7Hx0d38+ZNtR4cHKwbOnSo1THwGsOGDTOs81zctnLlSrXepk0bXY8ePVz8yQUheSE+akFIxrB3NLVUY7JkyWJ4XatWLZP3uH7w4EH1mhpuhQoVkD59esP7derUQXR0NE6dOqVM59evX0fjxo1tjqF8+fKG1zxXxowZcfv2bbXep08fpdHv378fzZo1Q7t27VC7dm0nP7UgJC9EUAtCMoaC0dwU7SroU7YHPz8/k3UKeAp7Qv/4pUuXsGLFCqxdu1YJfZrSv/76a7eMWRCSIuKjFoQUzK5du+KslypVSr3mX/qu6avW2L59O3x8fFCiRAkEBgaiYMGCWL9+vVNjYCBZt27dMHv2bEyZMgUzZsxw6nyCkNwQjVoQkjHh4eG4efOmybbUqVMbArYYIFa1alXUrVsXc+bMwe7du/HLL7+o9xj0NXLkSCVER40ahTt37uC9997Dm2++iZw5c6p9uL13797IkSOH0o5DQ0OVMOd+9jBixAhUqVJFRY1zrMuWLTNMFARB0COCWhCSMatWrVIpU8ZQGz558qQhInvevHno27ev2u/PP/9E6dKl1XtMp1q9ejUGDBiAatWqqXX6kydNmmQ4F4X4s2fPMHnyZHz44YdqAvDyyy/bPb40adJgyJAhuHjxojKl16tXT41HEIRYUjGizGhdEIQUAn3FCxcuVAFcgiB4L+KjFgRBEAQvRgS1IAiCIHgx4qMWhBSKeL0EIWkgGrUgCIIgeDEiqAVBEATBixFBLQiCIAhejAhqQRAEQfBiRFALgiAIghcjgloQBEEQvBgR1IIgCILgxYigFgRBEAQvRgS1IAiCIMB7+T+DfRfWK0G1/gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制损失曲线\n",
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-medium355M-Alpaca-sft.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-Alpaca-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name a famous movie with the input name\n",
      "\n",
      "### Input:\n",
      "Grace\n",
      "\n",
      "Correct response:\n",
      ">> Grace Unplugged (2013)\n",
      "\n",
      "Model response:\n",
      ">> of Monaco\n",
      "\n",
      "\n",
      "Grace of Monaco is a classic movie that is widely considered to be one of the greatest films ever made.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name a popular singer from the US\n",
      "\n",
      "Correct response:\n",
      ">> Taylor Swift\n",
      "\n",
      "Model response:\n",
      ">> .\n",
      "\n",
      "\n",
      "Miley Cyrus.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Make up a new word using the following letters\n",
      "\n",
      "### Input:\n",
      "erdnx\n",
      "\n",
      "Correct response:\n",
      ">> erdnxology: the study of the nature and process of change.\n",
      "\n",
      "Model response:\n",
      ">> Erdnx\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    # 输入格式化\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    # 生成回答\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    # 提取有效回答\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .replace(\"<|assistant|>\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    # 对比output\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5200/5200 [6:25:17<00:00,  4.45s/it]    \n"
     ]
    }
   ],
   "source": [
    "# 保存测试集的结果\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .replace(\"<|assistant|>\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"instruction-data-with-response-Alpaca.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "# 检测ollama是否正在运行\n",
    "# 运行ollama: ollama run llama3\n",
    "import psutil \n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Lanunch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "# 通过REST api访问ollama\n",
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "        prompt,\n",
    "        model=\"llama3\",\n",
    "        url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # 构造请求数据\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0.,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # dict转化json并编码\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # 解析返回结果\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    \n",
    "    return response_data\n",
    "\n",
    "model_name = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:   5%|▍         | 257/5200 [02:28<1:22:24,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: The correct output is:\n",
      "\n",
      "7 + 8 = 15\n",
      "3 + 4 + 8 = 15\n",
      "9 x (1 + 2/3) = 15\n",
      "\n",
      "Model response: y = 2x + 3\n",
      "\n",
      "Score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  12%|█▏        | 608/5200 [05:41<59:22,  1.29it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I cannot suggest a replacement for \"stupid\" that implies someone is incapable of completing a task. Can I help you with something else?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  16%|█▌        | 806/5200 [07:30<1:14:02,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Here's my attempt at generating an example of a multiple choice question:\n",
      "\n",
      "### Question:\n",
      "Which of the following is true of Entomology?\n",
      "\n",
      "A. It studies viruses\n",
      "B. **It is the study of insects**\n",
      "C. It focuses on galaxies\n",
      "D. It studies human behavior\n",
      "\n",
      "Correct output: The life and behavior of insects.\n",
      "\n",
      "Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  21%|██        | 1085/5200 [10:10<31:40,  2.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: **Model Response:** Green, Red, Yellow\n",
      "**Score:** 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  27%|██▋       | 1415/5200 [13:13<48:39,  1.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: **Model Response:** Acquire\n",
      "\n",
      "**Score:** 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  28%|██▊       | 1448/5200 [13:30<21:55,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Extract\n",
      "Score: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  28%|██▊       | 1451/5200 [13:31<31:44,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I'd be happy to help!\n",
      "\n",
      "### Model Response:\n",
      "A book about the Challenges and Opportunities in Developing Countries.\n",
      "\n",
      "Score: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  38%|███▊      | 1966/5200 [18:42<30:39,  1.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Here's my attempt:\n",
      "\n",
      "### Response:\n",
      "The sentence \"I just started knitting\" can be classified as **Hobbies**.\n",
      "\n",
      "### Model Response:\n",
      "A scarf.\n",
      "\n",
      "### Score:\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  48%|████▊     | 2487/5200 [23:47<25:08,  1.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Husky\n",
      "\n",
      "Score: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  54%|█████▍    | 2821/5200 [27:07<29:23,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: The model response is \"Pollution: Our Ever-Growing Challenge\", which is the correct output.\n",
      "\n",
      "Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  58%|█████▊    | 3021/5200 [29:06<23:09,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Responsibility\n",
      "\n",
      "Score: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  59%|█████▊    | 3052/5200 [29:27<19:32,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Pleasing.\n",
      "\n",
      "Score: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  72%|███████▏  | 3760/5200 [36:44<11:48,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Yo puedo hablar español. 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  80%|████████  | 4182/5200 [40:55<09:35,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: **Corrected Output:** Noun\n",
      "**Score:** 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  89%|████████▉ | 4617/5200 [45:21<12:45,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I'd rate this model response as **20**.\n",
      "\n",
      "The dialogue between the teacher and student seems to be repetitive and lacks depth. The student's responses are also unclear and don't provide much insight into their favorite hobby or board games. The conversation feels like it's stuck in a loop, with the same questions being asked multiple times without any meaningful answers.\n",
      "\n",
      "In contrast, the original sample dialogue provided a more engaging and informative conversation about the student's favorite hobby (video games) and their preferences within that genre.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  95%|█████████▍| 4918/5200 [48:20<02:20,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Here's my attempt:\n",
      "\n",
      "### Response:\n",
      "The word class of \"adopt\" is Verb.\n",
      "\n",
      "### Model Response Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  96%|█████████▌| 4977/5200 [48:56<02:14,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Footage\n",
      "\n",
      "Score: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 5200/5200 [51:08<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 5183 of 5200\n",
      "Average score: 46.87\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 封装上述功能\n",
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[95,\n",
       " 95,\n",
       " 92,\n",
       " 60,\n",
       " 20,\n",
       " 40,\n",
       " 60,\n",
       " 20,\n",
       " 1,\n",
       " 20,\n",
       " 95,\n",
       " 20,\n",
       " 85,\n",
       " 92,\n",
       " 85,\n",
       " 85,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 85,\n",
       " 95,\n",
       " 4,\n",
       " 64,\n",
       " 85,\n",
       " 20,\n",
       " 40,\n",
       " 20,\n",
       " 20,\n",
       " 80,\n",
       " 20,\n",
       " 85,\n",
       " 60,\n",
       " 20,\n",
       " 60,\n",
       " 64,\n",
       " 44,\n",
       " 20,\n",
       " 85,\n",
       " 44,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 50,\n",
       " 20,\n",
       " 80,\n",
       " 95,\n",
       " 20,\n",
       " 64,\n",
       " 20,\n",
       " 2,\n",
       " 20,\n",
       " 40,\n",
       " 80,\n",
       " 20,\n",
       " 4,\n",
       " 80,\n",
       " 2,\n",
       " 4,\n",
       " 20,\n",
       " 67,\n",
       " 20,\n",
       " 60,\n",
       " 4,\n",
       " 14,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 80,\n",
       " 80,\n",
       " 0,\n",
       " 14,\n",
       " 40,\n",
       " 40,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 44,\n",
       " 20,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 85,\n",
       " 12,\n",
       " 44,\n",
       " 95,\n",
       " 95,\n",
       " 80,\n",
       " 4,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 60,\n",
       " 4,\n",
       " 60,\n",
       " 67,\n",
       " 98,\n",
       " 20,\n",
       " 67,\n",
       " 4,\n",
       " 20,\n",
       " 85,\n",
       " 60,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 95,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 2,\n",
       " 85,\n",
       " 95,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 60,\n",
       " 98,\n",
       " 20,\n",
       " 95,\n",
       " 40,\n",
       " 20,\n",
       " 67,\n",
       " 92,\n",
       " 20,\n",
       " 20,\n",
       " 0,\n",
       " 20,\n",
       " 95,\n",
       " 20,\n",
       " 20,\n",
       " 85,\n",
       " 14,\n",
       " 85,\n",
       " 20,\n",
       " 85,\n",
       " 40,\n",
       " 4,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 80,\n",
       " 60,\n",
       " 64,\n",
       " 20,\n",
       " 4,\n",
       " 67,\n",
       " 60,\n",
       " 85,\n",
       " 2,\n",
       " 60,\n",
       " 85,\n",
       " 60,\n",
       " 14,\n",
       " 85,\n",
       " 44,\n",
       " 12,\n",
       " 2,\n",
       " 20,\n",
       " 94,\n",
       " 20,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 2,\n",
       " 85,\n",
       " 40,\n",
       " 20,\n",
       " 4,\n",
       " 95,\n",
       " 4,\n",
       " 95,\n",
       " 64,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 67,\n",
       " 67,\n",
       " 85,\n",
       " 95,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 60,\n",
       " 95,\n",
       " 40,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 44,\n",
       " 85,\n",
       " 95,\n",
       " 4,\n",
       " 20,\n",
       " 20,\n",
       " 4,\n",
       " 85,\n",
       " 60,\n",
       " 12,\n",
       " 20,\n",
       " 40,\n",
       " 85,\n",
       " 40,\n",
       " 20,\n",
       " 40,\n",
       " 85,\n",
       " 20,\n",
       " 85,\n",
       " 67,\n",
       " 20,\n",
       " 80,\n",
       " 80,\n",
       " 95,\n",
       " 20,\n",
       " 85,\n",
       " 4,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 80,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 85,\n",
       " 60,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 4,\n",
       " 95,\n",
       " 67,\n",
       " 80,\n",
       " 85,\n",
       " 85,\n",
       " 80,\n",
       " 0,\n",
       " 60,\n",
       " 20,\n",
       " 12,\n",
       " 44,\n",
       " 14,\n",
       " 12,\n",
       " 40,\n",
       " 4,\n",
       " 20,\n",
       " 40,\n",
       " 60,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 12,\n",
       " 40,\n",
       " 95,\n",
       " 20,\n",
       " 67,\n",
       " 87,\n",
       " 20,\n",
       " 67,\n",
       " 60,\n",
       " 60,\n",
       " 12,\n",
       " 85,\n",
       " 80,\n",
       " 85,\n",
       " 96,\n",
       " 85,\n",
       " 6,\n",
       " 20,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 60,\n",
       " 40,\n",
       " 80,\n",
       " 20,\n",
       " 20,\n",
       " 67,\n",
       " 40,\n",
       " 20,\n",
       " 2,\n",
       " 20,\n",
       " 85,\n",
       " 80,\n",
       " 4,\n",
       " 80,\n",
       " 20,\n",
       " 92,\n",
       " 60,\n",
       " 80,\n",
       " 60,\n",
       " 14,\n",
       " 4,\n",
       " 20,\n",
       " 20,\n",
       " 12,\n",
       " 4,\n",
       " 80,\n",
       " 60,\n",
       " 0,\n",
       " 95,\n",
       " 12,\n",
       " 85,\n",
       " 60,\n",
       " 20,\n",
       " 4,\n",
       " 85,\n",
       " 4,\n",
       " 4,\n",
       " 20,\n",
       " 0,\n",
       " 67,\n",
       " 4,\n",
       " 85,\n",
       " 82,\n",
       " 67,\n",
       " 20,\n",
       " 60,\n",
       " 80,\n",
       " 95,\n",
       " 20,\n",
       " 98,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 20,\n",
       " 12,\n",
       " 100,\n",
       " 4,\n",
       " 85,\n",
       " 80,\n",
       " 20,\n",
       " 20,\n",
       " 67,\n",
       " 85,\n",
       " 60,\n",
       " 20,\n",
       " 80,\n",
       " 85,\n",
       " 80,\n",
       " 20,\n",
       " 85,\n",
       " 76,\n",
       " 20,\n",
       " 80,\n",
       " 95,\n",
       " 80,\n",
       " 95,\n",
       " 60,\n",
       " 4,\n",
       " 4,\n",
       " 20,\n",
       " 0,\n",
       " 44,\n",
       " 87,\n",
       " 85,\n",
       " 20,\n",
       " 44,\n",
       " 85,\n",
       " 20,\n",
       " 4,\n",
       " 4,\n",
       " 20,\n",
       " 20,\n",
       " 80,\n",
       " 85,\n",
       " 98,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 94,\n",
       " 20,\n",
       " 2,\n",
       " 40,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 60,\n",
       " 40,\n",
       " 80,\n",
       " 20,\n",
       " 4,\n",
       " 85,\n",
       " 60,\n",
       " 60,\n",
       " 20,\n",
       " 95,\n",
       " 44,\n",
       " 50,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 95,\n",
       " 67,\n",
       " 85,\n",
       " 20,\n",
       " 60,\n",
       " 85,\n",
       " 80,\n",
       " 85,\n",
       " 85,\n",
       " 20,\n",
       " 64,\n",
       " 80,\n",
       " 94,\n",
       " 20,\n",
       " 80,\n",
       " 95,\n",
       " 60,\n",
       " 60,\n",
       " 85,\n",
       " 60,\n",
       " 80,\n",
       " 95,\n",
       " 67,\n",
       " 60,\n",
       " 60,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 0,\n",
       " 98,\n",
       " 2,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 95,\n",
       " 0,\n",
       " 4,\n",
       " 85,\n",
       " 0,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 60,\n",
       " 44,\n",
       " 20,\n",
       " 95,\n",
       " 20,\n",
       " 14,\n",
       " 12,\n",
       " 44,\n",
       " 20,\n",
       " 20,\n",
       " 85,\n",
       " 85,\n",
       " 40,\n",
       " 85,\n",
       " 14,\n",
       " 64,\n",
       " 60,\n",
       " 85,\n",
       " 95,\n",
       " 20,\n",
       " 95,\n",
       " 20,\n",
       " 85,\n",
       " 85,\n",
       " 4,\n",
       " 95,\n",
       " 92,\n",
       " 20,\n",
       " 20,\n",
       " 95,\n",
       " 95,\n",
       " 2,\n",
       " 20,\n",
       " 20,\n",
       " 4,\n",
       " 85,\n",
       " 40,\n",
       " 95,\n",
       " 60,\n",
       " 20,\n",
       " 67,\n",
       " 20,\n",
       " 20,\n",
       " 100,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 14,\n",
       " 100,\n",
       " 40,\n",
       " 64,\n",
       " 20,\n",
       " 85,\n",
       " 4,\n",
       " 40,\n",
       " 20,\n",
       " 20,\n",
       " 80,\n",
       " 60,\n",
       " 80,\n",
       " 20,\n",
       " 20,\n",
       " 96,\n",
       " 4,\n",
       " 20,\n",
       " 80,\n",
       " 20,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 90,\n",
       " 96,\n",
       " 80,\n",
       " 80,\n",
       " 98,\n",
       " 80,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 12,\n",
       " 44,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 60,\n",
       " 64,\n",
       " 4,\n",
       " 98,\n",
       " 20,\n",
       " 67,\n",
       " 4,\n",
       " 95,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 4,\n",
       " 95,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 60,\n",
       " 80,\n",
       " 14,\n",
       " 20,\n",
       " 85,\n",
       " 60,\n",
       " 2,\n",
       " 4,\n",
       " 20,\n",
       " 98,\n",
       " 98,\n",
       " 20,\n",
       " 12,\n",
       " 20,\n",
       " 4,\n",
       " 20,\n",
       " 60,\n",
       " 40,\n",
       " 5,\n",
       " 20,\n",
       " 80,\n",
       " 95,\n",
       " 60,\n",
       " 82,\n",
       " 82,\n",
       " 20,\n",
       " 4,\n",
       " 60,\n",
       " 85,\n",
       " 60,\n",
       " 20,\n",
       " 95,\n",
       " 95,\n",
       " 40,\n",
       " 20,\n",
       " 44,\n",
       " 20,\n",
       " 64,\n",
       " 20,\n",
       " 4,\n",
       " 44,\n",
       " 65,\n",
       " 20,\n",
       " 92,\n",
       " 20,\n",
       " 12,\n",
       " 2,\n",
       " 92,\n",
       " 85,\n",
       " 80,\n",
       " 95,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 96,\n",
       " 80,\n",
       " 44,\n",
       " 80,\n",
       " 95,\n",
       " 90,\n",
       " 4,\n",
       " 67,\n",
       " 85,\n",
       " 80,\n",
       " 95,\n",
       " 85,\n",
       " 64,\n",
       " 40,\n",
       " 20,\n",
       " 20,\n",
       " 60,\n",
       " 60,\n",
       " 85,\n",
       " 60,\n",
       " 40,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 95,\n",
       " 4,\n",
       " 20,\n",
       " 2,\n",
       " 4,\n",
       " 85,\n",
       " 20,\n",
       " 4,\n",
       " 14,\n",
       " 60,\n",
       " 85,\n",
       " 60,\n",
       " 4,\n",
       " 20,\n",
       " 60,\n",
       " 14,\n",
       " 40,\n",
       " 20,\n",
       " 100,\n",
       " 20,\n",
       " 85,\n",
       " 80,\n",
       " 20,\n",
       " 4,\n",
       " 4,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 98,\n",
       " 60,\n",
       " 95,\n",
       " 4,\n",
       " 20,\n",
       " 40,\n",
       " 87,\n",
       " 64,\n",
       " 85,\n",
       " 95,\n",
       " 85,\n",
       " 2,\n",
       " 85,\n",
       " 60,\n",
       " 85,\n",
       " 92,\n",
       " 20,\n",
       " 20,\n",
       " 85,\n",
       " 67,\n",
       " 85,\n",
       " 20,\n",
       " 80,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 85,\n",
       " 80,\n",
       " 20,\n",
       " 4,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 60,\n",
       " 2,\n",
       " 60,\n",
       " 2,\n",
       " 85,\n",
       " 20,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 67,\n",
       " 20,\n",
       " 90,\n",
       " 60,\n",
       " 85,\n",
       " 92,\n",
       " 64,\n",
       " 60,\n",
       " 4,\n",
       " 95,\n",
       " 44,\n",
       " 4,\n",
       " 80,\n",
       " 20,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 92,\n",
       " 4,\n",
       " 44,\n",
       " 64,\n",
       " 4,\n",
       " 20,\n",
       " 85,\n",
       " 80,\n",
       " 85,\n",
       " 20,\n",
       " 85,\n",
       " 44,\n",
       " 95,\n",
       " 20,\n",
       " 4,\n",
       " 20,\n",
       " 20,\n",
       " 96,\n",
       " 20,\n",
       " 4,\n",
       " 20,\n",
       " 80,\n",
       " 40,\n",
       " 20,\n",
       " 85,\n",
       " 80,\n",
       " 80,\n",
       " 85,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 4,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 95,\n",
       " 60,\n",
       " 20,\n",
       " 60,\n",
       " 85,\n",
       " 95,\n",
       " 20,\n",
       " 0,\n",
       " 60,\n",
       " 4,\n",
       " 80,\n",
       " 95,\n",
       " 95,\n",
       " 20,\n",
       " 20,\n",
       " 12,\n",
       " 40,\n",
       " 85,\n",
       " 4,\n",
       " 95,\n",
       " 80,\n",
       " 60,\n",
       " 4,\n",
       " 20,\n",
       " 60,\n",
       " 95,\n",
       " 40,\n",
       " 67,\n",
       " 85,\n",
       " 96,\n",
       " 4,\n",
       " 4,\n",
       " 80,\n",
       " 20,\n",
       " 95,\n",
       " 85,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 4,\n",
       " 20,\n",
       " 67,\n",
       " 85,\n",
       " 20,\n",
       " 85,\n",
       " 95,\n",
       " 4,\n",
       " 80,\n",
       " 64,\n",
       " 50,\n",
       " 67,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 4,\n",
       " 64,\n",
       " 40,\n",
       " 20,\n",
       " 20,\n",
       " 12,\n",
       " 12,\n",
       " 20,\n",
       " 20,\n",
       " 80,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 2,\n",
       " 94,\n",
       " 85,\n",
       " 0,\n",
       " 85,\n",
       " 60,\n",
       " 20,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 0,\n",
       " 90,\n",
       " 85,\n",
       " 95,\n",
       " 20,\n",
       " 95,\n",
       " 20,\n",
       " 80,\n",
       " 85,\n",
       " 85,\n",
       " 80,\n",
       " 4,\n",
       " 80,\n",
       " 20,\n",
       " 80,\n",
       " 100,\n",
       " 44,\n",
       " 44,\n",
       " 80,\n",
       " 20,\n",
       " 40,\n",
       " 4,\n",
       " 4,\n",
       " 14,\n",
       " 20,\n",
       " 85,\n",
       " 20,\n",
       " 80,\n",
       " 80,\n",
       " 74,\n",
       " 96,\n",
       " 94,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 60,\n",
       " 96,\n",
       " 4,\n",
       " 94,\n",
       " 50,\n",
       " 85,\n",
       " 98,\n",
       " 20,\n",
       " 20,\n",
       " 4,\n",
       " 64,\n",
       " 67,\n",
       " 20,\n",
       " 20,\n",
       " 64,\n",
       " 4,\n",
       " 20,\n",
       " 60,\n",
       " 85,\n",
       " 64,\n",
       " 64,\n",
       " 60,\n",
       " 85,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 12,\n",
       " 94,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 44,\n",
       " 4,\n",
       " 0,\n",
       " 40,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 4,\n",
       " 2,\n",
       " 85,\n",
       " 20,\n",
       " 67,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 4,\n",
       " 85,\n",
       " 20,\n",
       " 12,\n",
       " 60,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 2,\n",
       " 85,\n",
       " 67,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 80,\n",
       " 20,\n",
       " 95,\n",
       " 60,\n",
       " 2,\n",
       " 20,\n",
       " 85,\n",
       " 95,\n",
       " 20,\n",
       " 20,\n",
       " 44,\n",
       " 4,\n",
       " 67,\n",
       " 4,\n",
       " 40,\n",
       " 20,\n",
       " 20,\n",
       " 85,\n",
       " 60,\n",
       " 85,\n",
       " 85,\n",
       " 80,\n",
       " 0,\n",
       " 80,\n",
       " 4,\n",
       " 95,\n",
       " 20,\n",
       " 20,\n",
       " 80,\n",
       " 60,\n",
       " 95,\n",
       " 80,\n",
       " 2,\n",
       " 20,\n",
       " 20,\n",
       " 85,\n",
       " 95,\n",
       " 20,\n",
       " 20,\n",
       " 95,\n",
       " 20,\n",
       " 4,\n",
       " 80,\n",
       " 60,\n",
       " 85,\n",
       " 4,\n",
       " 4,\n",
       " 95,\n",
       " 80,\n",
       " 20,\n",
       " 85,\n",
       " 14,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 7.4 PARAMETER-EFFICIENT FINETUNING WITH LORA\n",
    "To instruction finetune an LLM more efficiently, modify the code in this chapter to use\n",
    "the low-rank adaptation method (LoRA) from appendix E. Compare the training\n",
    "runtime and model performance before and after the modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/ch07/01_main-chapter-code\n"
     ]
    }
   ],
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch07\", \"01_main-chapter-code\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    # 处理Input为空/非空的情况\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            # 拼接指令\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            # 拼接输出text\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            # 合并指令+输出\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            # 编码上述信息\n",
    "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加输入和目标\n",
    "# 填充方法\n",
    "def custom_collate_fn(\n",
    "        batch, \n",
    "        pad_token_id=50256, \n",
    "        ignore_index=-100,\n",
    "        allowed_max_length=None,\n",
    "        device=\"cpu\"\n",
    "):\n",
    "    # 填充至当前batch的最大长度+1\n",
    "    # 至少会填充一个<|endoftext|>\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # 填充endoftext\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # 去除最后一个token, 作为输入\n",
    "        # 相对的，如果去掉第一个token，则作为目标\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # targets中仅保留一个<|endoftext|>，其余填充为ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        # 最大长度截断\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    # stack to batch\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# 将部分参数提前填充，并生成一个新的函数，以适配collate函数的要求\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024 # 1024 -> 128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 使用gpt2的bpe编码器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    # else:\n",
    "    #     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    #         text_data = file.read()\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "# 0.85、0.1、0.05\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "<|assistant|>\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# 测试format效果\n",
    "# Input为空\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n<|assistant|>\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8  # 8 -> 1\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型初始化+LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout率\n",
    "    \"qkv_bias\": True         # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 构建LoRA layer\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        # A矩阵进行kaiming初始化，B矩阵进行全0初始化\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建整合了LoRALayer的线性层\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型中的所有线性层替换为LoRA层\n",
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # replace\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # 递归\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 406,286,336\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "# 冻结原模型的参数\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters after: 7,898,384\n"
     ]
    }
   ],
   "source": [
    "# 替换线性层为可训练的LoRA层\n",
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_key): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (W_value): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (out_proj): LinearWithLoRA(\n",
       "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora): LoRALayer()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (1): GELU()\n",
       "          (2): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): LinearWithLoRA(\n",
       "    (linear): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "    (lora): LoRALayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.509, Val loss 2.519\n",
      "Ep 1 (Step 000005): Train loss 1.243, Val loss 1.143\n",
      "Ep 1 (Step 000010): Train loss 0.896, Val loss 1.004\n",
      "Ep 1 (Step 000015): Train loss 0.878, Val loss 0.968\n",
      "Ep 1 (Step 000020): Train loss 0.804, Val loss 0.932\n",
      "Ep 1 (Step 000025): Train loss 0.780, Val loss 0.900\n",
      "Ep 1 (Step 000030): Train loss 0.811, Val loss 0.870\n",
      "Ep 1 (Step 000035): Train loss 0.741, Val loss 0.837\n",
      "Ep 1 (Step 000040): Train loss 0.695, Val loss 0.831\n",
      "Ep 1 (Step 000045): Train loss 0.653, Val loss 0.818\n",
      "Ep 1 (Step 000050): Train loss 0.705, Val loss 0.812\n",
      "Ep 1 (Step 000055): Train loss 0.784, Val loss 0.789\n",
      "Ep 1 (Step 000060): Train loss 0.756, Val loss 0.771\n",
      "Ep 1 (Step 000065): Train loss 0.669, Val loss 0.754\n",
      "Ep 1 (Step 000070): Train loss 0.565, Val loss 0.751\n",
      "Ep 1 (Step 000075): Train loss 0.585, Val loss 0.749\n",
      "Ep 1 (Step 000080): Train loss 0.631, Val loss 0.737\n",
      "Ep 1 (Step 000085): Train loss 0.528, Val loss 0.723\n",
      "Ep 1 (Step 000090): Train loss 0.585, Val loss 0.715\n",
      "Ep 1 (Step 000095): Train loss 0.534, Val loss 0.708\n",
      "Ep 1 (Step 000100): Train loss 0.535, Val loss 0.710\n",
      "Ep 1 (Step 000105): Train loss 0.595, Val loss 0.702\n",
      "Ep 1 (Step 000110): Train loss 0.589, Val loss 0.694\n",
      "Ep 1 (Step 000115): Train loss 0.528, Val loss 0.691\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The\n",
      "Ep 2 (Step 000120): Train loss 0.465, Val loss 0.703\n",
      "Ep 2 (Step 000125): Train loss 0.465, Val loss 0.710\n",
      "Ep 2 (Step 000130): Train loss 0.460, Val loss 0.702\n",
      "Ep 2 (Step 000135): Train loss 0.423, Val loss 0.703\n",
      "Ep 2 (Step 000140): Train loss 0.421, Val loss 0.709\n",
      "Ep 2 (Step 000145): Train loss 0.390, Val loss 0.718\n",
      "Ep 2 (Step 000150): Train loss 0.408, Val loss 0.696\n",
      "Ep 2 (Step 000155): Train loss 0.438, Val loss 0.692\n",
      "Ep 2 (Step 000160): Train loss 0.437, Val loss 0.703\n",
      "Ep 2 (Step 000165): Train loss 0.400, Val loss 0.707\n",
      "Ep 2 (Step 000170): Train loss 0.344, Val loss 0.696\n",
      "Ep 2 (Step 000175): Train loss 0.359, Val loss 0.678\n",
      "Ep 2 (Step 000180): Train loss 0.403, Val loss 0.673\n",
      "Ep 2 (Step 000185): Train loss 0.435, Val loss 0.681\n",
      "Ep 2 (Step 000190): Train loss 0.360, Val loss 0.674\n",
      "Ep 2 (Step 000195): Train loss 0.353, Val loss 0.655\n",
      "Ep 2 (Step 000200): Train loss 0.333, Val loss 0.653\n",
      "Ep 2 (Step 000205): Train loss 0.381, Val loss 0.651\n",
      "Ep 2 (Step 000210): Train loss 0.393, Val loss 0.647\n",
      "Ep 2 (Step 000215): Train loss 0.413, Val loss 0.646\n",
      "Ep 2 (Step 000220): Train loss 0.315, Val loss 0.653\n",
      "Ep 2 (Step 000225): Train loss 0.363, Val loss 0.659\n",
      "Ep 2 (Step 000230): Train loss 0.308, Val loss 0.652\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 2.89 minutes.\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")\n",
    "\n",
    "# 模型训练\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQoklEQVR4nO2dB3hU1fbFVzqBFBIggdB7702KooIUERR7R3zqs4NYeSq2v6KiiAULz8JTLIBKkSIiVRBEeu+9JKGFFNIz/2+dy0wmGEJCJplJsn7fd7lz79y5c84wmXX2Pvvs7WWz2WwQQgghhEfi7e4GCCGEEOL8SKiFEEIID0ZCLYQQQngwEmohhBDCg5FQCyGEEB6MhFoIIYTwYCTUQgghhAcjoRZCCCE8GAm1EEII4cFIqIUoRezbtw9eXl5Yt26du5sihHAREmohPAwKbV7byy+/7O4mCiGKEd/ifDMhxIU5evSo4/GkSZMwcuRIbN++3XEuKCjITS0TQrgDWdRCeBhVq1Z1bKGhocaKth9HRERgzJgxqFGjBgICAtCmTRv8+uuv571XZmYm7r33XjRp0gQHDhww56ZPn4527dqhXLlyqFevHl555RVkZGQ4XsP3+/zzzzFo0CCUL18eDRs2xIwZMxzPnzp1CnfccQeqVKmCwMBA8/xXX3113jb8+OOPaNmypbm2UqVK6NWrF5KSkhzP872aNm1q2sN2fvzxxzlef/DgQdx8882oWLEiwsPDce211xoXv5177rkH1113Hd555x1Uq1bNvMcjjzyC9PT0i/j0hfBAWD1LCOGZfPXVV7bQ0FDH8ZgxY2whISG277//3rZt2zbbM888Y/Pz87Pt2LHDPL93715Ww7OtXbvWlpKSYhs0aJCtbdu2ttjYWPP8kiVLzOsnTJhg2717t+23336z1alTx/byyy873oOvr1Gjhu27776z7dy50/b444/bgoKCbCdOnDDPP/LII7Y2bdrY/v77b/N+8+bNs82YMSPX9h85csTm6+tr2s1rN2zYYBs3bpwtISHBPD9x4kRbtWrVbD/99JNtz549Zh8eHm7aR9LS0mxNmza13Xvvvea1W7Zssd1+++22xo0b21JTU801gwcPNn168MEHbVu3brX98ssvtvLly9vGjx9fZP8vQhQnEmohSpBQR0VF2V5//fUc13Ts2NH28MMP5xDqP/74w9azZ09b9+7dbXFxcY5ree6NN97I8fpvvvnGiKUdvv6FF15wHCcmJppzc+bMMccDBgywDRkyJF/tX716tXntvn37cn2+fv36ZkDgzGuvvWbr0qWLo20U5aysLMfzFOjAwEDb3LlzHUJdu3ZtW0ZGhuOam266yXbLLbfkq41CeDqaoxaihBAfH48jR46gW7duOc7zeP369TnO3XbbbcY9vmDBAuNytsPrli1bhtdffz2HezwlJQVnzpwxrm7SqlUrx/MVKlRASEgIYmNjzfFDDz2EG264AWvWrEHv3r2N27lr1665trl169bo2bOncX336dPHXH/jjTciLCzMuL93796Nf/3rX7j//vsdr6Ebni5/e3t37dqF4ODgHPdle/laO82bN4ePj4/jmC7wjRs35vuzFcKTkVALUQq5+uqrMXHiRCxfvhxXXnml43xiYqKZk77++uv/8RrOEdvx8/PL8RznrbOysszjfv36Yf/+/Zg9ezbmzZtnhJhzwpwjPheKJ6/5888/8dtvv+HDDz/E888/j7/++ssxKPjvf/+Lzp07/+N19va2b98e33777T/uzTny/LRXiJKOhFqIEgKt2qioKGMR9+jRw3Gex506dcpxLa3eFi1aYODAgZg1a5bjegaRMYK8QYMGhWoLRXLw4MFmu/TSS/H000/nKtR20aTVz40R7LVr18bUqVMxfPhw0589e/aY4LTcYHsZ+c4gOvZfiLKIhFqIEgQF8aWXXkL9+vVNxDejrZncJDeL87HHHjNu7WuuuQZz5sxB9+7djVDyuFatWsYF7e3tbdzLmzZtwv/93//lqw28B61cuptTU1Mxc+ZME7WdG7Sc58+fb1zeFFseHzt2zHE9rfvHH3/cuLr79u1r7rdq1SoTWU4hp4CPHj3aRHq/+uqrxp1Pa/7nn3/GM888Y46FKO1IqIUoQVDUTp8+jSeffNLMGTdr1swsneISqdwYNmyYcQHTFc5lXJwnprBS9N566y3jMuaSqPvuuy/fbfD398eIESPMEinOf9Oi/uGHH3K9llbwkiVLMHbsWDPHTmv63XffNe5zwvelC5xizEEI58M5n812Ez7H1z/77LPGXZ+QkIDq1asbd7ssbFFW8GJEmbsbIYQQQojcUcITIYQQwoORUAshhBAejIRaCCGE8GAk1EIIIYQHI6EWQgghPBgJtRBCCOHBSKgvgnHjxqFOnTom5SJTH65cuRKexKhRo9CxY0eTH5lJJpiL2bmesT1XMtM+siQg6xszd3NMTEyOa1gWsX///mYtK+/Dda7O5RDJokWLTPYollxktqsJEya49fN68803TSYs+zrc0tjXw4cP48477zT94TpmrjtmkhA7XHHJpCTMd83nWVZy586dOe5x8uRJk0yEa5FZPpL5tpmu05kNGzaYNdLsS82aNfH222//oy1Tpkwx67B5DdvBtKKugslaXnzxRdStW9f0g0leXnvtNdO/0tBXrg8fMGCAyc7G7+y0adNyPO9JfctPWy62ryxHynXyfF+uo+c1d999t8lrXxL7WiS4uypISeOHH36w+fv727788kvb5s2bbffff7+tYsWKtpiYGJun0KdPH1N1adOmTbZ169bZrr76alutWrVMFSQ7LAlYs2ZN2/z5822rVq2yXXLJJbauXbs6nmclohYtWth69eplSibOnj3bVrlyZduIESMc17AsIcsJDh8+3JQf/PDDD20+Pj62X3/91S2f18qVK03JxlatWtmGDh1aKvt68uRJUynqnnvusf3111+mXawitWvXLsc1b775pqm4NW3aNNv69ettAwcOtNWtW9eWnJzsuKZv37621q1b21asWGEqbTVo0MB22223OZ4/ffq0LTIy0nbHHXeY7xHLarJi1Weffea4ZtmyZeYzePvtt81nwopbLLm5ceNGl/SVVcIqVapkmzlzpqkKNmXKFFNu8/333y8VfeX37Pnnn7f9/PPPpsLY1KlTczzvSX3LT1sutq+s7sa/vUmTJpnSrcuXL7d16tTJ1r59+xz36FtC+loUSKgLCL9ArMdrJzMz05QeHDVqlM1TYS1i/nEsXrzY8YfBLyd/+Oywji+v4R+J/Q/L29vbFh0d7bjmk08+MXV/7XWAWQu5efPmOd6LpQU5UCjuz4v1jRs2bGhqI/fo0cMh1KWtr88++6wpXXk+WA6yatWqttGjRzvO8TMICAgwP1yEP1DsP+tJ22EJSy8vL9vhw4fN8ccff2wLCwtz9N/+3iw5aefmm2+29e/fP8f7d+7c2fbvf//bJX3lvVmH2pnrr7/e/BCXtr6eK16e1Lf8tKUwfT3foJvX7d+/v0T31VXI9V0A0tLSsHr1auMKscNcyTxmlSJPhSknSXh4uNmzD3Q3OfeDriDmf7b3g3u6hSIjIx3XMP0k00Bu3rzZcY3zPezX2O9RnJ8XXdt0XZ/bntLWV6YL7dChA2666Sbjom/btq2pPmVn7969iI6OztEO5tGmG965v3Qd8j52eD3by1zc9msuu+wyky7Uub+cQmEe7vx8JoWFpTOZJ3zHjh3mmDnJly5d6kg/Wpr6ei6e1Lf8tKUofrPoImf/Sntf84OEugAcP37czJs5/6ATHvM/1xNhnmfO17JyEaspEbaVX2b7H0Fu/eA+t37an8vrGgpccnJysX1ezDPN2sicmz+X0tZXVpr65JNPTG7vuXPnmipZzP/9v//9L0d782oH9xR5Z3x9fc1AzhWfiav6+9xzz+HWW281AyvmJOeghN9le6Wt0tTXc/GkvuWnLa6EMSWcs2ZNdXs+9+hS2tf8oqIcpRxamqyMREukNHLw4EEMHTrU1Dx2rqdcWuHAi1bFG2+8YY4pXvz//fTTT03JydLE5MmTTVWw7777zlTqYpUwCjWDjUpbX4UFvV8333yzCejigFRYyKIuAJUrVzYF7c+NGOZx1apV4Wk8+uijplLSwoULc5QDZFvpqo2LiztvP7jPrZ/25/K6hqNgRksWx+dFdzOrSDEamyNsbosXL8YHH3xgHnMkXFr6ShiJyopZzrBkJKPWndubVzu452fmDCPcGVXris/EVf1l5L3dqubUxF133YUnnnjC4TkpTX09F0/qW37a4kqRZhlTDrydq6NVLWV9LSgS6gJAFyrr8HLezNnC4XGXLl3gKXA0SpGeOnUqFixYYJa3OMM+0JXo3A/O4/DH3t4P7jdu3Jjjj8P+x2MXCl7jfA/7NfZ7FMfnxXKHbCetLftGi5PuUfvj0tJXwimMc5facQ6X5SMJ/6/5g+LcDrrnOY/n3F8OXDjIscPvCdvLuTj7NVxSwx9P5/42btwYYWFh+fpMCsuZM2fMHKQzHAyxnaWtr+fiSX3LT1tcJdJcBvX777+bpYfOdClFfb0o3BbGVkLhEhxGAE6YMMFEIj7wwANmCY5zxLC7eeihh8zygkWLFtmOHj3q2M6cOZNjyRKXbC1YsMAsWerSpYvZzl2y1Lt3b7PEi8uQqlSpkuuSpaefftpEUo8bNy7XJUvF/Xk5R32Xtr4yGtbX19csXdq5c6ft22+/Ne2aOHFijuUlfN/p06fbNmzYYLv22mtzXdbTtm1bs8Rr6dKlJmLeeakLI1251OWuu+4yS13YN77PuUtd2JZ33nnHfCYvvfSSS5dnDR482Fa9enXH8iwu7eGyOUbgl4a+cqUClwNy40/xmDFjzGN7pLMn9S0/bbnYvqalpZklUDVq1DB/f86/Wc4R3H1LSF+LAgn1RcA1tPzh55pZLsnhuj5Pgn8IuW1cW22HX7qHH37YLGfgl3nQoEHmD8OZffv22fr162fWIvIH8sknn7Slp6fnuGbhwoW2Nm3amM+iXr16Od7DXZ/XuUJd2vr6yy+/mIEFBwVNmjSxjR8/PsfzXGLy4osvmh8tXtOzZ0/b9u3bc1xz4sQJ8yPHdclchjZkyBDzY+oM15ByKRjvQcHkD9i5TJ482daoUSPTXy5fmzVrlsv6GR8fb/4f+XmWK1fOfOZci+v8412S+8rvU25/pxygeFrf8tOWi+0rB2Hn+83i60paX4sCL/7jPnteCCGEEHmhOWohhBDCg5FQCyGEEB6MhFoIIYTwYCTUQgghhAcjoRZCCCE8GAm1EEII4cFIqC+S1NRUvPzyy2Zf2ilLfS1r/VVfSy9lqb+ppbyvWkd9kTCtHMufsRybc07a0khZ6mtZ66/6WnopS/2NL+V9lUUthBBCeDASaiGEEMKDKXP1qFkabe3atab84bmVeQpCQkKC2R8+fNi4XUozZamvZa2/6mvppSz1N6EE9pWVv1g+kzXlWZI3L8rcHPXff/+NTp06ubsZQgghBFauXImOHTvmeU2Zs6hpSds/nGrVqrm7OUIIIcogR48eNUajXZPyoswJtd3dTZGuUaOGu5sjhBCiDOOdjylYBZMJIYQQHoyEWgghhPBgJNRCCCGEB+PWOepRo0bh559/xrZt2xAYGIiuXbvirbfeQuPGjc/7mgkTJmDIkCE5zgUEBCAlJaUYWiyEKO1kZmYiPT3d3c0QJRw/Pz/4+PiUfKFevHgxHnnkEROazvXN//nPf9C7d29s2bIFFSpUOO/rmCJu+/btjmMvL69iarEQorTClarR0dGIi4tzd1NEKaFixYqoWrVqoTXKrUL966+//sNajoiIwOrVq3HZZZed93XsNDvvbuYsWYZTh3ejc8fOqN/g/F4AIYTnYxdp/gaVL19eBoAo1KDvzJkziI2NNceFXQrsUcuzmFCdhIeH53ldYmIiateubTK7tGvXDm+88QaaN2+e67WspuJcUcWewcYVVPrzdfRLWYZ1ASMBCbUQJdrdbRfpSpUqubs5ohQQGBho9hRrfq8K4wb3mGAyiu6wYcPQrVs3tGjR4rzXcf76yy+/xPTp0zFx4kTzOs5tHzp06Lzz4KyqYt+aNWvmsjanBYSZfWbicZfdUwhR/NjnpGlJC+Eq7N+nwsY8eIxQc65606ZN+OGHH/K8rkuXLrj77rvRpk0b9OjRwwSjValSBZ999lmu148YMcJY6vaN89+uIqucZfnbzkiohSgNyN0tPPH75BGu70cffRQzZ87EkiVLCpwtjJF1TGq+a9euXJ9nRDg3O65M2O5VwXKR+SSfctk9hRBCCI+xqDnhTpGeOnUqFixYgLp1617U3NLGjRvdkrfbJ6iK2funniz29xZCiKKiTp06GDt2bL6vX7RokbEeizpifsKECSaSuqzh7W53N+eZv/vuOwQHB5uoS27JycmOa+jmpvvazquvvorffvsNe/bswZo1a3DnnXdi//79uO+++4q9/QEhlc2+XIaWcwghih+KY17byy+/fNFVBh944IF8X884IRaZYByQcD1udX1/8sknZn/55ZfnOP/VV1/hnnvuMY8PHDiQI2n5qVOncP/99xtBDwsLQ/v27fHnn3+6NEgsvwRWjDD7oEwrWl0IIYoTiqOdSZMmYeTIkTlyTAQFBeXwYNIDeaHax4RxPwXB39/fI5bMllbc7vrObbOLtN2lQneHnffee89Y0FxyRbGeNWuWmaN2B8Hh1hczJKtkFCoXQpQuKI72jdasPccEN2Z8pKdyzpw5xqBhrM7SpUuxe/duXHvttaa8IoWcCad+//33PF3fvO/nn3+OQYMGmUjmhg0bYsaMGed1fdtd1HPnzkXTpk3N+/Tt2zfHwIJJrh5//HFzHZfEPfvssxg8eDCuu+66Aht89evXN4MFrgr65ptvHM9RT+hVqFWrlul/VFSUeU87H3/8selLuXLlzOdx4403whPxmKjvkkhoZUuoA73SkJzkuvXZQggPSVqRluGWje/tKp577jm8+eab2Lp1K1q1amXyUFx99dWYP38+1q5dawR0wIABxnuZF6+88gpuvvlmbNiwwbz+jjvuwMmT54/PYcKPd955xwgnA4V5/6eeesrxPNNFf/vtt8aDumzZMhPoO23atAL1berUqRg6dCiefPJJs2ro3//+t0kxvXDhQvP8Tz/9ZIw7rgrauXOnuX/Lli3Nc6tWrTKizelUeiGYgCuvRFvuxCOivksqwUGhSLX5IcArHXHHjyKwQrC7mySEcBHJ6ZloNnKuW957y6t9UN7fNT/PFKKrrrrKccyEUq1bt3Ycv/baa0bwaCEzuPd80NN52223mcdMMvXBBx9g5cqVRuhzg2uHP/30U2PtEt6bbbHz4YcfmvgjWunko48+wuzZswvUt3feece06+GHHzbHw4cPx4oVK8z5K664wgwO6F3o1auXWSFEy7pTp07mWj7HVNXXXHON8TwwiZa7vLMXQhZ1IfDy9sZpL0ucE07GuLs5QgjxDzp06JDjmBY1LVu6pOl2plua1vaFLGpa43YocKy5YE+RmRt0kdtFmnBljv165rSIiYlxiCZh5i666AvC1q1bTZIsZ3jM8+Smm24ywcn16tUzsU0ckNDlTjh4oTjzubvuustY9/QCeCKyqAtJok8oIjJP4kzc+b+wQoiSR6Cfj7Fs3fXeruLcAkcU6Xnz5hmrs0GDBibVJedm09LS8rwPLVJnOCfNzJAFud6VLv38ULNmTePW5hw8+0zLe/To0aYgFK1orhzi/DpXEjEQj/PZjHj3tCVgsqgLSbKv9R+aEq/sZEKUJigsdD+7YyvKDGmcD6a7mC5nztfSNbxv3z4UJwx8Y/AWRdEOI9IpnAWhadOmpj/O8Nh5FRAHIpyDp6ueorx8+XKTe4MwAp5u8bffftvMvfNzYE4PT0MWdSH5vvarmLLhBJ6o2AqXuLsxQghxARjlzNTLFC8OCF588cU8LeOi4rHHHjO1GGjVN2nSxMxZc/ltQQYpTz/9tAlw49wyBfeXX34xfbNHsTP6nAOAzp07G1c883ZQuOnyZjZM5uNgABmX+nJ+nJ8DI8c9DQl1IQkMrYxUJOBEYnaFLiGE8FTGjBmDe++91yQpqVy5slkW5crUyvmF78sltkxqxflpJljp06dPgapMXXfddXj//feNG5/R38xuyShye24OurAZ8c4gMwo2PQgUcy4H43MUdbq7U1JSzADm+++/P28lRnfiZSvuSQM3wypbnLc4ePBggfOK58ani3fjzTnbcH3b6hhzSxuXtFEIUbzwh3rv3r3mh55rakXxQ2uWrmxayIxEL+3fq0MF0CJZ1IWkUeomvOP3ObKONAAgoRZCiPzAxFUM4mIVRCaw4vIsitrtt9/u7qZ5HBLqQhJpO4ErfZZgQ5IqaAkhRH5hamjOITMKnY7dFi1amLllWtUiJxLqQuJdvQ3eTL8Vcb61kL3KUAghRF7Q7XtuxLbIHQl1IQmu3gSfZg6Ef7I3RtlsKjwvhBDCpWgddSGpVCHA7NMys5CQamW8EUIIIVyFhLqQBPr7oL3/AXT33oiTcaqiJYQQwrXI9e0CvvF+GeX9U7AxtjdQtZK7myOEEKIUIYvaBSR4h5p94inl+xZCCOFaJNQu4IyvJdSp8cfc3RQhhBClDAm1C0jzDzP79AQJtRCi5MGUm8OGDXMc16lTB2PHjs3zNVzhMm3atEK/t6vukxdME9qmTclNSCWhdgEZ5SyhtiWdcHdThBBlCBbW6Nu3b67P/fHHH0YEWRWqoLCqFXNvF4dYHj16FP369XPpe5U2JNQuwBZ4NoDsjIRaCFF8/Otf/zJ1lpk3+lxYnKJDhw5o1argqZiqVKliqk0VByyzGRBgLXMVuSOhdgE+QZZQ+6YqjagQovi45pprjKgyFacziYmJmDJlihHyEydO4LbbbkP16tWN+LKCFKtE5cW5ru+dO3eacpAsLMFazxwc5FYNq1GjRuY96tWrZ8pnpqenm+fYvldeeQXr1683Vj43e5vPdX2zVvSVV15pylGyytUDDzxg+mOHtbRZNYsVs6pVq2aueeSRRxzvld8CIK+++qophsFBAi39X3/91fF8WloaHn30UXN/9pllMVmSkzDdKb0DtWrVMq+NiorC448/jqJEy7NcgF9wFbMvlyahFqLUkZZU8Nf4BAA+Z39eMzOAzFTAyxvwC7zwff0r5PttfH19TZlIit7zzz/vyIxIkWZZRwo0Ra59+/ZGSENCQjBr1izcddddqF+/Pjp16pQvUbv++usRGRmJv/76C6dPn84xn20nODjYtIPCRbG9//77zblnnnkGt9xyCzZt2mTE0F4rOjTUCsJ1JikpyZS67NKli3G/x8bG4r777jOi6TwYWbhwoRFR7nft2mXuT7Hle+YHlsZ899138dlnn5la1l9++SUGDhyIzZs3m3KXH3zwAWbMmIHJkycbQWaFK27kp59+wnvvvYcffvjBlMRkqU4OQIoSCbULCAiJMPsKGXHubooQwtW8EVXw19w0AWg+yHq87Rdgyj1A7e7AkFnZ14xtmft02cunC/RWrC09evRoLF682FGHmW7vG264wYghNxa+sPPYY49h7ty5RoTyI9QU1m3btpnXUITJG2+88Y955RdeeCGHRc73pJhRqGkdBwUFmYEFXd3n47vvvjOlIb/++mtUqGANWD766CMzF//WW2+ZwQIJCwsz51m7ukmTJujfvz/mz5+fb6GmNc6By6233mqOeW+KPr0I48aNw4EDB4xgd+/e3Qx+aFHb4XPsQ69eveDn52eEPD+fY2GQ69sFBIVZQh2cFY/MrDJV3lsI4WYoVF27djVWIaGFyUAyur0JLWvWd6bLOzw83AgmRZeCkx+2bt1qCmjYRZrQ4j2XSZMmoVu3bkbE+B4U7vy+h/N7tW7d2iHSpFu3bsaq3759u+McLVmKtB1a17S+80N8fDyOHDli7usMj/n+dvf6unXr0LhxY+PWZjlOOzfddBOSk5ONe58Dg6lTpyIjo2jTR8uidgFB4WdHeV4JiDuThkpBCowQotTwnyMX5/q202SAdQ+6vp0ZthGugqJMS5nWIK1purVZ55nQ2qarl9YixZoiSNc152FdxfLly3HHHXeYeWi6rmnF05qme7ko8PPzy3FMq5di7iratWtnamPPmTPHeBRuvvlmY0H/+OOPZtDCQQPPc67+4Ycfdng0zm2Xq5BF7cI56lAk4UTCGXc3RwjhSjhnXNDNPj9N+JjnnOen87rvRUAhYX1nuo7pNqY73D5fzVKS1157Le68805jrdIS3LFjR77vzfrQnJ/lMio7K1asyHHNn3/+adzDnCdnpDndxvv378/ZXX9/Y91f6L0438u5ajvLli0zfaN16wo4T0/vwLklNnnMQDnn6zj3/d///td4Czg3ffLkSfMcXfl0x3Mue9GiRWagwnn5okIWtSsIDDc7by8b4k7GAtWsddVCCFEc0NVMURkxYoRx7dJ1a4eiSUuQYsq53TFjxiAmJiaHKOUFLUlGcw8ePNhYjrw/BdkZvgfd3LSiO3bsaALW6BJ2hvPWtFLpUma0NQPNzl2WRav8pZdeMu/FyOpjx44ZTwGD3+zz067g6aefNu9DzwOD0OiFYLu+/fZb8zw/I7rTGWjGQQKD8+jSr1ixoglq44Cjc+fOJsJ94sSJRrid57FLlUXNcHf+p/I/LCIiwoTcO89DnA9+aJyXYdg8XTmzZ8+GW/Hxxb+rfIvGKRMQmxns3rYIIcokdH+fOnXKuJ6d55M5V0xXLs8z2IyCw9/a/EKhouhyXpZBU4zCfv3113Ncw4jpJ554wkRnU/g4KODyLGcY3MbkLFdccYVZUpbbEjEKH+fPablSG2688Ub07NnTBI65Es47Dx8+HE8++aTREEajM8qbAw5CTXr77beNd4Dt2Ldvn9EZfhYUa1rZnNPmGnW6wH/55RezTKyo8LJxUZib4H8ao+74QXAy/j//+Y8J4d+yZUuOYAJn+AXgej6KPNcQ0tXDiL01a9agRYsWF3xPJgbgHANdORzVuYqHJq7GnE3ReGVgcwzuWsdl9xVCFD2MNKa1V7duXWMACFHU36uCaJFbXd/OC8wJXQq0rFevXm3EODcYFEGBp+uCMJqRE/occX366adwF5WC/M3+RGKq29oghBCi9OFRwWRcSE+4hOB8cNKecybO0KXD8+7kssS5eNfvY4QdXeLWdgghhChdeEwwGUPruWSAfv+8XNjMAnNuUAGPeT43UlNTzWYnISEBRUG95A1o4LMUU0+7JjJRCCGE8CiLmrlaOT/NqEFXwrlse3YebvmNdCwop2pfjVHpt2ElWhbJ/YUQQpRNPEKoGSk4c+ZMk8LtQpPqjFjk0gJneHy+tHRcrkCXun1joFpRkNngKnyWOQB/pdUtkvsLIYQom7hVqBlwTpFm6P+CBQtMZNyFYOo65nR1hsFkuaW0I1ynx4Xr9o1h90VB5bPBZMcVTCZEicWV2a2EyHLR98nX3e5uLq+aPn26EVD7PDNd1FxATlgZhuXZ7CXGhg4dalLjMTUdE7HTVb5q1SqMHz/enV1BJf9MNPfaC//UDKRlXAV/X49wVggh8gGzZnGNLHNAc40vj+2ZvYS4GCOUKVqZsIXfK36fSqxQf/LJJ2Zvr/hih1li7Jl1mO2GHbXD5PMUdy7i57prLlBnLdP8rKEuSkJPb8OsgOdxIKsKTiY9iKqhWospREmBvzH06DFNJsVaCFfABC6sruWsYSVOqPOTa4V5VM+F1Uu4eRLeFSqbfZhXIvYnpkqohShh0OrhjyqTL10oJ7UQF4LVvVjW0xWeGY9ZnlXiKW+t/Q72Ssap+ASg+j+LogshPBv+qLICUlFVQRLiYtBEqqsoVxGZZz/OxFP5q4sqhBBCXAgJtavw9kaSj2VFJ8dJqIUQQrgGCbULSfGraPap8cfd3RQhhBClBAm1C0kPsOpQZyQec3dThBBClBIk1C4kq5wVUOZ15oS7myKEEKKUIKF2IV4VrMLh3imn3N0UIYQQpQQJtQvxCbLWUvulSqiFEEK4Bgm1CwkIqWL2gelx7m6KEEKIUoKE2oWUrxhh9iG2eJxJy3B3c4QQQpQCJNRFYFFX8orHicQ0dzdHCCFEKUApRF2IV91LMTDgC2w97YcpSWmoGV7e3U0SQghRwpFF7Ur8AmELqop0+OKE6lILIYRwARJqF1MpyKo7Kte3EEIIVyChdjF3J36FMX4fI+nkYXc3RQghRClAc9QupkP8PIT4HMPncRJqIYQQhUdC7WI21bkbC7ccRVpasLubIoQQohQg17eLOdrkXvw38xrsSbNKXgohhBCFQULtYsIVTCaEEMKFSKhdTKRPElp67UH5xL3ubooQQohSgITaxdTYNRG/BLyAQSnTYbPZ3N0cIYQQJRwJtYsJDLXyfVdEPOKTle9bCCFE4ZBQuxi/YKvUZTgScSJJ2cmEEEIUDgm1qylfyezCvBJwIkkBZUIIIQqHhLqIhNqqoCWLWgghROGQUBeVRY0EHE+QUAshhCgcEmpXExhudr5eWUg4fdLdrRFCCFEWhfrgwYM4dOiQ43jlypUYNmwYxo8fX6D7LFmyBAMGDEBUVBS8vLwwbdq0PK9ftGiRue7cLTo6Gh6DXzmkeVt1qFPjY93dGiGEEGVRqG+//XYsXLjQPKZIXnXVVUasn3/+ebz66qv5vk9SUhJat26NcePGFej9t2/fjqNHjzq2iAhrSZSnkBoQZvbpCcfd3RQhhBBlsSjHpk2b0KlTJ/N48uTJaNGiBZYtW4bffvsNDz74IEaOHJmv+/Tr189sBYXCXLFiRXgqGeXCgeTDsCVJqIUQQrjBok5PT0dAQIB5/Pvvv2PgwIHmcZMmTYyFW9S0adMG1apVM5Y8Bwiehu3sPLXXGc1RCyGEcINQN2/eHJ9++in++OMPzJs3D3379jXnjxw5gkqVrKjnooDizPf96aefzFazZk1cfvnlWLNmzXlfk5qaivj4eMeWkJCAosYnyEp64psqoRZCCOEG1/dbb72FQYMGYfTo0Rg8eLCZZyYzZsxwuMSLgsaNG5vNTteuXbF792689957+Oabb3J9zahRo/DKK6/AHdnJyqXHITPLBh9vr2J9fyGEEGVcqGnFHj9+3FioYWFW4BR54IEHUL68FfFcXHBgsHTp0vM+P2LECAwfPtxxfPjwYTRr1qxI2+R/+dPo+GdrnLIF4aYzaagcZE0TCCGEEMUi1MnJyaYylF2k9+/fj6lTp6Jp06bo06cPipN169YZl/j54Fy6fT6dcHBR1PgGV0Fm+QhkJKWZutQSaiGEEMUq1Ndeey2uv/56E+EdFxeHzp07w8/Pz1jZY8aMwUMPPZSv+yQmJmLXrl2O47179xrhDQ8PR61atYw1TAv466+/Ns+PHTsWdevWNXPkKSkp+Pzzz7FgwQITbe5pVKrgj5NGqJmdLNjdzRFCCFGWgskYvHXppZeaxz/++CMiIyONVU1B/eCDD/J9n1WrVqFt27ZmI3RR87F9eRcjyA8cOOC4Pi0tDU8++SRatmyJHj16YP369SbqvGfPnvAo4g7iiYwv8LzvRBxXYQ4hhBDFbVGfOXMGwcGWlUhrlta1t7c3LrnkEiPYBZnrpgv9fEyYMCHH8TPPPGM2jyctCVefmY5TPkGYpsIcQgghituibtCggUn3yVSic+fORe/evc352NhYhISEFKY9pYOQalgSeSc+yrjWzFELIYQQxSrUdE0/9dRTqFOnjom67tKli8O6truxyzTlQrG24TB8kdlfNamFEEIUv+v7xhtvRPfu3c0csn0NNeFcMddXC6BSkL/Zqya1EEKIYhdqUrVqVbPZq2jVqFGjSJOdlDSqe51AK6/dSE246I9YCCGEuDjXd1ZWlqmSFRoaitq1a5uNRTJee+0185wAOqx6CjMCXkSt+NXubooQQogSzEWZeyxn+cUXX+DNN99Et27dzDlmB3v55ZfN+ubXX38dZR3vCpWAY4Bvyil3N0UIIURZE+r//e9/JtmIvWoWadWqFapXr46HH35YQm3yfVcx+8CMOKRmZCLA18fdTRJCCFFWXN8nT540JS3Phef4nMguzBHulWAylAkhhBDFJtSM9P7oo4/+cZ7naFkLwIuubwBhXglaSy2EEKJ4Xd9vv/02+vfvb9J32tdQL1++3CRAmT179sW3pjRR3hLqcCTguJZoCSGEKE6Lmnm2d+zYYdZMsygHN6YR3bx583nrQpdVoZZFLYQQojBc9CLfqKiofwSNsUgGo8HHjx9fqEaVJqGuhASsTJJFLYQQohgtalFAi1rBZEIIIS4SCXVRUT7c7IK8UnD0eJy7WyOEEKKEIqEuKgJCYfOy1k5v3rUPaRnK2CaEEKKI56gZMJYXDCoTZ/H2ttzfSbHwTzuFFXtO4LJGVhIUIYQQokiEmrm9L/T83XffXZBblmq8zgo156nnbYmRUAshhChaof7qq68K/g5lmdsnYen+JPz1wy7s2RKDVwY2h7e3l7tbJYQQogShOeqiJKw2OjRvjHL+/oiOT8HGw6fd3SIhhBAlDAl1EVPOzwc9Glsub7q/hRBCiIIgoS5K9i4BZj+DfwcuNIe/bYl2d4uEEEKUMCTURUlCDLDyM7Tc8SFCvVOwIyYR+44nubtVQgghShAS6qKkxQ1A+yHwHjwDLepVN6fk/hZCCFEQJNRFvZZ6wFigWmv0blbVnJL7WwghREGQUBcTVzWLRGuvXWh48EeVvRRCCJFvJNTFRFT6QfwY8Cpe8/0SG/+Y6e7mCCGEKCFIqIuLyg2xM7IffLxsaP/3cCD+iLtbJIQQogQgoS4uvLzgc8272JJVGyFZccicPBjIUPlLIYQQHizUS5YswYABAxAVFQUvLy9Mmzbtgq9ZtGgR2rVrh4CAADRo0AATJkxASaFRjQi8Wv45xNvKw+fQSmDeSHc3SQghhIfjVqFOSkpC69atMW7cuHxdv3fvXvTv3x9XXHEF1q1bh2HDhuG+++7D3LlzURLgYKR5izYYnv6QdeKvT4BNP7m7WUIIIUpLUQ5X069fP7Pll08//RR169bFu+++a46bNm2KpUuX4r333kOfPn1QEujdLBK3LG2PLzAI/8JUYPpjQERzIKKJu5smhBDCAylRc9TLly9Hr169cpyjQPP8+UhNTUV8fLxjS0hIgDtpXzsM4RX88UbK9ThdtQuQngSM7wFMugvY9DOQpsxlQgghSqhQR0dHIzIyMsc5HlOAk5OTc33NqFGjTJ1s+9asWTO4E18fb1zZJAKZ8MH4iOdNMhRkpABbZwA/DgHerg/s/cOtbRRCCOE5lCihvhhGjBiB06dPO7YtW7Z4hPubTNuRDtv9i4B//wF0Hw6E1QFsmUDVltkX75gLbP0FSM99ICKEEKJ049Y56oJStWpVxMTkzJXN45CQEAQGBub6GkaHc7ND69vdXNqwCsr5eeNwXDK2RieiWVQroForoOdI4OQeILBi9sWL3gSOrAH6vwt0vM+dzRZCCOEGSpRF3aVLF8yfPz/HuXnz5pnzJYlAfx8j1v/I/e3lBVSqn32clQnU7gpUrA00HZh9fu23wE/3AdtmAekpxdl0IYQQZUmoExMTzTIrbvblV3x84MABh9v67rvvdlz/4IMPYs+ePXjmmWewbds2fPzxx5g8eTKeeOIJlDTs7u/fNudRTcvbB+jzOjB0PRAUkX1+/ffAxinAD7cDoxsAP91vLfNKPlUMLRdCCFFmXN+rVq0ya6LtDB8+3OwHDx5sEpkcPXrUIdqES7NmzZplhPn9999HjRo18Pnnn5eYpVnO9GwaCW8vYMvReBw6dQY1wsqf/2Ja2jle/BKweSqwZRoQfxjYONnavLyBmp2BhlcBDXsDkS3++VohhBAlCi+bzWZDGeLQoUOoWbMmDh48aITendzy2XL8tfckXhrQDEO61S34DbKygMOrrIjxnb8Dx7bmfD64WrZo17scCAh2WduFEEIUjxaVqGCy0lj6kkL9yaLd2Ho0Ho0igx1bZEiAyWR2wXrXNTtZW+//A+IOADvnWdvexUDCUWDN19Z2x09Aw7Nr0BlB7ltO1rYQQpQAJNRupF/Lanh77nbEJqRi8qpDOZ4LKedrBLthZDA61Q3Dta2rw5u+8ryoWAvo+C9rY5DZ/mVnRXsJUKdb9nUL37DmuK94Hmh3VxH1TgghhCuQULuR6hUDseipy7HmwCnsiEnEjugE7IhNwL7jSYhPycCq/afM9v3KA/h+5UG8e1Nr1AzPYy7bGb9yQIOe1nYu+5Za1raf05K2E7uB7bOBxlfnjDwXQgjhViTUbiaqYqDZnElJz8SeY0nYGZuAzUfiMXHFfqzcexJ9xy7BC9c0w60da17YLZ4XQ+ZY1nb19tnnNv8MLPg/4LcXgMqNgMb9LNGu0dGKPhdCCOEWFExWAjhw4gyemrIeK/edNMeXN66Ct25ohciQcq57k83TgNVfWdZ2Vkb2+fKVgEZ9gbo9rMcMSCsXYu0DwwH/fFr4QgghLkqLJNQlhMwsG75cuhejf9uOtIwshAb64dVrm2Nga6uWt8tIOQ3s+h3YPgfY+Zt1fD46/Ru4+m3rcWoC8M0gILQGcP3ngM9ZZ03cQStwrUJlBa8JIcRZFPVdCvHx9sL9l9Uz1vTwyeux8fBpDP1hnUmY8tp1LUxFrouBbvZ1B+NQJTgA9asEAeVCgRY3WFtmOnBguSXaRzcAqactQeaWEm9Z1nZOHwIO/Q0c35Et0mTmMEv4fQKA8uGWJR5w1iJ3fsx7cTlZjQ5AlaY57yGEEGUY/RqWMBgF/vPDXTFu4S58tGAXZm08ihV7TqBX00i0rVURbWuFoUFEkBH288FgtcU7jplt+e4TSE7PNMbu4C518HSfxqgQcPZr4eMH1L3M2s6FjhhbVvYxRfbmr4G0Mzmvy0il4wbITLUC2LhdiLZ3Add+lJ1G9cyJnJnZhBCiDCHXdwlm46HTGD55HXbGJuY4X8HfB61rUrQrom3NMDSLCsH26AQs2h5rxHnfiZxiWqmCP04kpTki0V8f1AKXN3ahMGakAQlHgOS4bIvcbPE5H5/YBRxaDVz1irXEjERvAj7thsyIFljVZzoaVwtBxfL+wOnDlnhzMCGEECUMub7LCC1rhGLm492xZMdxrD1wCmsPxGH9oTgkpWXiz90nzJYbfj5e6FA7HD0aVzGu9MaRwfhj53H8Z+pGHDqVjHu++huD2lbHi9c0u2iXeg58/a0SnmH5uJYWtHMw27FtsMELa45545b//mUs/+ZRIfg24T4EZ5wAwuvBO6IJUIVbY6ByY6BSA2t5mhBClAJkUZcyGHTGZV0Ubbt40+KmpWyEuVEVdG1QGUF297YTZ9Iy8O5vO/DVsr3IslmW9sgBzVwfsFaAvny8cBc+/30dQpCAU/7VkZiagQCkYXXAgwjyOk/lMOY858CAol2lkSXifBzRVFHqQgiPQFHfZVioc4NR4rSi8yu2DC579scN2B6TYI6vbBKB/7uuxT/WexclsfEpGDZpncMrcEO7GibKPSk1w5xbvisWO3ftQHDCbjTwOoQGXkfQ0PswmvocQQVbzqkAB7d+BzTpbz1e/T+r1nfTAdmR67Tmv+wL+PhbwWxm7w94Oz32DQCCq1pz8iHcqltlSDUAEEIUALm+RQ78fQtWzbRNzYr45bHu+Gzxbny4YBcWbItFz3cX48b2NTC4ax0TrFaULNlxzMy9H09MQ3l/H7x2bQvc0N76IjPQ7bq21c1ms7XBwZPJWLb7OJbtOo5Xt8YiOTkDrcNS8WGvCqiVeRA4th04vh04tsOyqu1w2RnnzZ2Xn2WmAYdWFrzBDKJrdq31eP+fVr3w2l2AtncW+rMQQghZ1CJPdsUm4LmfNppUpnYubVgZ93arix6Nqlw4/3gBSM/Mwph5O0yREtKkajA+ur1dvgcGDJi7/+tVOHDyjBF4plxlPvVcSTxmlQjlsrDweta5zAxgx69AVrq1NI3CbbaM7McsaGKPXo/ndhi440egZkfrHis+BX591hJuCri9ytmYJpYlHlbXssIDw4DAimf352xcssaCK6Jo4E9eSpy1ZFCeEOEm5PrOAwl1weFXZPmeE/hq2T78vjXG/M6ROpXKGwublnZwucJFXx+OS8bj36/F6rMDgjs61zLBbOX8Cpa+NO5MGh79bi2W7jpujh+/sgGG9Wrk0gFFnhxZB+yYa82NNx9knWOE+nvN8n8PzrFTrG/9FqjT3Tq3eyGwYZJVb7zDkOxo+nkjswcWJhCP+wzAr7yVSc55Y9IZrmUPjir6YDu254yVSQ/BkWfPZQArxwPJpyyh5MDHDE7CrXY59mfPcSBjPg+f7IEL75t+5uxn5FS29eDfZ9f5J1qrCNLO7vleSceApONnt2PAmePWZ3TDF0DLG7Nz3S8fB9Tumn3OJZ9DhtVXtpd9s59jboH0JGs5I//PKkRYUymcUuFj5REo9RyS61u4Es5td61f2WwHT57B18v34Ye/D5plXq/8ssUEoFGsKa5c511QgR6/eLe5X2pGFoIDfPHmDa3Qv9V5LOELwKVbE4Z0xKg52/DF0r34YMEubDmagPduaV3owUS+iGpjbc5wGdnDK4CTe4FTe4HEGEtAzBbn9PiUJUJcn263+OzEbAbWf28JjF2oYQP++qTgbbz5G6DZQOsx65gvGW1VV+s5MvuatROt9e8UdGaW49y8b2D2Y87XU/TijwDxh6x9pweAyg2t1//9heVZYAzALRPPlmX1sXLJ2zIL1t6b/gc0v856vPUX4MchQJ1LgXtmZl8z8XpriV9B4Odth4l9Vn1hJexxFmpWmjOfQ6A1+OHnYfY85ucRaL2v8bJEW/vuw4Gw2tbrl40FFrwGtLoFuH68dY79//6W87eLoh4UaYm2PRaCKxmYe99+X1GmkFCLAsHqXc/3b2as1KlrD2PCn/uwKzbR7Lm1rx2GWzrUNELrSJySC3uOJRoXN++RwRBzAB3rhOHdm9qgVqXCuSN9fbyNNd6sWghGTN1ovACDPv4T/727A+pWroBih2u9GXHO7UKwPClFmpnfKtbMPk9L76pXgUpnhZB4+1miYILd/HLuaVHSoqUVyYQxZjtp7e2WHeHA4eAKy9q2Q5fJL0NzLpPLD7T+7UIdEnU20U169vMMZuS8PdtYrqK1tw9Q2Lbkk9l759gB58Q69oBI53OEny0HOf7MdhdkWdv+QZZVXqGKtZWvbPXTHFe2BhyO1zcDuj6WPQ1COIha/BYKTNOB2YJKoSXOnwMHOSx243tW9DmA4eCNUyncU8hzSw7Ettnvy9K1m34C6l2RPYjhFEtaAuBX4fwWOf9v6Wng98B83qeAWp2zvRPMMMi0v6EMkqxV8L6LIkGub1Eo+PWhm/nr5ftN0BmXVNmTrgxsE4VbOtZC6xqhjojzLUfiMW7RLszeeNThQu9avxIevaIButSv5PJlYOsPxuGBb1YhJj7V1PjmnPdljaqgTMMP3v45n9oHHF1viZi9Zjmzyf14r7XPSHHazh5zMMFMc3Snc76dosx9i+uBqi2z3bu0DC92rp1ufGMh05otb63Ft5/nAILu8KJ2DyfGAss/stzpjE3ISLb2HBDwM7A/5sDAWL9nVwO0uBGo3CBboM3nkM8pHPbP7qkwcRD0WBwGju8E+r1l5dInC0cBi9+0Bj7XjrPOJZ0ARp8daNjn3zlY4efHNtgHQnS1O/PA4mwv0J8fWl6PljcDN/w3e4plbEtrkGAGnM2ByGbW4MZ50CcKhFzfotigsF7asIrZYhNS8POaw5j090HsPZ5kamhzY0IVRmmv2ncS87fFOl7bq2kEHr6iAdrVyk8mlIuDGdp+ebQ7Hpy4GmsOxOHeCX/j/VvbXrRrvVTgPBgyiWjq5HyelibnxwtDYUWUwsa56tzOF1fZVU5Z0ItRGAqaOY99M4JfNe/r6l1uCW71DtnnOOdthwOp5NSc7n1n6LKnyBqhdbLV6M6n5W4fEBB6eBKjre3gXznvE1TVEu/I5tZr7EsYOVBo1NuqHUDiDlgeAxNQWTt7MHf6oJMnyO/sVEug64IpszIt74zzNJO9ZgG9Vs4ZErs/ke0RYszDph+Bqq2Atndk3+/ACqBGp2IP9pRFLVwOv1Ksn03BZi5yzj3bYUxX/1ZRePjy+mhazamoRxGTmpGJp6dswIz1R0wbRt/Y2rHkS4hSAX/KGaCXxiC1JMva5xQIA9boTncO2itItDu9AjGbrIC72C1AzBZrH7c/79c9uipb+Oa/BvzxTs6KexRurobIDRML4RwPUN4ScQ4Ern7HsujtMQvMiVD3UqDbUOscxXj8FWeDFjmFkk+Ju/MnoEGv7BiLWcOBJtdkD1r5+b5eFXg+2iWVAGVRC7db2Z3rVTLbSwObG3H8bXM0aoSVxwOX1XPLPHGArw/eu6WNWbbFwLUnp6w3xUjuvMR1wTlcHjZj/WFc0TgCHerIJSiKGYqHCfo7W6nOVdDajWprbc5wSuDYtmzxToq1pkc4WOCebnc7tKzpuWF8gB0OHjifzqkMegec4w7s0y205s+FgxA7nLrZNS9nf/m+jL1wxsQrcJASCgSEZlfsc67gx6WTdqq1Bi59EqjcKPsc+0TPgRuyNMqiFmWKrCwbXp25xQS+kRf6N8V9lzoFEBUQzslzbp5pV+1Z1Px9vDHujna4qtnZZUlCiPy5qU0MhD0WwB4PkGzFB5ilbBlWNT+7MMduBQ6vsQYB9hgLcuCv7DwFDFy0xzh4ELKohTgPXE/90oBmxrL+eNFu/N+srTiTlonHrmxQoEC2+JR0TP77oAmiY4IVc28vGG/B7mNJeGjiaoy9tQ2uacXoZyFEvubn/StYW36JOM9qCkaylyIk1KLMQUF+pm8TI9bv/LbDZEOjWD/bt/EFxXr3sUT87899+HH1IfMaEhroh1s71cTdXeogMjgAT/+4wSw7YwKXlPQss8bcU9h/IgnvzduBbdEJCC7ni5ByftY+0C/H48iQAFzWsIpZ6iaEcC8SalFmefTKhibzGa3qTxfvRnJaBl4a0NxY3RmZWSahy7boeGw7mmDtoxNMGVA7jSKDcE/XuriubRTK+2f/KTF1aTk/bxPx/tSU9UjJ51z4HzuP4Z25241FPqB1FB7sUQ+1K7lmPv/0mXR8uGAn/rd8H9Iz8zfbxdStz/dvaubchRDuQ3PUoszz7V/78cK0TSaos1OdcBNktiMmIUe0uh0a3D2bRGBIt7pm/ff5LHD+WTFrW37mwrnW++2527BsV8764XSl03X+UCEi5Jk/feKK/Xh//k7EnUl35Gqn9c/nElLSEZ+cYe1TMhCfbO1X7z+JU2ev57pztr9RAbPOCSHOj3J954GEWuTGz2sOGev3bL4WQ6CfDxpXDUbTasFmLXiTaiGmUAjTlOYH/mm9PXe7o8jIk1c1wmM9G+Zwo7/723bM3hjtCEKj5X1po8qYsGwfFu845riWgwOuOWfmt/y+97wtMSaVKte02z0A/7m6KS7Ph4V8Ojkd4xbuMkFytMA5aLitUy0Mv6oRKgU5ZfQSQpQNoR43bhxGjx6N6OhotG7dGh9++CE6deqU67UTJkzAkCH2XMcWAQEBSElJydd7SajF+WCpzDX7T5l85RTnmmHlC13Mg39eHy3YhXfn7TDHXD9+V5fa+GD+TkxedchEjdMov75tDQzr1dCkaLWz6fBpI/KzN2VncetcNxwP9qhv0qzy3hxYZHGfZe15XVxymnnPv/ZaRTEqB/njiasamdSuBZ1z5pz2qNnb8OtmazDBXOyPXtkA93SrY5a8CSHKgFBPmjQJd999Nz799FN07twZY8eOxZQpU7B9+3ZERETkKtRDhw41z9uh+zEyMn9LYSTUwh38d8kevD57q3ns4+3lSLXaq2kknu7T2FjueeVF/2zxHvy89lC+55ftdcjv617XuM4LW5BkxZ4T+L9ZW7DpsFX4omZ4IJ6/uin6NK/q8rSvQpQFDpUkoaY4d+zYER999JE5zsrKMo1/7LHH8Nxzz+Uq1MOGDUNcXC4L4fOBhFq4i2+W78OL0zc7CpA827dJgRKjHD2djP8u2WsSyKRlZBpr38fLywglDX/vs3sr4Uy4cVMzyYwr16D/vPYwRs/dZnKnk24NKpkAPM1fC1FK11GnpaVh9erVGDFihOOct7c3evXqheXLl5/3dYmJiahdu7YR9Xbt2uGNN95A8+bNi6nVQlwcd3Wpg8ZVQ0xE+cUUIKkWGoiRA5qZzR1wYMClZle3rGpc8p8t2WMC4Pq9/wfu7lLbVFTjUjUhhGtx6yLJ48ePIzMz8x9uax5zvjo3GjdujC+//BLTp0/HxIkTjVh37drVjE5yIzU1FfHx8Y4tISGhSPoiRH7oVDccXRtULtHuYi5Fe7J3Y8wf3gN9mkcaN/5Xy/bhincW4fuVBxxufSGEayhx2Qy6dOli5rTbtGmDHj164Oeff0aVKlXw2Wef5Xr9qFGjEBoa6tiaNXOPNSJEaYOBb5/d1QHf/KuTWXN9MikNI37eiGvHLTXLu4QQrsGtru/KlSvDx8cHMTExOc7zuGrVC5R5O4ufnx/atm2LXbt25fo83erDhw93HB8+fFhiLYQLYYnTOUMvxTfL9+O933eYgLMbPlmOhhFBJqLdHpnOaBjnSHXOqTM7XIUAX2vv74vyAT6Ox8yQ1qNRFTSPCinRHgghSrRQ+/v7o3379pg/fz6uu+46c46ubB4/+uij+boHXecbN27E1VdfnevzXLrFzQ7d30II1+Ln4417u9fFwDZRJrvapFUHsTM2sdD3HT13u8mfPqBVNZOtjUvnhChruD2FKK3dwYMHo0OHDmbtNJdnJSUlOdZK081dvXp148Imr776Ki655BI0aNDARH5z/fX+/ftx3333ubknQojKQQF484ZWZq334bhkY1HTcvY6G4xmj0rnMa1q5ktPSs3EmbQMJKVl4kyqtWc614Mnk7Fwe6xJ2PLBgl1mY8IZCvY1raq5LL3qxcD15Ut2HMPSXcdNchhi9SpnFUQ+rhIUgH4tq+HyxlU8bu15YmoG/thxDHuOJ6F/y2qo44YStKIECPUtt9yCY8eOYeTIkSaAjHPPv/76qyPA7MCBAyYS3M6pU6dw//33m2vDwsKMRf7nn3/KnS2EB8EffFf86FNI5m+NwS/rj5hMbcy3vi16u7G0W9UIRf0qQWZdup+Pl9n7envDl8vWfLzg5+2NahXLmUxs1SsGFqodSakZZi05xZntYB74gjBt3RFT8KRfi6oY2Lq6ifpne90Bl/n9vjUWv2+JwfLdJ5CWaaXKZZKcF65pits71dJUg4fh9nXUxY3WUQtRMmFhkbmbo/HLhiOm9ndBostpiV/RJAJXNolA25oV88zQxp/E6PgU7IxJxOYj8aZYyqp9pxyCRjgYaFc7zMyh1wovD3tLzv055SEzzLHN9rXnds8DvQL0DrSrVbFIhZFtYj9+3xpjNnvSGju1K5U3aXGZc96erpZekSrBnp0qNik14+zALR5bj1rFc8Iq+OOVgc0RVciBWXFQohKeFDcSaiFKPscTU7FgW6wR7/SsLGRm2pCRxS3L2mfaTNGRLUfisebAqRw53LnWmwJL0W5RPRQHTiYZUd4Vm2jm1bmnJX8uNcICzetYpIQFWQqS7Y2DipV7T5pkNXM2HXUUSCEU+n/3qIebO9Q0c/2uglH4P60+ZJbM0bVth2OCdrXCTFa8q5pFGK8EVeDLZXtNbvq0jCyEV/DHm9e3RO/m+QvqLSqYZOd4UiqOxKXgaFwydsQkOoR5/8kzjtS6zlQLLYev7+3k8fEMEuo8kFALUbY4lZRm3NUUdu7tc8p5QYuZrntGrnPtOwWaQW2usHwphEt3HcOMdUfw25YYR11zCvYTVzU0rvGLdYvz53zFnpP4buUBzN0U7fACsOwq64v3ahZpBii06HNje3QChv6w1liqhPnhXxzQDEEBRTtLGncmDb9uijbiS0E+cjrFuOijT6fkmTY3IjjAFMthbv6GEcGmXC0HWhyMfXlPB7Svnf/Mf8WNhDoPJNRClF2YFW7twTgj2gu3WYFqFGCuA+cPfcNI7oNMoBpzpRc1yWmZ+OHvA6ZS2fHENHOO7/9k70YFyqN+Puu5ZfVQ3N65lnGx51dsUzMyMea3HRj/xx5jsXIA8d4tbfJdua0g7DueZCz5KasOmfKyueHtRUEuZ+IN6laqYEq+WlvwPyq5cVB27//+xtoDcWZwMu72dujZNH91IIobCXUeSKiFEJ4Go95Zu5zFV+wWP4PlmAHusobZmezoCo5JSMHeY0nYeyLJCB0tSKZytVvPFfx9MLBNdRMU1rJG6EW3icFzT05eb6L3KZYdaocjtLyfsVZDyln70EBrvTsfVw0tZwY8F4psp+T8ve8UPv9jD+ZtjXG4rym+neqEoVrFQDPHHBVKcQ40VnNBpgQ4+HnkuzVmMEbPxKjrW5pphbzgFMkXS/eaGAgOAFhtrmv9yihKJNR5IKEWQngqFGkK2JdL95plaqRD7TAT2EXrf9+JJKSkZwe1OXMx1vOFiE9Jx0vTN2Pq2sP5up5TBhTrZlEhaFYtxLFnsBq9GbM3ReOLP/Zg/aHTjtfQFc8qbxeT//58MD7huZ824qc1VmrpZ/o2xkM96ue4Pwc9nAr5fKmVs/5cGIdAz0ZRuc8l1HkgoRZCeDonElNN4ZOvV+w3c9rO0EqsGRZoXPacR+eewWEMjCsqNh46jf0nk8xAglt8csbZfboRcz7ef+LMeef/uTyOAXWMpicBvt64vl0N/Kt7HTSIKJqgL5vNZoLj+DmSe7rWwchrmhnPw89rDuOLpXuw+1iS4zPl0rlbOtbE/K2x+O6vAw4PBde/P3lV40J5J3JDQp0HEmohREmBwVS0CgP9fBzCzOhzV0aHuwpKCYPA6EZmVDb3W47G48DJ7DXnlYP8cdcldXDnJbX+Mb9cVHyxdC9em7nFPGZgIKcKOKdPggN8cWunmhjctU6OkrB093+0YCcmrzrkWAbIAjR0iTepGuKSdkmo80BCLYQQxQctbq5xTkhJR7cGlVHOr/izs01fdxhPTVnviCCnhc+Utzd3qJHnMjvGAHwwfyemrjts5tLpOb+mVRRe6N8UkSHlCtUmCXUeSKiFEKLs8efu4/hh5UH0bVEVvZtF5pn05lx2xiRg7O87MWvjUWOF//HsFWbevbi0yO0pRIUQQoiipmv9yhcdyc3kKePuaIeHj5zGnmNJhRbpgiKhFkIIIfJB86hQsxU3nheRIIQQQggHEmohhBDCg5FQCyGEEB6MhFoIIYTwYCTUQgghhAdT5qK+s7KstHBHjx51d1OEEEKUUY6e1SC7JuVFmRPqmJgYs+/UqZO7myKEEKKMExMTg1q1auV5TZnLTJaRkYG1a9ciMjIS3t6F8/wnJCSgWbNm2LJlC4KDiyaxvBCeiL77oiyS4MLvPS1pinTbtm3h65u3zVzmhNqVxMfHIzQ0FKdPn0ZIiGsStQtREtB3X5RF4t30vVcwmRBCCOHBSKiFEEIID0ZCXQgCAgLw0ksvmb0QZQl990VZJMBN33vNUQshhBAejCxqIYQQwoORUAshhBAejIRaCCGE8GAk1IVg3LhxqFOnDsqVK4fOnTtj5cqV7m6SEEXKkiVLMGDAAERFRcHLywvTpk1zd5OEKHJGjRqFjh07miQnERERuO6667B9+3YUFxLqi2TSpEkYPny4iQBcs2YNWrdujT59+iA2NtbdTROiyEhKSjLfdQ5ShSgrLF68GI888ghWrFiBefPmIT09Hb179zZ/D8WBor4vElrQHGF99NFHjnRwNWvWxGOPPYbnnnvO3c0TosihRT116lRjXQhRljh27JixrCngl112WZG/nyzqiyAtLQ2rV69Gr169HOeYN5zHy5cvd2vbhBBCFC1MIUrCw8NRHEioL4Ljx48jMzPTFPZwhsfR0dFua5cQQoiihd7TYcOGoVu3bmjRogWKgzJX5lIIIYS4WDhXvWnTJixduhTFhYT6IqhcuTJ8fHwcta3t8Lhq1apua5cQQoii49FHH8XMmTPN6ocaNWqguJDr+yLw9/dH+/btMX/+/BzuEB536dLFrW0TQgjhWhhzTZFm8OSCBQtQt25dFCeyqC8SLs0aPHgwOnTogE6dOmHs2LEmVH/IkCHubpoQRUZiYiJ27drlON67dy/WrVtngmpq1arl1rYJUZTu7u+++w7Tp083a6ntsUisTR0YGIiiRsuzCgGXZo0ePdr8p7Vp0wYffPCBWbYlRGll0aJFuOKKK/5xnoPWCRMmuKVNQhTHUsTc+Oqrr3DPPfcU/ftLqIUQQgjPRXPUQgghhAcjoRZCCCE8GAm1EEII4cFIqIUQQggPRkIthBBCeDASaiGEEMKDkVALIYQQHoyEWgghhPBgJNRCiCLN6DRt2jR3N0OIEo2EWohSClMbUijP3fr27evupgkhCoCKcghRiqEoMx+xMwEBAW5rjxCi4MiiFqIUQ1FmjXTnLSwszDxH6/qTTz5Bv379TAWgevXq4ccff8zx+o0bN+LKK680z1eqVAkPPPCAqaDlzJdffonmzZub96pWrZopB+jM8ePHMWjQIJQvXx4NGzbEjBkzHM+dOnUKd9xxB6pUqWLeg8+fO7AQoqwjoRaiDPPiiy/ihhtuwPr1641g3nrrrdi6dat5jmVb+/TpY4T977//xpQpU/D777/nEGIKPUsAUsAp6hThBg0a5HiPV155BTfffDM2bNiAq6++2rzPyZMnHe+/ZcsWzJkzx7wv71e5cuVi/hSE8HBYPUsIUfoYPHiwzcfHx1ahQoUc2+uvv26e55//gw8+mOM1nTt3tj300EPm8fjx421hYWG2xMREx/OzZs2yeXt726Kjo81xVFSU7fnnnz9vG/geL7zwguOY9+K5OXPmmOMBAwbYhgwZ4uKeC1G60By1EKUY1o6mlepMeHi443GXLl1yPMfjdevWmce0cFu3bo0KFSo4nu/WrRuysrKwfft24zo/cuQIevbsmWcbWrVq5XjMe4WEhCA2NtYcP/TQQ8aiX7NmDXr37o3rrrsOXbt2LWSvhShdSKiFKMVQGM91RbsKzinnBz8/vxzHFHiKPeH8+P79+zF79mzMmzfPiD5d6e+8806RtFmIkojmqIUow6xYseIfx02bNjWPuefcNeeq7Sxbtgze3t5o3LgxgoODUadOHcyfP79QbWAg2eDBgzFx4kSMHTsW48ePL9T9hChtyKIWohSTmpqK6OjoHOd8fX0dAVsMEOvQoQO6d++Ob7/9FitXrsQXX3xhnmPQ10svvWRE9OWXX8axY8fw2GOP4a677kJkZKS5hucffPBBREREGOs4ISHBiDmvyw8jR45E+/btTdQ42zpz5kzHQEEIYSGhFqIU8+uvv5olU87QGt62bZsjIvuHH37Aww8/bK77/vvv0axZM/Mcl1PNnTsXQ4cORceOHc0x55PHjBnjuBdFPCUlBe+99x6eeuopMwC48cYb890+f39/jBgxAvv27TOu9EsvvdS0RwiRjRcjypyOhRBlBM4VT5061QRwCSE8F81RCyGEEB6MhFoIIYTwYDRHLUQZRbNeQpQMZFELIYQQHoyEWgghhPBgJNRCCCGEByOhFkIIITwYCbUQQgjhwUiohRBCCA9GQi2EEEJ4MBJqIYQQwoORUAshhBDwXP4f+8e17md6aYEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制损失曲线\n",
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-medium355M-lora-sft.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-lora-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as an elephant.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a cumulus cloud.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    # 输入格式化\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    # 生成回答\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    # 提取有效回答\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .replace(\"<|assistant|>\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    # 对比output\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [03:05<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# 保存测试集的结果\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .replace(\"<|assistant|>\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"instruction-data-with-response-lora.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "# 检测ollama是否正在运行\n",
    "# 运行ollama: ollama run llama3\n",
    "import psutil \n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Lanunch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "# 通过REST api访问ollama\n",
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "        prompt,\n",
    "        model=\"llama3\",\n",
    "        url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # 构造请求数据\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0.,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # dict转化json并编码\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # 解析返回结果\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    \n",
    "    return response_data\n",
    "\n",
    "model_name = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  49%|████▉     | 54/110 [00:17<00:33,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: The correct output for the given instruction is:\n",
      "\n",
      "Irony is a figure of speech in which words are used in such a way that their intended meaning is different from the actual meaning of the words.\n",
      "\n",
      "Score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [00:36<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 109 of 110\n",
      "Average score: 44.74\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 封装上述功能\n",
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20,\n",
       " 60,\n",
       " 98,\n",
       " 60,\n",
       " 20,\n",
       " 0,\n",
       " 80,\n",
       " 100,\n",
       " 60,\n",
       " 40,\n",
       " 95,\n",
       " 20,\n",
       " 20,\n",
       " 60,\n",
       " 4,\n",
       " 40,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 4,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 60,\n",
       " 95,\n",
       " 20,\n",
       " 95,\n",
       " 20,\n",
       " 40,\n",
       " 60,\n",
       " 40,\n",
       " 20,\n",
       " 98,\n",
       " 20,\n",
       " 100,\n",
       " 95,\n",
       " 20,\n",
       " 12,\n",
       " 85,\n",
       " 100,\n",
       " 20,\n",
       " 60,\n",
       " 60,\n",
       " 92,\n",
       " 20,\n",
       " 80,\n",
       " 95,\n",
       " 67,\n",
       " 95,\n",
       " 90,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 0,\n",
       " 20,\n",
       " 60,\n",
       " 95,\n",
       " 20,\n",
       " 20,\n",
       " 2,\n",
       " 60,\n",
       " 40,\n",
       " 20,\n",
       " 20,\n",
       " 44,\n",
       " 85,\n",
       " 20,\n",
       " 60,\n",
       " 4,\n",
       " 80,\n",
       " 80,\n",
       " 92,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 100,\n",
       " 95,\n",
       " 100,\n",
       " 20,\n",
       " 20,\n",
       " 75,\n",
       " 20,\n",
       " 20,\n",
       " 0,\n",
       " 40,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 60,\n",
       " 20,\n",
       " 20,\n",
       " 0,\n",
       " 40,\n",
       " 95,\n",
       " 20,\n",
       " 85,\n",
       " 85,\n",
       " 20,\n",
       " 20,\n",
       " 60,\n",
       " 0,\n",
       " 20,\n",
       " 20,\n",
       " 80,\n",
       " 20,\n",
       " 20]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
