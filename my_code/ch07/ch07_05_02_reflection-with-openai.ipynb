{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch07\", \"03_model-evaluation\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 OpenAI API (使用deepseek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatgpt(prompt, client, model=\"gpt-4-turbo\", system_prompt=None):\n",
    "    messages = []\n",
    "\n",
    "    # 增加system prompt\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        seed=123,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from config_manager import config_manager\n",
    "\n",
    "config = config_manager.config\n",
    "api_key = config.get('api', {}).get('deepseek_api_key')\n",
    "if not api_key:\n",
    "    raise ValueError(\"需要提供API密钥\")\n",
    "model = config.get('api', {}).get('model', 'deepseek-chat')\n",
    "\n",
    "# 初始化OpenAI客户端来访问Deepseek API\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=config.get('api', {}).get('deepseek_api_url'),\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备输入\n",
    "prompt = f\"Respond with 'hello world' if you got this message.\"\n",
    "run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载json数据\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = \"../../ch07/01_main-chapter-code/\"\n",
    "json_file =  \"instruction-data.json\"\n",
    "\n",
    "with open(data_dir + json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp as pprint\n",
    "\n",
    "pprint(json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化指令数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化指令数据的工具函数\n",
    "def instr_prompt_no_input(ins, outp):\n",
    "\n",
    "    sys_prompt = \"You are a helpful, precise but picky assistant for checking the quality of a given instruction.\"\n",
    "    prompt_template = \"[Instruction]\\n{ins}\\n\\n[The Start of Answer]\\n{outp}\\n\\n[The End of Answer]\\n\\n[System]\\n{criteria}\\n\\n\"\n",
    "    criteria = \"We would like you to answer several questions related to the quality of a given instruction. \\n\" + \\\n",
    "                \"1. Why this instruction is not good? First analyse the instruction based on Complexity of the Topic, Level of Detail Required, Knowledge Required, Ambiguity of the Instruction and Logical Reasoning or Problem-Solving Involved. \\n\" + \\\n",
    "                \"Then analyse why this answer is not good for the given instruction? Analyse based on the Helpfulness, Relevance, Accuracy and Level of Details. \\n\" + \\\n",
    "                \"Finally analyse why this bad instruction lead to a bad answer. \" +\\\n",
    "                \"2. Based on the reason you provided, generate a new and complete instruction which is complex and difficult to answer directly. \" + \\\n",
    "                \"Make sure the new instruction is relevent but independent to the original instruction, which can be answered without knowing the original instruction, put the new instruction in the format of [New Instruction] your instruction [End]\" +\\\n",
    "                \"3. Answer the newly generated instruction as detailed as possible, in the format of [New Answer] your answer [End] \\n\"\n",
    "    prompt = prompt_template.format(\n",
    "        ins=ins, outp=outp, criteria=criteria\n",
    "    )\n",
    "    return sys_prompt, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看原始的指令数据\n",
    "pprint(json_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用工具函数优化指令数据\n",
    "entry = json_data[2]\n",
    "sys_prompt, prompt = instr_prompt_no_input(entry[\"instruction\"], entry[\"output\"])\n",
    "output = run_chatgpt(prompt, client, system_prompt=sys_prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取优化后的指令和回答\n",
    "import re\n",
    "\n",
    "# 提取优化后指令的工具函数\n",
    "def extract_ins(text, no_input=True):\n",
    "    if '[New Instruction]' in text:\n",
    "        pattern = r'(\\[New Instruction\\])(.*?)(\\[End\\]|\\[New Answer\\]|New Answer:)'\n",
    "    else:\n",
    "        pattern = r'(New Instruction:)(.*?)(\\[End\\]|\\[New Answer\\]|New Answer:)'\n",
    "    segments = re.findall(pattern, text, re.DOTALL)\n",
    "    if len(segments) == 0:\n",
    "        seg_ins = ''\n",
    "    else:\n",
    "        seg_ins = segments[0][1].strip()\n",
    "    if seg_ins.endswith(\"\\n\\n3.\"):\n",
    "        seg_ins = seg_ins[:-4]\n",
    "    return seg_ins\n",
    "\n",
    "# 提取优化后回答的工具函数\n",
    "def extract_oup(text, no_input=True):\n",
    "    if '[New Answer]' in text:\n",
    "        pattern = r'(\\[New Answer\\])(.*?)(\\[End\\]|$)'\n",
    "    else:\n",
    "        pattern = r'(New Answer:)(.*?)(\\[End\\]|$)'\n",
    "        # pattern = r'(\\[New Answer\\]|New Answer:)(.*?)(\\[End\\]|$)'\n",
    "    segments = re.findall(pattern, text, re.DOTALL)\n",
    "    if len(segments) == 0:\n",
    "        seg_oup = ''\n",
    "    else:\n",
    "        seg_oup = segments[0][1].strip()\n",
    "    return seg_oup\n",
    "\n",
    "# 提取优化后指令和回答的工具函数\n",
    "def extract_instruction(text):\n",
    "    if text == '':\n",
    "        return []\n",
    "    seg_ins = extract_ins(text, no_input=True)\n",
    "    seg_oup = extract_oup(text, no_input=True)\n",
    "    return [seg_ins, seg_oup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备输入\n",
    "prompt = f\"Respond with 'hello world' if you got this message.\"\n",
    "run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从之前的output中提取优化后的指令和回答\n",
    "new_instr, new_outp = extract_instruction(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 响应优化的工具函数\n",
    "def res_gen_prompt_no_input(ins, outp):\n",
    "\n",
    "    sys_prompt = \"You are a helpful, precise but picky assistant for checking the quality of the answer to a given instruction.\"\n",
    "    prompt_template = \"[Instruction]\\n{ins}\\n\\n[The Start of Answer]\\n{outp}\\n\\n[The End of Answer]\\n\\n[System]\\n{criteria}\\n\\n\"\n",
    "    criteria = \"We would like you to answer several questions related to the quality of the answer to the given instruction. \\n\" + \\\n",
    "                \"1. Why this answer is not good for the given instruction? Analyse based on the Helpfulness, Relevance, Accuracy and Level of Details. \\n\" + \\\n",
    "                \"2. Based on the reason you provided, generate a better answer, new and complete, as detailed as possible, in the format of [Better Answer] your answer [End] \\n\" \n",
    "    prompt = prompt_template.format(\n",
    "        ins=ins, outp=outp, criteria=criteria\n",
    "    )\n",
    "    return sys_prompt, prompt\n",
    "\n",
    "\n",
    "def res_gen_prompt_input(ins, inp, outp):\n",
    "\n",
    "    sys_prompt = \"You are a helpful and precise assistant for checking the quality of the answer to a given instruction and its input.\"\n",
    "    prompt_template = \"[Instruction]\\n{ins}\\n\\n[The Start of Input]\\n{inp}\\n\\n[The End of Input]\\n\\n[The Start of Answer]\\n{outp}\\n\\n[The End of Answer]\\n\\n[System]\\n{criteria}\\n\\n\"\n",
    "    criteria = \"We would like you to answer several questions related to the quality of the answer to the given instruction and corresponding input. \\n\" + \\\n",
    "                \"1. Why this answer is not good for the given instruction and corresponding input? Analyse based on the Helpfulness, Relevance, Accuracy and Level of Details. \\n\" + \\\n",
    "                \"2. Based on the reason you provided, generate a better answer, new and complete, as detailed as possible, in the format of [Better Answer] your answer [End] \\n\" \n",
    "    prompt = prompt_template.format(\n",
    "        ins=ins, inp=inp, outp=outp, criteria=criteria\n",
    "    )\n",
    "    return sys_prompt, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看响应优化的效果\n",
    "entry = json_data[2]\n",
    "\n",
    "system_prompt, prompt = res_gen_prompt_no_input(ins=entry[\"instruction\"], outp=entry[\"output\"])\n",
    "output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取优化后的响应\n",
    "def extract_response(text):\n",
    "    if text.count('[Better Answer]') >= 2:\n",
    "        pattern = r'\\[(Better Answer)\\](.*?)(\\[End\\]|\\[Better Answer\\]|$)'\n",
    "        segments = re.findall(pattern, text, re.DOTALL)\n",
    "    else:\n",
    "        # pattern = r'\\[(Better Answer)\\](.*?)\\[End\\]'\n",
    "        pattern = r'\\[(Better Answer)\\](.*?)(\\[End\\]|End|$)'\n",
    "        segments = re.findall(pattern, text, re.DOTALL)\n",
    "    return [segment[1].strip() for segment in segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = extract_response(output)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_process = json_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
