{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/ch07/03_model-evaluation\n"
     ]
    }
   ],
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch07\", \"03_model-evaluation\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 OpenAI API (使用deepseek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatgpt(prompt, client, model=\"gpt-4-turbo\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        seed=123,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from config_manager import config_manager\n",
    "\n",
    "config = config_manager.config\n",
    "api_key = config.get('api', {}).get('deepseek_api_key')\n",
    "if not api_key:\n",
    "    raise ValueError(\"需要提供API密钥\")\n",
    "model = config.get('api', {}).get('model', 'deepseek-chat')\n",
    "\n",
    "# 初始化OpenAI客户端来访问Deepseek API\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=config.get('api', {}).get('deepseek_api_url')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备输入\n",
    "prompt = f\"Respond with 'hello world' if you got this message.\"\n",
    "run_chatgpt(prompt, client, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建训练用样本保存为json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 100\n"
     ]
    }
   ],
   "source": [
    "# 加载测试用原始数据\n",
    "import json\n",
    "\n",
    "data_dir = \"../../ch07/03_model-evaluation/\"\n",
    "json_file = \"eval-example-data.json\"\n",
    "\n",
    "with open(data_dir + json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Calculate the hypotenuse of a right triangle with legs of 6 cm and 8 cm.',\n",
       " 'input': '',\n",
       " 'output': 'The hypotenuse of the triangle is 10 cm.',\n",
       " 'model 1 response': '\\nThe hypotenuse of the triangle is 3 cm.',\n",
       " 'model 2 response': '\\nThe hypotenuse of the triangle is 12 cm.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格式化输入数据\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Wrtie a response that \"\n",
    "        f\"appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else \"\"\n",
    "    \n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The hypotenuse of the triangle is 10 cm.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "The hypotenuse of the triangle is 3 cm.\n",
      "\n",
      "Score:\n",
      ">> To score the model response, we'll consider the accuracy of the answer compared to the correct output.\n",
      "\n",
      "**Correct Output:**  \n",
      "\"The hypotenuse of the triangle is 10 cm.\"  \n",
      "*(This is correct because, using the Pythagorean theorem: \\( \\sqrt{6^2 + 8^2} = \\sqrt{36 + 64} = \\sqrt{100} = 10 \\).)*\n",
      "\n",
      "**Model Response:**  \n",
      "\"The hypotenuse of the triangle is 3 cm.\"  \n",
      "*(This is incorrect, as the correct hypotenuse is 10 cm.)*\n",
      "\n",
      "### Scoring:\n",
      "- **Accuracy:** The model's answer is completely wrong (3 cm vs. 10 cm).  \n",
      "- **Relevance:** The response is relevant to the task but fails to provide the correct calculation.  \n",
      "- **Confidence:** The model states the answer confidently, but it is incorrect.  \n",
      "\n",
      "Given that the answer is entirely incorrect, the score should reflect a complete failure to solve the task correctly.  \n",
      "\n",
      "**Score: 0/100**  \n",
      "\n",
      "*(If the model had provided a partially correct or closer answer, the score could be adjusted accordingly, but 3 cm is not even close to the correct value.)*\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> 1. Squirrel\n",
      "2. Eagle\n",
      "3. Tiger\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "1. Squirrel\n",
      "2. Tiger\n",
      "3. Eagle\n",
      "4. Cobra\n",
      "5. Tiger\n",
      "6. Cobra\n",
      "\n",
      "Score:\n",
      ">> To score the model response, let's consider the following criteria:\n",
      "\n",
      "1. **Accuracy**: Are the animals listed actually active during the day (diurnal)?\n",
      "2. **Correctness**: Does the response follow the instruction to name 3 animals?\n",
      "3. **Repetition**: Are there duplicate entries?\n",
      "4. **Relevance**: Are all listed animals relevant to the instruction?\n",
      "\n",
      "### Evaluation:\n",
      "- **Squirrel**: Correct (diurnal).\n",
      "- **Tiger**: Incorrect (tigers are primarily crepuscular/nocturnal, not strictly diurnal).\n",
      "- **Eagle**: Correct (diurnal).\n",
      "- **Cobra**: Incorrect (many cobras are crepuscular/nocturnal, not strictly diurnal).\n",
      "- The response lists 6 animals instead of 3, with duplicates (\"Tiger\" and \"Cobra\" repeated).\n",
      "\n",
      "### Deductions:\n",
      "- +1 for \"Squirrel\" (correct).\n",
      "- +1 for \"Eagle\" (correct).\n",
      "- -1 for \"Tiger\" (incorrect, and repeated).\n",
      "- -1 for \"Cobra\" (incorrect, and repeated).\n",
      "- -1 for exceeding the requested 3 animals.\n",
      "- -1 for duplicate entries.\n",
      "\n",
      "### Scoring:\n",
      "- Base score for correct animals: 2/3 (since only 2 of the first 3 are correct).\n",
      "- Deductions: -4 (for incorrect animals, duplicates, and exceeding the count).\n",
      "- Adjusted score: (2/3 * 100) - 40 = ~66.67 - 40 = ~26.67.\n",
      "\n",
      "However, since the scale is 0-100 and the response has significant issues (incorrect animals, duplicates, and extra entries), a more realistic score is:\n",
      "\n",
      "**Score: 30/100**\n",
      "\n",
      "### Justification:\n",
      "- The response includes some correct animals but fails on accuracy (listing non-diurnal animals), correctness (not following the 3-animal limit), and clarity (repetitions). A score of 30 reflects partial credit for the correct entries but significant deductions for errors.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> I must ascertain what is incorrect.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "What is incorrect?\n",
      "\n",
      "Score:\n",
      ">> Here's the scored evaluation of the model response `What is incorrect?` based on the given criteria:\n",
      "\n",
      "1. **Formality**: The response is somewhat formal but lacks the full formality of the correct output. It simplifies the original sentence but doesn't elevate it as much as the correct output does.  \n",
      "   - Score: 60/100  \n",
      "\n",
      "2. **Accuracy**: The response captures the essence of the original sentence but is more of a question than a statement, which slightly deviates from the original intent (\"I need to find out...\").  \n",
      "   - Score: 70/100  \n",
      "\n",
      "3. **Clarity**: The response is clear and concise, but it doesn't fully convey the proactive intent of the original sentence (i.e., the act of \"finding out\").  \n",
      "   - Score: 75/100  \n",
      "\n",
      "4. **Comparison to Correct Output**: The correct output (\"I must ascertain what is incorrect.\") is more formal, precise, and maintains the original intent. The model response falls short in these aspects.  \n",
      "   - Score: 50/100  \n",
      "\n",
      "### **Overall Weighted Score**: **63/100**  \n",
      "\n",
      "**Breakdown**:  \n",
      "- The response is acceptable but not ideal. It simplifies the original sentence into a question, losing some formality and intent.  \n",
      "- It is not incorrect, but it doesn't meet the same standard as the provided correct output.  \n",
      "- A stronger response would retain the declarative structure and use more formal phrasing (e.g., \"I must determine what is incorrect.\").  \n",
      "\n",
      "Would you like a more detailed rubric or adjustments to the scoring criteria?\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The interjection in the sentence is 'Wow'.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "The interjection in the sentence is 'Wow'.\n",
      "\n",
      "Score:\n",
      ">> The model response is perfectly correct and matches the expected output exactly. Therefore, it deserves the highest score.\n",
      "\n",
      "**Score: 100**\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of sentence is interrogative.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "The type of sentence is exclamatory.\n",
      "\n",
      "Score:\n",
      ">> The correct type of sentence for \"Did you finish the report?\" is **interrogative**, as it is a question. The model response incorrectly identified it as **exclamatory**, which is a sentence that expresses strong emotion or surprise (e.g., \"What a great day!\").\n",
      "\n",
      "### Scoring:\n",
      "- **Accuracy**: 0 (completely incorrect classification)  \n",
      "- **Relevance**: 0 (the response does not match the correct type)  \n",
      "- **Correctness**: 0 (the answer is wrong)  \n",
      "\n",
      "**Final Score: 0/100**  \n",
      "\n",
      "The model failed to correctly identify the sentence type, so it receives the lowest possible score.\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# 少量样本测试\n",
    "for entry in json_data[:5]:\n",
    "    text = entry[\"output\"]\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model 1 response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model 1 response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", run_chatgpt(prompt, client, model))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_model_scores(json_data, json_key, client):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the number only.\"\n",
    "        )\n",
    "\n",
    "        score = run_chatgpt(prompt, client, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Error converting score to int: {score}\")\n",
    "            continue\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  98%|█████████▊| 98/100 [15:39<00:41, 20.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting score to int: The model response correctly classifies the given animals (Eagle, Shark, Cobra) but includes many incorrect and redundant entries (Squirrel, Tiger, repeated Cobra). \n",
      "\n",
      "Scoring:\n",
      "- Correct classifications: 3 (Eagle, Shark, Cobra)\n",
      "- Incorrect classifications: 9 (Squirrel, Tiger, repeated Cobra)\n",
      "- Redundant entries: 6 (repeated Cobra, Tiger, Squirrel)\n",
      "\n",
      "The score should reflect the accuracy of the response relative to the correct output. Since the correct part is only 3 out of 12 lines (25%), but the core task (classifying the 3 given animals) is fully correct, the score should be adjusted to account for both the correctness and the noise.\n",
      "\n",
      "A fair score would be **50**. \n",
      "\n",
      "(Note: If the scoring is strictly based on exact match to the correct output, the score would be lower, but considering the core task is done correctly, 50 is a balanced score.) \n",
      "\n",
      "Final answer: `50`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 100/100 [15:55<00:00,  9.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model 1 response\n",
      "Number of scores: 99 of 100\n",
      "Average score: 81.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 100/100 [09:00<00:00,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model 2 response\n",
      "Number of scores: 100 of 100\n",
      "Average score: 68.25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for m in (\"model 1 response\", \"model 2 response\"):\n",
    "    scores = generate_model_scores(json_data, m, client)\n",
    "    print(f\"\\n{m}\")\n",
    "    print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
    "    print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
    "\n",
    "    # 保存打分结果\n",
    "    # save_path = Path(f\"model-{model}-scores.json\")\n",
    "    # with open(save_path, \"w\") as f:\n",
    "    #     json.dump(scores, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50,\n",
       " 100,\n",
       " 50,\n",
       " 100,\n",
       " 50,\n",
       " 100,\n",
       " 5,\n",
       " 100,\n",
       " 50,\n",
       " 50,\n",
       " 60,\n",
       " 20,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 95,\n",
       " 50,\n",
       " 90,\n",
       " 100,\n",
       " 100,\n",
       " 50,\n",
       " 0,\n",
       " 100,\n",
       " 50,\n",
       " 50,\n",
       " 100,\n",
       " 10,\n",
       " 10,\n",
       " 95,\n",
       " 20,\n",
       " 80,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 95,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 10,\n",
       " 50,\n",
       " 0,\n",
       " 50,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 50,\n",
       " 100,\n",
       " 10,\n",
       " 100,\n",
       " 85,\n",
       " 100,\n",
       " 100,\n",
       " 20,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 50,\n",
       " 95,\n",
       " 10,\n",
       " 75,\n",
       " 95,\n",
       " 100,\n",
       " 85,\n",
       " 0,\n",
       " 85,\n",
       " 100,\n",
       " 0,\n",
       " 20,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 50,\n",
       " 100,\n",
       " 50,\n",
       " 75,\n",
       " 100,\n",
       " 100,\n",
       " 50,\n",
       " 100,\n",
       " 10,\n",
       " 20,\n",
       " 100,\n",
       " 90,\n",
       " 100,\n",
       " 70,\n",
       " 100,\n",
       " 100,\n",
       " 10,\n",
       " 100,\n",
       " 100,\n",
       " 50,\n",
       " 10,\n",
       " 10]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
