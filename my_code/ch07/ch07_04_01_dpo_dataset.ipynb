{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:08.487175Z",
     "start_time": "2025-04-28T14:34:06.544574Z"
    }
   },
   "source": [
    "# ä½¿ç”¨sys.pathæ·»åŠ ä¸Šçº§ç›®å½•\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch07\", \"01_main-chapter-code\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\project\\LLMs-from-scratch-CN\\ch07\\01_main-chapter-code\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:08.500886Z",
     "start_time": "2025-04-28T14:34:08.494186Z"
    }
   },
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"tqdm\",    # è¿›åº¦æ¡\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ã€todoã€‘ä½¿ç”¨Ollamaçš„REST API"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:08.542579Z",
     "start_time": "2025-04-28T14:34:08.540302Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨deepseekæ„é€ æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:09.727263Z",
     "start_time": "2025-04-28T14:34:08.552852Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "from config_manager import config_manager\n",
    "\n",
    "config = config_manager.config\n",
    "api_key = config.get('api', {}).get('deepseek_api_key')\n",
    "if not api_key:\n",
    "    raise ValueError(\"éœ€è¦æä¾›APIå¯†é’¥\")\n",
    "\n",
    "# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯æ¥è®¿é—®Deepseek API\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=config.get('api', {}).get('deepseek_api_url')\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:30.679117Z",
     "start_time": "2025-04-28T14:34:09.736740Z"
    }
   },
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "def query_model(prompt, client, config, batch_mode=False):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    if batch_mode:\n",
    "        messages = []\n",
    "        for pt in prompt:\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            })\n",
    "    # æ„å»ºè¯·æ±‚æ•°æ®\n",
    "    data = {\n",
    "        \"model\": config.get('api', {}).get('model', 'deepseek-chat'),\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.\n",
    "    }\n",
    "    # å‘é€è¯·æ±‚\n",
    "    response = client.chat.completions.create(**data)\n",
    "    # è§£æå“åº”\n",
    "    try:\n",
    "        content = response.choices[0].message.content\n",
    "        # # æ¸…ç†å¯èƒ½çš„å‰å¯¼å’Œå°¾éšç©ºç™½å­—ç¬¦\n",
    "        # content = content.strip()\n",
    "        \n",
    "        # # ç§»é™¤å¯èƒ½çš„Markdownä»£ç å—æ ‡è®°\n",
    "        # if content.startswith('```json'):\n",
    "        #     content = content[7:]  # ç§»é™¤å¼€å¤´çš„```json\n",
    "        # if content.endswith('```'):\n",
    "        #     content = content[:-3]  # ç§»é™¤ç»“å°¾çš„```\n",
    "        \n",
    "        # æ¸…ç†å¹¶è§£æJSON\n",
    "        content = content.strip()\n",
    "        response_data = json.loads(content)\n",
    "\n",
    "        return response_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        # print(f\"JSONè§£æå¤±è´¥: {content}\")\n",
    "        # print(f\"é”™è¯¯ä¿¡æ¯: {str(e)}\")\n",
    "        return content\n",
    "\n",
    "result = query_model(\"What do Llamas eat?\", client, config)\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores with a diet primarily consisting of grasses, hay, and other plant materials. Hereâ€™s a breakdown of what they typically eat:\n",
      "\n",
      "### **1. Main Diet:**\n",
      "- **Grass & Hay** â€“ The bulk of their diet, especially in captivity. They graze on fresh pasture or are fed high-quality grass hay (e.g., timothy, orchard, or brome hay).\n",
      "- **Forage** â€“ They browse on leaves, shrubs, and other available vegetation in their natural habitat (Andes Mountains).\n",
      "\n",
      "### **2. Supplemental Foods:**\n",
      "- **Pellets/Grain Mixes** â€“ Specially formulated llama feed (in moderation) can provide extra nutrients.\n",
      "- **Vegetables & Fruits** â€“ Occasionally, they can have small amounts of carrots, apples, or leafy greens as treats (avoid sugary or high-oxalate foods like spinach).\n",
      "\n",
      "### **3. Important Notes:**\n",
      "- **Fresh Water** â€“ Must always be available.\n",
      "- **Avoid Overfeeding Grains** â€“ Too much grain can cause digestive issues like bloat.\n",
      "- **Mineral Licks** â€“ Llamas may need salt or mineral supplements, especially if grazing on nutrient-deficient land.\n",
      "\n",
      "### **4. Foods to Avoid:**\n",
      "- **Toxic Plants** â€“ Some plants (e.g., rhododendron, azaleas, nightshade) are poisonous.\n",
      "- **High-Sugar or Processed Foods** â€“ Can lead to obesity and metabolic problems.\n",
      "\n",
      "Llamas have efficient digestive systems (similar to cows, but with a three-chambered stomach), so a high-fiber, low-sugar diet is best. If raising llamas, consult a vet or livestock nutritionist for a balanced feeding plan. ğŸ¦™ğŸŒ¿\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŠ è½½JSONæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:30.721048Z",
     "start_time": "2025-04-28T14:34:30.713182Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "json_file = Path(\"..\", \"..\", \"ch07\", \"01_main-chapter-code\", \"instruction-data.json\")\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:30.769007Z",
     "start_time": "2025-04-28T14:34:30.761760Z"
    }
   },
   "source": [
    "json_data[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Evaluate the following phrase by transforming it into the spelling given.',\n",
       " 'input': 'freind --> friend',\n",
       " 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:30.804315Z",
     "start_time": "2025-04-28T14:34:30.800243Z"
    }
   },
   "source": [
    "# æ ¼å¼åŒ–è¾“å…¥\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that \"\n",
    "        f\"appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    instruction_text + input_text\n",
    "\n",
    "    return instruction_text + input_text"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:57.436606Z",
     "start_time": "2025-04-28T14:34:30.817857Z"
    }
   },
   "source": [
    "# æµ‹è¯•ç”Ÿæˆæ•°æ®\n",
    "import random\n",
    "\n",
    "for entry in json_data[:5]:\n",
    "    politeness = random.choice([\"polite\", \"impolite\"])\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}`,\"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"slightly rewrite the output to be more {politeness}.\"\n",
    "        \"Keep the modification minimal.\"\n",
    "        \"Only return the generated response and nothing else.\"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(f\"\\n{politeness} response:\")\n",
    "    print(\">>\", query_model(prompt, client, config))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n",
      "\n",
      "impolite response:\n",
      ">> The spelling of \"freind\" is wrong, it's \"friend.\" Learn to spell.\n",
      "\n",
      "Dataset response:\n",
      ">> He goes to the park every day.\n",
      "\n",
      "polite response:\n",
      ">> He goes to the park every day, thank you.\n",
      "\n",
      "Dataset response:\n",
      ">> 45 kilometers is 45000 meters.\n",
      "\n",
      "polite response:\n",
      ">> 45 kilometers is equal to 45000 meters.\n",
      "\n",
      "Dataset response:\n",
      ">> Although it was raining, they went for a walk.\n",
      "\n",
      "impolite response:\n",
      ">> Although it was raining, they still went for a walk, like idiots.\n",
      "\n",
      "Dataset response:\n",
      ">> 1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\n",
      "\n",
      "impolite response:\n",
      ">> Here, take these stupid square numbers: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100. Ugh.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:57.459026Z",
     "start_time": "2025-04-28T14:34:57.454455Z"
    }
   },
   "source": [
    "# ç”Ÿæˆæ•°æ®é›†\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_model_responses(json_data):\n",
    "    for i, entry in enumerate(tqdm(json_data, desc=\"Writing entryies\")):\n",
    "        politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"slightly rewrite the output to be more {politeness}.\"\n",
    "            \"Keep the modification minimal.\"\n",
    "            \"Only return return the generated response and nothing else.\"\n",
    "        )\n",
    "        response = query_model(prompt, client, config)\n",
    "        \n",
    "        if politeness == \"polite\":\n",
    "            json_data[i][\"chosen\"] = response\n",
    "            json_data[i][\"rejected\"] = entry[\"output\"]\n",
    "        else:\n",
    "            json_data[i][\"rejected\"] = response\n",
    "            json_data[i][\"chosen\"] = entry[\"output\"] "
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-28T14:34:57.469940Z"
    }
   },
   "source": [
    "generate_model_responses(json_data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing entryies:   0%|          | 0/1100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"instruction-data-with-preference.json\", \"w\") as file:\n",
    "    json.dump(json_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
