{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:08.487175Z",
     "start_time": "2025-04-28T14:34:06.544574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/ch07/01_main-chapter-code\n"
     ]
    }
   ],
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch07\", \"01_main-chapter-code\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:08.500886Z",
     "start_time": "2025-04-28T14:34:08.494186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"tqdm\",    # 进度条\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Ollama的REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格式化输入\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that \"\n",
    "        f\"appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    instruction_text + input_text\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:08.542579Z",
     "start_time": "2025-04-28T14:34:08.540302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet consists of:\n",
      "\n",
      "1. **Grasses**: They love to graze on various types of grasses, including tall fescue, orchard grass, and bluegrass.\n",
      "2. **Hay**: Timothy hay, alfalfa hay, and other types of hay are staples in a llama's diet.\n",
      "3. **Fruits**: Apples, carrots, and sweet potatoes are all treats that llamas enjoy.\n",
      "4. **Grains**: Oats, corn, and barley can be given to llamas as supplements or treats.\n",
      "5. **Leafy greens**: Llamas will eat leafy greens like kale, spinach, and collard greens.\n",
      "\n",
      "In the wild, llamas would typically roam in herds and graze on a variety of plants, including shrubs and trees. In captivity, their diet is often supplemented with commercial llama feed or pellets to ensure they receive all the necessary nutrients.\n",
      "\n",
      "Some interesting facts about llama eating habits:\n",
      "\n",
      "* Llamas have a three-part stomach, similar to cows, which allows them to digest plant material more efficiently.\n",
      "* They can go without water for long periods of time, but will still need access to fresh water regularly.\n",
      "* In the wild, llamas may eat up to 2-3% of their body weight in dry matter per day.\n",
      "\n",
      "Overall, a balanced diet that includes a mix of hay, grasses, fruits, and grains is essential for maintaining a healthy llama.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "\n",
    "def query_model_ollama(prompt, model=\"llama3.1\", url=\"http://localhost:11434/api/chat\"):\n",
    "    # 创建数据负载作为字典\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 将字典转换为 JSON 格式的字符串并编码为字节\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # 创建请求对象，设置方法为 POST 并添加必要的头信息\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # 发送请求并捕获响应\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # 读取并解码响应\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "result = query_model_ollama(\"What do Llamas eat?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据集\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def process_single_entry_ollama(args):\n",
    "    \"\"\"处理单个数据条目\"\"\"\n",
    "    i, entry = args\n",
    "    politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"slightly rewrite the output to be more {politeness}.\"\n",
    "        \"Keep the modification minimal.\"\n",
    "        \"Only return the generated response and nothing else.\"\n",
    "    )\n",
    "    \n",
    "    # 添加重试机制\n",
    "    max_retries = 3\n",
    "    retry_delay = 1\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = query_model_ollama(prompt)\n",
    "            result = {\n",
    "                \"index\": i,\n",
    "                \"politeness\": politeness,\n",
    "                \"response\": response\n",
    "            }\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # 指数退避\n",
    "            else:\n",
    "                print(f\"处理条目 {i} 失败: {str(e)}\")\n",
    "                return {\n",
    "                    \"index\": i,\n",
    "                    \"politeness\": None,\n",
    "                    \"response\": None\n",
    "                }\n",
    "\n",
    "def generate_model_responses_ollama(json_data, max_workers=5):\n",
    "    \"\"\"使用并发处理生成模型响应\"\"\"\n",
    "    \n",
    "    # 准备参数列表\n",
    "    args_list = [(i, entry) for i, entry in enumerate(json_data)]\n",
    "    \n",
    "    # 使用进度条包装结果处理\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_entry_ollama, args) for args in args_list]\n",
    "        \n",
    "        # 使用tqdm显示进度\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"处理数据\"):\n",
    "            result = future.result()\n",
    "            if result[\"politeness\"] is not None:\n",
    "                results.append(result)\n",
    "    \n",
    "    # 结果处理\n",
    "    for result in results:\n",
    "        i = result[\"index\"]\n",
    "        politeness = result[\"politeness\"]\n",
    "        response = result[\"response\"]\n",
    "        \n",
    "        if politeness == \"polite\":\n",
    "            json_data[i][\"chosen\"] = response\n",
    "            json_data[i][\"rejected\"] = json_data[i][\"output\"]\n",
    "        else:\n",
    "            json_data[i][\"rejected\"] = response\n",
    "            json_data[i][\"chosen\"] = json_data[i][\"output\"]\n",
    "    \n",
    "    # 检查未处理的条目\n",
    "    processed_indices = set(result[\"index\"] for result in results)\n",
    "    all_indices = set(range(len(json_data)))\n",
    "    unprocessed = all_indices - processed_indices\n",
    "    \n",
    "    if unprocessed:\n",
    "        print(f\"警告: {len(unprocessed)} 个条目未成功处理\")\n",
    "\n",
    "def generate_model_responses_ollama_orig(json_data):\n",
    "\n",
    "    for i, entry in enumerate(tqdm(json_data, desc=\"Writing entries\")):\n",
    "        politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"slightly rewrite the output to be more {politeness}.\"\n",
    "            \"Keep the modification minimal.\"\n",
    "            \"Only return the generated response and nothing else.\"\n",
    "        )\n",
    "        response = query_model_ollama(prompt)\n",
    "        \n",
    "        if politeness == \"polite\":\n",
    "            json_data[i][\"chosen\"] = response\n",
    "            json_data[i][\"rejected\"] = entry[\"output\"]\n",
    "        else:\n",
    "            json_data[i][\"rejected\"] = response\n",
    "            json_data[i][\"chosen\"] = entry[\"output\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "json_file = Path(\"..\", \"..\", \"ch07\", \"01_main-chapter-code\", \"instruction-data.json\")\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据: 100%|██████████| 1100/1100 [14:20<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_model_responses_ollama(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"instruction-data-with-preference-ollama.json\", \"w\") as file:\n",
    "    json.dump(json_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用deepseek构造数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:09.727263Z",
     "start_time": "2025-04-28T14:34:08.552852Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from config_manager import config_manager\n",
    "\n",
    "config = config_manager.config\n",
    "api_key = config.get('api', {}).get('deepseek_api_key')\n",
    "if not api_key:\n",
    "    raise ValueError(\"需要提供API密钥\")\n",
    "\n",
    "# 初始化OpenAI客户端来访问Deepseek API\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=config.get('api', {}).get('deepseek_api_url')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:30.679117Z",
     "start_time": "2025-04-28T14:34:09.736740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores with a diet primarily consisting of grasses, hay, and other plant materials. Here’s a breakdown of their typical diet:\n",
      "\n",
      "### **1. Main Food Sources:**\n",
      "   - **Grasses & Hay:** The bulk of their diet consists of fresh pasture grasses (if available) or high-quality grass hay (such as timothy, orchard, or brome hay).\n",
      "   - **Forage:** They graze on a variety of plants, including clover and other leafy greens.\n",
      "\n",
      "### **2. Supplemental Foods:**\n",
      "   - **Pellets/Grain:** Some llamas are given small amounts of specially formulated llama or alpaca pellets to ensure balanced nutrition, especially in winter or for pregnant/nursing females.\n",
      "   - **Vegetables & Fruits (in moderation):** Carrots, apples, and leafy greens can be given as treats, but too much sugar (e.g., from fruits) can cause digestive issues.\n",
      "\n",
      "### **3. Minerals & Salt:**\n",
      "   - Llamas need access to a **mineral block** or loose mineral mix formulated for camelids (low in copper, as excess copper is toxic to them).\n",
      "   - Fresh, clean water should always be available.\n",
      "\n",
      "### **4. What to Avoid:**\n",
      "   - **Toxic plants** (e.g., rhododendron, azaleas, nightshade).\n",
      "   - **High-protein or high-carb feeds** (e.g., cattle or horse feed, which can cause digestive problems).\n",
      "   - **Moldy or spoiled hay**, which can lead to illness.\n",
      "\n",
      "Llamas have efficient digestive systems (similar to other camelids) and thrive on a high-fiber, low-sugar diet. If raising llamas, consult a veterinarian or livestock nutritionist for specific dietary recommendations based on their age, health, and activity level.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "def query_model(prompt, client, config):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    # 构建请求数据\n",
    "    data = {\n",
    "        \"model\": config.get('api', {}).get('model', 'deepseek-chat'),\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.\n",
    "    }\n",
    "    # 发送请求\n",
    "    response = client.chat.completions.create(**data)\n",
    "    # 解析响应\n",
    "    try:\n",
    "        content = response.choices[0].message.content\n",
    "        # # 清理可能的前导和尾随空白字符\n",
    "        # content = content.strip()\n",
    "        \n",
    "        # # 移除可能的Markdown代码块标记\n",
    "        # if content.startswith('```json'):\n",
    "        #     content = content[7:]  # 移除开头的```json\n",
    "        # if content.endswith('```'):\n",
    "        #     content = content[:-3]  # 移除结尾的```\n",
    "        \n",
    "        # 清理并解析JSON\n",
    "        content = content.strip()\n",
    "        response_data = json.loads(content)\n",
    "\n",
    "        return response_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        # print(f\"JSON解析失败: {content}\")\n",
    "        # print(f\"错误信息: {str(e)}\")\n",
    "        return content\n",
    "\n",
    "result = query_model(\"What do Llamas eat?\", client, config)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载JSON数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:30.721048Z",
     "start_time": "2025-04-28T14:34:30.713182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "json_file = Path(\"..\", \"..\", \"ch07\", \"01_main-chapter-code\", \"instruction-data.json\")\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:30.769007Z",
     "start_time": "2025-04-28T14:34:30.761760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Evaluate the following phrase by transforming it into the spelling given.',\n",
       " 'input': 'freind --> friend',\n",
       " 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:34:57.436606Z",
     "start_time": "2025-04-28T14:34:30.817857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n",
      "\n",
      "impolite response:\n",
      ">> The spelling of \"freind\" is wrong, it's \"friend.\" Learn to spell.\n",
      "\n",
      "Dataset response:\n",
      ">> He goes to the park every day.\n",
      "\n",
      "polite response:\n",
      ">> He goes to the park every day, thank you.\n",
      "\n",
      "Dataset response:\n",
      ">> 45 kilometers is 45000 meters.\n",
      "\n",
      "polite response:\n",
      ">> 45 kilometers is equal to 45000 meters.\n",
      "\n",
      "Dataset response:\n",
      ">> Although it was raining, they went for a walk.\n",
      "\n",
      "impolite response:\n",
      ">> Although it was raining, they still went for a walk, like idiots.\n",
      "\n",
      "Dataset response:\n",
      ">> 1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\n",
      "\n",
      "impolite response:\n",
      ">> Here, take these stupid square numbers: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100. Ugh.\n"
     ]
    }
   ],
   "source": [
    "# 测试生成数据\n",
    "import random\n",
    "\n",
    "for entry in json_data[:5]:\n",
    "    politeness = random.choice([\"polite\", \"impolite\"])\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}`,\"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"slightly rewrite the output to be more {politeness}.\"\n",
    "        \"Keep the modification minimal.\"\n",
    "        \"Only return the generated response and nothing else.\"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(f\"\\n{politeness} response:\")\n",
    "    print(\">>\", query_model(prompt, client, config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据集\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def process_single_entry(args):\n",
    "    \"\"\"处理单个数据条目\"\"\"\n",
    "    i, entry, client, config = args\n",
    "    politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"slightly rewrite the output to be more {politeness}.\"\n",
    "        \"Keep the modification minimal.\"\n",
    "        \"Only return the generated response and nothing else.\"\n",
    "    )\n",
    "    \n",
    "    # 添加重试机制\n",
    "    max_retries = 3\n",
    "    retry_delay = 1\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = query_model(prompt, client, config)\n",
    "            result = {\n",
    "                \"index\": i,\n",
    "                \"politeness\": politeness,\n",
    "                \"response\": response\n",
    "            }\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # 指数退避\n",
    "            else:\n",
    "                print(f\"处理条目 {i} 失败: {str(e)}\")\n",
    "                return {\n",
    "                    \"index\": i,\n",
    "                    \"politeness\": None,\n",
    "                    \"response\": None\n",
    "                }\n",
    "\n",
    "def generate_model_responses(json_data, max_workers=5):\n",
    "    \"\"\"使用并发处理生成模型响应\"\"\"\n",
    "    \n",
    "    # 准备参数列表\n",
    "    args_list = [(i, entry, client, config) for i, entry in enumerate(json_data)]\n",
    "    \n",
    "    # 使用进度条包装结果处理\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_entry, args) for args in args_list]\n",
    "        \n",
    "        # 使用tqdm显示进度\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"处理数据\"):\n",
    "            result = future.result()\n",
    "            if result[\"politeness\"] is not None:\n",
    "                results.append(result)\n",
    "    \n",
    "    # 结果处理\n",
    "    for result in results:\n",
    "        i = result[\"index\"]\n",
    "        politeness = result[\"politeness\"]\n",
    "        response = result[\"response\"]\n",
    "        \n",
    "        if politeness == \"polite\":\n",
    "            json_data[i][\"chosen\"] = response\n",
    "            json_data[i][\"rejected\"] = json_data[i][\"output\"]\n",
    "        else:\n",
    "            json_data[i][\"rejected\"] = response\n",
    "            json_data[i][\"chosen\"] = json_data[i][\"output\"]\n",
    "    \n",
    "    # 检查未处理的条目\n",
    "    processed_indices = set(result[\"index\"] for result in results)\n",
    "    all_indices = set(range(len(json_data)))\n",
    "    unprocessed = all_indices - processed_indices\n",
    "    \n",
    "    if unprocessed:\n",
    "        print(f\"警告: {len(unprocessed)} 个条目未成功处理\")\n",
    "\n",
    "def generate_model_responses_orig(json_data):\n",
    "    for i, entry in enumerate(tqdm(json_data, desc=\"Writing entryies\")):\n",
    "        politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"slightly rewrite the output to be more {politeness}.\"\n",
    "            \"Keep the modification minimal.\"\n",
    "            \"Only return the generated response and nothing else.\"\n",
    "        )\n",
    "        response = query_model(prompt, client, config)\n",
    "        \n",
    "        if politeness == \"polite\":\n",
    "            json_data[i][\"chosen\"] = response\n",
    "            json_data[i][\"rejected\"] = entry[\"output\"]\n",
    "        else:\n",
    "            json_data[i][\"rejected\"] = response\n",
    "            json_data[i][\"chosen\"] = entry[\"output\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理数据: 100%|██████████| 1100/1100 [1:10:07<00:00,  3.83s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_model_responses(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"instruction-data-with-preference-deepseek.json\", \"w\") as file:\n",
    "    json.dump(json_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
