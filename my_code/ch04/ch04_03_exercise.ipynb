{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 4.1 NUMBER OF PARAMETERS IN FEED FORWARD AND ATTENTION MODULES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and compare the number of parameters that are contained in the feed\n",
    "forward module and those that are contained in the multi-head attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "#初始化定义需要的各种超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\project\\LLMs-from-scratch-CN\\ch04\\02_performance-analysis\n"
     ]
    }
   ],
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch04\", \"02_performance-analysis\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed forward module: 4,722,432.0 parameters\n",
      "Attention module: 2,360,064.0 parameters\n"
     ]
    }
   ],
   "source": [
    "ff_numel = 0\n",
    "attn_numel = 0\n",
    "for name, p in model.named_parameters():\n",
    "    if \"ff\" in name:\n",
    "        ff_numel += p.numel()\n",
    "    elif \"att\" in name:\n",
    "        attn_numel += p.numel()\n",
    "print(f\"Feed forward module: {ff_numel / 12:,} parameters\")\n",
    "print(f\"Attention module: {attn_numel / 12:,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 4.2 INITIALIZING LARGER GPT MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we initialized a 124 million parameter GPT model, which is known as\n",
    "\"GPT-2 small.\" Without making any code modifications besides updating the\n",
    "configuration file, use the GPTModel class to implement GPT-2 medium (using 1024-\n",
    "dimensional embeddings, 24 transformer blocks, 16 multi-head attention heads),\n",
    "GPT-2 large (1280-dimensional embeddings, 36 transformer blocks, 20 multi-head\n",
    "attention heads), and GPT-2 XL (1600-dimensional embeddings, 48 transformer\n",
    "blocks, 25 multi-head attention heads). As a bonus, calculate the total number of\n",
    "parameters in each GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本配置\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词汇表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # 丢弃率\n",
    "    \"qkv_bias\": True         # 是否使用查询-键-值偏置\n",
    "}\n",
    "\n",
    "# 不同规模的GPT模型配置\n",
    "model_configs = {\n",
    "    \"gpt-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},  # 小型模型\n",
    "    \"gpt-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16}, # 中型模型\n",
    "    \"gpt-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},  # 大型模型\n",
    "    \"gpt-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},    # 超大模型\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-small (124M) parameters is 163037184.\n",
      "gpt-medium (355M) parameters is 406286336.\n",
      "gpt-large (774M) parameters is 838359040.\n",
      "gpt-xl (1558M) parameters is 1638022400.\n"
     ]
    }
   ],
   "source": [
    "for name, config in model_configs.items():\n",
    "    BASE_CONFIG.update(config)\n",
    "    model = GPTModel(BASE_CONFIG)\n",
    "    numel = sum([p.numel() for p in model.parameters()])\n",
    "    print(f\"{name} parameters is {numel}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 4.3 USING SEPARATE DROPOUT PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of this chapter, we defined a global \"drop_rate\" setting in the\n",
    "GPT_CONFIG_124M dictionary to set the dropout rate in various places throughout the\n",
    "GPTModel architecture. Change the code to specify a separate dropout value for the\n",
    "various dropout layers throughout the model architecture. (Hint: there are three\n",
    "distinct places where we used dropout layers: the embedding layer, shortcut layer,\n",
    "and multi-head attention module.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters_mod import GPTModel as GPTModel_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"emb_drop_rate\": 0.1,\n",
    "    \"att_drop_rate\": 0.2,\n",
    "    \"shortcut_drop_rate\": 0.3,\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_emb, 0.1\n",
      "trf_blocks.0.att.dropout, 0.2\n",
      "trf_blocks.0.drop_shortcut, 0.3\n",
      "trf_blocks.1.att.dropout, 0.2\n",
      "trf_blocks.1.drop_shortcut, 0.3\n",
      "trf_blocks.2.att.dropout, 0.2\n",
      "trf_blocks.2.drop_shortcut, 0.3\n",
      "trf_blocks.3.att.dropout, 0.2\n",
      "trf_blocks.3.drop_shortcut, 0.3\n",
      "trf_blocks.4.att.dropout, 0.2\n",
      "trf_blocks.4.drop_shortcut, 0.3\n",
      "trf_blocks.5.att.dropout, 0.2\n",
      "trf_blocks.5.drop_shortcut, 0.3\n",
      "trf_blocks.6.att.dropout, 0.2\n",
      "trf_blocks.6.drop_shortcut, 0.3\n",
      "trf_blocks.7.att.dropout, 0.2\n",
      "trf_blocks.7.drop_shortcut, 0.3\n",
      "trf_blocks.8.att.dropout, 0.2\n",
      "trf_blocks.8.drop_shortcut, 0.3\n",
      "trf_blocks.9.att.dropout, 0.2\n",
      "trf_blocks.9.drop_shortcut, 0.3\n",
      "trf_blocks.10.att.dropout, 0.2\n",
      "trf_blocks.10.drop_shortcut, 0.3\n",
      "trf_blocks.11.att.dropout, 0.2\n",
      "trf_blocks.11.drop_shortcut, 0.3\n"
     ]
    }
   ],
   "source": [
    "model_mod = GPTModel_mod(GPT_CONFIG_124M)\n",
    "for name, m in model_mod.named_modules():\n",
    "    if isinstance(m, torch.nn.Dropout):\n",
    "        print(f\"{name}, {m.p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_emb, 0.1\n",
      "trf_blocks.0.att.dropout, 0.1\n",
      "trf_blocks.0.drop_shortcut, 0.1\n",
      "trf_blocks.1.att.dropout, 0.1\n",
      "trf_blocks.1.drop_shortcut, 0.1\n",
      "trf_blocks.2.att.dropout, 0.1\n",
      "trf_blocks.2.drop_shortcut, 0.1\n",
      "trf_blocks.3.att.dropout, 0.1\n",
      "trf_blocks.3.drop_shortcut, 0.1\n",
      "trf_blocks.4.att.dropout, 0.1\n",
      "trf_blocks.4.drop_shortcut, 0.1\n",
      "trf_blocks.5.att.dropout, 0.1\n",
      "trf_blocks.5.drop_shortcut, 0.1\n",
      "trf_blocks.6.att.dropout, 0.1\n",
      "trf_blocks.6.drop_shortcut, 0.1\n",
      "trf_blocks.7.att.dropout, 0.1\n",
      "trf_blocks.7.drop_shortcut, 0.1\n",
      "trf_blocks.8.att.dropout, 0.1\n",
      "trf_blocks.8.drop_shortcut, 0.1\n",
      "trf_blocks.9.att.dropout, 0.1\n",
      "trf_blocks.9.drop_shortcut, 0.1\n",
      "trf_blocks.10.att.dropout, 0.1\n",
      "trf_blocks.10.drop_shortcut, 0.1\n",
      "trf_blocks.11.att.dropout, 0.1\n",
      "trf_blocks.11.drop_shortcut, 0.1\n"
     ]
    }
   ],
   "source": [
    "model_mod = GPTModel(GPT_CONFIG_124M)\n",
    "for name, m in model_mod.named_modules():\n",
    "    if isinstance(m, torch.nn.Dropout):\n",
    "        print(f\"{name}, {m.p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
