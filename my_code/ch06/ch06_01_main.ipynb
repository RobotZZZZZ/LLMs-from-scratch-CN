{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:15:09.585474Z",
     "start_time": "2025-04-27T12:15:07.687056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/ch06/01_main-chapter-code\n"
     ]
    }
   ],
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch06\", \"01_main-chapter-code\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.4\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.6.0\n",
      "tensorflow version: 2.18.0\n",
      "pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", # 用于加载OpenAI的预训练权重的TensorFlow库\n",
    "        \"pandas\"      # 用于加载数据集的库\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "#查看版本号Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:20:10.680106Z",
     "start_time": "2025-04-27T12:20:10.676203Z"
    }
   },
   "outputs": [],
   "source": [
    "# 防止某些单元格执行两次\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "executed_cells = set()\n",
    "#使用一个 set 数据结构来存储已经执行过的单元格标识符\n",
    "@register_line_cell_magic\n",
    "#注册了一个名为 run_once 的魔法命令\n",
    "def run_once(line, cell):\n",
    "    if line not in executed_cells:\n",
    "        get_ipython().run_cell(cell)\n",
    "        executed_cells.add(line)\n",
    "    else:\n",
    "        print(f\"Cell '{line}' has already been executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:20:00.113812Z",
     "start_time": "2025-04-27T12:19:58.309973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# 下载数据\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "    \n",
    "    # 下载文件\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "    \n",
    "    # 解压文件\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "    \n",
    "    # 添加.tsv扩展名\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:20:03.922602Z",
     "start_time": "2025-04-27T12:20:03.438431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据加载\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:20:06.061483Z",
     "start_time": "2025-04-27T12:20:06.037065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:20:17.051077Z",
     "start_time": "2025-04-27T12:20:16.972352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "%%run_once balance_df\n",
    "\n",
    "# 平衡正负样本\n",
    "def create_balanced_dataset(df):\n",
    "    # 下采样，以使正负样本数量相等\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:20:20.275337Z",
     "start_time": "2025-04-27T12:20:20.230358Z"
    }
   },
   "outputs": [],
   "source": [
    "%%run_once label_mapping\n",
    "\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:20:22.693912Z",
     "start_time": "2025-04-27T12:20:22.685836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww dat is sweet! We can think of something t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>0</td>\n",
       "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>0</td>\n",
       "      <td>This is wishing you a great day. Moji told me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank you. do you generally date the brothas?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>1</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>1</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>1</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "4307      0  Awww dat is sweet! We can think of something t...\n",
       "4138      0                             Just got to  &lt;#&gt;\n",
       "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
       "4461      0  This is wishing you a great day. Moji told me ...\n",
       "5440      0      Thank you. do you generally date the brothas?\n",
       "...     ...                                                ...\n",
       "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567      1  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:20:25.091631Z",
     "start_time": "2025-04-27T12:20:25.033329Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # 根据给定的比例，计算划分索引\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # 划分数据集\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 创建数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 创建分词器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 创建数据集\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        # 编码输入数据\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "        # 最大长度截断\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            self.encode_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        # 填充\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 用预训练权重初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout率\n",
    "    \"qkv_bias\": True         # qkv向量是否使用Bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "# 上下文长度应小于max_length\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "# 下载预训练模型权重\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 添加分类头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 冻结参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换输出层，用于分类\n",
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 除了输出层外，令trf的最后一个模块和final_norm的参数可训练\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Do'\n",
      "' you'\n",
      "' have'\n",
      "' time'\n"
     ]
    }
   ],
   "source": [
    "for token_id in inputs[0].tolist():\n",
    "    word = tokenizer.decode([token_id])\n",
    "    print(f\"'{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "# (batch_size, seq_len, num_classes)\n",
    "print(\"Outputs dimensions:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "# 最后一个输出，包含有最完整的信息\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 计算分类损失和准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# 若只想要分类结果，则可以跳过softmax操作\n",
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算分类准备率\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            # 将数据移到指定设备\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "            # 模型预估\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "            # 获取分类结果\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            # 统计总样本量、分类正确样本量\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "# 计算不同数据集上的准确率\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "print(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义batch训练损失\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    # 只关注最后一个输出\n",
    "    logits = model(input_batch)[:, -1, :]\n",
    "    # 交叉熵损失\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义loader训练损失\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "# 查看训练前，各数据集上的损失表现\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 在有监督数据上微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # 初始化梯度\n",
    "            optimizer.zero_grad()\n",
    "            # loss计算\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 权重更新\n",
    "            optimizer.step()\n",
    "            \n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}) \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy * 100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000) Train loss 2.153, Val loss 2.392\n",
      "Ep 1 (Step 000050) Train loss 0.617, Val loss 0.637\n",
      "Ep 1 (Step 000100) Train loss 0.523, Val loss 0.557\n",
      "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
      "Ep 2 (Step 000150) Train loss 0.561, Val loss 0.489\n",
      "Ep 2 (Step 000200) Train loss 0.419, Val loss 0.397\n",
      "Ep 2 (Step 000250) Train loss 0.409, Val loss 0.353\n",
      "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
      "Ep 3 (Step 000300) Train loss 0.333, Val loss 0.320\n",
      "Ep 3 (Step 000350) Train loss 0.340, Val loss 0.306\n",
      "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
      "Ep 4 (Step 000400) Train loss 0.136, Val loss 0.200\n",
      "Ep 4 (Step 000450) Train loss 0.153, Val loss 0.132\n",
      "Ep 4 (Step 000500) Train loss 0.222, Val loss 0.137\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550) Train loss 0.207, Val loss 0.143\n",
      "Ep 5 (Step 000600) Train loss 0.083, Val loss 0.074\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 0.88 minutes.\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 2))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.21%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 95.67%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 使用LLM作为垃圾消息分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 垃圾邮件分类器\n",
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "    # 编码输入文本\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    # 根据pos_emb，确定模型支持的上下文长度\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    # 截断\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "    # 填充\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    # 转换为tensor\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "    # 预估\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device, weights_only=True)\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
