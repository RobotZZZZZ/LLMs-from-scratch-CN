{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:45:59.324074Z",
     "start_time": "2025-04-27T12:45:59.239489Z"
    }
   },
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\",\n",
    "        \"pandas\"\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.1\n",
      "numpy version: 2.1.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.6.0+cu126\n",
      "tensorflow version: 2.19.0\n",
      "pandas version: 2.2.3\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:46:02.626983Z",
     "start_time": "2025-04-27T12:46:00.442807Z"
    }
   },
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"appendix-E\", \"01_main-chapter-code\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\project\\LLMs-from-scratch-CN\\appendix-E\\01_main-chapter-code\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.2 准备数据集"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from previous_chapters import (\n",
    "#     download_and_unzip_spam_data,\n",
    "#     create_balanced_dataset,\n",
    "#     random_split\n",
    "# )\n",
    "#\n",
    "# url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "# zip_path = \"sms_spam_collection.zip\"\n",
    "# extracted_path = \"sms_spam_collection\"\n",
    "# data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "#\n",
    "# download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "#\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:58:38.177163Z",
     "start_time": "2025-04-27T12:58:36.914404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from previous_chapters import SpamDataset\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "# 构建dataset和dataloader\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:58:39.189642Z",
     "start_time": "2025-04-27T12:58:39.097905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 查看一个batch的数据\n",
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T12:58:40.943051Z",
     "start_time": "2025-04-27T12:58:40.938388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 查看batch数\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "18 validation batches\n",
      "37 test batches\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.3 模型的初始化"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:00:52.375504Z",
     "start_time": "2025-04-27T13:00:40.719621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词汇表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout比率\n",
    "    \"qkv_bias\": True         # 查询-键-值 偏置\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:04:04.792322Z",
     "start_time": "2025-04-27T13:04:03.424285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from previous_chapters import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer).to(device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:06:03.786942Z",
     "start_time": "2025-04-27T13:06:03.761709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)\n",
    "\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:06:12.383001Z",
     "start_time": "2025-04-27T13:06:04.511213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from previous_chapters import calc_accuracy_loader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.4 使用LoRA进行参数高效微调"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:14:51.407197Z",
     "start_time": "2025-04-27T13:14:51.402375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "# 构建LoRA layer\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        # A矩阵进行kaiming初始化，B矩阵进行全0初始化\n",
    "        # 该初始化方式，保证添加LoRA层后微调前，模型的输出不变\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:14:52.666996Z",
     "start_time": "2025-04-27T13:14:52.661995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构建整合了LoRA Layer的线性层\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:25:07.833147Z",
     "start_time": "2025-04-27T13:25:07.828826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将模型中的所有线性层替换为LoRA层\n",
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # replace\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # 递归\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:25:08.601592Z",
     "start_time": "2025-04-27T13:25:08.594164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 冻结原模型的参数\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:25:20.627769Z",
     "start_time": "2025-04-27T13:25:20.598530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 替换线性层为可训练的LoRA层\n",
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters after: 2,666,528\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:25:55.472540Z",
     "start_time": "2025-04-27T13:25:55.393362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:26:15.119249Z",
     "start_time": "2025-04-27T13:26:11.994332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:36:23.893839Z",
     "start_time": "2025-04-27T13:31:59.981030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 模型训练\n",
    "import time\n",
    "from previous_chapters import train_classifier_simple\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Total training time: {execution_time_minutes:.2f} mins\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.820, Val loss 3.462\n",
      "Ep 1 (Step 000050): Train loss 0.396, Val loss 0.364\n",
      "Ep 1 (Step 000100): Train loss 0.111, Val loss 0.229\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 2 (Step 000150): Train loss 0.135, Val loss 0.073\n",
      "Ep 2 (Step 000200): Train loss 0.007, Val loss 0.055\n",
      "Ep 2 (Step 000250): Train loss 0.020, Val loss 0.188\n",
      "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.140, Val loss 0.062\n",
      "Ep 3 (Step 000350): Train loss 0.006, Val loss 0.089\n",
      "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
      "Ep 4 (Step 000400): Train loss 0.005, Val loss 0.033\n",
      "Ep 4 (Step 000450): Train loss 0.010, Val loss 0.074\n",
      "Ep 4 (Step 000500): Train loss 0.020, Val loss 0.518\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.007, Val loss 0.469\n",
      "Ep 5 (Step 000600): Train loss 0.000, Val loss 0.326\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Total training time: 4.40 mins\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:38:28.060593Z",
     "start_time": "2025-04-27T13:38:24.930088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from previous_chapters import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T13:38:46.146563Z",
     "start_time": "2025-04-27T13:38:28.067599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.65%\n",
      "Validation accuracy: 99.31%\n",
      "Test accuracy: 96.28%\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
