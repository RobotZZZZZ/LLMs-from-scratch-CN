{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.字节对编码（BPE）的主要思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bits和bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(len(text))\n",
    "print(byte_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_ary)\n",
    "print(len(ids))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 创建词汇表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 BPE算法大览"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BPE的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:22:52.313849Z",
     "start_time": "2025-03-19T02:22:52.293479Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json\n",
    "import re\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # 映射 token_id 到 token_str（例如：{11246: \"some\"}）\n",
    "        self.vocab = {}\n",
    "        # 映射 token_str 到 token_id（例如：{\"some\": 11246}）\n",
    "        self.inverse_vocab = {}\n",
    "        # BPE 合并字典：{(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "        # GPT-2 用于合并的rank字典：{(string_A, string_B): rank}，其中较低的rank = 较高的优先级\n",
    "        self.bpe_ranks = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        从头开始训练 BPE 分词器。\n",
    "\n",
    "        参数：\n",
    "            text (str): 训练文本。\n",
    "            vocab_size (int): 目标词汇表大小。\n",
    "            allowed_special (set): 要包含的特殊令牌集。\n",
    "        \"\"\"\n",
    "\n",
    "        # 预处理：将空格替换为 'Ġ'\n",
    "        # 注意，Ġ 是 GPT-2 BPE 实现的一个特性\n",
    "        # 例如，\"Hello world\" 可能被标记为 [\"Hello\", \"Ġworld\"]\n",
    "        # （GPT-4 BPE 会将其标记为 [\"Hello\", \" world\"]）\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # 初始化词汇表\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # 扩展 unique_chars，包含处理后的文本中未包含的字符\n",
    "        unique_chars.extend([char for char in sorted(set(processed_text)) if char not in unique_chars])\n",
    "        \n",
    "        # 可选：确保 'Ġ' 包含在内\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "\n",
    "        # 创建词汇表和逆词汇表\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # 添加允许的特殊token\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # 将文本转化为token ID\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE 步骤 1-3：反复查找并替换“出现次数最多”的字节对\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "        \n",
    "        # 使用合并后的token更新词汇表\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "    \n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        从 OpenAI 的 GPT-2 文件加载预训练的词汇表和 BPE 合并。\n",
    "\n",
    "        参数：\n",
    "            vocab_path (str): 词汇文件路径（GPT-2 称其为 'encoder.json'）。\n",
    "            bpe_merges_path (str): BPE 合并文件路径（GPT-2 称其为 'vocab.bpe'）。\n",
    "        \"\"\"\n",
    "        # 加载词汇表\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # loaded_vocab 将 token_str 映射到 token_id\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # 如果词汇表中没有'\\n'，则使用一个已有的token ID作为'\\n'的占位符\n",
    "        # 选择的这几个token ID都是在encode时不会被处理为子词的token\n",
    "        if \"\\n\" not in self.inverse_vocab:\n",
    "            # 如果词汇表中存在\"<|endoftext|>\"，则使用它作为'\\n'的占位符\n",
    "            # 否则，使用\"Ġ\"或\"\"作为'\\n'的占位符\n",
    "            fallback_token = next((token for token in [\"<|endoftext|>\", \"Ġ\", \"\"] if token in self.inverse_vocab), None)\n",
    "            if fallback_token is not None:\n",
    "                newline_token_id = self.inverse_vocab[fallback_token]\n",
    "            else:\n",
    "                # If no fallback token is available, raise an error\n",
    "                raise KeyError(\"No suitable token found in vocabulary to map '\\\\n'.\")\n",
    "\n",
    "            self.inverse_vocab[\"\\n\"] = newline_token_id\n",
    "            self.vocab[newline_token_id] = \"\\n\"\n",
    "\n",
    "        # 加载BPE合并，并存储它们，并分配一个\"rank\"\n",
    "        self.bpe_ranks = {}  # reset ranks\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            rank = 0\n",
    "            for line in lines:\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) == 2:\n",
    "                    token1, token2 = pair\n",
    "                    # 如果token1或token2不在词汇表中，则跳过\n",
    "                    if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                        self.bpe_ranks[(token1, token2)] = rank\n",
    "                        rank += 1\n",
    "                    else:\n",
    "                        print(f\"Skipping pair {pair} as one token is not in the vocabulary.\")\n",
    "\n",
    "\n",
    "        # 加载词汇表\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # loaded_vocab 将 token_str 映射到 token_id\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "        # 加载BPE合并\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            # 如果有头部注释行，跳过它\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            for rank, line in enumerate(lines):\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) != 2:\n",
    "                    print(f\"第 {rank+1} 行有超过 2 个条目：{line.strip()}\")\n",
    "                    continue\n",
    "                token1, token2 = pair\n",
    "                if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                    token_id1 = self.inverse_vocab[token1]\n",
    "                    token_id2 = self.inverse_vocab[token2]\n",
    "                    merged_token = token1 + token2\n",
    "                    if merged_token in self.inverse_vocab:\n",
    "                        merged_token_id = self.inverse_vocab[merged_token]\n",
    "                        self.bpe_merges[(token_id1, token_id2)] = merged_token_id\n",
    "                    else:\n",
    "                        print(f\"合并的token '{merged_token}' 未在词汇表中找到，跳过。\")\n",
    "                else:\n",
    "                    print(f\"跳过配对 {pair}，因为其中一个token不在词汇表中。\")\n",
    "\n",
    "    def encode(self, text, allowed_special=None):\n",
    "        \"\"\"\n",
    "        将输入文本编码为token ID 列表。\n",
    "\n",
    "        参数：\n",
    "            text (str): 要编码的文本。\n",
    "            allowed_special (set or None): 要包含的特殊token集。如果为None，则禁用特殊处理。\n",
    "\n",
    "        返回：\n",
    "            List[int]: token ID 列表。\n",
    "        \"\"\"\n",
    "        token_ids = []\n",
    "\n",
    "        # 如果允许特殊token处理，则构建一个正则表达式来匹配允许的特殊token\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            # 构建一个正则表达式来匹配允许的特殊token\n",
    "            special_pattern = (\n",
    "                \"(\" + \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) + \")\"\n",
    "            )\n",
    "    \n",
    "            # 将文本根据特殊token进行分割，分别进行编码\n",
    "            last_index = 0\n",
    "            for match in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index:match.start()]\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None))  # 按正常处理编码前缀\n",
    "    \n",
    "                special_token = match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token {special_token} not found in vocabulary.\")\n",
    "                last_index = match.end()\n",
    "    \n",
    "            text = text[last_index:]  # 剩余部分按正常处理\n",
    "    \n",
    "            # 检查剩余部分是否包含不允许的特殊token\n",
    "            disallowed = [\n",
    "                tok for tok in self.inverse_vocab\n",
    "                if tok.startswith(\"<|\") and tok.endswith(\"|>\") and tok in text and tok not in allowed_special\n",
    "            ]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "\n",
    "        # 如果没有任何特殊token，或者剩余文本在特殊token分割后：\n",
    "        tokens = []\n",
    "        # 将文本\"\\n\"拆分为token，并处理空格\n",
    "        lines = text.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if i > 0:\n",
    "                tokens.append(\"\\n\")\n",
    "            words = line.split()\n",
    "            for j, word in enumerate(words):\n",
    "                if j == 0 and i > 0:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "                elif j == 0:\n",
    "                    tokens.append(word)\n",
    "                else:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "    \n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                # token 已经存在于词汇表中\n",
    "                token_ids.append(self.inverse_vocab[token])\n",
    "            else:\n",
    "                # 尝试通过 BPE 处理未登录词\n",
    "                token_ids.extend(self.tokenize_with_bpe(token))\n",
    "    \n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        使用 BPE 合并对单个token进行分词。\n",
    "\n",
    "        参数：\n",
    "            token (str): 要分词的token。\n",
    "\n",
    "        返回：\n",
    "            List[int]: 应用 BPE 后的token ID 列表。\n",
    "        \"\"\"\n",
    "        # 将token分词为单个字符（作为初始token ID）\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        # 若没有ranks信息，则使用下面的处理方法\n",
    "        # 从左到右，依次合并token_ids中相邻的token，直到无法继续合并\n",
    "        if not self.bpe_ranks:\n",
    "            can_merge = True\n",
    "            while can_merge and len(token_ids) > 1:\n",
    "                can_merge = False\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(token_ids) - 1:\n",
    "                    pair = (token_ids[i], token_ids[i + 1])\n",
    "                    if pair in self.bpe_merges:\n",
    "                        merged_token_id = self.bpe_merges[pair]\n",
    "                        new_tokens.append(merged_token_id)\n",
    "                        # Uncomment for educational purposes:\n",
    "                        # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                        i += 2  # 跳过下一个token，因为它已经被合并\n",
    "                        can_merge = True\n",
    "                    else:\n",
    "                        new_tokens.append(token_ids[i])\n",
    "                        i += 1\n",
    "                if i < len(token_ids):\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                token_ids = new_tokens\n",
    "            return token_ids\n",
    "\n",
    "        # 若有ranks信息，则采用GPT-2-style的合并方法\n",
    "        # 1) 将token_ids转换为字符串（为了适配bpe_ranks通过字符串存储key的形式）\n",
    "        symbols = [self.vocab[id_num] for id_num in token_ids]\n",
    "\n",
    "        # 重复合并所有出现rank最低（优先级最高）的pair\n",
    "        while True:\n",
    "            # 收集所有相邻的pair\n",
    "            pairs = set(zip(symbols, symbols[1:]))\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            # 找到出现次数最多的pair\n",
    "            min_rank = float(\"inf\")\n",
    "            bigram = None\n",
    "            for p in pairs:\n",
    "                r = self.bpe_ranks.get(p, float(\"inf\"))\n",
    "                if r < min_rank:\n",
    "                    min_rank = r\n",
    "                    bigram = p\n",
    "\n",
    "            # 如果没有任何有效的合并pair，则结束\n",
    "            if bigram is None or bigram not in self.bpe_ranks:\n",
    "                break\n",
    "\n",
    "            # 合并所有出现的该pair\n",
    "            first, second = bigram\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                # 位置i的pair，如果匹配(frist, second)，则进行合并\n",
    "                if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
    "                    new_symbols.append(first + second)  # 合并\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "\n",
    "            if len(symbols) == 1:\n",
    "                break\n",
    "\n",
    "        # 最后，将合并后的symbols转换成token IDs\n",
    "        merged_ids = [self.inverse_vocab[sym] for sym in symbols]\n",
    "        return merged_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        将token ID 列表解码回字符串。\n",
    "\n",
    "        参数：\n",
    "            token_ids (List[int]): 要解码的token ID 列表。\n",
    "\n",
    "        返回：\n",
    "            str: 解码后的字符串。\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token == \"\\n\":\n",
    "                if decoded_string and not decoded_string.endswith(\" \"):\n",
    "                    decoded_string += \" \"  # 如果行尾没有空格，则添加一个空格\n",
    "                decoded_string += token\n",
    "            elif token.startswith(\"Ġ\"):\n",
    "                # 用空格替换 'Ġ'\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        将词汇表和 BPE 合并保存到 JSON 文件。\n",
    "\n",
    "        参数：\n",
    "            vocab_path (str): 保存词汇表的路径。\n",
    "            bpe_merges_path (str): 保存 BPE 合并的路径。\n",
    "        \"\"\"\n",
    "        # 保存词汇表\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump({k: v for k, v in self.vocab.items()}, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # 保存 BPE 合并作为字典列表\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        从 JSON 文件加载词汇表和 BPE 合并。\n",
    "\n",
    "        参数：\n",
    "            vocab_path (str): 词汇表文件路径。\n",
    "            bpe_merges_path (str): BPE 合并文件路径。\n",
    "        \"\"\"\n",
    "        # 加载词汇表\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # 加载 BPE 合并\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge['pair'])\n",
    "                new_id = merge['new_id']\n",
    "                self.bpe_merges[pair] = new_id\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"无效模式。选择 'most' 或 'least'。\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 训练、编码与解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:18:43.468692Z",
     "start_time": "2025-03-19T02:18:43.464069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the-verdict.txt already exists in ./the-verdict.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def download_file_if_absent(url, filename, search_dirs):\n",
    "    for directory in search_dirs:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"{filename} already exists in {file_path}\")\n",
    "            return file_path\n",
    "\n",
    "    target_path = os.path.join(search_dirs[0], filename)\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response, open(target_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "        print(f\"Downloaded {filename} to {target_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}. Error: {e}\")\n",
    "    return target_path\n",
    "\n",
    "verdict_path = download_file_if_absent(\n",
    "    url=(\n",
    "         \"https://raw.githubusercontent.com/rasbt/\"\n",
    "         \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "         \"the-verdict.txt\"\n",
    "    ),\n",
    "    filename=\"the-verdict.txt\",\n",
    "    search_dirs=\".\"\n",
    ")\n",
    "\n",
    "with open(verdict_path, \"r\", encoding=\"utf-8\") as f: # added ../01_main-chapter-code/\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:10.438336Z",
     "start_time": "2025-03-19T02:23:09.443991Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:11.072947Z",
     "start_time": "2025-03-19T02:23:11.069987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:11.539323Z",
     "start_time": "2025-03-19T02:23:11.537059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.bpe_merges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:11.921848Z",
     "start_time": "2025-03-19T02:23:11.919785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:12.343953Z",
     "start_time": "2025-03-19T02:23:12.341512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 42\n",
      "Number of token IDs: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:12.798054Z",
     "start_time": "2025-03-19T02:23:12.795863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:13.243768Z",
     "start_time": "2025-03-19T02:23:13.241556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:13.992760Z",
     "start_time": "2025-03-19T02:23:13.989942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 -> 'Jack'\n",
      "256 -> ' '\n",
      "654 -> 'em'\n",
      "531 -> 'br'\n",
      "302 -> 'ac'\n",
      "311 -> 'ed'\n",
      "256 -> ' '\n",
      "296 -> 'be'\n",
      "97 -> 'a'\n",
      "465 -> 'ut'\n",
      "121 -> 'y'\n",
      "595 -> ' through'\n",
      "841 -> ' ar'\n",
      "116 -> 't'\n",
      "287 -> ' a'\n",
      "466 -> 'nd'\n",
      "256 -> ' '\n",
      "326 -> 'li'\n",
      "972 -> 'fe'\n",
      "46 -> '.'\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> '{tokenizer.decode([token_id])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:15.162105Z",
     "start_time": "2025-03-19T02:23:15.158571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"This is some text.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:15.872930Z",
     "start_time": "2025-03-19T02:23:15.864711Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save trained tokenizer\n",
    "tokenizer.save_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:22.691589Z",
     "start_time": "2025-03-19T02:23:22.687351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer2 = BPETokenizerSimple()\n",
    "tokenizer2.load_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:31.175985Z",
     "start_time": "2025-03-19T02:23:31.174172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### 3.3 加载来自 OpenAI 的原始 GPT-2 BPE 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:27:42.838669Z",
     "start_time": "2025-03-19T02:27:38.655173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.bpe already exists\n",
      "encoder.json already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def download_file_if_absent(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        try:\n",
    "            with urllib.request.urlopen(url) as response, open(filename, 'wb') as out_file:\n",
    "                out_file.write(response.read())\n",
    "            print(f\"Downloaded {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {filename}. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists\")\n",
    "\n",
    "files_to_download = {\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe\": \"vocab.bpe\",\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json\": \"encoder.json\"\n",
    "}\n",
    "\n",
    "for url, filename in files_to_download.items():\n",
    "    download_file_if_absent(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:28:36.467615Z",
     "start_time": "2025-03-19T02:28:36.412005Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_gpt2 = BPETokenizerSimple()\n",
    "tokenizer_gpt2.load_vocab_and_merges_from_openai(\n",
    "    vocab_path=\"encoder.json\", bpe_merges_path=\"vocab.bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:28:57.975765Z",
     "start_time": "2025-03-19T02:28:57.971680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_gpt2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:29:07.786691Z",
     "start_time": "2025-03-19T02:29:07.783553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 617, 2420]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"This is some text\"\n",
    "token_ids = tokenizer_gpt2.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:29:13.226099Z",
     "start_time": "2025-03-19T02:29:13.224320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some text\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_gpt2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:29:23.760007Z",
     "start_time": "2025-03-19T02:29:19.424473Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1212, 318, 617, 2420]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tik_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tik_tokenizer.encode(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
