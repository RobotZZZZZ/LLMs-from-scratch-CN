{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.字节对编码（BPE）的主要思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bits和bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(len(text))\n",
    "print(byte_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_ary)\n",
    "print(len(ids))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 创建词汇表"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.3 BPE算法大览"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. BPE的实现"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:22:52.313849Z",
     "start_time": "2025-03-19T02:22:52.293479Z"
    }
   },
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # 映射 token_id 到 token_str（例如：{11246: \"some\"}）\n",
    "        self.vocab = {}\n",
    "        # 映射 token_str 到 token_id（例如：{\"some\": 11246}）\n",
    "        self.inverse_vocab = {}\n",
    "        # BPE 合并字典：{(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        从头开始训练 BPE 分词器。\n",
    "\n",
    "        参数：\n",
    "            text (str): 训练文本。\n",
    "            vocab_size (int): 目标词汇表大小。\n",
    "            allowed_special (set): 要包含的特殊令牌集。\n",
    "        \"\"\"\n",
    "\n",
    "        # 预处理：将空格替换为 'Ġ'\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # 初始化词汇表\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # 扩展 unique_chars，包含处理后的文本中未包含的字符\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "        \n",
    "        # 可选：确保 'Ġ' 包含在内（如果它对文本处理相关）\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "\n",
    "        # 创建词汇表和逆词汇表\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # 添加允许的特殊token\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # 将处理后的文本标记化为令牌 ID\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE 步骤 1-3：反复查找并替换频繁的字节对\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "        \n",
    "        # 使用合并后的token构建词汇表\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "    \n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        从 OpenAI 的 GPT-2 文件加载预训练的词汇表和 BPE 合并。\n",
    "\n",
    "        参数：\n",
    "            vocab_path (str): 词汇文件路径（GPT-2 称其为 'encoder.json'）。\n",
    "            bpe_merges_path (str): BPE 合并文件路径（GPT-2 称其为 'vocab.bpe'）。\n",
    "        \"\"\"\n",
    "        # 加载词汇表\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # loaded_vocab 将 token_str 映射到 token_id\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "        # 加载BPE合并\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            # 如果有头部注释行，跳过它\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            for rank, line in enumerate(lines):\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) != 2:\n",
    "                    print(f\"第 {rank+1} 行有超过 2 个条目：{line.strip()}\")\n",
    "                    continue\n",
    "                token1, token2 = pair\n",
    "                if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                    token_id1 = self.inverse_vocab[token1]\n",
    "                    token_id2 = self.inverse_vocab[token2]\n",
    "                    merged_token = token1 + token2\n",
    "                    if merged_token in self.inverse_vocab:\n",
    "                        merged_token_id = self.inverse_vocab[merged_token]\n",
    "                        self.bpe_merges[(token_id1, token_id2)] = merged_token_id\n",
    "                    else:\n",
    "                        print(f\"合并的令牌 '{merged_token}' 未在词汇表中找到，跳过。\")\n",
    "                else:\n",
    "                    print(f\"跳过配对 {pair}，因为其中一个令牌不在词汇表中。\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        将输入文本编码为令牌 ID 列表。\n",
    "\n",
    "        参数：\n",
    "            text (str): 要编码的文本。\n",
    "\n",
    "        返回：\n",
    "            List[int]: 令牌 ID 列表。\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # 将文本拆分为令牌，确保换行符 intact\n",
    "        words = text.replace(\"\\n\", \" \\n \").split()  # 确保 '\\n' 被视为独立的令牌\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and not word.startswith(\"\\n\"):\n",
    "                tokens.append(\"Ġ\" + word)  # 如果单词前有空格或换行符，添加 'Ġ'\n",
    "            else:\n",
    "                tokens.append(word)  # 处理第一个单词或独立的 '\\n'\n",
    "\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                # token 已经存在于词汇表中\n",
    "                token_id = self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                # 尝试通过 BPE 处理子词分词\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        使用 BPE 合并对单个令牌进行分词。\n",
    "\n",
    "        参数：\n",
    "            token (str): 要分词的令牌。\n",
    "\n",
    "        返回：\n",
    "            List[int]: 应用 BPE 后的令牌 ID 列表。\n",
    "        \"\"\"\n",
    "        # 将令牌拆分为单个字符（作为初始令牌 ID）\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"未在词汇表中找到的字符：{missing_chars}\")\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    i += 2\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        将令牌 ID 列表解码回字符串。\n",
    "\n",
    "        参数：\n",
    "            token_ids (List[int]): 要解码的令牌 ID 列表。\n",
    "\n",
    "        返回：\n",
    "            str: 解码后的字符串。\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"未在词汇表中找到令牌 ID {token_id}。\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token.startswith(\"Ġ\"):\n",
    "                # 用空格替换 'Ġ'\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        将词汇表和 BPE 合并保存到 JSON 文件。\n",
    "\n",
    "        参数：\n",
    "            vocab_path (str): 保存词汇表的路径。\n",
    "            bpe_merges_path (str): 保存 BPE 合并的路径。\n",
    "        \"\"\"\n",
    "        # 保存词汇表\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump({k: v for k, v in self.vocab.items()}, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # 保存 BPE 合并作为字典列表\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        从 JSON 文件加载词汇表和 BPE 合并。\n",
    "\n",
    "        参数：\n",
    "            vocab_path (str): 词汇表文件路径。\n",
    "            bpe_merges_path (str): BPE 合并文件路径。\n",
    "        \"\"\"\n",
    "        # 加载词汇表\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # 加载 BPE 合并\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge['pair'])\n",
    "                new_id = merge['new_id']\n",
    "                self.bpe_merges[pair] = new_id\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"无效模式。选择 'most' 或 'least'。\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 训练、编码与解码"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:18:43.468692Z",
     "start_time": "2025-03-19T02:18:43.464069Z"
    }
   },
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:10.438336Z",
     "start_time": "2025-03-19T02:23:09.443991Z"
    }
   },
   "source": [
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:11.072947Z",
     "start_time": "2025-03-19T02:23:11.069987Z"
    }
   },
   "source": "print(len(tokenizer.vocab))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:11.539323Z",
     "start_time": "2025-03-19T02:23:11.537059Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(tokenizer.bpe_merges))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:11.921848Z",
     "start_time": "2025-03-19T02:23:11.919785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Jack embraced beauty through art and life.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:12.343953Z",
     "start_time": "2025-03-19T02:23:12.341512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 42\n",
      "Number of token IDs: 20\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:12.798054Z",
     "start_time": "2025-03-19T02:23:12.795863Z"
    }
   },
   "cell_type": "code",
   "source": "print(token_ids)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:13.243768Z",
     "start_time": "2025-03-19T02:23:13.241556Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode(token_ids))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:13.992760Z",
     "start_time": "2025-03-19T02:23:13.989942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 -> Jack\n",
      "256 ->  \n",
      "654 -> em\n",
      "531 -> br\n",
      "302 -> ac\n",
      "311 -> ed\n",
      "256 ->  \n",
      "296 -> be\n",
      "97 -> a\n",
      "465 -> ut\n",
      "121 -> y\n",
      "595 ->  through\n",
      "841 ->  ar\n",
      "116 -> t\n",
      "287 ->  a\n",
      "466 -> nd\n",
      "256 ->  \n",
      "326 -> li\n",
      "972 -> fe\n",
      "46 -> .\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:15.162105Z",
     "start_time": "2025-03-19T02:23:15.158571Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(tokenizer.encode(\"This is some text.\"))",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:15.872930Z",
     "start_time": "2025-03-19T02:23:15.864711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save trained tokenizer\n",
    "tokenizer.save_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:22.691589Z",
     "start_time": "2025-03-19T02:23:22.687351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load tokenizer\n",
    "tokenizer2 = BPETokenizerSimple()\n",
    "tokenizer2.load_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:23:31.175985Z",
     "start_time": "2025-03-19T02:23:31.174172Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer2.decode(token_ids))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "&nbsp;\n",
    "### 3.3 加载来自 OpenAI 的原始 GPT-2 BPE 分词器"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:27:42.838669Z",
     "start_time": "2025-03-19T02:27:38.655173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def download_file_if_absent(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        try:\n",
    "            with urllib.request.urlopen(url) as response, open(filename, 'wb') as out_file:\n",
    "                out_file.write(response.read())\n",
    "            print(f\"Downloaded {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {filename}. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists\")\n",
    "\n",
    "files_to_download = {\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe\": \"vocab.bpe\",\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json\": \"encoder.json\"\n",
    "}\n",
    "\n",
    "for url, filename in files_to_download.items():\n",
    "    download_file_if_absent(url, filename)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded vocab.bpe\n",
      "Downloaded encoder.json\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:28:36.467615Z",
     "start_time": "2025-03-19T02:28:36.412005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer_gpt2 = BPETokenizerSimple()\n",
    "tokenizer_gpt2.load_vocab_and_merges_from_openai(\n",
    "    vocab_path=\"encoder.json\", bpe_merges_path=\"vocab.bpe\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:28:57.975765Z",
     "start_time": "2025-03-19T02:28:57.971680Z"
    }
   },
   "cell_type": "code",
   "source": "len(tokenizer_gpt2.vocab)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:29:07.786691Z",
     "start_time": "2025-03-19T02:29:07.783553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"This is some text\"\n",
    "token_ids = tokenizer_gpt2.encode(input_text)\n",
    "print(token_ids)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 617, 2420]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:29:13.226099Z",
     "start_time": "2025-03-19T02:29:13.224320Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer_gpt2.decode(token_ids))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some text\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T02:29:23.760007Z",
     "start_time": "2025-03-19T02:29:19.424473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "tik_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tik_tokenizer.encode(input_text)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/LLMs-from-scratch-CN/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1212, 318, 617, 2420]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
