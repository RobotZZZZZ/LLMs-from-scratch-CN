{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\project\\LLMs-from-scratch-CN\\ch05\\01_main-chapter-code\n"
     ]
    }
   ],
   "source": [
    "# 使用sys.path添加上级目录\n",
    "import sys\n",
    "import os\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "file_path = os.path.join(package_path, \"ch05\", \"01_main-chapter-code\")\n",
    "print(file_path)\n",
    "sys.path.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.1\n",
      "numpy version: 2.1.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.6.0+cu126\n",
      "tensorflow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 评估文本生成大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 用GPT来生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "# 124M模型配置\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "# 预测模式，dropout层不起作用\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "# text -> ids\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "# ids -> text\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# 生成文本测试\n",
    "start_context = \"Every effort moves you\"\n",
    "# 使用gpt2的tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# 将文本转换为token ids, 并输入模型\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    # 生成10个新token\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "# 将模型结果ids转换为文本\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 计算文本生成的损失：交叉熵(cross-entropy)和困惑度（perplexity）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一组测试输入和目标，以ids list的形式表式\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# 计算当前inputs的预测的下一个token的概率\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "# 当输入的shape为[2, 3, 50257]时，对于每个输入的每个token，预测下一个token的概率\n",
    "# 由于使用了causal mask，每个token的预测只依赖于它前面的token\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "# 取max，得到预测的下一个token\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# 将预测的ids转换为文本，与targets对比\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "# 查看当前样本的targets的概率\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# 计算对数概率\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# 计算平均对数概率\n",
    "# 在对数概率下，越接近0，表示预测越准确\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# 负平均对数概率, 即交叉熵损失\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 查看logits和targets的shape\n",
    "# (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "# (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# 使用torch的cross_entropy计算交叉熵损失\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "# 与neg_avg_log_probas相同\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "# 计算困惑度, 困惑度是交叉熵的指数\n",
    "perplexity = torch.exp(loss)\n",
    "# 困惑度越小，表示预测越准确\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 计算训练集和验证集的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载训练数据\n",
    "import os \n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode(\"utf-8\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# 查看文本数据\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "# 统计文本长度及编码后的token个数\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建dataloader\n",
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 训练集\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# 验证集\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速检测(sanity check)\n",
    "# 检查训练集和验证集的token个数是否足够\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# 确认数据导入成功\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "# 另一个方式，确认数据导入成功\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算batch loss\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "# 计算数据集的loss\n",
    "def calc_loss_loader(data_loader, model ,device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若有GPU，则使用GPU计算\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n",
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.981104850769043\n"
     ]
    }
   ],
   "source": [
    "# 注意：\n",
    "# 如果取消注释以下代码块，代码可以在 Apple Silicon 芯片上运行（如果适用），\n",
    "# 在 M3 MacBook Air 上测量速度大约是 Apple CPU 的两倍。\n",
    "# 然而，计算得到的损失值可能会略有不同。\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "   train_loss = calc_loss_loader(train_loader, model, device)\n",
    "   val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 训练大语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                       num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练模式\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 计算batch loss\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            # 更新token计数\n",
    "            tokens_seen += input_batch.numel()\n",
    "            # 更新全局步数\n",
    "            global_step += 1\n",
    "            # 每eval_freq个batch，计算一次loss, 查看训练效果\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                # 记录loss\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                # 记录token计数\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # 每个epoch结束，生成和打印文本\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "# 模型评估\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "# 生成和打印文本\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.820, Val loss 9.926\n",
      "Ep 1 (Step 000005): Train loss 8.067, Val loss 8.335\n",
      "Every effort moves you,,,,,,,,,,,,,,.                                   \n",
      "Ep 2 (Step 000010): Train loss 6.624, Val loss 7.049\n",
      "Ep 2 (Step 000015): Train loss 6.048, Val loss 6.601\n",
      "Every effort moves you, and,, and,,,,,,,,,.                                   \n",
      "Ep 3 (Step 000020): Train loss 5.545, Val loss 6.507\n",
      "Ep 3 (Step 000025): Train loss 5.419, Val loss 6.376\n",
      "Every effort moves you, and, and of the of the to the, and I had. Gis, and, and, and, and, and, and I had, and, and, and, and, and, and, and, and, and,\n",
      "Ep 4 (Step 000030): Train loss 4.915, Val loss 6.272\n",
      "Ep 4 (Step 000035): Train loss 4.699, Val loss 6.297\n",
      "Every effort moves you of the picture.      \"I had the of the of the picture--and--and     \"I\"I the picture and I had been the picture to the picture and I had been.  \n",
      "Ep 5 (Step 000040): Train loss 4.032, Val loss 6.165\n",
      "Every effort moves you know the                                                \n",
      "Ep 6 (Step 000045): Train loss 3.658, Val loss 6.195\n",
      "Ep 6 (Step 000050): Train loss 3.077, Val loss 6.112\n",
      "Every effort moves you know it was not that the picture.                            \"Oh, I had the donkey.      \n",
      "Ep 7 (Step 000055): Train loss 2.974, Val loss 6.163\n",
      "Ep 7 (Step 000060): Train loss 2.239, Val loss 6.119\n",
      "Every effort moves you know,\" was not that, and he was--I told Mrs.  \"I was no great, the fact, and that, and I was his pictures--I looked up his pictures with his pictures, and down the room, I was\n",
      "Ep 8 (Step 000065): Train loss 1.793, Val loss 6.145\n",
      "Ep 8 (Step 000070): Train loss 1.490, Val loss 6.241\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"I looked up, and up, I had been to the display of his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.144, Val loss 6.251\n",
      "Ep 9 (Step 000080): Train loss 0.864, Val loss 6.278\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  \"Oh, and I remember getting off a prodigious phrase about the honour being _mine_--because he didn't want\n",
      "Ep 10 (Step 000085): Train loss 0.630, Val loss 6.375\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Total training time: 0.94 minutes\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "num_epochs = 10\n",
    "# 训练模型\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total training time: {(end_time - start_time) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# 绘制Loss\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 控制随机性的解码策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "#经典的载入\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 温度缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "#插入\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "#softmax归一化\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "#选个可能性最大\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    #从概率分布 probas 中按照权重进行一次采样,并生成索引\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    #然后变成单词\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "#统计采样过程中每个词的出现频率\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入温度系数的softmax\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# 不同的温度系数\n",
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制不同温度系数下的概率分布\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n"
     ]
    }
   ],
   "source": [
    "# 温度系数为0.1时\n",
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n"
     ]
    }
   ],
   "source": [
    "# 温度系数为5时\n",
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k 取样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top possitions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "# 取前k个概率最大的词\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top possitions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "# 将非前k个logits设为-inf\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")),\n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k的概率\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 优化文本更新功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加topk和temperature参数的文本生成函数\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 取context_size个词\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # 取最后一个词的预测logits\n",
    "        # 实际的场景下，会通过kv cache的方式来减少冗余计算\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # top k 采样\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        \n",
    "        # 温度校正\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to my surprise, poor StI was such a good; and\n"
     ]
    }
   ],
   "source": [
    "# 新的生成策略测试\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 在Pytorch中加载并保留权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型参数\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数，同时，保存优化器参数\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型参数和优化器参数\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 从OpenAI导入训练好的模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载OpenAI的预训练模型\n",
    "from gpt_download import download_and_load_gpt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 77.0kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.36MiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 89.9kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [17:18<00:00, 479kiB/s] \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 2.61MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 938kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 736kiB/s] \n"
     ]
    }
   ],
   "source": [
    "# 加载124M模型\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型参数前，进行shape检查\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 加载模型参数\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    # 加载embedding层\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "    # 加载transformer层\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # 加载带bias的qkv\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "        \n",
    "        # 加载输出层\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # 加载ff层\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # 加载norm层\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "    \n",
    "    # 加载最后的norm层\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    # 加载输出层\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型加载\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
      "\n",
      "This would remove you from a battle\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
